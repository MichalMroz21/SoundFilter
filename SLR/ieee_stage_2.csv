Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Publication Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,Mesh_Terms,Article Citation Count,Patent Citation Count,Reference Count,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Optimizing Speech Recognition for Medical Transcription: Fine-Tuning Whisper and Developing a Web Application,R. Roushan; H. Mishra; L. Yadav; S. Koppula; N. Tiwari; K. S. Nataraj,"ECE Department, IIIT Dharwad, Dharwad, India; ECE Department, IIIT Dharwad, Dharwad, India; CSE Department, IIIT Dharwad, Dharwad, India; ECE Department, IIIT Dharwad, Dharwad, India; SECS, IIT Bhubaneswar, Bhubaneswar, India; ECE Department, IIIT Dharwad, Dharwad, India",2024 IEEE Conference on Engineering Informatics (ICEI),12 Mar 2025,2024,,,1,6,"Utilizing automatic speech recognition in medical transcription can greatly improve healthcare professionals’ productivity. It eliminates the need for manual tasks like physician note-taking, data retrieval, and medical information searches, which can be time-consuming and divert their attention from patient care. Fine-tuning automatic speech recognition (ASR) systems for medical transcription using domain specific data is essential to enhance the performance. In this work, we fine-tuned the Whisper ASR system, which is known for its state-of-the-art speech recognition capabilities, using medical speech data. The fine-tuned model achieved a word error rate (WER) of 7.5% for medical data, demonstrating its potential for accurate transcription in clinical settings. Futher, we have developed a web application tailored for medical transcription. This application uses the robust Whisper ASR engine, renowned for its resilience to background noise. The web application offers user-friendly features such as audio recording, secure access via user-specific logins, and the seamless preservation of medical speech reports. We tested the web application with six Indian doctors by recording 362 utterances of prescriptions. The speech transcription achieved a WER of 19.4%, indicating the need for further fine-tuning for the Indian context with a larger dataset and an advanced Whisper model variant.",,979-8-3315-0577-6,10.1109/ICEI64305.2024.10912421,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912421,Automatic speech recognition;word error rate;medical transcription;web application,Training;Productivity;Error analysis;Medical services;Manuals;Audio recording;Data models;Automatic speech recognition;Context modeling;Resilience,,,,17,IEEE,12 Mar 2025,,,IEEE,IEEE Conferences
Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels,P. Ma; A. Haliassos; A. Fernandez-Lopez; H. Chen; S. Petridis; M. Pantic,"Imperial College London, UK; Imperial College London, UK; Meta AI, UK; Meta AI, UK; Imperial College London, UK; Imperial College London, UK","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",5 May 2023,2023,,,1,5,"Audio-visual speech recognition has received a lot of attention due to its robustness against acoustic noise. Recently, the performance of automatic, visual, and audio-visual speech recognition (ASR, VSR, and AV-ASR, respectively) has been substantially improved, mainly due to the use of larger models and training sets. However, accurate labelling of datasets is time-consuming and expensive. Hence, in this work, we investigate the use of automatically-generated transcriptions of unlabelled datasets to increase the training set size. For this purpose, we use publicly-available pre-trained ASR models to automatically transcribe unlabelled datasets such as AVSpeech and Vox-Celeb2. Then, we train ASR, VSR and AV-ASR models on the augmented training set, which consists of the LRS2 and LRS3 datasets as well as the additional automatically-transcribed data. We demonstrate that increasing the size of the training set, a recent trend in the literature, leads to reduced WER despite using noisy transcriptions. The proposed model achieves new state-of-the-art performance on AV-ASR on LRS2 and LRS3. In particular, it achieves a WER of 0.9 % on LRS3, a relative improvement of 30 % over the current state-of-the–art approach, and outperforms methods that have been trained on non-publicly available datasets with 26 times more training data.",2379-190X,978-1-7281-6327-7,10.1109/ICASSP49357.2023.10096889,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10096889,audio-visual speech recognition;unlabelled audio-visual data;automatically generated transcriptions,Training;Visualization;Training data;Speech recognition;White noise;Signal processing;Robustness,,51,,40,IEEE,5 May 2023,,,IEEE,IEEE Conferences
Automatic Speech Recognition in Diverse English Accents,H. Mohyuddin; D. Kwak,"Department of Computer Science and Technology, Kean University, Union, NJ, USA; Department of Computer Science and Technology, Kean University, Union, NJ, USA",2023 International Conference on Computational Science and Computational Intelligence (CSCI),19 Jul 2024,2023,,,714,718,"Advancements in automatic speech recognition (ASR) systems have led to their widespread integration into daily life, significantly altering our interaction with technology. However, this interaction is not always seamless for all users. Specifically, speakers with accents frequently face difficulties using ASR technologies and often need to deliberately adjust their pronunciation for better recognition. This study aims to compare leading ASR models' ability to transcribe speech from accented speakers of various nationalities against their native American English-speaking counterparts. We utilize two speech corpora: the L2-ARCTIC (L2A) and the Speech Accent Archive (SAA), which provide the original ‘clean’ audio samples. From there, two additional files are created by adding background noise to the original samples. These files are then processed through the respective APIs of each ASR model to obtain transcriptions. The accuracy of these transcriptions is then assessed by calculating the Word Error Rate (WER) for each speaker and model. The primary objective of this study is to highlight the challenges faced by speakers with diverse accents in using ASR technology. By highlighting these issues, we aim to encourage proactive measures to take steps towards their resolution. We believe it emphasizes the importance of fostering a more equitable and inclusive user experience.",2769-5654,979-8-3503-6151-3,10.1109/CSCI62032.2023.00122,National Science Foundation(grant numbers:DUE-2129795); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590378,Voice Assistants;Automatic Speech Recognition;Accented Speech;ASR Accuracy;Accent Recognition,Scientific computing;Filtering;Face recognition;Working environment noise;Noise;Speech enhancement;User experience,,1,,21,IEEE,19 Jul 2024,,,IEEE,IEEE Conferences
Automatic and Multilingual Speech Recognition and Translation by using Google Cloud API,P. Yellamma; P. R. Varun; N. C. N. L. Narayana; Y. Chowdary; P. Manikanth; K. H. G. Sai,"Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India",2024 5th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI),11 Apr 2024,2024,,,566,571,"The speech recognition is plays a vital role in the technology. The proposed work introduces a web application that leverages state-of-the-art technologies for audio-to-text recognition and multilingual text translation. Developed using Flask, the application integrates the googletrans library for translation, speech_recognition for audio processing, and mysql.connector for seamless database integration. Users have the convenience of uploading audio files, which undergo automatic transcription with language detection. The recognized text is subsequently translated into languages such as Hindi, Tamil, and Telugu, offering users the flexibility to choose their desired target language. Additionally, the system ensures secure storage of both transcribed speech and its translations in a MySQL database for future retrieval. The user-friendly web interface simplifies the entire process, making it a valuable tool for language learning, content localization, and accessibility services. The project effectively highlights the harmonious integration of audio recognition and machine translation technologies, delivering a powerful solution for various applications.",,979-8-3503-9523-5,10.1109/ICMCSI61536.2024.00089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493971,Automatic Speech Recognition (ASR);Multilingual Speech Recognition;Machine Translation;Cloud Infrastructure;Speech-to-Text Conversion,Text recognition;Databases;Target recognition;Heuristic algorithms;Speech recognition;Regulation;Real-time systems,,1,,15,IEEE,11 Apr 2024,,,IEEE,IEEE Conferences
Self-Training and Error Correction using Large Language Models for Medical Speech Recognition,S. N. Dungavath; K. S. Nataraj; N. Tiwari,"Computer Science and Engineering IIIT Dharwad, Dharwad, India; Electronics and Communication Engg. IIIT Dharwad, Dharwad, India; SECS IIT Bhubaneswar, Bhubaneswar, India",2024 IEEE Conference on Engineering Informatics (ICEI),12 Mar 2025,2024,,,1,6,"In the healthcare sector, medical professionals must dedicate substantial time and effort to documentation, which directly impacts patient care and clinical decision-making. Automatic speech recognition (ASR) systems offer a potential solution to reduce the burden of these documentation tasks. However, conventional ASR systems often perform poorly in the medical domain due to the use of specialized terminology, as well as variations in accent and pronunciation. Fine-tuning ASR models with large amounts of medical speech data is challenging due to confidentiality concerns and limitations in recording conditions. This study explores two approaches to enhance the performance of ASR for medical speech. The first approach utilizes a self-training mechanism, where transcriptions generated by a baseline ASR model are used to train the model further. The pseudo-labels produced by the baseline model introduce greater data diversity, particularly when the model transcribes with high confidence. The second approach employs open-source large language models (LLMs) to select the most probable transcription from the fivebest hypotheses generated by the ASR model. We conducted experiments using the Whisper ASR model as the baseline and investigated the effectiveness of these two approaches. Our findings indicate that self-training reduced the word error rate (WER) by an absolute $1-2 \%$. However, the use of five-best hypothesis selection resulted in an increased WER, which we attribute to the limited medical knowledge of the open-source LLMs.",,979-8-3315-0577-6,10.1109/ICEI64305.2024.10912300,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912300,Key Words: Automatic Speech Recognition;SelfTraining;Error Correction;Medical Transcriptions,Accuracy;Terminology;Large language models;Decision making;Documentation;Medical services;Speech enhancement;Data models;Error correction;Automatic speech recognition,,,,13,IEEE,12 Mar 2025,,,IEEE,IEEE Conferences
Enhancing Speech-to-Text Transcription Accuracy for the Bahraini Dialect,A. Almahmood; H. Al-Ammal; F. Albalooshi,"College of Information Technology, University of Bahrain, Sakhir, Bahrain; Department of Computer Science, University of Bahrain, Sakhir, Bahrain; Department of Computer Engineering, University of Bahrain, Sakhir, Bahrain","2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",13 Jan 2025,2024,,,508,514,"This study investigates means of enhancing the accuracy of Automated Speech Recognition (ASR) systems for the Bahraini dialect, a variant that has received minimal attention in natural language processing research. This is achieved through increasing the accuracy of the OpenAI Whisper model's transcription for the Bahraini dialect. Two tailored audio datasets were created: one with a local Bahraini audio which was manually transcribed, and another that included a wider range of audio sources such TV broadcasts, podcasts, and parliament sessions. To optimize the Whisper model, several methods were used, such as sequential training on both datasets, data augmentation, and hyperparameter optimization with Optuna. The Word Error Rate (WER) significantly decreased from 181.608% to 13.54% in the results, demonstrating the efficacy of the fine-tuning techniques in raising the model's accuracy above its original benchmarks.",2770-7466,979-8-3315-3313-7,10.1109/3ict64318.2024.10824280,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10824280,Automated Speech Recognition;Dialectal Vari-ability;Speech-to- Text Transcription;Arabic Language Processing;Fine-tuning Models,Training;Technological innovation;Accuracy;TV;Text recognition;Computational modeling;Speech enhancement;Natural language processing;Informatics;Speech to text,,,,23,IEEE,13 Jan 2025,,,IEEE,IEEE Conferences
Evaluating Automatic Transcription Models Utilising Cloud Platforms,K. Meehan; F. McDermott; N. Petropoulos,"Department of Computing, Atlantic Technical University, Letterkenny, Ireland; ImpactReady, Sector 3 Solutions Ltd., Belfast, Northern Ireland; School of HAPP, Queen’s University Belfast, Belfast, Northern Ireland",2024 5th International Conference on Data Analytics for Business and Industry (ICDABI),20 Dec 2024,2024,,,91,96,"Automatic Speech Recognition (ASR) technology is becoming pervasive in society and is being used for language translation, customer service and disability support. ASR and transcription is also rapidly becoming a popular manner of enabling qualitative research. Traditionally transcribing interviews and focus groups would have been very time consuming and labour intensive. In recent years, online tools have become available to help with automatic transcription. These tools have varying levels of accuracy, and most will require manual correction. Moreover, these tools require a researcher to manually upload audio files that have been processed or edited.This research proposes the development of an automatic framework for completing ASR and automatic transcription without the need for the researcher to perform any manual processes. The research is completed within an industrial context in an organisation that completes qualitative analysis and evaluation on behalf of clients in the third sector. The proposed framework utilises a cloud-based API for completing the automatic transcription. This research evaluates multiple APIs for completing automatic transcription and selects one service for inclusion within the framework. This evaluation is completed on a self-created audio dataset named “S3QualitativeAudio” using Word Error Rate (WER) calculation on the transcription and also based on cost-benefit analysis. The research has determined that the Whisper ASR model developed by OpenAI provides the lowest error rate of those evaluated (most accurate with 96% of the audio files providing the lowest WER value). The average WER for the Whisper ASR model was 0.07246. Further evaluation was completed using this model in an attempt to decrease the error rate further. The final automatic transcriptions could be used for sentiment analysis and text summarisation to complete further qualitative analysis.",,979-8-3503-6871-0,10.1109/ICDABI63787.2024.10800465,InterTradeIreland; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800465,Automatic Transcription;Automatic Speech Recognition;Word Error Rate;Cloud Transcription;Qualitative Research,Industries;Sentiment analysis;Accuracy;Translation;Data analysis;Error analysis;Manuals;Data models;Interviews;Automatic speech recognition,,,,24,IEEE,20 Dec 2024,,,IEEE,IEEE Conferences
"Speech Recognition Paradigms: A Comparative Evaluation of SpeechBrain, Whisper and Wav2Vec2 Models",D. Reddy Yerramreddy; J. Marasani; P. S. Venkata Gowtham; G. Harshit; Anjali,"Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Electronics & Communication Engineering, School of Engineering, Amrita Vishwa Vidyapeetham, Amritapuri, India",2024 IEEE 9th International Conference for Convergence in Technology (I2CT),10 Jun 2024,2024,,,1,6,"Speech recognition plays a pivotal role in the realm of natural language processing that deals in converting the language into the written text, providing human-computer interaction and enables us to use it widely for applications starting with voice assistants and delving upto the transcription services. Due to the complexity present in performing the task of speech recognition has led to the development of various models to enhance accuracy and efficiency. Our project mainly delves into three prominent speech recognition models that are Whisper, Wav2Vec2 and Speechbrain each of them representing distinct approaches of transcribing spoken language. The significance of the models used lies in their potential to perform better for realtime applications by improving the accuracy and reliability of speech recognition. To evaluate the best model that performs effectively, an array of metrics are used including levenshtein distance and it’s similarity percentage, jaccard similarity along with semantic similarity that provides an additional layer of evaluation describing the model’s understanding of contextual meaning in spoken language. Out of the three models, Speech Brain model outperformed all the other models through the calculation of Word Error Rate (WER), Character Error rate (CER) and BLEU score. The results have shown the model’s efficiency in converting spoken language(audio files) into precise and contextually relevant text. The results shows that these models contribute to the field of speech recognition highlighting the strengths of each approach and among them considering speechbrain as an ideal solution for accurate and meaningful transcription.",,979-8-3503-9447-4,10.1109/I2CT61223.2024.10544133,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544133,Speech Recognition;Whsiper;Wav2Vec2;SpeechBrain;Jaccard Similarity;Levenshtein Distance,Measurement;Human computer interaction;Adaptation models;Technological innovation;Error analysis;Semantics;Speech recognition,,,,15,IEEE,10 Jun 2024,,,IEEE,IEEE Conferences
Increasing Context for Estimating Confidence Scores in Automatic Speech Recognition,A. Ragni; M. J. F. Gales; O. Rose; K. M. Knill; A. Kastanos; Q. Li; P. M. Ness,"Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing",6 Apr 2022,2022,30,,1319,1329,"Accurate confidence measures for predictions from machine learning techniques play a critical role in the deployment and training of many speech and language processing applications. For example, confidence scores are important when making use of automatically generated transcriptions in training automatic speech recognition (ASR) systems, as well as down-stream applications, such as information retrieval and conversational assistants. Previous work on improving confidence scores for these systems has focused on two main directions: designing features correlated with improved confidence prediction; and employing sequence models to account for the importance of contextual information. Few studies, however, have explored incorporating contextual information more broadly, such as from the future, in addition to the past, or making use of alternative multiple hypotheses in addition to the most likely one. This article introduces two general approaches for encapsulating contextual information from lattices. Experimental results illustrating the importance of increasing contextual information for estimating confidence scores are presented on a range of limited resource languages where word error rates range between 30% and 60%. The results show that the novel approaches provide significant gains in the accuracy of confidence estimation.",2329-9304,,10.1109/TASLP.2022.3161153,ALTA Institute; University of Cambridge; Office of the Director of National Intelligence; Intelligence Advanced Research Projects Activity; Air Force Research Laboratory(grant numbers:FA8650-17-C-9117); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739834,Attention;confidence;graph structures;recurrent neural network;speech recognition,Lattices;Hidden Markov models;Speech processing;Estimation;History;Feature extraction;Stability criteria,,2,,74,IEEE,22 Mar 2022,,,IEEE,IEEE Journals
Crossmodal ASR Error Correction With Discrete Speech Units,Y. Li; P. Chen; P. Bell; C. Lai,"University of Edinburgh, UK; University of Edinburgh, UK; University of Edinburgh, UK; University of Edinburgh, UK",2024 IEEE Spoken Language Technology Workshop (SLT),16 Jan 2025,2024,,,431,438,"ASR remains unsatisfactory in scenarios where the speaking style diverges from that used to train ASR systems, resulting in erroneous transcripts. To address this, ASR Error Correction (AEC), a post-ASR processing approach, is required. In this work, we tackle an understudied issue: the Low-Resource Out-of-Domain (LROOD) problem, by investigating crossmodal AEC on very limited downstream data with 1-best hypothesis transcription. We explore pretraining and fine-tuning strategies and uncover an ASR domain discrepancy phenomenon, shedding light on appropriate training schemes for LROOD data. Moreover, we propose the incorporation of discrete speech units to align with and enhance the word embeddings for improving AEC quality. Results from multiple corpora and several evaluation metrics demonstrate the feasibility and efficacy of our proposed AEC approach on LROOD data as well as its generalizability and superiority on large-scale data. Finally, a study on speech emotion recognition confirms that our model produces ASR error-robust transcripts suitable for downstream applications.",,979-8-3503-9225-8,10.1109/SLT61566.2024.10832240,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832240,ASR Error Correction;Discrete Speech Units;Low-Resource Speech;Out-of-Domain Data,Training;Measurement;Emotion recognition;Conferences;Speech recognition;Speech enhancement;Error correction,,2,,38,IEEE,16 Jan 2025,,,IEEE,IEEE Conferences
Confidence Score Based Speaker Adaptation of Conformer Speech Recognition Systems,J. Deng; X. Xie; T. Wang; M. Cui; B. Xue; Z. Jin; G. Li; S. Hu; X. Liu,"Chinese University of Hong Kong, Central Ave, Hong Kong; Institute of Software, Chinese Academy of Sciences, Beijing, China; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing",15 Mar 2023,2023,31,,1175,1190,"Speaker adaptation techniques provide a powerful solution to customise automatic speech recognition (ASR) systems for individual users. Practical application of unsupervised model-based speaker adaptation techniques to data intensive end-to-end ASR systems is hindered by the scarcity of speaker-level data and performance sensitivity to transcription errors. To address these issues, a set of compact and data efficient speaker-dependent (SD) parameter representations are used to facilitate both speaker adaptive training and test-time unsupervised speaker adaptation of state-of-the-art Conformer ASR systems. The sensitivity to supervision quality is reduced using a confidence score-based selection of the less erroneous subset of speaker-level adaptation data. Two lightweight confidence score estimation modules are proposed to produce more reliable confidence scores. The data sparsity issue, which is exacerbated by data selection, is addressed by modelling the SD parameter uncertainty using Bayesian learning. Experiments on the benchmark 300-hour Switchboard and the 233-hour AMI datasets suggest that the proposed confidence score-based adaptation schemes consistently outperformed the baseline speaker-independent (SI) Conformer model and conventional non-Bayesian, point estimate-based adaptation using no speaker data selection. Similar consistent performance improvements were retained after external Transformer and LSTM language model rescoring. In particular, on the 300-hour Switchboard corpus, statistically significant WER reductions of 1.0%, 1.3%, and 1.4% absolute (9.5%, 10.9%, and 11.3% relative) were obtained over the baseline SI Conformer on the NIST Hub5’00, RT02, and RT03 evaluation sets respectively. Similar WER reductions of 2.7% and 3.3% absolute (8.9% and 10.2% relative) were also obtained on the AMI development and evaluation sets.",2329-9304,,10.1109/TASLP.2023.3250842,"Hong Kong RGC GRF(grant numbers:14200021,14200220); Innovation & Technology Fund(grant numbers:ITS/254/19,ITS/218/21); National Key R&D Program of China(grant numbers:2020YFC2004100); National Natural Science Foundation of China(grant numbers:62106255); Guangzhou Civil Affairs Science and Technology Foundation(grant numbers:2022MZK02); Open Research Fund of Guangxi Key Lab of Human-machine Interaction and Intelligent Decision(grant numbers:GXHIID2202); ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057052,Speech recognition;speaker adaptation;confidence score estimation;bayesian learning;conformer,Adaptation models;Hidden Markov models;Data models;Acoustics;Transformers;Switches;Task analysis,,5,,109,IEEE,1 Mar 2023,,,IEEE,IEEE Journals
Romanian Speech-to-Text Transcription for Medical Applications,N. Nitu; A. Catruna; E. Radoi,"National Univ. of Science and Technology Politehnica, Bucharest; National Univ. of Science and Technology Politehnica, Bucharest; National Univ. of Science and Technology Politehnica, Bucharest",2024 IEEE 20th International Conference on Intelligent Computer Communication and Processing (ICCP),17 Dec 2024,2024,,,1,7,"Speech recognition models have an important role in improving efficiency and accessibility across various industries. Due to its advantages and benefits, this technology has transformed sectors such as healthcare, automotive, and customer service. However, the speech recognition in low-resource languages such as Romanian remains relatively underdeveloped due to the lack of data for training deep learning models. For this reason it cannot be utilized in the healthcare industry where Romanian specialized terms are unknown to the speech-to-text models. To address this problem, we propose to fine-tune large speech recognition models on a specialized dataset with Romanian audio and medical discourse. We obtain this dataset in a semi-automatic manner by lever-aging speech recognition and large language models. By adapting state-of-the-art fine-tuning approaches that are used for LLMs, we improve the performance of the model on Romanian medical discourse by 19.9% in substitution errors, 28% in deletion errors and 9 % in word error rate. This approach paves the way for speech recognition technology usage across many Romanian industries.",2766-8495,979-8-3315-3997-9,10.1109/ICCP63557.2024.10793032,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10793032,,Industries;Training;Adaptation models;Analytical models;Error analysis;Statistical analysis;Computational modeling;Large language models;Medical services;Speech to text,,,,22,IEEE,17 Dec 2024,,,IEEE,IEEE Conferences
ASR Error Correction Using Large Language Models,R. Ma; M. Qian; M. Gales; K. Knill,"ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.; ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.; ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.; ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.","IEEE Transactions on Audio, Speech and Language Processing",1 Apr 2025,2025,33,,1389,1401,"Error correction (EC) models play a crucial role in refining Automatic Speech Recognition (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. This work investigates the use of large language models (LLMs) for error correction across diverse scenarios. 1-best ASR hypotheses are commonly used as the input to EC models. We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process. Additionally, the generation process of a standard EC model is unrestricted in the sense that any output sequence can be generated. For some scenarios, such as unseen domains, this flexibility may impact performance. To address this, we introduce a constrained decoding approach based on the N-best list or an ASR lattice. Finally, most EC models are trained for a specific ASR system requiring retraining whenever the underlying ASR system is changed. This paper explores the ability of EC models to operate on the output of different ASR systems. This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT. Experiments on three standard datasets demonstrate the efficacy of our proposed methods for both Transducer and attention-based encoder-decoder ASR systems. In addition, the proposed method can serve as an effective method for model ensembling.",2998-4173,,10.1109/TASLPRO.2025.3551083,Engineering and Physical Sciences Research Council(grant numbers:EP/V006223/1); Cambridge University Press & Assessment; University of Cambridge; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930744,Automatic speech recognition;error correction;large language model;supervised training;zero-shot prompting,Error correction;Decoding;Hidden Markov models;Biological system modeling;Adaptation models;Training;Data models;Context modeling;Chatbots;Training data,,,,52,IEEE,18 Mar 2025,,,IEEE,IEEE Journals
A Study of Audio-to-Text Conversion Software Using Whispers Model,A. L. Haz; E. D. Fajrianti; N. Funabiki; S. Sukaridhoto,"Department of Information and Communication Systems, Okayama University, Okayama, Japan; Department of Information and Communication Systems, Okayama University, Okayama, Japan; Department of Information and Communication Systems, Okayama University, Okayama, Japan; Informatics and Computer Department, Politeknik Elektro Negeri Surabaya, Surabaya, Indonesia",2023 Sixth International Conference on Vocational Education and Electrical Engineering (ICVEE),14 Dec 2023,2023,,,268,273,"This paper explores the potential of utilizing the Whispers model to create unified interfaces for audio-to-text in the context of Natural Language Processing (NLP). It offers possibilities for accurately converting spoken language into written texts. Whispers model by OpenAI is a state-of-the-art model in the field of NLP and is employe. In this study, various metrics and criteria are considered to evaluate the performance of the developed audio-to-text conversion software, including loading time, stress test, and transcription accuracy and speed. The proposal is capable of handling up to 180 concurrent users with an average response time of 309 ms and 471.5 requests per second. The results and findings of this study provide valuable insights into the effectiveness and limitations of the Whispers model in the context of audio-to-text conversion.",,979-8-3503-2664-2,10.1109/ICVEE59738.2023.10348186,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10348186,Audio-to-text;Streamlit;Web Application;Whisper Model,Loading;Natural language processing;Software;Data models;Time factors;Task analysis;Unsupervised learning,,6,,26,IEEE,14 Dec 2023,,,IEEE,IEEE Conferences
WhisperSum: Unified Audio-to-Text Summarization,S. Ganguly; S. Mandal; N. Das; B. Sadhukhan; S. Sarkar; S. Paul,"Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India",2024 International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS),24 Oct 2024,2024,,,1,7,"In an era overwhelmed by information, efficiently extracting relevant data from audio sources is crucial. This study introduces WhisperSum, a solution combining OpenAI's Whisper for accurate voice transcription with spaCy's advanced natural language processing (NLP) for extractive text summarization. WhisperSum transcribes audio files into text, analyzes the content, and produces concise summaries to help users quickly understand key information. The web application, built using Flask, offers a user-friendly interface for easy audio uploads, real-time transcription, and summary retrieval. The system integrates Whisper's transcription capabilities with the NLP tools of spaCy, ensuring accuracy and efficiency. WhisperSum achieved high performance, with precision, recall, and F1-scores of 91.98%, 92.12%, and 92.04%, respectively, highlighting its effectiveness in accurately transcribing and summarizing audio content while minimizing errors and maintaining comprehensive coverage. The application is valuable for content management, education, and journalism, providing a streamlined solution to information overload. WhisperSum enhances decision-making by ensuring the accessible availability of crucial information, demonstrating the significant potential of integrating advanced NLP and transcription technologies.",,979-8-3503-6066-0,10.1109/IACIS61494.2024.10721926,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721926,extractive summarization;NLP;spacy;whisper;text;flask;audio-to-text,Accuracy;Education;Decision making;Text summarization;Speech recognition;Journalism;Feature extraction;Natural language processing;Real-time systems;Data mining,,,,22,IEEE,24 Oct 2024,,,IEEE,IEEE Conferences
