@article{GANESH2024623,
title = {Flask-based ASR for Automated Disorder Speech Recognition},
journal = {Procedia Computer Science},
volume = {233},
pages = {623-637},
year = {2024},
note = {5th International Conference on Innovative Data Communication Technologies and Application (ICIDCA 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.252},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924006112},
author = {Devalla Bhaskar Ganesh and Yellamma Pachipala and Syed Sania Rizvi and Teena Chowdary Manne and Himavanth Swamy Atchi and V V R Maheswara Rao},
keywords = {Cloud computing, Speech recognition, Disorder speech recognition, Machine learning, Artificial intelligence, Natural language processing},
abstract = {Speech disorders encompass a diverse range of conditions hindering effective communication, stemming from developmental, neurological, or physical factors. These challenges impact daily life and opportunities. Automatic Speech Recognition (ASR) technology emerges as a pivotal solution. ASR, powered by sophisticated algorithms, transcribes spoken language into written text, transcending traditional communication barriers. Tailorable to accommodate unique speech disorders, ASR offers the promise of empowering individuals to articulate their thoughts. Our Python Flask-based ASR(FBASR) applications meticulously crafted to cater to individuals, including children, grappling with speech disorders. It provides a user-friendly platform. Users simply submit a speech audio file in WAV format, and the application transcribes it. Alongside recognized text, it furnishes vital audio metrics. Additionally, it generates visual audio signal and spectrogram representations, all stored efficiently for future reference. The application's profound advantage lies in granting a voice to individuals with speech disorders, facilitating education, employment, and healthcare interactions. Furthermore, it serves as a resource for research and therapy development, promising inclusivity and enriching lives. The proposed work concludes by discussing the future outlook of cloud-based ASR and identifying some of the key areas of research that need to be addressed to make cloud-based DSR systems more accurate, reliable, and accessible.}
}
@article{GE20243215,
title = {Audio-Text Multimodal Speech Recognition via Dual-Tower Architecture for Mandarin Air Traffic Control Communications},
journal = {Computers, Materials and Continua},
volume = {78},
number = {3},
pages = {3215-3245},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.046746},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824003047},
author = {Shuting Ge and Jin Ren and Yihua Shi and Yujun Zhang and Shunzhi Yang and Jinfeng Yang},
keywords = {Speech-text multimodal, automatic speech recognition, semantic alignment, air traffic control communications, dual-tower architecture},
abstract = {In air traffic control communications (ATCC), misunderstandings between pilots and controllers could result in fatal aviation accidents. Fortunately, advanced automatic speech recognition technology has emerged as a promising means of preventing miscommunications and enhancing aviation safety. However, most existing speech recognition methods merely incorporate external language models on the decoder side, leading to insufficient semantic alignment between speech and text modalities during the encoding phase. Furthermore, it is challenging to model acoustic context dependencies over long distances due to the longer speech sequences than text, especially for the extended ATCC data. To address these issues, we propose a speech-text multimodal dual-tower architecture for speech recognition. It employs cross-modal interactions to achieve close semantic alignment during the encoding stage and strengthen its capabilities in modeling auditory long-distance context dependencies. In addition, a two-stage training strategy is elaborately devised to derive semantics-aware acoustic representations effectively. The first stage focuses on pre-training the speech-text multimodal encoding module to enhance inter-modal semantic alignment and aural long-distance context dependencies. The second stage fine-tunes the entire network to bridge the input modality variation gap between the training and inference phases and boost generalization performance. Extensive experiments demonstrate the effectiveness of the proposed speech-text multimodal speech recognition method on the ATCC and AISHELL-1 datasets. It reduces the character error rate to 6.54% and 8.73%, respectively, and exhibits substantial performance gains of 28.76% and 23.82% compared with the best baseline model. The case studies indicate that the obtained semantics-aware acoustic representations aid in accurately recognizing terms with similar pronunciations but distinctive semantics. The research provides a novel modeling paradigm for semantics-aware speech recognition in air traffic control communications, which could contribute to the advancement of intelligent and efficient aviation safety management.}
}
@article{KADAM2021100,
title = {Cognitive Evaluation of Machine Learning Agents},
journal = {Cognitive Systems Research},
volume = {66},
pages = {100-121},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300978},
author = {Suvarna Kadam and Vinay Vaidya},
keywords = {Cognition Framework, Machine Learning, Cognitive Evaluation, Machine Cognition, Cognition Metrics, Evaluation Metrics},
abstract = {Advances in applying statistical Machine Learning (ML) led to several claims of human-level or near-human performance in tasks such as image classification & speech recognition. Such claims are unscientific primarily for two reasons, (1) They incorrectly enforce the notion that task-specific performance can be treated as manifestation of General Intelligence and (2) They are not verifiable as currently there is no set benchmark for measuring human-like cognition in a machine learning agent. Moreover, ML agent’s performance is influenced by knowledge ingested in it by its human designers. Therefore, agent’s performance may not necessarily reflect its true cognition. In this paper, we propose a framework that draws parallels from human cognition to measure machine’s cognition. Human cognitive learning is quite well studied in developmental psychology with frameworks and metrics in place to measure actual learning. To either believe or refute the claims of human-level performance of machine learning agent, we need scientific methodology to measure its cognition. Our framework formalizes incremental implementation of human-like cognitive processes in ML agents with an implicit goal to measure it. The framework offers guiding principles for measuring, (1) Task-specific machine cognition and (2) General machine cognition that spans across tasks. The framework also provides guidelines for building domain-specific task taxonomies to cognitively profile tasks. We demonstrate application of the framework with a case study where two ML agents that perform Vision and NLP tasks are cognitively evaluated.}
}
@article{KHALAFALLAH2024101864,
title = {Speech corpus for Medina dialect},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {2},
pages = {101864},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101864},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823004184},
author = {Haneen Bahjat Khalafallah and Mohamed Abdel Fattah and Ruqayya Abdulrahman},
keywords = {Machine Learning (ML), Natural Language Processing (NLP), Automatic Speech Recognition, Arabic ASR Speech Corpus, Arabic Dialects, CMU Sphinx},
abstract = {Automatic Speech Recognition (ASR) has standard rules which must be followed and considered carefully. Some difficulties that lead to less ASR performance is variations in pronunciation and small words misrecognition. Arabic ASR faces some challenges like difficulty in obtaining corpora for spoken dialects. Obtaining a wide range of diacritized text as well as the enormous number of word forms is considered a major challenge due to the Arabic language morphology richness and its’ letters capability to be written without diacritics. Although Arabic is one of the most popular languages, Arabic ASR systems are still rare compared with other languages. As ASR systems depend primarily on speech corpuses, Arabic ASR systems requires specific-dialect speech corpuses. Such speech corpuses are still deficient, costly, nor sometimes exists. In this research, we contribute to overcome the lack of speech recognition and misunderstanding for one of the most famous dialects in Saudi Arabia, Medina. We created a brand-new corpus “Haneen Corpus“, which consists of 70,364 tokens that have been uttered using Medina dialect, and constructed a dictionary using 64 phonemes to analyse the correct pronunciation of words. Our Medina Dialect ASR System exploited Hidden Markov Models (HMM) that achieved 92.09 % speech recognition accuracy.}
}
@article{MEHRISH2023101869,
title = {A review of deep learning techniques for speech processing},
journal = {Information Fusion},
volume = {99},
pages = {101869},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101869},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523001859},
author = {Ambuj Mehrish and Navonil Majumder and Rishabh Bharadwaj and Rada Mihalcea and Soujanya Poria},
keywords = {Deep learning, Speech processing, Transformers, Survey, Trends},
abstract = {The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field’s evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field.}
}
@article{YU2024103720,Scaling speech technologies to 1000+ languages
title = {An explainable deepfake of speech detection method with spectrograms and waveforms},
journal = {Journal of Information Security and Applications},
volume = {81},
pages = {103720},
year = {2024},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2024.103720},
url = {https://www.sciencedirect.com/science/article/pii/S2214212624000231},
author = {Ning Yu and Long Chen and Tao Leng and Zigang Chen and Xiaoyin Yi},
keywords = {Explainable artificial intelligence, SHAP, Spoofed speech detection, Feature splice},
abstract = {Research on deepfake techniques for speech is crucial for combatting the spread of fake information, safeguarding public privacy, and advancing forensic techniques. However, the lack of transparency and explainability of spoofed speech detection models raises concerns about their reliability. In this paper, we suggest using raw waveform signals and spectrograms as fused features of the spoofed speech detection model. We use the SHAP method to analyze the feature distribution of spoofed speech detection and explain the likelihood of fake speech. Our experimental results demonstrate that our approach achieves better classification results with lighter model parameters than other feature fusion methods. Finally, the feature contribution values are calculated under the SHAP method to visualize them as heat maps. It helps researchers to analyze the feature distribution of spoofed speech to identify the most critical features that distinguish between spoofed and bona fide and to ensure transparency in their use.}
}
@article{TRABELSI20222242,
title = {Evaluation of the efficiency of state-of-the-art Speech Recognition engines},
journal = {Procedia Computer Science},
volume = {207},
pages = {2242-2252},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.534},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014338},
author = {Asma Trabelsi and Sébastien Warichet and Yassine Aajaoun and Séverine Soussilane},
keywords = {Speech Recognition, ASR, Data Sovereignty, Kaldi, DeepSpeech},
abstract = {Speech Recognition is one of the several Artificial Intelligence applications. It helps us converting spoken words into text. It can be part of various daily use cases in order to deal with accessibility. Google Assistant and Amazon's Alexa are in the top of list of the well-known Speech Recognition tools. European companies cannot use these solutions as they should guarantee data sovereignty. Another important point is that these mentioned solutions are not customized. So that, it is not possible to deal with new accents or new vocabularies. To cope with these problems, one can either use European Automatic Speech Recognition (ASR) solutions or build his own personalized models using well-known open-source tools like Deep Speech or Kaldi. Choosing the best solution between both, Kaldi and DeepSpeech, is an important task. The criteria for judging the finest method are the Accuracy and the Inference Time. In this paper, we make theoretical and experimental study between DeepSpeech and Kaldi. Also, Vosk and LinTO, open-source solutions build in top of Kaldi, will be included in the comparison study.}
}
@article{BISOGNI2024104145,
title = {Acoustic features analysis for explainable machine learning-based audio spoofing detection},
journal = {Computer Vision and Image Understanding},
volume = {249},
pages = {104145},
year = {2024},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2024.104145},
url = {https://www.sciencedirect.com/science/article/pii/S1077314224002261},
author = {Carmen Bisogni and Vincenzo Loia and Michele Nappi and Chiara Pero},
keywords = {Deepfake audio, Deepfake detection, Audio spoofing, Explainable AI, Acoustic features},
abstract = {The rapid evolution of synthetic voice generation and audio manipulation technologies poses significant challenges, raising societal and security concerns due to the risks of impersonation and the proliferation of audio deepfakes. This study introduces a lightweight machine learning (ML)-based framework designed to effectively distinguish between genuine and spoofed audio recordings. Departing from conventional deep learning (DL) approaches, which mainly rely on image-based spectrogram features or learning-based audio features, the proposed method utilizes a diverse set of hand-crafted audio features – such as spectral, temporal, chroma, and frequency-domain features – to enhance the accuracy of deepfake audio content detection. Through extensive evaluation and experiments on three well-known datasets, ASVSpoof2019, FakeAVCelebV2, and an In-The-Wild database, the proposed solution demonstrates robust performance and a high degree of generalization compared to state-of-the-art methods. In particular, our method achieved 89% accuracy on ASVSpoof2019, 94.5% on FakeAVCelebV2, and 94.67% on the In-The-Wild database. Additionally, the experiments performed on explainability techniques clarify the decision-making processes within ML models, enhancing transparency and identifying crucial features essential for audio deepfake detection.}
}