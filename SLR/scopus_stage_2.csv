Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
Mehrish A.; Majumder N.; Bharadwaj R.; Mihalcea R.; Poria S.,"Mehrish, Ambuj (57189058040); Majumder, Navonil (57203239752); Bharadwaj, Rishabh (59098910400); Mihalcea, Rada (8619220500); Poria, Soujanya (55316592700)",57189058040; 57203239752; 59098910400; 8619220500; 55316592700,A review of deep learning techniques for speech processing,2023,Information Fusion,99,,101869,,,,153,10.1016/j.inffus.2023.101869,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162051266&doi=10.1016%2fj.inffus.2023.101869&partnerID=40&md5=90a58330b1ebc9e04605b2cf2b3d9351,"The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field's evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field. © 2023 Elsevier B.V.",Deep learning; Speech processing; Survey; Transformers; Trends,Character recognition; Emotion Recognition; Learning algorithms; Learning systems; Speech processing; Speech recognition; Speech synthesis; Automatic speech recognition; Deep learning; Emotion recognition; Learning techniques; Multiple processing; Processing layer; Speech data; Speech emotions; Transformer; Trend; Deep learning,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85162051266
Jain R.; Barcovschi A.; Yiwere M.Y.; Corcoran P.; Cucu H.,"Jain, Rishabh (57568296800); Barcovschi, Andrei (58135704300); Yiwere, Mariam Yahayah (57204890157); Corcoran, Peter (57190839462); Cucu, Horia (36439147500)",57568296800; 58135704300; 57204890157; 57190839462; 36439147500,Exploring Native and Non-Native English Child Speech Recognition With Whisper,2024,IEEE Access,12,,,41601,41610,9,1,10.1109/ACCESS.2024.3378738,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188422344&doi=10.1109%2fACCESS.2024.3378738&partnerID=40&md5=7353fc3facfb112bc6da009384e5e868,"Modern end-to-end Automatic Speech Recognition (ASR) systems struggle to recognise children's speech. This challenge is due to the high acoustic variability in children's voices and the scarcity of child speech training data, particularly for accented or low-resource languages. This study focuses on improving the performance of ASR on native and non-native English child speech using publicly available datasets. We evaluate how the large-scale whisper models (trained with a large amount of adult speech data) perform with child speech. In addition, we perform finetuning experiments using different child speech datasets to investigate the performance of whisper ASR on non-native English-speaking children's speech. Our findings indicate relative Word Error Rate (WER) improvements ranging from 29% to 89% over previous benchmarks on the same datasets. Notably, these gains were achieved by finetuning with only a 10% sample of unseen non-native datasets. These results demonstrate the potential of whisper for improving ASR in a low-resource scenario for non-native child speech.  © 2013 IEEE.",Child automatic speech recognition; CMU_Kids; large-scale supervision; MyST; non-native child speech; PFSTAR; speechocean762; whisper,Electric transformer testing; Adaptation models; Automatic speech recognition; Child automatic speech recognition; Children's speech; CMU_kid; Decoding; Large-scale supervision; Large-scales; MyST; Non-native; Non-native child speech; PFSTAR; Speechocean762; Transformer; Whisper; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85188422344
Avdeeva A.; Gusev A.; Andzhukaev T.; Ivanov A.,"Avdeeva, Anastasia (57200257783); Gusev, Aleksei (57211643521); Andzhukaev, Tseren (57210788227); Ivanov, Artem (57213403201)",57200257783; 57211643521; 57210788227; 57213403201,Streaming ASR Encoder for Whisper-to-Speech Online Voice Conversion,2024,IEEE Open Journal of Signal Processing,5,,,160,167,7,2,10.1109/OJSP.2023.3343342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180281381&doi=10.1109%2fOJSP.2023.3343342&partnerID=40&md5=7de112ea4bc83c192c33161a8671b61a,"Whispered speech is a quiet voice without vocalization. One of the common cases of using whispered speech is a technique that can help overcome stuttering. But whispered speech can be uncomfortable and difficult to understand in everyday communication. To address these problems, we propose a method of low-delayed whisper-to-speech voice conversion, which can be useful in real life communication of people with disordered speech. As part of our research, we study the impact of streaming Automatic Speech Recognition models on the quality of voice conversion, comparing different streaming models and methods for model adaptation to streaming settings, and showing the importance of using such models in cases of low-delayed voice conversion. © 2023 The Authors.",disordered speech; Speech recognition; voice conversion; whisper-to-speech processing,Speech communication; Speech processing; Adaptation models; Automatic speech recognition; Context models; Decoding; Disordered speech; Recognition models; Streaming model; Voice conversion; Whisper-to-speech processing; Whispered speech; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85180281381
Kawade R.; Jagtap S.,"Kawade, Rupali (56592787100); Jagtap, Sonal (55532222700)",56592787100; 55532222700,Comprehensive Study of Automatic Speech Emotion Recognition Systems,2023,International Journal on Recent and Innovation Trends in Computing and Communication,11,9s,,709,717,8,0,10.17762/ijritcc.v11i9s.7743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173650781&doi=10.17762%2fijritcc.v11i9s.7743&partnerID=40&md5=f1a7332dafe8db3e65dec65b36111215,"Speech emotion recognition (SER) is the technology that recognizes psychological characteristics and feelings from the speech signals through techniques and methodologies. SER is challenging because of more considerable variations in different languages arousal and valence levels. Various technical developments in artificial intelligence and signal processing methods have encouraged and made it possible to interpret emotions.SER plays a vital role in remote communication. This paper offers a recent survey of SER using machine learning (ML) and deep learning (DL)-based techniques. It focuses on the various feature representation and classification techniques used for SER. Further, it describes details about databases and evaluation metrics used for speech emotion recognition. © 2023 Auricle Global Society of Education and Research.",Affective Computing; Deep Learning; Machine Learning; Speech Emotion Recognition; Speech Recognition,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85173650781
Li Q.; Mai Q.; Wang M.; Ma M.,"Li, Qiang (55694488200); Mai, Qianyu (57216692028); Wang, Mandou (58860317400); Ma, Mingjuan (55430733600)",55694488200; 57216692028; 58860317400; 55430733600,Chinese dialect speech recognition: a comprehensive survey,2024,Artificial Intelligence Review,57,2,25,,,,4,10.1007/s10462-023-10668-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183905725&doi=10.1007%2fs10462-023-10668-0&partnerID=40&md5=2098da499f378e481966e75ed54ea273,"As a multi-ethnic country with a large population, China is endowed with diverse dialects, which brings considerable challenges to speech recognition work. In fact, due to geographical location, population migration, and other factors, the research progress and practical application of Chinese dialect speech recognition are currently at different stages. Therefore, exploring the significant regional heterogeneities in specific recognition approaches and effects, dialect corpus, and other resources is of vital importance for Chinese speech recognition work. Based on this, we first start with the regional classification of dialects and analyze the pivotal acoustic characteristics of dialects, including specific vowels and tones patterns. Secondly, we comprehensively summarize the existing dialect phonetic corpus in China, which is of some assistance in exploring the general construction methods of dialect phonetic corpus. Moreover, we expound on the general process of dialect recognition. Several critical dialect recognition approaches are summarized and introduced in detail, especially the hybrid method of Artificial Neural Network (ANN) combined with the Hidden Markov Model(HMM), as well as the End-to-End (E2E). Thirdly, through the in-depth comparison of their principles, merits, disadvantages, and recognition performance for different dialects, the development trends and challenges in dialect recognition in the future are pointed out. Finally, some application examples of dialect speech recognition are collected and discussed. © 2024, The Author(s).",Automatic speech recognition; Chinese dialect; Deep neural network; Dialect corpus; Dialectal acoustic modeling; End-to-end,Acoustic Modeling; Hidden Markov models; Linguistics; Population dynamics; Speech recognition; Acoustics model; Automatic speech recognition; Chinese dialects; Dialect corpus; Dialectal acoustic modeling; Different stages; End to end; Geographical locations; Large population; Specific recognition; Deep neural networks,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85183905725
Qu L.; Weber C.; Wermter S.,"Qu, Leyuan (57189588967); Weber, Cornelius (7402376952); Wermter, Stefan (7003826680)",57189588967; 7402376952; 7003826680,Emphasizing unseen words: New vocabulary acquisition for end-to-end speech recognition,2023,Neural Networks,161,,,494,504,10,6,10.1016/j.neunet.2023.01.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148327640&doi=10.1016%2fj.neunet.2023.01.027&partnerID=40&md5=4d16c55b7afa07484258204a901372cb,"Due to the dynamic nature of human language, automatic speech recognition (ASR) systems need to continuously acquire new vocabulary. Out-Of-Vocabulary (OOV) words, such as trending words and new named entities, pose problems to modern ASR systems that require long training times to adapt their large numbers of parameters. Different from most previous research focusing on language model post-processing, we tackle this problem on an earlier processing level and eliminate the bias in acoustic modeling to recognize OOV words acoustically. We propose to generate OOV words using text-to-speech systems and to rescale losses to encourage neural networks to pay more attention to OOV words. Specifically, we enlarge the classification loss used for training neural networks’ parameters of utterances containing OOV words (sentence-level), or rescale the gradient used for back-propagation for OOV words (word-level), when fine-tuning a previously trained model on synthetic audio. To overcome catastrophic forgetting, we also explore the combination of loss rescaling and model regularization, i.e. L2 regularization and elastic weight consolidation (EWC). Compared with previous methods that just fine-tune synthetic audio with EWC, the experimental results on the LibriSpeech benchmark reveal that our proposed loss rescaling approach can achieve significant improvement on the recall rate with only a slight decrease on word error rate. Moreover, word-level rescaling is more stable than utterance-level rescaling and leads to higher recall rates and precision rates on OOV word recognition. Furthermore, our proposed combined loss rescaling and weight consolidation methods can support continual learning of an ASR system. © 2023 The Author(s)",Automatic speech recognition; Continual learning; End-to-end learning; Loss rescaling; Out-of-vocabulary word recognition,"Humans; Language; Neural Networks, Computer; Speech; Speech Perception; Vocabulary; Acoustic Modeling; Backpropagation; Modeling languages; Vocabulary control; Automatic speech recognition; Automatic speech recognition system; Continual learning; End to end; End-to-end learning; Loss rescaling; Out-of-vocabulary word recognition; Outof-vocabulary words (OOV); Rescaling; Word recognition; adult; Article; artificial neural network; attention; automatic speech recognition; back propagation; classification algorithm; clinical article; elastic weight consolidation; female; human; image reconstruction; learning; machine learning; male; nerve cell network; recall; recognition; speech discrimination; support vector machine; training; vocabulary; word recognition; language; speech; speech perception; Speech recognition",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85148327640
Zhou Y.; Wu Z.; Zhang M.; Tian X.; Li H.,"Zhou, Yi (58940556100); Wu, Zhizheng (55500643500); Zhang, Mingyang (57193807130); Tian, Xiaohai (56022979300); Li, Haizhou (8615868400)",58940556100; 55500643500; 57193807130; 56022979300; 8615868400,TTS-Guided Training for Accent Conversion Without Parallel Data,2023,IEEE Signal Processing Letters,30,,,533,537,4,4,10.1109/LSP.2023.3270079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159721330&doi=10.1109%2fLSP.2023.3270079&partnerID=40&md5=0fdcacbf61b2ee160f836c353cd2b5b0,"Accent Conversion (AC) seeks to change the accent of speech from one (source) to another (target) while preserving the speech content and speaker identity. However, many existing AC approaches rely on source-target parallel speech data during training or reference speech at run-time. We propose a novel accent conversion framework without the need for either parallel data or reference speech. Specifically, a text-to-speech (TTS) system is first pretrained with target-accented speech data. This TTS model and its hidden representations are expected to be associated only with the target accent. Then, a speech encoder is trained to convert the accent of the speech under the supervision of the pretrained TTS model. In doing so, the source-accented speech and its corresponding transcription are forwarded to the speech encoder and the pretrained TTS, respectively. The output of the speech encoder is optimized to be the same as the text embedding in the TTS system. At run-time, the speech encoder is combined with the pretrained speech decoder to convert the source-accented speech toward the target. In the experiments, we converted English with two source accents (Chinese/Indian) to the target accent (American/British/Canadian). Both objective metrics and subjective listening tests successfully validate that the proposed approach generates speech samples that are close to the target accent with high speech quality.  © 1994-2012 IEEE.",Accent conversion (AC); text-to-speech (TTS),Data handling; Signal encoding; Speech recognition; Accent conversion; Accented speech; Decoding; Features extraction; Parallel data; Runtimes; Speech data; Text to speech; Text-to-speech; Text-to-speech system; Decoding,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85159721330
Aggarwal S.; Vasukidevi G.; Selvakanmani S.; Pant B.; Kaur K.; Verma A.; Binegde G.N.,"Aggarwal, Shruti (57201487047); Vasukidevi, G. (57222657121); Selvakanmani, S. (56156565700); Pant, Bhaskar (35316278700); Kaur, Kiranjeet (57542481600); Verma, Amit (57304749700); Binegde, Geleta Negasa (58026126200)",57201487047; 57222657121; 56156565700; 35316278700; 57542481600; 57304749700; 58026126200,Audio Segmentation Techniques and Applications Based on Deep Learning,2022,Scientific Programming,2022,,7994191,,,,8,10.1155/2022/7994191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144493279&doi=10.1155%2f2022%2f7994191&partnerID=40&md5=231780439a6c2155777f6972130a1227,"Audio processing has become an inseparable part of modern applications in domains ranging from health care to speech-controlled devices. In automated audio segmentation, deep learning plays a vital role. In this article, we are discussing audio segmentation based on deep learning. Audio segmentation divides the digital audio signal into a sequence of segments or frames and then classifies these into various classes such as speech recognition, music, or noise. Segmentation plays an important role in audio signal processing. The most important aspect is to secure a large amount of high-quality data when training a deep learning network. In this study, various application areas, citation records, documents published year-wise, and source-wise analysis are computed using Scopus and Web of Science (WoS) databases. The analysis presented in this paper supports and establishes the significance of the deep learning techniques in audio segmentation. Copyright © 2022 Shruti Aggarwal et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.",,Audio acoustics; Audio signal processing; Deep learning; Learning systems; Music; Application area; Audio processing; Audio segmentation; Digital audio signals; High quality data; Large amounts; Learning network; Modern applications; Segmentation techniques; Web of Science; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144493279
Jain R.; Barcovschi A.; Yiwere M.Y.; Bigioi D.; Corcoran P.; Cucu H.,"Jain, Rishabh (57568296800); Barcovschi, Andrei (58135704300); Yiwere, Mariam Yahayah (57204890157); Bigioi, Dan (57223669237); Corcoran, Peter (57190839462); Cucu, Horia (36439147500)",57568296800; 58135704300; 57204890157; 57223669237; 57190839462; 36439147500,A WAV2VEC2-Based Experimental Study on Self-Supervised Learning Methods to Improve Child Speech Recognition,2023,IEEE Access,11,,,46938,46948,10,15,10.1109/ACCESS.2023.3275106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159797544&doi=10.1109%2fACCESS.2023.3275106&partnerID=40&md5=1c0a7d12a2de7292b277e3c7b19b5eb1,"Despite recent advancements in deep learning technologies, Child Speech Recognition remains a challenging task. Current Automatic Speech Recognition (ASR) models require substantial amounts of annotated data for training, which is scarce. In this work, we explore using the ASR model, wav2vec2, with different pretraining and finetuning configurations for self-supervised learning (SSL) toward improving automatic child speech recognition. The pretrained wav2vec2 models were finetuned using different amounts of child speech training data, adult speech data, and a combination of both, to discover the optimum amount of data required to finetune the model for the task of child ASR. Our trained model achieves the best Word Error Rate (WER) of 7.42 on the MyST child speech dataset, 2.91 on the PFSTAR dataset and 12.77 on the CMU KIDS dataset using cleaned variants of each dataset. Our models outperformed the unmodified wav2vec2 BASE 960 on child speech using as little as 10 hours of child speech data in finetuning. The analysis of different types of training data and their effect on inference is provided by using a combination of custom datasets in pretraining, finetuning and inference. These 'cleaned' datasets are provided for use by other researchers to provide comparisons with our results.  © 2013 IEEE.",automatic speech recognition; Child speech recognition; CMU-kids dataset; MyST dataset; PFSTAR dataset; self-supervised learning; wav2vec2,Deep learning; Job analysis; Perturbation techniques; Speech recognition; Automatic speech recognition; Children's speech recognition; CMU_kid dataset; MyST dataset; Perturbation method; PFSTAR dataset; Self-supervised learning; Task analysis; Transformer; Wav2vec2; Supervised learning,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85159797544
Kąkol K.; Korvel G.; Tamulevičius G.; Kostek B.,"Kąkol, Krzysztof (57193491284); Korvel, Gražina (56719707600); Tamulevičius, Gintautas (36562459300); Kostek, Bożena (6603685068)",57193491284; 56719707600; 36562459300; 6603685068,Detecting Lombard Speech Using Deep Learning Approach,2023,Sensors,23,1,315,,,,2,10.3390/s23010315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145874079&doi=10.3390%2fs23010315&partnerID=40&md5=22a9d50c99446acac404f53dac4c7d50,"Robust Lombard speech-in-noise detecting is challenging. This study proposes a strategy to detect Lombard speech using a machine learning approach for applications such as public address systems that work in near real time. The paper starts with the background concerning the Lombard effect. Then, assumptions of the work performed for Lombard speech detection are outlined. The framework proposed combines convolutional neural networks (CNNs) and various two-dimensional (2D) speech signal representations. To reduce the computational cost and not resign from the 2D representation-based approach, a strategy for threshold-based averaging of the Lombard effect detection results is introduced. The pseudocode of the averaging process is also included. A series of experiments are performed to determine the most effective network structure and the 2D speech signal representation. Investigations are carried out on German and Polish recordings containing Lombard speech. All 2D signal speech representations are tested with and without augmentation. Augmentation means using the alpha channel to store additional data: gender of the speaker, F0 frequency, and first two MFCCs. The experimental results show that Lombard and neutral speech recordings can clearly be discerned, which is done with high detection accuracy. It is also demonstrated that the proposed speech detection process is capable of working in near real-time. These are the key contributions of this work. © 2022 by the authors.",2D feature representations; Lombard speech; machine learning; speech recognition; threshold-based averaging strategy,Deep Learning; Language; Noise; Speech; Speech Perception; Audio recordings; Convolutional neural networks; Deep learning; Learning systems; Real time systems; Speech communication; 2d feature representation; Feature representation; Lombard effects; Lombard speech; Machine-learning; Near-real time; Signal representations; Speech detection; Speech signals; Threshold-based averaging strategy; language; noise; speech; speech perception; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85145874079
Qian Y.; Gong X.; Huang H.,"Qian, Yanmin (35103128400); Gong, Xun (57220842525); Huang, Houjun (57215929660)",35103128400; 57220842525; 57215929660,Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2842,2853,11,30,10.1109/TASLP.2022.3198546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136906044&doi=10.1109%2fTASLP.2022.3198546&partnerID=40&md5=efaa3e54c60785202d5a824657ba1384,"The variety and complexity of accents pose a huge challenge to robust Automatic Speech Recognition (ASR). Some previous work has attempted to address such problems, however most of the current approaches either require prior knowledge about the target accent, or cannot handle unseen accents and accent-unspecific standard speech. In this work, we aim to improve multi-accent speech recognition in the end-to-end (E2E) framework with a novel layer-wise adaptation architecture. Firstly, we propose a robust deep accent representation learning architecture to obtain accurate accent embedding, and some advanced schemes are designed to further boost the quality of accent embeddings, including phone posteriorgram (PPG) feature, TTS based data augmentation in the training stage, test-time augmentation and multi-embedding fusion in the testing stage. Then, the layer-wise adaptation with accent embeddings is developed for fast accent adaptation in ASR, and two types of adapter layers are designed, including the gated adapter layer and multi-basis adapter layer. Compared to the usual two-pass adaptation, these adapter layers are injected between the ASR encoder layers to encode the accent information in ASR flexibly, and perform fast adaption on the corresponding speech accent. The experiments on Accent AESRC corpus show that the proposed deep accent representation learning can capture accurate accent knowledge, and get high performance on accent classification. The new layer-wise adaptation architecture with the accurate accent embedding outperforms the other traditional methods, and obtains consistent ∼15% relative word error rate (WER) reduction on all kinds of testing scenarios, including seen accents, unseen accents and accent-unspecific standard speech.  © 2014 IEEE.",accent embedding; End-to-end speech recognition; layer-wise adaptation; multi-accent,Architecture; Deep learning; Job analysis; Personnel training; Speech recognition; Accent embedding; Adaptation models; Decoding; Embeddings; End to end; End-to-end speech recognition; Layer-wise; Layer-wise adaptation; Multi-accent; Representation learning; Task analysis; Embeddings,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85136906044
Bell P.; Fainberg J.; Klejch O.; Li J.; Renals S.; Swietojanski P.,"Bell, Peter (23092749700); Fainberg, Joachim (57191857441); Klejch, Ondrej (57022010500); Li, Jinyu (35488220000); Renals, Steve (7003714982); Swietojanski, Pawel (37125204800)",23092749700; 57191857441; 57022010500; 35488220000; 7003714982; 37125204800,Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview,2021,IEEE Open Journal of Signal Processing,2,,9296327,33,66,33,62,10.1109/OJSP.2020.3045349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126667298&doi=10.1109%2fOJSP.2020.3045349&partnerID=40&md5=932138248b1896ecafb833dee7dc7a9f,"We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model/neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature. © 2020 IEEE.",Accent adaptation; data augmentation; domain adaptation; regularization; semi-supervised learning; speaker adaptation; speaker embeddings; speech recognition; structured linear transforms,Hidden Markov models; Neural networks; Adaptation algorithms; Data augmentation; Domain adaptation; Model parameters; Network systems; Neural network systems; Relative error rates; Speaker adaptation; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85126667298
Arslan R.S.; Barişçi N.; Arici N.; Koçer S.,"Arslan, Recep Sinan (57205619566); Barişçi, Necaattin (56038499500); Arici, Nursal (42460944900); Koçer, Sabri (8590703300)",57205619566; 56038499500; 42460944900; 8590703300,Detecting and correcting automatic speech recognition errors with a new model,2021,Turkish Journal of Electrical Engineering and Computer Sciences,25,9,,2298,2311,13,1,10.3906/ELK-2010-117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117134759&doi=10.3906%2fELK-2010-117&partnerID=40&md5=62bbd1e25f54c82e885c08b4463fb2f8,"The purpose of automatic speech recognition (ASR) systems is to recognize speech signals obtained from people and convert them into text so that they can be processed by a computer. Although many ASR applications are versatile and widely used in the real world, they still generate relatively inaccurate results. They tend to generate spelling errors in recognized words, especially in noisy environments, in situations where the vocabulary size is increased, and at times when the input speech is of poor quality. The permanent presence of errors in ASR systems has led to the need to find alternative methods for automatic detection and correction of such errors. In this study, the basic principles of ASR evaluation are first summarized, and then a new approach based on the suggestion of an alternative hypothesis is proposed for the detection and correction of these errors generated by ASR systems. The proposed method involves a series of processes such as identifying incorrect words, selecting the ones that can be corrected, and identifying candidate words to replace these words. As a result of the tests carried out by creating different test environments, significant performance improvements for Turkish were achieved and an average of 4.60 % performance improvement was provided. © 2021 Turkiye Klinikleri. All rights reserved.",Alternative hypothesis suggestion; Artificial intelligence; Automatic speech recognition; Automatic speech recognition error correction; Natural language processing,Character recognition; Error correction; Natural language processing systems; Signal processing; Speech; Alternative hypothesis; Alternative hypothesis suggestion; Automatic speech recognition; Automatic speech recognition error correction; Automatic speech recognition system; Errors correction; Performance; Recognition error; Speech signals; Speech recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85117134759
