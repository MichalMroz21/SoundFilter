"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"WENETSPEECH: A 10000+ Hours Multi-Domain Mandarin Corpus for Speech Recognition","B. Zhang; H. Lv; P. Guo; Q. Shao; C. Yang; L. Xie; X. Xu; H. Bu; X. Chen; C. Zeng; D. Wu; Z. Peng","Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University; Mobvoi Inc.; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University; Beijing Shell Shell Technology Co., Ltd.; Beijing Shell Shell Technology Co., Ltd.; Mobvoi Inc.; Mobvoi Inc.; Mobvoi Inc.; Mobvoi Inc.","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6182","6186","In this paper, we present WenetSpeech, a multi-domain Mandarin corpus consisting of 10000+ hours high-quality labeled speech, 2400+ hours weakly labeled speech, and about 10000 hours unlabeled speech, with 22400+ hours in total. We collect the data from YouTube and Podcast, which covers a variety of speaking styles, scenarios, domains, topics and noisy conditions. An optical character recognition (OCR) method is introduced to generate the audio/text segmentation candidates for the YouTube data on the corresponding video subtitles, while a high-quality ASR transcription system is used to generate audio/text pair candidates for the Podcast data. Then we propose a novel end-to-end label error detection approach to further validate and filter the candidates. We also provide three manually labelled high-quality test sets along with WenetSpeech for evaluation – Dev for cross-validation purpose in training, Test_Net, collected from Internet for matched test, and Test_Meeting, recorded from real meetings for more challenging mismatched test. Baseline systems trained with WenetSpeech are provided for three popular speech recognition toolkits, namely Kaldi, ESPnet, and WeNet, and recognition results on the three test sets are also provided as benchmarks. To the best of our knowledge, WenetSpeech is the current largest open-source Mandarin speech corpus with transcriptions, which benefits research on production-level speech recognition.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746682","automatic speech recognition;corpus;multi-domain","Optical filters;Training;Matched filters;Speech recognition;Optical character recognition software;Noise measurement;Speech processing","","82","","41","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Optimizing Speech Recognition for Medical Transcription: Fine-Tuning Whisper and Developing a Web Application","R. Roushan; H. Mishra; L. Yadav; S. Koppula; N. Tiwari; K. S. Nataraj","ECE Department, IIIT Dharwad, Dharwad, India; ECE Department, IIIT Dharwad, Dharwad, India; CSE Department, IIIT Dharwad, Dharwad, India; ECE Department, IIIT Dharwad, Dharwad, India; SECS, IIT Bhubaneswar, Bhubaneswar, India; ECE Department, IIIT Dharwad, Dharwad, India",2024 IEEE Conference on Engineering Informatics (ICEI),"12 Mar 2025","2024","","","1","6","Utilizing automatic speech recognition in medical transcription can greatly improve healthcare professionals’ productivity. It eliminates the need for manual tasks like physician note-taking, data retrieval, and medical information searches, which can be time-consuming and divert their attention from patient care. Fine-tuning automatic speech recognition (ASR) systems for medical transcription using domain specific data is essential to enhance the performance. In this work, we fine-tuned the Whisper ASR system, which is known for its state-of-the-art speech recognition capabilities, using medical speech data. The fine-tuned model achieved a word error rate (WER) of 7.5% for medical data, demonstrating its potential for accurate transcription in clinical settings. Futher, we have developed a web application tailored for medical transcription. This application uses the robust Whisper ASR engine, renowned for its resilience to background noise. The web application offers user-friendly features such as audio recording, secure access via user-specific logins, and the seamless preservation of medical speech reports. We tested the web application with six Indian doctors by recording 362 utterances of prescriptions. The speech transcription achieved a WER of 19.4%, indicating the need for further fine-tuning for the Indian context with a larger dataset and an advanced Whisper model variant.","","979-8-3315-0577-6","10.1109/ICEI64305.2024.10912421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912421","Automatic speech recognition;word error rate;medical transcription;web application","Training;Productivity;Error analysis;Medical services;Manuals;Audio recording;Data models;Automatic speech recognition;Context modeling;Resilience","","","","17","IEEE","12 Mar 2025","","","IEEE","IEEE Conferences"
"Auto-AVSR: Audio-Visual Speech Recognition with Automatic Labels","P. Ma; A. Haliassos; A. Fernandez-Lopez; H. Chen; S. Petridis; M. Pantic","Imperial College London, UK; Imperial College London, UK; Meta AI, UK; Meta AI, UK; Imperial College London, UK; Imperial College London, UK","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Audio-visual speech recognition has received a lot of attention due to its robustness against acoustic noise. Recently, the performance of automatic, visual, and audio-visual speech recognition (ASR, VSR, and AV-ASR, respectively) has been substantially improved, mainly due to the use of larger models and training sets. However, accurate labelling of datasets is time-consuming and expensive. Hence, in this work, we investigate the use of automatically-generated transcriptions of unlabelled datasets to increase the training set size. For this purpose, we use publicly-available pre-trained ASR models to automatically transcribe unlabelled datasets such as AVSpeech and Vox-Celeb2. Then, we train ASR, VSR and AV-ASR models on the augmented training set, which consists of the LRS2 and LRS3 datasets as well as the additional automatically-transcribed data. We demonstrate that increasing the size of the training set, a recent trend in the literature, leads to reduced WER despite using noisy transcriptions. The proposed model achieves new state-of-the-art performance on AV-ASR on LRS2 and LRS3. In particular, it achieves a WER of 0.9 % on LRS3, a relative improvement of 30 % over the current state-of-the–art approach, and outperforms methods that have been trained on non-publicly available datasets with 26 times more training data.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10096889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10096889","audio-visual speech recognition;unlabelled audio-visual data;automatically generated transcriptions","Training;Visualization;Training data;Speech recognition;White noise;Signal processing;Robustness","","51","","40","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Data-Filtering Methods for Self-Training of Automatic Speech Recognition Systems","A. -L. Georgescu; C. Manolache; D. Oneaţă; H. Cucu; C. Burileanu","Speech and Dialogue Research Laboratory, University POLITEHNICA of Bucharest, Romania; Speech and Dialogue Research Laboratory, University POLITEHNICA of Bucharest, Romania; Speech and Dialogue Research Laboratory, University POLITEHNICA of Bucharest, Romania; Speech and Dialogue Research Laboratory, University POLITEHNICA of Bucharest, Romania; Speech and Dialogue Research Laboratory, University POLITEHNICA of Bucharest, Romania",2021 IEEE Spoken Language Technology Workshop (SLT),"25 Mar 2021","2021","","","1","7","Self-training is a simple and efficient way of leveraging un-labeled speech data: (i) start with a seed system trained on transcribed speech; (ii) pass the unlabeled data through this seed system to automatically generate transcriptions; (iii) en-large the initial dataset with the self-labeled data and retrain the speech recognition system. However, in order not to pol-lute the augmented dataset with incorrect transcriptions, an important intermediary step is to select those parts of the self-labeled data that are accurate. Several approaches have been proposed in the community, but most of the works address only a single method. In contrast, in this paper we inspect three distinct classes of data-filtering for self-training, leveraging: (i) confidence scores, (ii) multiple ASR hypotheses and (iii) approximate transcriptions. We evaluate these approaches from two perspectives: quantity vs. quality of the selected data and improvement of the seed ASR by including this data. The proposed methodology achieves state-of-the-art results on Romanian speech, obtaining 25% relative improvement over prior work. Among the three methods, approximate transcriptions bring the highest performance gain, even if they yield the least quantity of data.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383577","automatic speech recognition;self training;speech corpora annotation","Training;Annotations;Conferences;Speech recognition;Performance gain;Data models;Automatic speech recognition","","1","","39","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"Multilingual Audio-Visual Speech Recognition with Hybrid CTC/RNN-T Fast Conformer","M. Burchi; K. C. Puvvada; J. Balam; B. Ginsburg; R. Timofte","NVIDIA, USA; NVIDIA, USA; NVIDIA, USA; NVIDIA, USA; Computer Vision Lab, CAIDAS & IFI, University of Würzburg, Germany","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","10211","10215","Humans are adept at leveraging visual cues from lip movements for recognizing speech in adverse listening conditions. Audio-Visual Speech Recognition (AVSR) models follow similar approach to achieve robust speech recognition in noisy conditions. In this work, we present a multilingual AVSR model incorporating several enhancements to improve performance and audio noise robustness. Notably, we adapt the recently proposed Fast Conformer model to process both audio and visual modalities using a novel hybrid CTC/RNN-T architecture. We increase the amount of audio-visual training data for six distinct languages, generating automatic transcriptions of unlabelled multilingual datasets (VoxCeleb2 and AVSpeech). Our proposed model achieves new state-of-the-art performance on the LRS3 dataset, reaching WER of 0.8%. On the recently introduced MuAViC benchmark, our model yields an absolute average-WER reduction of 11.9% in comparison to the original baseline. Finally, we demonstrate the ability of the proposed model to perform audio-only, visual-only, and audio-visual speech recognition at test time.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10445891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10445891","audio-visual speech recognition;multi-lingual;noise robustness;generated transcriptions","Adaptation models;Visualization;Training data;Speech recognition;Benchmark testing;Noise robustness;Speech processing","","4","","40","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Foundation Model Assisted Automatic Speech Emotion Recognition: Transcribing, Annotating, and Augmenting","T. Feng; S. Narayanan","Signal Analysis and Interpretation Laboratory, University of Southern California, Los Angeles, USA; Signal Analysis and Interpretation Laboratory, University of Southern California, Los Angeles, USA","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","12116","12120","Significant advances are being made in speech emotion recognition (SER) using deep learning models. Nonetheless, training SER systems remains challenging, requiring both time and costly resources. Like many other machine learning tasks, acquiring datasets for SER requires substantial data annotation efforts, including transcription and labeling. These annotation processes present challenges when attempting to scale up conventional SER systems. Recent developments in foundational models have had a tremendous impact, giving rise to applications such as ChatGPT. These models have enhanced human-computer interactions including bringing unique possibilities for streamlining data collection in fields like SER. In this research, we explore the use of foundational models to assist in automating SER from transcription and annotation to augmentation. Our study demonstrates that these models can generate transcriptions to enhance the performance of SER systems that rely solely on speech data. Furthermore, we note that annotating emotions from transcribed speech remains a challenging task. However, combining outputs from multiple LLMs enhances the quality of annotations. Lastly, our findings suggest the feasibility of augmenting existing speech emotion datasets by annotating unlabeled speech samples.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10448130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448130","Speech;Emotion recognition;Foundation model;Large Language Model","Training;Emotion recognition;Annotations;Speech recognition;Speech enhancement;Signal processing;Data models","","12","","30","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Automatic Speech Recognition using the Melspectrogram-based method for English Phonemes","M. Soundarya; P. R. Karthikeyan; K. Ganapathy; G. Thangarasu","Institute of Electronics about Communication Engineering, Saveetha School of Engineering SIMATS, Chennai, India; Institute of Electronics about Communication Engineering, Saveetha School of Engineering SIMATS, Chennai, India; Institute of Electronics about Communication Engineering, Saveetha School of Engineering SIMATS, Chennai, India; Department of Professional Industry-Driven Education, MAHSA University, Selangor, Malaysia","2022 International Conference on Computer, Power and Communications (ICCPC)","23 Mar 2023","2022","","","270","273","An automatic speech recognition (ASR) technique may be set up to forecast the pronunciation of textual identifiers (such as song names) based on assumptions about the language or languages in which the textual identifier was originally written. To identify mispronunciation, custom acoustic-phonetic elements are typically used. This study examines the use of deep convolutional neural networks to identify English phonemes that have been mispronounced in musical samples. Convolutional neural networks (CNNs) are now often employed in systems recognizing speech. In this work, a decoded-based architecture is proposed in which the spectrogram feature that corresponds with the auditory features is proposed by comparing the various inputs to the model. Following the selection of the input features, this research examines the design principles of learning parameters and their application to voice recognition with various parameters. To identify mispronunciation, custom acoustic-phonetic elements are typically used. This research work also examines the application of learning models. The proposed method achieves better results with 85% of accuracy and a Word Error Rate of 8.1 on comparing with existing works.","","979-8-3503-9784-0","10.1109/ICCPC55978.2022.10072076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10072076","Speech recognition;Melspectrogram;CNN;audio;text;phoneme","Error analysis;Computational modeling;Music;Computer architecture;Convolutional neural networks;Spectrogram;Automatic speech recognition","","1","","10","IEEE","23 Mar 2023","","","IEEE","IEEE Conferences"
"Speech Mastery Detection Using Advanced Natural Language Processing (NLP) and Automatic Speech Recognition (ASR) Techniques","P. Raut; K. Anitha; S. Sowmya; K. Vinnu","Department of Electronics and Communication Engineering, Velagapudi Ramakrishna Siddhartha Engineering, College Deemed to be University, Andhra Pradesh, India; Department of Electronics and Communication Engineering, Velagapudi Ramakrishna Siddhartha Engineering, College Deemed to be University, Andhra Pradesh, India; Department of Electronics and Communication Engineering, Velagapudi Ramakrishna Siddhartha Engineering, College Deemed to be University, Andhra Pradesh, India; Department of Electronics and Communication Engineering, Velagapudi Ramakrishna Siddhartha Engineering, College Deemed to be University, Andhra Pradesh, India",2024 4th International Conference on Artificial Intelligence and Signal Processing (AISP),"12 Feb 2025","2024","","","1","5","In today's globalized world, English communication skills are essential for career advancement and cross-cultural collaboration, enhancing access to information and opportunities. Automatic speech recognition, or ASR, is a separate machine-driven method for transcription and decoding spoken language. An ASR system typically uses a microphone to capture a speaker's audio input, analyze it using a model, algorithm, or pattern, and output the results, which are often text messages (Lai, Karat, Yankelovich, 2008). This paper provides a thorough method for utilizing Python-based tools and modules to extract and analyze linguistic characteristics from audio data. The suggested approach turns spoken language into text using voice recognition technology, and then it uses natural language processing (NLP) methods to extract different linguisticmetrics. Word count, sentence count, vocabulary size, average sentence length, average word length, sentiment score, speech pace, frequency of pauses, and average length of pauses are some of these measures. The technique also determines the speaker's speaking style. Keywords: audio analysis, linguistic features, natural language processing, pause detection, speech recognition, sentiment analysis, visualization.","2640-5768","979-8-3503-5065-4","10.1109/AISP61711.2024.10870738","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10870738","Audio Analysis;Linguistic Features;Natural Language Processing;Pause Detection;Speech Recognition;Sentiment Analysis;Visualization","Training;Vocabulary;Sentiment analysis;Signal processing algorithms;Linguistics;Feature extraction;Size measurement;Frequency measurement;Speech processing;Automatic speech recognition","","","","20","IEEE","12 Feb 2025","","","IEEE","IEEE Conferences"
"Speech Retrieval-Augmented Generation without Automatic Speech Recognition","D. J. Min; K. Mundnich; A. Lapastora; E. Soltanmohammadi; S. Ronanki; K. Han",University of Michigan; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs; AWS AI Labs,"ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","One common approach for question answering over speech data is to first transcribe speech using automatic speech recognition (ASR) and then employ text-based retrieval-augmented generation (RAG) on the transcriptions. While this cascaded pipeline has proven effective in many practical settings, ASR errors can propagate to the retrieval and generation steps. To overcome this limitation, we introduce SpeechRAG, a novel framework designed for open-question answering over spoken data. Our proposed approach fine-tunes a pre-trained speech encoder into a speech adapter fed into a frozen large language model (LLM)–based retrieval model. By aligning the embedding spaces of text and speech, our speech retriever directly retrieves audio passages from text-based queries, leveraging the retrieval capacity of the frozen text retriever. Our retrieval experiments on spoken question answering datasets show that direct speech retrieval does not degrade over the text-based baseline, and outperforms the cascaded systems using ASR. For generation, we use a speech language model (SLM) as a generator, conditioned on audio passages rather than transcripts. Without fine-tuning of the SLM, this approach outperforms cascaded text-based models when there is high WER in the transcripts.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888900","speech retrieval-augmented generation;spoken content retrieval;cross-modal retrieval;multimodal retrieval;open question answering;audio language model;speech language model","Adaptation models;Large language models;Retrieval augmented generation;Pipelines;Signal processing;Question answering (information retrieval);Noise measurement;Speech processing;Automatic speech recognition;Indexing","","","","34","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Automatic Speech Recognition in Diverse English Accents","H. Mohyuddin; D. Kwak","Department of Computer Science and Technology, Kean University, Union, NJ, USA; Department of Computer Science and Technology, Kean University, Union, NJ, USA",2023 International Conference on Computational Science and Computational Intelligence (CSCI),"19 Jul 2024","2023","","","714","718","Advancements in automatic speech recognition (ASR) systems have led to their widespread integration into daily life, significantly altering our interaction with technology. However, this interaction is not always seamless for all users. Specifically, speakers with accents frequently face difficulties using ASR technologies and often need to deliberately adjust their pronunciation for better recognition. This study aims to compare leading ASR models' ability to transcribe speech from accented speakers of various nationalities against their native American English-speaking counterparts. We utilize two speech corpora: the L2-ARCTIC (L2A) and the Speech Accent Archive (SAA), which provide the original ‘clean’ audio samples. From there, two additional files are created by adding background noise to the original samples. These files are then processed through the respective APIs of each ASR model to obtain transcriptions. The accuracy of these transcriptions is then assessed by calculating the Word Error Rate (WER) for each speaker and model. The primary objective of this study is to highlight the challenges faced by speakers with diverse accents in using ASR technology. By highlighting these issues, we aim to encourage proactive measures to take steps towards their resolution. We believe it emphasizes the importance of fostering a more equitable and inclusive user experience.","2769-5654","979-8-3503-6151-3","10.1109/CSCI62032.2023.00122","National Science Foundation(grant numbers:DUE-2129795); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590378","Voice Assistants;Automatic Speech Recognition;Accented Speech;ASR Accuracy;Accent Recognition","Scientific computing;Filtering;Face recognition;Working environment noise;Noise;Speech enhancement;User experience","","1","","21","IEEE","19 Jul 2024","","","IEEE","IEEE Conferences"
"Benchmarking Automatic Speech Recognition Technology for Natural Language Samples of Children With and Without Developmental Delays","E. McGonigle; M. VanDam; C. Wilkinson; K. T. Johnson","Dept. of Psychology, Northeastern University, Boston, MA, USA; Dept. of Speech & Hearing Sciences, Elson S. Floyd College of Medicine, Washington State University, Spokane, WA, USA; Div. of Developmental Medicine, Boston Children’s Hospital, Boston, MA, USA; Div. of Developmental Medicine, Boston Children’s Hospital, Boston, MA, USA",2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),"17 Dec 2024","2024","","","1","5","Natural language sampling (NLS) offers rich insights into real-world speech and language usage across diverse groups; yet, human transcription is time-consuming and costly. Automatic speech recognition (ASR) technology has the potential to revolutionize NLS research. However, its performance in clinical-research settings with young children and those with developmental delays remains unknown. This study evaluates the OpenAI Whisper ASR model on n=34 NLS sessions of toddlers with and without language delays. Manual comparison of ASR to human transcriptions of children with Down Syndrome (DS; n=19; 2-5 years old) and typically-developing children (TD; n=15; 2-3 years old) revealed ASR accurately captured 50% of words spoken by TD children but only 14% for those with DS. About 20% of words were missed in both groups, and 21% (TD) and 6% (DS) of words were replaced. ASR also struggled with developmentally informative sounds, such as non-speech vocalizations, missing almost 50% in the DS data and misinterpreting most of the rest. While ASR shows potential in reducing transcription time, its limitations underscore the need for human-in-the-loop clinical machine learning systems, especially for underrepresented groups.","2694-0604","979-8-3503-7149-9","10.1109/EMBC53108.2024.10782773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10782773","speech;ecological validity;real-world data;audio;artificial intelligence;digital health;toddlers;nonverbal vocalizations;unintelligible speech;autism","Pediatrics;Speech analysis;Natural languages;Transforms;Benchmark testing;Predictive models;Human in the loop;Data models;Delays;Automatic speech recognition","Humans;Child, Preschool;Speech Recognition Software;Benchmarking;Natural Language Processing;Developmental Disabilities;Male;Female;Language Development Disorders;Down Syndrome","","","24","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Improving Accented Speech Recognition Using Data Augmentation Based on Unsupervised Text-to-Speech Synthesis","C. -T. Do; S. Imai; R. Doddipatla; T. Hain","Toshiba Research Europe, Cambridge, UK; Tohoku University, Sendai, Japan; Toshiba Research Europe, Cambridge, UK; University of Sheffield, Sheffield, UK",2024 32nd European Signal Processing Conference (EUSIPCO),"23 Oct 2024","2024","","","136","140","This paper investigates the use of unsupervised text-to-speech synthesis (TTS) as a data augmentation method to improve accented speech recognition. TTS systems are trained with a small amount of accented speech training data and their pseudo-labels rather than manual transcriptions, and hence un-supervised. This approach enables the use of accented speech data without manual transcriptions to perform data augmentation for accented speech recognition. Synthetic accented speech data, generated from text prompts by using the TTS systems, are then combined with available non-accented speech data to train automatic speech recognition (ASR) systems. ASR experiments are performed in a self-supervised learning framework using a Wav2vec2.0 model which was pre-trained on large amount of unsupervised accented speech data. The accented speech data for training the unsupervised TTS are read speech, selected from L2-ARCTIC and British Isles corpora, while spontaneous conversational speech from the Edinburgh international accents of English corpus are used as the evaluation data. Experimental results show that Wav2vec2.0 models which are fine-tuned to downstream ASR task with synthetic accented speech data, generated by the unsupervised TTS, yield up to 6.1% relative word error rate reductions compared to a Wav2vec2.0 baseline which is fine-tuned with the non-accented speech data from Librispeech corpus.","2076-1465","978-9-4645-9361-7","10.23919/EUSIPCO63174.2024.10715166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10715166","Accented speech recognition;text-to-speech synthesis;data augmentation;self-supervised learning;Wav2vec2.0","Training;Training data;Speech recognition;Manuals;Self-supervised learning;Signal processing;Data augmentation;Data models;Text to speech;Speech processing","","","","30","","23 Oct 2024","","","IEEE","IEEE Conferences"
"Automatic and Multilingual Speech Recognition and Translation by using Google Cloud API","P. Yellamma; P. R. Varun; N. C. N. L. Narayana; Y. Chowdary; P. Manikanth; K. H. G. Sai","Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India; Dept of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram, India",2024 5th International Conference on Mobile Computing and Sustainable Informatics (ICMCSI),"11 Apr 2024","2024","","","566","571","The speech recognition is plays a vital role in the technology. The proposed work introduces a web application that leverages state-of-the-art technologies for audio-to-text recognition and multilingual text translation. Developed using Flask, the application integrates the googletrans library for translation, speech_recognition for audio processing, and mysql.connector for seamless database integration. Users have the convenience of uploading audio files, which undergo automatic transcription with language detection. The recognized text is subsequently translated into languages such as Hindi, Tamil, and Telugu, offering users the flexibility to choose their desired target language. Additionally, the system ensures secure storage of both transcribed speech and its translations in a MySQL database for future retrieval. The user-friendly web interface simplifies the entire process, making it a valuable tool for language learning, content localization, and accessibility services. The project effectively highlights the harmonious integration of audio recognition and machine translation technologies, delivering a powerful solution for various applications.","","979-8-3503-9523-5","10.1109/ICMCSI61536.2024.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493971","Automatic Speech Recognition (ASR);Multilingual Speech Recognition;Machine Translation;Cloud Infrastructure;Speech-to-Text Conversion","Text recognition;Databases;Target recognition;Heuristic algorithms;Speech recognition;Regulation;Real-time systems","","1","","15","IEEE","11 Apr 2024","","","IEEE","IEEE Conferences"
"A Novel Approach for Bootstrapping and Automatic Transcription of Low Resourced Language Speech Corpus","M. K. Roy; K. K. Arora; J. Basu; S. Basu; S. Arora; S. S. Agarwal","SNLP Dept., CDAC, Noida, India; SNLP Dept., CDAC, Noida, India; ASPG Dept., CDAC, Kolkata, India; CSE Dept., MAKAUT, Kolkata, India; SNLP Dept., CDAC, Noida, India; CSE Dept., KIIT, Gurgaon, India",2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"2 Apr 2024","2023","","","1","5","Automatic Speech Recognition (ASR) systems have made significant advancements in the context of high-resource languages, primarily attributable to the abundant availability of extensive and diverse speech datasets. Nevertheless, the dearth of annotated data remains a substantial hurdle when it comes to low-resource languages. This study delves into the feasibility of development of an ASR system for low-resource languages by leveraging pre-trained models from other languages. The fine-tuned model is then deployed to transcribe speech segments from news bulletins and audio content found on the web. Subsequently, the generated transcript is heuristically aligned with existing news script. This newly aligned speech corpus is used incrementally to augment the existing corpus, and thus progressively bootstrapping the ASR models. The proposed work has been effectively carried out for Dogri, a low resource language of India. The proposed approach of incremental learning and data augmentation can be applied to other low resource languages as well, and thus would help in bridging the resource gap.","2472-7695","979-8-3503-4402-8","10.1109/O-COCOSDA60357.2023.10482938","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482938","Speech Recognition;Speech Corpus;Low Resource language;Dogri","Databases;Data augmentation;Automatic speech recognition","","","","20","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"CTC-Based End-to-End Speech Recognition for Low Resource Language Sanskrit","Suhani; A. Dev; P. Bansal","Dept. of Information Technology, Indira Gandhi Delhi Technical University for Women (IGDTUW), New Delhi, India; Dept. of Information Technology, Indira Gandhi Delhi Technical University for Women (IGDTUW), New Delhi, India; Dept. of AI & DS, Indira Gandhi Delhi Technical University for Women (IGDTUW), New Delhi, India",2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"2 Apr 2024","2023","","","1","5","Automatic Speech Recognition (ASR) has grown enormously over the past ten years, attracting much interest and attention. Implementing their systems, language adaption, and performance robustness remain some of the major obstacles. Sanskrit presents a challenge for developing such systems since it is a more complex language than other languages and lacks common databases. Deep learning is widely applied in numerous study domains and has established greater importance. The capability of a machine or a program to recognize spoken statements or to translate speech is known as automated speech recognition. It requires the ability to contrast a vocal pattern with a pre-existing or previously learned set of words. This study aims to develop an effectively optimized recurrent neural network (RNN) and CNN-based Sanskrit speech recognition system. Additionally, the Connectionist Temporal Classification (CTC) loss function is employed to increase the likelihood of accurate transcription. Using 46,000 utterances from 27 distinct speakers, the algorithm research has been trained to recognize Sanskrit. The experimental results show potential for automated processing of valuable information extraction. Accurately processing language accents is essential for successful human interaction, and this plays a role in advancing more streamlined and efficient approaches to accomplish this objective.","2472-7695","979-8-3503-4402-8","10.1109/O-COCOSDA60357.2023.10482943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482943","Automatic Speech Recognition (ASR);Sanskrit;MFCC;CNN;RNN;CTC","Deep learning;Recurrent neural networks;Databases;Information retrieval;Robustness;Classification algorithms;Automatic speech recognition","","","","22","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Self-Training and Error Correction using Large Language Models for Medical Speech Recognition","S. N. Dungavath; K. S. Nataraj; N. Tiwari","Computer Science and Engineering IIIT Dharwad, Dharwad, India; Electronics and Communication Engg. IIIT Dharwad, Dharwad, India; SECS IIT Bhubaneswar, Bhubaneswar, India",2024 IEEE Conference on Engineering Informatics (ICEI),"12 Mar 2025","2024","","","1","6","In the healthcare sector, medical professionals must dedicate substantial time and effort to documentation, which directly impacts patient care and clinical decision-making. Automatic speech recognition (ASR) systems offer a potential solution to reduce the burden of these documentation tasks. However, conventional ASR systems often perform poorly in the medical domain due to the use of specialized terminology, as well as variations in accent and pronunciation. Fine-tuning ASR models with large amounts of medical speech data is challenging due to confidentiality concerns and limitations in recording conditions. This study explores two approaches to enhance the performance of ASR for medical speech. The first approach utilizes a self-training mechanism, where transcriptions generated by a baseline ASR model are used to train the model further. The pseudo-labels produced by the baseline model introduce greater data diversity, particularly when the model transcribes with high confidence. The second approach employs open-source large language models (LLMs) to select the most probable transcription from the fivebest hypotheses generated by the ASR model. We conducted experiments using the Whisper ASR model as the baseline and investigated the effectiveness of these two approaches. Our findings indicate that self-training reduced the word error rate (WER) by an absolute $1-2 \%$. However, the use of five-best hypothesis selection resulted in an increased WER, which we attribute to the limited medical knowledge of the open-source LLMs.","","979-8-3315-0577-6","10.1109/ICEI64305.2024.10912300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912300","Key Words: Automatic Speech Recognition;SelfTraining;Error Correction;Medical Transcriptions","Accuracy;Terminology;Large language models;Decision making;Documentation;Medical services;Speech enhancement;Data models;Error correction;Automatic speech recognition","","","","13","IEEE","12 Mar 2025","","","IEEE","IEEE Conferences"
"Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo Language","T. Abu; Y. Shi; T. F. Zheng; D. Wang","Center for Speech and Language Technologies, BNRist, Beijing; Center for Speech and Language Technologies, BNRist, Beijing; Center for Speech and Language Technologies, BNRist, Beijing; Center for Speech and Language Technologies, BNRist, Beijing","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","We present a novel Automatic Speech Recognition (ASR) dataset for the Oromo language, a widely spoken language in Ethiopia and neighboring regions. The dataset was collected through a crowdsourcing initiative, encompassing a diverse range of speakers and phonetic variations. It consists of 100 hours of real-world audio recordings paired with transcriptions, covering read speech in both clean and noisy environments. This dataset addresses the critical need for ASR resources for the Oromo language which is underrepresented. To show its applicability for the ASR task, we conducted experiments using the Conformer model, achieving a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss and WER of 18.74% with pure CTC loss. Additionally, fine-tuning the Whisper model resulted in a significantly improved WER of 10.82%. These results establish baselines for Oromo ASR, highlighting both the challenges and the potential for improving ASR performance in Oromo. The dataset is publicly available at https://github.com/turinaf/sagalee and we encourage its use for further research and development in Oromo speech processing.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890761","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890761","Speech Recognition;Afaan Oromo;Dataset;Speech processing","Crowdsourcing;Error analysis;Signal processing;Phonetics;Audio recording;Acoustics;Noise measurement;Speech processing;Research and development;Automatic speech recognition","","","","27","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Speech Therapy Assistance through Gamification","N. E. Syam; A. S. Vempeny; A. M. Biju; E. M. Abraham; S. Anjali","Department of Computer Science and Engineering, Mar Baselios College of Engineering and Technology (Autonomous), Thiruvananthapuram, India; Department of Computer Science and Engineering, Mar Baselios College of Engineering and Technology (Autonomous), Thiruvananthapuram, India; Department of Computer Science and Engineering, Mar Baselios College of Engineering and Technology (Autonomous), Thiruvananthapuram, India; Department of Computer Science and Engineering, Mar Baselios College of Engineering and Technology (Autonomous), Thiruvananthapuram, India; Department of Computer Science and Engineering, Mar Baselios College of Engineering and Technology (Autonomous), Thiruvananthapuram, India",2024 International Conference on Computing and Intelligent Reality Technologies (ICCIRT),"18 Mar 2025","2024","","","1","5","Speech therapy, or speech-language pathology, is a specialized field that helps people with communication and swallowing difficulties and break down the barriers that result from speech impediments. The goals are to improve pronunciation, strengthen the muscles used in speech, and learn to speak correctly. This work aims to bring in an effective method of phonetic training primarily focusing on children developing the capability to speak. In this therapy tool, enhanced speech is used for providing auditory feedback with a delay to instill confidence in patients, so that they can improve their speech intelligibility gradually through relearning. The phenome level conversion technique-based transformer model allows the comparison of texts derived from audio which are converted using an Automatic Speech Recognition ASR model. Pheromone transcriptions from the CMU dictionary and words from the Merriam-Webster Dictionary have been used to help this proposed algorithm. This study introduces a gamified approach to speech therapy, focusing on phonetic training for children. Utilizing a transformer-based model and ASR technology, the system offers real-time feedback to improve pronunciation and speech intelligibility. The model attains 83% accuracy in phoneme analysis, demonstrating significant improvements in user engagement and speech outcomes, making it a robust tool for speech therapy applications.","","979-8-3315-1029-9","10.1109/ICCIRT59484.2024.10922076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10922076","Deep Learning;Natural Language Processing;Speech Recognition;Automatic Speech Recognition;Transformer Model","Training;Dictionaries;Speech analysis;Medical treatment;Focusing;Speech enhancement;Phonetics;Transformers;Real-time systems;Automatic speech recognition","","","","15","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"End-to-End Multi-Modal Speech Recognition on an Air and Bone Conducted Speech Corpus","M. Wang; J. Chen; X. -L. Zhang; S. Rahardja","School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China; School of Marine Science and Technology, Northwestern Polytechnical University, Xi'an, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Dec 2022","2023","31","","513","524","Automatic speech recognition (ASR) has been significantly improved in the past years. However, most robust ASR systems are based on air-conducted (AC) speech, and their performances in low signal-to-noise-ratio (SNR) conditions are not satisfactory. Bone-conducted (BC) speech is intrinsically insensitive to environmental noise, and therefore can be used as an auxiliary source for improving the performance of an ASR at low SNR. In this paper, we first develop a multi-modal Mandarin corpus, which contains air- and bone-conducted synchronized speech (ABCS). The multi-modal speeches are recorded with a headset equipped with both AC and BC microphones. To our knowledge, it is by far the largest corpus for conducting bone conduction ASR research. Then, we propose a multi-modal conformer ASR system based on a novel multi-modal transducer (MMT). The proposed system extracts semantic embeddings from the AC and BC speech signals by a conformer-based encoder and a transformer-based truncated decoder. The semantic embeddings of the two speech sources are fused dynamically with adaptive weights by the MMT module. Experimental results demonstrate the proposed multi-modal system outperforms single-modal systems with either AC or BC modality and multi-modal baseline system by a large margin at various SNR levels. It also shows the two modalities complement with each other, and our method can effectively utilize the complementary information of different sources.","2329-9304","","10.1109/TASLP.2022.3224305","Oversea Expertise Introduction Project for Discipline Innovation(grant numbers:B18041); National Natural Science Foundation of China(grant numbers:62176211); Innovation Foundation for Doctor Dissertation of Northwestern Polytechnical University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961873","Speech recognition;multi-modal speech processing;bone conduction;air- and bone-conducted speech corpus","Speech recognition;Speech processing;Signal to noise ratio;Spectrogram;Headphones;Microphones;Synchronization","","12","","43","IEEE","23 Nov 2022","","","IEEE","IEEE Journals"
"Corpus Development for Dzongkha Automatic Speech Recognition","P. Galey; S. Tanachutiwat","The Sirindhorn International Thai-German Graduate School of Engineering, King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand; The Sirindhorn International Thai-German Graduate School of Engineering, King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand",2021 International Conference on Intelligent Technologies (CONIT),"4 Aug 2021","2021","","","1","4","Speech recognition technology has been very popular and reached an advanced stage in computational linguistics that enables the recognition and transcription of spoken language into text by computers. However, the research on automatic speech recognition (ASR) for Dzongkha is the first of its kind and none has carried out any research and development on Dzongkha language; whereby making the data resource preparation information very limited. More than 11000 Dzongkha utterances of raw data are collected from Dzongkha Development Commission. The data wrangling is carried out on the limited data and the Dzongkha ASR corpus has been developed. In corpus, a total of 10566 usable utterances is prepared which includes Dzongkha text, audio recordings, and transcriptions. The pronunciation dictionary (lexicon) is generated which contains a total of 12605 unique Dzongkha words; and also, a phone list is recorded using Dzongkha phonemic inventory table for the representation of the smallest unit of the sound at the phoneme level. Such speech corpus is the primary requirement for developing the new speech recognition system for Dzongkha language.","","978-1-7281-8583-5","10.1109/CONIT51480.2021.9498329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9498329","Natural Language Processing (NLP);Automatic Speech Recognition (ASR);Dzongkha language;Corpus;Phonemes;Lexicon","Computers;Dictionaries;Text recognition;Annotations;Documentation;Audio recording;Natural language processing","","2","","11","IEEE","4 Aug 2021","","","IEEE","IEEE Conferences"
"Integration of Speech Separation, Diarization, and Recognition for Multi-Speaker Meetings: System Description, Comparison, and Analysis","D. Raj; P. Denisov; Z. Chen; H. Erdogan; Z. Huang; M. He; S. Watanabe; J. Du; T. Yoshioka; Y. Luo; N. Kanda; J. Li; S. Wisdom; J. R. Hershey","Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD; Institute for Natural Language Processing, University of Stuttgart, Germany; Microsoft Corp, Redmond, WA; Google Research, Cambridge, MA; Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD; University of Science and Technology of China, HeFei, China; Center for Language and Speech Processing, The Johns Hopkins University, Baltimore, MD; University of Science and Technology of China, HeFei, China; Microsoft Corp, Redmond, WA; Department of Electrical Engineering, Columbia University, NY; Microsoft Corp, Redmond, WA; Microsoft Corp, Redmond, WA; Google Research, Cambridge, MA; Google Research, Cambridge, MA",2021 IEEE Spoken Language Technology Workshop (SLT),"25 Mar 2021","2021","","","897","904","Multi-speaker speech recognition of unsegmented recordings has diverse applications such as meeting transcription and automatic subtitle generation. With technical advances in systems dealing with speech separation, speaker diarization, and automatic speech recognition (ASR) in the last decade, it has become possible to build pipelines that achieve reasonable error rates on this task. In this paper, we propose an end-to-end modular system for the LibriCSS meeting data, which combines independently trained separation, diarization, and recognition components, in that order. We study the effect of different state-of-the-art methods at each stage of the pipeline, and report results using task-specific metrics like SDR and DER, as well as downstream WER. Experiments indicate that the problem of overlapping speech for diarization and ASR can be effectively mitigated with the presence of a well-trained separation module. Our best system achieves a speaker-attributed WER of 12.7%, which is close to that of a non-overlapping ASR.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383556","Speech separation;diarization;speech recognition;multi-speaker","Measurement;Error analysis;Conferences;Pipelines;Speech recognition;Task analysis;Automatic speech recognition","","43","","43","IEEE","25 Mar 2021","","","IEEE","IEEE Conferences"
"DARIJA-C: towards a Moroccan DARIJA Speech recognition and speech-to-text Translation Corpus","M. Labied; A. Belangour; M. Banane","Ben M’sik Faculty of Sciences, Laboratory of Information Technology and Modeling LTIM Hassan II University, Casablanca, Morocco; Ben M’sik Faculty of Sciences, Laboratory of Information Technology and Modeling LTIM Hassan II University, Casablanca, Morocco; Faculty of Legal, Economic, and Social Sciences, Laboratory of Artificial Intelligence & Complex Systems Engineering Hassan II University, Casablanca, Morocco",2023 1st International Conference on Advanced Innovations in Smart Cities (ICAISC),"3 Apr 2023","2023","","","1","4","This paper introduces an automated collection of a speech corpus for the Moroccan Arabic dialect “Darija” (DARIJA-C) which is intended for speech-to-text translation from Moroccan Darija into classical Arabic language. The DARIJA-C corpus is designed for Moroccan Darija automatic speech-to-text translation purposes. Nevertheless, it can be useful for automatic speech recognition of this dialect. To address both scale and sustainability, the DARIJA-C project uses crowdsourcing to collect and validate speech transcriptions and translations. By providing an automatic web platform for recording speech, along with their corresponding translation by distinct unknown speakers. The first versions of the Darija-C dataset will include only the translation of Moroccan Darija speech to classical Arabic. In later versions, we will include the translation of Moroccan Darija into other international languages such as French, and English, … The goal of this work is to build the largest crowdsourced corpus of Darija speech, which to our knowledge will be the first corpus for Moroccan Darija Speech-to-text translation.","","978-1-6654-7275-3","10.1109/ICAISC56366.2023.10085164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10085164","Moroccan Darija Speech corpus;Moroccan Arabic Dialect;Automatic speech recognition;Speech-to-Text Translation;DARIJA-C","Training;Crowdsourcing;Technological innovation;Smart cities;Buildings;Speech recognition;Recording","","2","","16","IEEE","3 Apr 2023","","","IEEE","IEEE Conferences"
"Can Differential Testing Improve Automatic Speech Recognition Systems?","M. H. Asyrofi; Z. Yang; J. Shi; C. W. Quan; D. Lo","School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University",2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),"24 Nov 2021","2021","","","674","678","Due to the widespread adoption of Automatic Speech Recognition (ASR) systems in many critical domains, ensuring the quality of recognized transcriptions is of great importance. A recent work, CrossASR++, can automatically uncover many failures in ASR systems by taking advantage of the differential testing technique. It employs a Text-To-Speech (TTS) system to synthesize audios from texts and then reveals failed test cases by feeding them to multiple ASR systems for cross-referencing. However, no prior work tries to utilize the generated test cases to enhance the quality of ASR systems. In this paper, we explore the subsequent improvements brought by leveraging these test cases from two aspects, which we collectively refer to as a novel idea, evolutionary differential testing. On the one hand, we fine-tune a target ASR system on the corresponding test cases generated for it. On the other hand, we fine-tune a cross-referenced ASR system inside CrossASR++, with the hope to boost CrossASR++'s performance in uncovering more failed test cases. Our experiment results empirically show that the above methods to leverage the test cases can substantially improve both the target ASR system and CrossASR++ itself. After fine-tuning, the number of failed test cases uncovered decreases by 25.81% and the word error rate of the improved target ASR system drops by 45.81%. Moreover, by evolving just one cross-referenced ASR system, CrossASR++ can find 5.70%, 7.25%, 3.93%, and 1.52% more failed test cases for 4 target ASR systems, respectively.","2576-3148","978-1-6654-2882-8","10.1109/ICSME52107.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609154","Automatic Speech Recognition;Test Case Generation;Differential Testing","Software maintenance;Error analysis;Conferences;Automatic speech recognition","","8","","11","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"An Effort Towards Improving Automatic-Transcription Systems","A. Biswas; L. S. Sabela; P. K. Sahu; R. K. Samanta","Wayland Academy, Durgapur, West Bengal, India; Pelita Harapan University, Jakarta, Indonesia; Dr B.C. Roy Engineering College, Durgapur, West Bengal, India; Dr B.C. Roy Engineering College, Durgapur, West Bengal, India","2022 International Interdisciplinary Conference on Mathematics, Engineering and Science (MESIICON)","10 Apr 2023","2022","","","1","6","The automatic speech recognition (ASR) system is a modern popular emerging technological system with lots of scope for applications in different domains. However, developing a mature ASR system needs to address lots of challenges to accommodate the diversity of the accents of speakers from diverse countries, regions, communities, languages, dialects and many more. This work is an effort to understand the different aspects of improving modern automatic transcription systems. The performance on significant aspects like Accent detection, Gender Detection and Voice Recognition across five major audio features have been captured, analyzed and studied. Standard machine learning architecture has been applied to analyze the features and determine their impact on the performance of the transcription system. We observed a high level of accuracy on combined features and hope this study will help those who are working in ASR, and NLP.","","978-1-6654-7072-8","10.1109/MESIICON55227.2022.10093297","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093297","Transcription;Voice Recognition;Gender Recognition;Accent Detection;Machine Learning;Feature Extraction","Diversity reception;Machine learning;Feature extraction;Mathematics;Standards;Testing;Automatic speech recognition","","","","19","IEEE","10 Apr 2023","","","IEEE","IEEE Conferences"
"Enhancing Forensic Audio Transcription with Neural Network-Based Speaker Diarization and Gender Classification","R. Ullah; I. Asghar; H. Malik; G. Evans; J. Ahmad; D. A. Roberts","School of Computer Science and Electronic Engineering, University of Essex, Colchester, UK; School of Computing, Engineering and Digital Technologies, Teesside University, Middlesbrough, UK; School of Computing Sciences, University of East Anglia, Norwich, UK; Faculty of Computing Engineering and Science, Uiversity of South Wales, Pontypridd, UK; Cybersecurity Center, Prince Mohammad Bin Fahd University, Al Khobar, Saudi Arabia; Posib Ltd Y Gilfach, Nercwys, Flintshire, UK",2024 International Conference on Engineering and Emerging Technologies (ICEET),"12 Mar 2025","2024","","","1","6","Forensic audio transcription is often compromised by low-quality recordings, where indistinct speech can hinder the accuracy of conventional Automatic Speech Recognition (ASR) systems. This study addresses this limitation by developing a machine learning-based approach to improve speaker diarization, a process critical for distinguishing between speakers in sensitive audio data. Previous research highlights the inadequacy of traditional ASR in forensic settings, particularly where audio quality is poor and speaker overlap is common. This paper presents a neural network specifically designed for gender classification, using 20 key acoustic features extracted from real forensic audio data. The model architecture includes input, hidden, and output layers tailored to differentiate male and female voices, with dropout regularization to prevent overfitting and hyperparameter optimization ensuring robust generalization across test data. The neural network achieved an average recall of 86.81%, F1 score of 85.67%, precision of 87.95%, and accuracy of 86.83% across varied audio conditions. This model significantly improves transcription accuracy, reducing errors in legal contexts and supporting judicial processes with more reliable, interpretable evidence from sensitive audio data.","2831-3682","979-8-3315-3289-5","10.1109/ICEET65156.2024.10913726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10913726","Forensic linguistics;speech diarization;Speech transcription;Automatic speech recognition;ML","Biometrics;Accuracy;Forensics;Neural networks;Speech enhancement;Feature extraction;Data models;Recording;Reliability;Automatic speech recognition","","","","15","IEEE","12 Mar 2025","","","IEEE","IEEE Conferences"
"Enhancing Phoneme Recognition in the Bengali Language Through Fine-Tuning of Multilingual Model","A. Deep; P. Bharati; S. Chandra; D. Pramanik; K. S. Naik; S. Kumar Das Mandal","Indian Institute of Technology, Kharagpur; Indian Institute of Technology, Kharagpur; Indian Institute of Technology, Kharagpur; Indian Institute of Technology, Kharagpur; Indian Institute of Technology, Kharagpur; Indian Institute of Technology, Kharagpur",2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"20 Dec 2024","2024","","","1","5","Phoneme recognition is essential in speech recognition systems, allowing for precise transcription of spoken language into text. This technology underpins virtual assistants, voice search, and automated transcription services. It is also vital in language learning applications, where it helps with pronunciation training and accent reduction by offering real-time feedback on spoken language accuracy. Phonemes are the distinct sounds that create meaningful contrasts within a specific language, while phones refer to the actual sounds produced, which are generally language-independent. Allophones are the set of phones that correspond to a particular phoneme. Phoneme recognition of low resource languages using Automatic Speech Recognition (ASR) model produce suboptimal results, hence to improve the result, we can use pre-trained model with fine-tuning. In this work, we are utilizing the multilingual pre-trained model by fine-tuning it for the Bengali language. A multilingual pre-trained model called Allosaurus makes predictions over a common phone inventory before using an allophone layer to map onto a particular phoneme. For fine-tuning, we require phoneme transcribed dataset of Bengali language which is not available. Here we have used one of the text to phoneme model to get phoneme transcribed data. Through our proposed approach, we are able achieve to significant improvements in the phoneme recognition of Bengali language that we can see as the Phoneme Error Rate (PER) of our test file reduces by 95.65% compared to the model without fine-tuning.","2472-7695","979-8-3315-0603-2","10.1109/O-COCOSDA64382.2024.10800277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800277","Allophone;automatic speech recognition;multilingual;phonemizer","Training;Text recognition;Error analysis;Databases;Virtual assistants;Speech recognition;Speech enhancement;Predictive models;Real-time systems;Multilingual","","","","20","IEEE","20 Dec 2024","","","IEEE","IEEE Conferences"
"Multilingual Phonetic Dataset for Low Resource Speech Recognition","X. Li; D. R. Mortensen; F. Metze; A. W. Black",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","6958","6962","Phone Recognition is one of the most important tasks in the field of multilingual speech recognition, especially for low-resource languages whose orthographies are not available. However, most speech recognition datasets so far only focus on high-resource languages, there are very few datasets available for low-resource languages, especially datasets with detailed phone annotation. In this work, we present a large multilingual phonetic dataset, which is preprocessed and aligned from the UCLA phonetic dataset. The dataset contains around 100 low-resource languages and 7000 utterances in total. This dataset would provide an ideal training/evaluation set for universal phone recognition.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413720","Multilingual Phonetic Dataset;Multilingual Speech Alignment;Low-Resource Speech recognition","Annotations;Conferences;Speech recognition;Phonetics;Signal processing;Acoustics;Speech processing","","5","","17","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Enhancing Communication: Utilizing Transfer Learning for Improved Speech-to-Text Transcription","S. D; S. H. Fazil","Department of Computer Science and Engineering, Amrita School of Computing Amrita Vishwa Vidyapeetham, Chennai, Tamil Nadu, India; Department of Computer Science and Engineering, Amrita School of Computing Amrita Vishwa Vidyapeetham, Chennai, Tamil Nadu, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","6","Automatic Speech Recognition (ASR) transforms spoken language into text facilitating interaction with technology and easing access to information for individuals facing challenges in communication through conventional text-based methods due to conditions like mobility impairments or speech impairments. The objective of this work is to develop a custom speech-to-text system that aids people with disabilities. In this study, transfer learning approach is explored with wav 2 vec a pretrained model. Fine-tuned wav2vec on the TIMIT dataset achieved a Word Error Rate (WER) of 30%, demonstrating the merits of transfer learning. In summary, this work reveals that transfer learning with pre-trained models like wav2vec 2.0 can enable accurate ASR for assistive technology applications with limited training data. This work aims to leverage speech-to-text transcription more accessible for people with disabilities.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10725694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10725694","Automatic Speech Recognition (ASR);Transfer Learning;Word Error Rate;speech-to-text;wav2vec","Error analysis;Computational modeling;Transfer learning;Training data;Transforms;Assistive technologies;People with disabilities;Data models;Speech to text;Automatic speech recognition","","","","23","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Accent-Enhanced Automatic Speech Recognition for Multilingual Classrooms","M. Jagadeeshvaran; H. Sarah Michelle; S. Shruti Bala; M. Srinivasa Vinayak; T. Deepika","Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Coimbatore, India; Department of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidyapeetham, Coimbatore, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","7","In an increasingly interconnected world, universities face the challenge of hosting diverse students from different backgrounds. The biggest obstacle to effective communication and learning in multicultural classrooms is teacher and student diversity. To overcome this challenge, we propose the development of noise-enhanced automatic speech recognition (ASR), specifically for multilingual classrooms. The system is designed to provide instant feedback on lessons while preserving the subtleties of speech, thus improving understanding and creating a learning environment. Using the advanced capabilities of the Transformer model together with the convolutional neural network (CNN) highlight classifier, the proposed system provides powerful and accurate transcription capabilities. This article provides an overview of the design, implementation, and evaluation of voice-enhanced ASR, demonstrating its potential to transform communication and learning in a variety of educational settings.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10724822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724822","Automatic Speech Recognition;Multilingual Classrooms;Accent Enhancement;Inclusive Learning;Speech Transcription;Accent Nullification","Accuracy;Transducers;Face recognition;Neural networks;Transforms;Speech enhancement;Transformers;Complexity theory;Convolutional neural networks;Automatic speech recognition","","","","12","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Enhancing Automatic Speech Recognition: Effects of Semantic Audio Filtering on Models Performance","Y. Perezhohin; T. Santos; V. Costa; F. Peres; M. Castelli","MyNorth AI Research, Oeiras, Portugal; MyNorth AI Research, Oeiras, Portugal; MyNorth AI Research, Oeiras, Portugal; MyNorth AI Research, Oeiras, Portugal; NOVA Information Management School (NOVA IMS), Universidade NOVA de Lisboa, Campus de Campolide, Lisbon, Portugal",IEEE Access,"29 Oct 2024","2024","12","","155136","155150","This paper presents a novel methodology for enhancing Automatic Speech Recognition (ASR) performance by utilizing contrastive learning to filter synthetic audio data. We address the challenge of incorporating synthetic data into ASR training, especially in scenarios with limited real-world data or unique linguistic characteristics. The method utilizes a contrastive learning model to align representations of synthetic audio and its corresponding text transcripts, enabling the identification and removal of low-quality samples that do not align well semantically. We evaluate the methodology on a medium-resource language across two distinct datasets: a general-domain dataset and a regionally specific dataset characterized by unique pronunciation patterns. Experimental results reveal that the optimal filtering strategy depends on both model capacity and dataset characteristics. Larger models, like Whisper Large V3, particularly benefit from aggressive filtering, while smaller models may not require such stringent filtering, especially on non-normalized text. This work highlights the importance of adjusting synthetic data augmentation and filtering to specific model architectures and target domains. The proposed method, robust and adaptable, enhances ASR performance across diverse language settings. We have open-sourced the entire work, which includes 140 hours of synthetically generated Portuguese speech, as well as the pipeline and parameter settings used to create these samples. Additionally, we provide the fine-tuned Whisper models and the code required to reproduce this research. Our code will be available at https://github.com/my-north-ai/semantic_audio_filtering.","2169-3536","","10.1109/ACCESS.2024.3482970","MyNorth Artificial Intelligence (AI) Research; national funds through Fundação para a Ciência e a Tecnologia(FCT) (DOI: 10.54499/UIDB/04152/2020)-Centro de Investigação em Gestão de Informação (MagIC)/NOVA Information Management School (IMS)(grant numbers:UIDB/04152/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10720758","Automatic speech recognition;contrastive learning;data augmentation;embeddings;synthetic data filtering;text-to-speech","Hidden Markov models;Feature extraction;Filtering;Data models;Synthetic data;Training;Contrastive learning;Accuracy;Adaptation models;Transformers;Automatic speech recognition;Contrastive learning;Text to speech","","","","102","CCBYNCND","17 Oct 2024","","","IEEE","IEEE Journals"
"Enhancing Speech-to-Text Transcription Accuracy for the Bahraini Dialect","A. Almahmood; H. Al-Ammal; F. Albalooshi","College of Information Technology, University of Bahrain, Sakhir, Bahrain; Department of Computer Science, University of Bahrain, Sakhir, Bahrain; Department of Computer Engineering, University of Bahrain, Sakhir, Bahrain","2024 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)","13 Jan 2025","2024","","","508","514","This study investigates means of enhancing the accuracy of Automated Speech Recognition (ASR) systems for the Bahraini dialect, a variant that has received minimal attention in natural language processing research. This is achieved through increasing the accuracy of the OpenAI Whisper model's transcription for the Bahraini dialect. Two tailored audio datasets were created: one with a local Bahraini audio which was manually transcribed, and another that included a wider range of audio sources such TV broadcasts, podcasts, and parliament sessions. To optimize the Whisper model, several methods were used, such as sequential training on both datasets, data augmentation, and hyperparameter optimization with Optuna. The Word Error Rate (WER) significantly decreased from 181.608% to 13.54% in the results, demonstrating the efficacy of the fine-tuning techniques in raising the model's accuracy above its original benchmarks.","2770-7466","979-8-3315-3313-7","10.1109/3ict64318.2024.10824280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10824280","Automated Speech Recognition;Dialectal Vari-ability;Speech-to- Text Transcription;Arabic Language Processing;Fine-tuning Models","Training;Technological innovation;Accuracy;TV;Text recognition;Computational modeling;Speech enhancement;Natural language processing;Informatics;Speech to text","","","","23","IEEE","13 Jan 2025","","","IEEE","IEEE Conferences"
"Improving automatic transcription of call center speech using data simulation","V. Chunwijitra; N. Kurpukdee","National Electronics and Computer Technology Center (NECTEC), National Science and Technology Development Agency (NSTDA), Pathumthani, Thailand; National Electronics and Computer Technology Center (NECTEC), National Science and Technology Development Agency (NSTDA), Pathumthani, Thailand","2021 18th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","18 Jun 2021","2021","","","842","845","Due to the various channel distortions and the limited real call center data, the simulation data is an essential resource to train an appropriate acoustic model for automatic call transcription. In this work, in case of in-domain telephony data are unavailable, we imitate the distorted voices by utilizing various colored noises and audio codecs simulation-based data augmentation. To simulate the same channel distortion, since only wide-band data are available, the 16 kHz is down-sampled to the 8 kHz data and then applied such data to the variety of colored noises and codecs. The augmentation conditions consist of using different noises and audio codes with changed SNR and bit rate, respectively. Such distorted speech data is used to train the domain-invariant acoustic model to improve recognition performance. We conduct experiments on a real call center evaluation set to show our proposed data simulation technique's effectiveness. The result demonstrates that distorted data simulation is intimate to the actual call center data since our target model reduces the word error rate by 31.69% relative compared to the baseline.","","978-1-6654-0382-5","10.1109/ECTI-CON51831.2021.9454803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454803","audio codec;colored noise;data simulation;speech augmentation;call center","Training;Acoustic distortion;Codecs;Computational modeling;Training data;Speech recognition;Telephony","","","","17","IEEE","18 Jun 2021","","","IEEE","IEEE Conferences"
"Automatic Speech Recognition for Dementia Screening using ILSE-Interviews","A. Ablimit; T. Schultz",NA; NA,Speech Communication; 14th ITG Conference,"21 Dec 2021","2021","","","1","5","Spoken language skills are strong biomarkers for detecting cognitive decline. Studies like the Interdisciplinary Longitudinal Study of Adult Development and Aging (ILSE) are of particular interest to quantify the predictive power of biomarkers in terms of acoustic/linguistic features. ILSE consists of ca. 6500 hours of interviews and only 10% were manually transcribed. To extract linguistic features, we need to build reliable ASR to provide transcriptions. The ILSE-corpus is challenging for ASR, due to a combination of factors. In this study, we present our effort to overcome some of these challenges. We automatically segmented 45-minutes of interviews into shorter segments and time aligned. Using these segments, we developed HMMDNN based ASR and achieved 33.55% of WER. Based on this system, we recreated the time-alignments for manualtranscriptions and derived acoustic and linguistic features for classifier-training. we applied the resulting system for dementia screening and achieved UAR of 0.867 for a threeclass problem.","","978-3-8007-5627-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9657504","","","","1","","","","21 Dec 2021","","","VDE","VDE Conferences"
"Transformer Based Bangla-English Code-Switching Speech Recognition and Language Identification Model","F. T. Z. Akhi; M. S. Arefin","Department of Computer Science & Engineering, Chittagong University of Engineering & Technology, Chattogram, Bangladesh; Department of Computer Science & Engineering, Chittagong University of Engineering & Technology, Chattogram, Bangladesh","2024 IEEE International Conference on Computing, Applications and Systems (COMPAS)","19 Dec 2024","2024","","","1","5","The growing ubiquity of bilingual and multilingual communication presents serious difficulties for conventional Automatic Speech Recognition (ASR) system in the field of voice recognition. Code-switching is a major problem for traditional ASR systems because it enables two or more languages to be switched into one conversation. In this paper, the model can detect the language (whether Bangla or English) from a spoken sentence and also perform the ASR task from this mixed voice. For this context, XLSR Wav2vec 2.0 encoder has been used for feature extraction, CTC (Connectionist Temporal Classification) is used for ASR purpose and LID (Language Identification) for language identification task. By applying this approach, it is found that the Word Error Rate (WER) is 21.5% and the Language Identification (LID) accuracy is 70%. The suggested approach shows notable gains over any other models and provides a viable method for precise and effective ASR and LID in code switching.","","979-8-3315-2976-5","10.1109/COMPAS60761.2024.10796602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10796602","Code-switching;automatic speech recognition;language identification;transformer;XLSR Wav2Vec 2.0;joint CTC-LID","Accuracy;Codes;Speech coding;Error analysis;Computational modeling;Switches;Oral communication;Feature extraction;Transformers;Multilingual","","","","33","IEEE","19 Dec 2024","","","IEEE","IEEE Conferences"
"Toward Semi-supervised Transcription of NAKO+ILSE: Influence of Automatic Speech Recognition Performance on Manual Transcription Effort","E. Brausse; K. Scheck; T. Schultz",NA; NA; NA,Speech Communication; 15th ITG Conference,"18 Dec 2023","2023","","","106","110","Acoustic and linguistic information of spoken communication were found to be a reliable estimator for early detection of cognitive decline. The extraction of linguistic features, however, requires transcription from spoken content to text, either manually or by automatic speech recognition (ASR). We propose a semi-automatic transcription system using manual correction of ASR output. It is applied to our new project, with which we envision to fuse information of the Interdisciplinary Longitudinal Study of Adult Development and Aging (ILSE) with the German National Cohort (NAKO) study by creating a core overlap dataset NAKO+ILSE. We compare the performance of our ASR system with the zero-shot Whisper system on ILSE interview data and NAKO+ILSE test data NI0. Due to differences in ASR performance between data sets, we analyze the effect of noise levels on ASR performance. Lastly, we analyze the influence of using ASR hypotheses as basis for manual transcriptions. With a minimum word error rate of 45.1 %, pre-trained Whisper models do not outperform our own ASR system of 33.55 %. However, manual transcription time is reduced by a factor of three when using Whisper large as a basis for semi-automatic transcription.","","978-3-8007-6164-7","10.30420/456164020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363008","","","","","","","","18 Dec 2023","","","VDE","VDE Conferences"
"End-to-End Speech Recognition Contextualization with Large Language Models","E. Lakomkin; C. Wu; Y. Fathullah; O. Kalinli; M. L. Seltzer; C. Fuegen",Meta AI; Meta AI; Meta AI; Meta AI; Meta AI; Meta AI,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","12406","12410","In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We use audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system implicitly learns how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively, improving by 7.5% WER overall and 17% WER on rare words, compared to a baseline contextualized RNN-T system that has been trained on a speech dataset more than twenty-five times larger. Overall, we demonstrate that by adding only a handful of trainable parameters via adapters, we can unlock the contextualized speech recognition capability of the pretrained LLM while maintaining the same text-only input functionality.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446898","contextual biasing;large language models;speech recognition","Training;Adaptation models;Speech recognition;Signal processing;Acoustics;Task analysis;Speech processing","","5","","26","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"An Automatic Pipeline For Building Emotional Speech Dataset","N. -A. N. Thi; B. Thang Ta; N. M. Le; V. Hai Do","Viettel Cyberspace Center, Viettel Group, Vietnam; Viettel Cyberspace Center, Viettel Group, Vietnam; Viettel Cyberspace Center, Viettel Group, Vietnam; Viettel Cyberspace Center, Viettel Group, Vietnam",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","1030","1035","Emotional speech synthesis has gained attention for enhancing the naturalness and expressiveness of synthesized speech. However, developing emotional speech synthesis presents challenges due to the lack of high-quality emotional speech datasets, especially for low-resource languages. This paper proposes an automated process for building datasets of emotional speech that reduces dataset generation time by 92%. The pipeline combines trained models and leverages publicly available data. The effectiveness of the proposed pipeline is examined and demonstrated using a Vietnamese case study.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317420","","Emotion recognition;Pipelines;Buildings;Asia;Speech recognition;Information processing;Speech enhancement","","2","","30","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Investigation of Whisper ASR Hallucinations Induced by Non-Speech Audio","M. Barański; J. Jasiński; J. Bartolewska; S. Kacprzak; M. Witkowski; K. Kowalczyk","Signal Processing Group, Institute of Electronics, AGH University of Krakow, Poland; Signal Processing Group, Institute of Electronics, AGH University of Krakow, Poland; Signal Processing Group, Institute of Electronics, AGH University of Krakow, Poland; Signal Processing Group, Institute of Electronics, AGH University of Krakow, Poland; Signal Processing Group, Institute of Electronics, AGH University of Krakow, Poland; Signal Processing Group, Institute of Electronics, AGH University of Krakow, Poland","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Hallucinations of deep neural models are amongst key challenges in automatic speech recognition (ASR). In this paper, we investigate hallucinations of the Whisper ASR model induced by non-speech audio segments present during inference. By inducting hallucinations with various types of sounds, we show that there exists a set of hallucinations that appear frequently. We then study hallucinations caused by the augmentation of speech with such sounds. Finally, we describe the creation of a bag of hallucinations (BoH) that allows to remove the effect of hallucinations through the post-processing of text transcriptions. The results of our experiments show that such post-processing is capable of reducing word error rate (WER) and acts as a good safeguard against problematic hallucinations.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890105","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890105","automatic speech recognition;hallucinations;Whisper;error detection","Correlation;Error analysis;Medical services;Signal processing;Acoustics;Speech processing;Automatic speech recognition","","","","24","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Refining Synthesized Speech Using Speaker Information and Phone Masking for Data Augmentation of Speech Recognition","S. Ueno; A. Lee; T. Kawahara","Nagoya Institute of Technology, Nagoya-shi, Japan; Nagoya Institute of Technology, Nagoya-shi, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","6 Sep 2024","2024","32","","3924","3933","While end-to-end automatic speech recognition (ASR) has shown impressive performance, it requires a huge amount of speech and transcription data. The conversion of domain-matched text to speech (TTS) has been investigated as one approach to data augmentation. The quality and diversity of the synthesized speech are critical in this approach. To ensure quality, a neural vocoder is widely used to generate speech waveforms in conventional studies, but it requires a huge amount of computation and another conversion to spectral-domain features such as the log-Mel filterbank (lmfb) output typically used for ASR. In this study, we explore the direct refinement of these features. Unlike conventional speech enhancement, we can use information on the ground-truth phone sequences of the speech and designated speaker to improve the quality and diversity. This process is realized as a Mel-to-Mel network, which can be placed after a text-to-Mel synthesis system such as FastSpeech 2. These two networks can be trained jointly. Moreover, semantic masking is applied to the lmfb features for robust training. Experimental evaluations demonstrate the effect of phone information, speaker information, and semantic masking. For speaker information, x-vector performs better than the simple speaker embedding. The proposed method achieves even better ASR performance with a much shorter computation time than the conventional method using a vocoder.","2329-9304","","10.1109/TASLP.2024.3451982","JSPS KAKENHI(grant numbers:JP23K16944); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10664004","Data augmentation;domain adaptation;speech recognition;speech synthesis","Speech recognition;Data models;Vocoders;Data augmentation;Training;Computational modeling;Predictive models","","","","57","IEEE","3 Sep 2024","","","IEEE","IEEE Journals"
"Recognition of Fricative Phoneme based Hindi Words in Speech-to-Text System using Wav2Vec2.0 Model","S. P. Gupta; S. V; S. G. Koolagudi","Department of Computer Science & Engineering, National Institute of Technology Karnataka Surathkal, Mangalore, India; Department of Computer Science & Engineering, National Institute of Technology Karnataka Surathkal, Mangalore, India; Department of Computer Science & Engineering, National Institute of Technology Karnataka Surathkal, Mangalore, India","2022 IEEE Global Conference on Computing, Power and Communication Technologies (GlobConPT)","14 Nov 2022","2022","","","1","5","In this work, we have discussed issues with Microsoft's state-of-the-art Speech-to-Text (STT) system. Two key issues have been identified: recognition of Hindi words starting with the fricative phoneme (/ha/) and recognition power of the system with background noise. The solution for correctly identifying the unrecognized Hindi fricative phoneme is by training the Wav2Vec2.0 model on the OpenSLR Hindi dataset. The evaluation of the proposed model is given by the performance metric Char-acter Error Rate (CER). To test the performance of the proposed model, 20 fricative words in both clean and noisy conditions are fed to the trained model. The second issue of handling noisy speech samples is resolved using an amplitude-based automatic noise detection method. The results achieved from the proposed model are observed to be better than the state-of-the-art STT model when trained with and without the language model in terms of CER in clean conditions.","","978-1-6654-9365-9","10.1109/GlobConPT57482.2022.9938222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938222","Fricative phonemes;Phoneme recognition;OpenSLR speech corpus","Training;Error analysis;Computational modeling;Speech recognition;Communications technology;Noise measurement;Background noise","","","","19","IEEE","14 Nov 2022","","","IEEE","IEEE Conferences"
"Evaluating Automatic Transcription Models Utilising Cloud Platforms","K. Meehan; F. McDermott; N. Petropoulos","Department of Computing, Atlantic Technical University, Letterkenny, Ireland; ImpactReady, Sector 3 Solutions Ltd., Belfast, Northern Ireland; School of HAPP, Queen’s University Belfast, Belfast, Northern Ireland",2024 5th International Conference on Data Analytics for Business and Industry (ICDABI),"20 Dec 2024","2024","","","91","96","Automatic Speech Recognition (ASR) technology is becoming pervasive in society and is being used for language translation, customer service and disability support. ASR and transcription is also rapidly becoming a popular manner of enabling qualitative research. Traditionally transcribing interviews and focus groups would have been very time consuming and labour intensive. In recent years, online tools have become available to help with automatic transcription. These tools have varying levels of accuracy, and most will require manual correction. Moreover, these tools require a researcher to manually upload audio files that have been processed or edited.This research proposes the development of an automatic framework for completing ASR and automatic transcription without the need for the researcher to perform any manual processes. The research is completed within an industrial context in an organisation that completes qualitative analysis and evaluation on behalf of clients in the third sector. The proposed framework utilises a cloud-based API for completing the automatic transcription. This research evaluates multiple APIs for completing automatic transcription and selects one service for inclusion within the framework. This evaluation is completed on a self-created audio dataset named “S3QualitativeAudio” using Word Error Rate (WER) calculation on the transcription and also based on cost-benefit analysis. The research has determined that the Whisper ASR model developed by OpenAI provides the lowest error rate of those evaluated (most accurate with 96% of the audio files providing the lowest WER value). The average WER for the Whisper ASR model was 0.07246. Further evaluation was completed using this model in an attempt to decrease the error rate further. The final automatic transcriptions could be used for sentiment analysis and text summarisation to complete further qualitative analysis.","","979-8-3503-6871-0","10.1109/ICDABI63787.2024.10800465","InterTradeIreland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800465","Automatic Transcription;Automatic Speech Recognition;Word Error Rate;Cloud Transcription;Qualitative Research","Industries;Sentiment analysis;Accuracy;Translation;Data analysis;Error analysis;Manuals;Data models;Interviews;Automatic speech recognition","","","","24","IEEE","20 Dec 2024","","","IEEE","IEEE Conferences"
"IIIT-Speech Twins 1.0: An English-Hindi Parallel Speech Corpora for Speech-to-Speech Machine Translation and Automatic Dubbing","A. Mondal; A. K. Vuppala; C. Yarra","Language Technologies Research Center, IIIT, Hyderabad, India; Language Technologies Research Center, IIIT, Hyderabad, India; Language Technologies Research Center, IIIT, Hyderabad, India",2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"20 Dec 2024","2024","","","1","6","The demand for high-quality parallel speech data has been increasing as deep-learning based Speech to Speech Machine Translation (SSMT) and automatic dubbing approaches gain popularity in speech applications. Traditional and well-established speech applications such as Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) heavily rely on large corpus of monolingual speech and the corresponding text. While there is a wealth of parallel text data available for both English and Indic languages, parallel speech data is available only for English and other European languages, yet it often lacks natural prosody and semantic alignment between the languages. For achieving cross-lingual prosody transfer, end-to-end SSMT models, and high-quality dubbing from English to Hindi, in this work, an English-Hindi parallel bilingual speech-text corpus named lIlT-Speech Twins 1.0 is created. This data contains twin-like English and Hindi speech-text pairs obtained from publicly available children's stories in both the languages, through manual and automatic processing. Starting with 8 stories in each language, totaling around 4 hours of audio, the final outcome was a 2-hour dataset. This was achieved through systematic segmentation, re-moval of non-speech background audio, and sentence-by-sentence alignment to ensure accurate meaning in both languages. In addition to ensuring proper alignment and transcription, this dataset offers a rich source of natural prosody, expressions, and emotions, due to the narrative diversity within the stories. The dataset also provides sig-nificant speaker variability, with different characters being voiced by various speakers, enhancing the richness of the lIlT-Speech Twins 1.0 corpus.","2472-7695","979-8-3315-0603-2","10.1109/O-COCOSDA64382.2024.10800086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10800086","SSMT;audio dubbing;cross-lingual prosody transfer;parallel bilingual speech-text corpus","Systematics;Accuracy;Databases;Semantics;Europe;Manuals;Text to speech;Machine translation;Automatic speech recognition","","","","21","IEEE","20 Dec 2024","","","IEEE","IEEE Conferences"
"Findings of the 2024 Mandarin Stuttering Event Detection and Automatic Speech Recognition Challenge","H. Xue; R. Gong; M. Shao; X. Xu; L. Wang; L. Xie; H. Bu; J. Zhou; Y. Qin; J. Du; M. Li; B. Zhang; B. Jia","ASLP@NPU, Northwestern Polytechnical University; StammerTalk; ASLP@NPU, Northwestern Polytechnical University; AIShell Inc; StammerTalk; ASLP@NPU, Northwestern Polytechnical University; AIShell Inc; Nankai University; Nankai University; University of Science and Technology of China; Wuhan University; WeNet Open Source Community; StammerTalk",2024 IEEE Spoken Language Technology Workshop (SLT),"16 Jan 2025","2024","","","385","392","The StutteringSpeech Challenge focuses on advancing speech technologies for people who stutter, specifically targeting Stuttering Event Detection (SED) and Automatic Speech Recognition (ASR) in Mandarin. The challenge comprises three tracks: (1) SED, which aims to develop systems for detection of stuttering events; (2) ASR, which focuses on creating robust systems for recognizing stuttered speech; and (3) Research track for innovative approaches utilizing the provided dataset. We utilizes an open-source Mandarin stuttering dataset AS-70, which has been split into new training and test sets for the challenge. This paper presents the dataset, details the challenge tracks, and analyzes the performance of the top systems, highlighting improvements in detection accuracy and reductions in recognition error rates. Our findings underscore the potential of specialized models and augmentation strategies in developing stuttered speech technologies.","","979-8-3503-9225-8","10.1109/SLT61566.2024.10832208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832208","Mandarin stuttered speech;stuttering event detection;speech recognition","Training;Target tracking;Accuracy;Event detection;Error analysis;Conferences;Data augmentation;Robustness;Data models;Automatic speech recognition","","","","42","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition via Weakly Phonetic Supervision","S. Yusuyin; T. Ma; H. Huang; W. Zhao; Z. Ou","School of Computer Science and Technology, Xinjiang University, Urumqi, China; School of Computer Science and Technology, Xinjiang University, Urumqi, China; School of Computer Science and Technology, Xinjiang University, Urumqi, China; China Unicom (Guangdong) Industrial Internet Company Ltd, Guangzhou, China; Speech Processing and Machine Intelligence (SPMI) Lab, Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Audio, Speech and Language Processing","1 Apr 2025","2025","33","","1440","1453","There exist three approaches for multilingual and crosslingual automatic speech recognition (MCL-ASR) - supervised pretraining with phonetic or graphemic transcription, and self-supervised pretraining. We find that pretraining with phonetic supervision has been underappreciated so far for MCL-ASR, while conceptually it is more advantageous for information sharing between different languages. This paper explores the approach of pretraining with weakly phonetic supervision towards data-efficient MCL-ASR, which is called Whistle. We relax the requirement of gold-standard human-validated phonetic transcripts, and obtain International Phonetic Alphabet (IPA) based transcription by leveraging the LanguageNet grapheme-to-phoneme (G2P) models. We construct a common experimental setup based on the CommonVoice dataset, called CV-Lang10, with 10 seen languages and 2 unseen languages. A set of experiments are conducted on CV-Lang10 to compare, as fair as possible, the three approaches under the common setup for MCL-ASR. Experiments demonstrate the advantages of phoneme-based models (Whistle) for MCL-ASR, in terms of speech recognition for seen languages, crosslingual performance for unseen languages with different amounts of few-shot data, overcoming catastrophic forgetting, and training efficiency. It is found that when training data is more limited, phoneme supervision can achieve better results compared to subword supervision and self-supervision, thereby providing higher data-efficiency.","2998-4173","","10.1109/TASLPRO.2025.3550683","National Science and Technology Major Project(grant numbers:2023ZD0121401); Guangxi Science and Technology(grant numbers:2022AC16002); National Natural Science Foundation of China(grant numbers:62466055); TasiTech; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10923750","Crosslingual;data-efficient;IPA;multilingual;speech recognition","Phonetics;Multilingual;Speech recognition;Training;Hidden Markov models;Data models;Symbols;Error analysis;Pipelines;Information sharing","","","","66","IEEE","12 Mar 2025","","","IEEE","IEEE Journals"
"SlideSpeech: A Large Scale Slide-Enriched Audio-Visual Corpus","H. Wang; F. Yu; X. Shi; Y. Wang; S. Zhang; M. Li","School of Computer Science, Wuhan University, Wuhan, China; Speech Lab of DAMO Academy, Alibaba Group, China; Speech Lab of DAMO Academy, Alibaba Group, China; Speech Lab of DAMO Academy, Alibaba Group, China; Speech Lab of DAMO Academy, Alibaba Group, China; School of Computer Science, Wuhan University, Wuhan, China","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","11076","11080","Multi-Modal automatic speech recognition (ASR) techniques aim to leverage additional modalities to improve the performance of speech recognition systems. While existing approaches primarily focus on video or contextual information, the utilization of extra supplementary textual information has been overlooked. Recognizing the abundance of online conference videos with slides, which provide rich domain-specific information in the form of text and images, we release SlideSpeech, a large-scale audio-visual corpus enriched with slides. The corpus contains 1,705 videos, 1,000+ hours, with 473 hours of high-quality transcribed speech. Moreover, the corpus contains a significant amount of real-time synchronized slides. In this work, we present the pipeline for constructing the corpus and propose baseline methods for utilizing text information in the visual slide context. Through the application of keyword extraction and contextual ASR methods in the benchmark system, we demonstrate the potential of improving speech recognition performance by incorporating textual information from supplementary video slides.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10448079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448079","audio visual speech recognition;corpus;slides","Visualization;Text recognition;Pipelines;Streaming media;Benchmark testing;Web conferencing;Signal processing","","4","","38","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Application of Automatic Speech Recognition Theory in Improving Pronunciation and Listening Skills for EFL Learners","D. Li; M. Yang; T. Lyu; B. Li; Y. Zhao; S. Qin","International College, Northwestern Polytechnical University, Xi'an, China; School of English Studies, Xi’an International Studies University, Xi'an, China; International College, Northwestern Polytechnical University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China; School of English Studies, Xi’an International Studies University, Xi'an, China; College of Humanities and Foreign Languages, Xi'an University of Science and Technology, Xi'an, China",2024 International Conference on Cyber-Physical Social Intelligence (ICCSI),"18 Dec 2024","2024","","","1","6","This paper explores how Automatic Speech Recognition (ASR) technology, rooted in human speech recognition research, can be leveraged to enhance English as a Foreign Language learners' pronunciation and listening skills. Inspired by insights gained from ASR, this study introduces a novel theory called the Duality of Pronunciation and a new approach to English pronunciation instruction, known as the Parallel Phonetic Transcription Teaching Method. This approach emphasizes a more holistic understanding of the sound system, focusing on the interconnectedness of sounds within words and phrases. By combining traditional pronunciation drills with advanced techniques and investigating the underlying principles of sound change, this method aims to enhance both pronunciation accuracy and listening comprehension.","","979-8-3503-7673-9","10.1109/ICCSI62669.2024.10799420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10799420","Automatic Speech Recognition (ASR);duality of pronunciation;pronunciation and listening skills;Parallel Phonetic Transcription Teaching Method;sound change","Accuracy;Audio systems;Education;Focusing;Phonetics;Social intelligence;Automatic speech recognition","","","","16","IEEE","18 Dec 2024","","","IEEE","IEEE Conferences"
"Speech Disorders Classification in Phonetic Exams with MFCC and DTW","J. Liu; M. Speights; D. Bailey; S. Li; H. Zhou; Y. Luan; T. Xie; C. Seals","CSSE, Auburn University, Auburn, USA; CSD, Northwestern University, Evanston, USA; CMDS, Auburn University, Auburn, USA; CSSE, Auburn University, Auburn, USA; ECE, Auburn University, Auburn, USA; CSSE, Auburn University, Auburn, USA; CSSE, Auburn University, Auburn, USA; CSSE, Auburn University, Auburn, USA",2021 IEEE 7th International Conference on Collaboration and Internet Computing (CIC),"14 Feb 2022","2021","","","35","40","Recognizing disordered speech is a challenge to Automatic Speech Recognition (ASR) systems. This research focuses on classifying disordered speech vs. non-disordered speech through signal processing coupled with machine learning techniques. We have found little evidence of ASR that correctly classifies disordered vs. ordered speech at the level of expert-based classification. This research supports the Automated Phonetic Transcription - Grading Tool (APTgt). APTgt is an online E-Learning system that supports Communications Disorders (CMDS) faculty during linguistic courses and provides reinforcement activities for phonetic transcription with the potential to improve the quality of students' learning efficacy and teachers' pedagogical experience. In addition, APTgt generates interactive practice sessions and exams, automatic grading, and exam analysis. This paper will focus on the classification module to classify disordered speech and non-disordered speech supporting APTgt. We utilize Mel-frequency cepstral coefficients (MFCCs) and dynamic time warping (DTW) to preprocess the audio files and calculate the similarity, and the Support Vector Machine (SVM) algorithm for classification and regression.","","978-1-6654-1625-2","10.1109/CIC52973.2021.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707175","E-Learning;Phonetic Transcription;International Phonetic Alphabet;MFCC;Dynamic Time Warping;Speech Classification;Support Vector Machine","Support vector machines;Training;Heuristic algorithms;Support vector machine classification;Signal processing algorithms;Transforms;Phonetics","","2","","16","IEEE","14 Feb 2022","","","IEEE","IEEE Conferences"
"Controlling Whisper: Universal Acoustic Adversarial Attacks to Control Multi-Task Automatic Speech Recognition Models","V. Raina; M. Gales",University of Cambridge; University of Cambridge,2024 IEEE Spoken Language Technology Workshop (SLT),"16 Jan 2025","2024","","","208","215","Speech enabled foundation models, either in the form of flexible speech recognition based systems or audio-prompted large language models (LLMs), are becoming increasingly popular. One of the interesting aspects of these models is their ability to perform tasks other than automatic speech recognition (ASR) using an appropriate prompt. For example, the OpenAI Whisper model can perform both speech transcription and speech translation. With the development of audio-prompted LLMs there is the potential for even greater control options. In this work we demonstrate that with this greater flexibility the systems can be susceptible to model-control adversarial attacks. Without any access to the model prompt it is possible to modify the behaviour of the system by appropriately changing the audio input. To illustrate this risk, we demonstrate that it is possible to prepend a short universal adversarial acoustic segment to any input speech signal to override the prompt setting of an ASR foundation model. Specifically, we successfully use a universal adversarial acoustic segment to control Whisper to always perform speech translation, despite being set to perform speech transcription. Overall, this work demonstrates a new form of adversarial attack on multi-tasking speech enabled foundation models that needs to be considered prior to the deployment of this form of model.","","979-8-3503-9225-8","10.1109/SLT61566.2024.10832273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832273","ASR;Adversarial Attacks;Control","Presses;Translation;Foundation models;Large language models;Force;Switches;Multitasking;Acoustics;Security;Automatic speech recognition","","","","39","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Non-Autoregressive End-To-End Automatic Speech Recognition Incorporating Downstream Natural Language Processing","M. Omachi; Y. Fujita; S. Watanabe; T. Wang",Yahoo Japan Corporation; Yahoo Japan Corporation; Carnegie Mellon University; Johns Hopkins University,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6772","6776","We propose a fast and accurate end-to-end (E2E) model, which executes automatic speech recognition (ASR) and downstream natural language processing (NLP) simultaneously. The proposed approach predicts a single-aligned sequence of transcriptions and linguistic annotations such as part-of-speech (POS) tags and named entity (NE) tags from speech. We use non-autoregressive (NAR) decoding instead of autoregressive (AR) decoding to reduce execution time since NAR can output multiple tokens in parallel across time. We use the connectionist temporal classification (CTC) model with mask-predict, i.e., Mask-CTC, to predict the single-aligned sequence accurately. Mask-CTC improves performance by joint training of CTC and a conditioned masked language model and refining output tokens with low confidence conditioned on reliable output tokens and audio embeddings. The proposed method jointly performs the ASR and downstream NLP task, i.e., POS or NE tagging, in a NAR manner. Experiments using the Corpus of Spontaneous Japanese and Spoken Language Understanding Resource Package show that the proposed E2E model can predict transcriptions and linguistic annotations with consistently better performance than vanilla CTC using greedy decoding and 15–97x faster than Transformer-based AR model.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746067","Speech recognition;natural language processing;linguistic annotation;end-to-end;non-autoregressive","Annotations;Signal processing algorithms;Predictive models;Linguistics;Tagging;Transformers;Natural language processing","","6","","32","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"The Development of a Thai Telephone Conversational Speech Corpus","S. Thatphithakkul; K. Thangthai; S. Sriphol; V. Chunwijitra","National Science and Technology Development Agency (NSTDA), NECTEC, Pathumthani, Thailand; National Science and Technology Development Agency (NSTDA), NECTEC, Pathumthani, Thailand; National Science and Technology Development Agency (NSTDA), NECTEC, Pathumthani, Thailand; National Science and Technology Development Agency (NSTDA), NECTEC, Pathumthani, Thailand",2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"2 Apr 2024","2023","","","1","6","To be the crucial language resource for acoustic model and language model training of the Automatic Speech Recognition system over telephone channel, a Thai telephone conversational speech corpus is created. This paper described the design and development of a Thai telephone conversational speech corpus. We obtained 37 hours and 28 minutes in 8 kilohertz of recorded real telephone conversational speech data from a private company leading in chemical products. This conversation related to a customers’ interview to finish a company’s satisfaction survey by agent. The speech data contains 26 hours and 3 minutes from 6 agents and 11 hours and 25 minutes from 4,383 customers. A few linguists were asked to transcribe all speech data to Thai text transcription and annotated the characteristics of speech and non-speech by the tags designed by research team. The statistical analysis of the corpus in terms of the number of utterances, tokens, vocabulary and the occurrences of sound loss tokens and English tokens are reported. The total of utterances found in the corpus are 36,122 utterances. There are 309,046 tokens which can be extracted to 5,522 vocabulary size. the percentage of sound loss tokens and English tokens found is 3.73% and 5.45% of all tokens in the corpus respectively. The top 5 words occurrences consist of Thai final particles showed politeness of male and female speakers and the adjective expressed the satisfy feeling. In conclusion, we examine the efficacy of models trained using our corpus in terms of recognition performance. According to the results of the experiment, the models we propose outperform the baseline model.","2472-7695","979-8-3503-4402-8","10.1109/O-COCOSDA60357.2023.10482925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482925","telephone speech corpus;conversational speech;Thai speech corpus for ASR","Training;Surveys;Vocabulary;Dictionaries;Statistical analysis;Databases;Oral communication","","","","27","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"Automatic Alignment of Human Generated Transcripts to Speech Signals","B. Mocanu; R. Tapu","Department of Telecommunications, Faculty of ETTI, University “Politehnica” of Bucharest, Romania; Télécom SudParis, ARTEMIS Department, Institut Polytechnique de Paris, Evry, France",2022 E-Health and Bioengineering Conference (EHB),"2 Jan 2023","2022","","","1","4","This paper introduces a novel, completely automatic, audio-subtitle synchronization algorithm designed to increase the accessibility and comprehension of the hearing-impaired people over the video documents. The major contribution of the paper involves the anchor words matching strategy that can reliably put in correspondence the subtitle document generated by a human transcriber and the automatic speech recognition (ASR) textual file. In addition, we propose a subtitle positioning strategy that automatically determines the optimal location for each phrase on the user screen, with respect to the video semantic content. The experimental evaluation validates the proposed method with average accuracy scores superior to 90%.","2575-5145","978-1-6654-8557-9","10.1109/EHB55594.2022.9991512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991512","subtitle synchronization;anchor words;automatic speech recognition;dynamic subtitle positioning","Semantics;Synchronization;Reliability;Biomedical engineering;Automatic speech recognition","","","","13","IEEE","2 Jan 2023","","","IEEE","IEEE Conferences"
"Detection of Vowel Errors in Children’s Speech using Synthetic Phonetic Transcripts","I. Baumann; D. Wagner; K. Riedhammer; E. Nöth; T. Bocklet","Technische Hochschule Nürnberg Georg Simon Ohm, Germany; Technische Hochschule Nürnberg Georg Simon Ohm, Germany; Technische Hochschule Nürnberg Georg Simon Ohm, Germany; Pattern Recognition Lab, Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany; Technische Hochschule Nürnberg Georg Simon Ohm, Germany",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","8","The analysis of phonological processes is crucial in evaluating speech development disorders in children, but encounters challenges due to limited children audio data. This work focuses on automatic vowel error detection using a two-stage pipeline. The first stage uses a fine-tuned cross-lingual phone recognizer (wav2vec 2.0) to extract phone sequences from audio. The second stage employs a language model (BERT) for classification from a phone sequence, entirely trained on synthetic transcripts, to counteract the very broad range of potential mistakes. We evaluate the system on nonword audio recordings recited by preschool children from a speech development test. The results show that the classifier trained on synthetic data performs well, but its efficacy relies on the quality of the phone recognizer. The best classifier achieves an 94.7% F1 score when evaluated against phonetic ground truths, whereas the F1 score is 76.2% when using automatically recognized phone sequences.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389704","children’s speech;vowel errors;nonwords","Conferences;Pipelines;Phonetics;Audio recording;Speech processing;Synthetic data;Automatic speech recognition","","","","26","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"BIGOS - Benchmark Intended Grouping of Open Speech Corpora for Polish Automatic Speech Recognition","M. Junczyk",Adam Mickiewicz University,2023 18th Conference on Computer Science and Intelligence Systems (FedCSIS),"8 Nov 2023","2023","","","585","590","This paper presents a Benchmark Intended Grouping of Open Speech (BIGOS), a new corpus designed for Polish Automatic Speech Recognition (ASR) systems. This initial version of the benchmark leverages 1,900 audio recordings from 71 distinct speakers, sourced from 10 publicly available speech corpora. Three proprietary ASR systems and five open-source ASR systems were evaluated on a diverse set of recordings and the corresponding original transcriptions. Interestingly, it was found that the performance of the latest open-source models is on par with that of more established commercial services. Furthermore, a significant influence of the model size on system accuracy was observed, as well as a decrease in scenarios involving highly specialized or spontaneous speech. The challenges of using public datasets for ASR evaluation purposes and the limitations based on this inaugural benchmark are critically discussed, along with recommendations for future research. BIGOS corpus and associated tools that facilitate replication and customization of the benchmark are made publicly available.","","978-83-967447-8-4","10.15439/2023F1609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10306084","","Computer science;Benchmark testing;Audio recording;Automatic speech recognition","","","","33","","8 Nov 2023","","","IEEE","IEEE Conferences"
"Joint Speech Recognition and Audio Captioning","C. Narisetty; E. Tsunoo; X. Chang; Y. Kashiwagi; M. Hentschel; S. Watanabe","Carnegie Mellon University, USA; Sony Group Corporation, Japan; Carnegie Mellon University, USA; Sony Group Corporation, Japan; Sony Group Corporation, Japan; Carnegie Mellon University, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","7892","7896","Speech samples recorded in both indoor and outdoor environments are often contaminated with secondary audio sources. Most end-to-end monaural speech recognition systems either remove these background sounds using speech enhancement or train noise-robust models. For better model interpretability and holistic understanding, we aim to bring together the growing field of automated audio captioning (AAC) and the thoroughly studied automatic speech recognition (ASR). The goal of AAC is to generate natural language descriptions of contents in audio samples. We propose several approaches for end-to-end joint modeling of ASR and AAC tasks and demonstrate their advantages over traditional approaches, which model these tasks independently. A major hurdle in evaluating our proposed approach is the lack of labeled audio datasets with both speech transcriptions and audio captions. Therefore we also create a multi-task dataset by mixing the clean speech Wall Street Journal corpus with multiple levels of background noises chosen from the AudioCaps dataset. We also perform extensive experimental evaluation and show improvements of our proposed methods as compared to existing state-of-the-art ASR and AAC methods.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746601","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746601","ASR;AAC;speech recognition;audio captioning;joint modeling","Conferences;Natural languages;Speech enhancement;Signal processing;Multitasking;Transformers;Noise robustness","","5","","28","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Context-aware RNNLM Rescoring for Conversational Speech Recognition","K. Wei; P. Guo; H. Lv; Z. Tu; L. Xie","Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xian, China; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xian, China; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xian, China; Zhuiyi Technology, Shenzhen, China; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xian, China",2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP),"1 Mar 2021","2021","","","1","5","Conversational speech recognition is regarded as a challenging task due to its free-style speaking and long-term contextual dependencies. Prior work has explored the modeling of long-range context through RNNLM rescoring with improved performance. To further take advantage of the persisted nature during a conversation, such as topics or speaker turn, we extend the rescoring procedure to a new context-aware manner. For RNNLM training, we capture the contextual dependencies by concatenating adjacent sentences with various tag words, such as speaker or intention information. For lattice rescoring, the lattice of adjacent sentences are also connected with the first-pass decoded result by tag words. Besides, we also adopt a selective concatenation strategy based on tf-idf, making the best use of contextual similarity to improve transcription performance. Results on four different conversation test sets show that our approach yields up to 13.1% and 6% relative char-error-rate (CER) reduction compared with 1st-pass decoding and common lattice-rescoring, respectively.","","978-1-7281-6994-1","10.1109/ISCSLP49672.2021.9362109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9362109","conversational speech recognition;recurrent neural network language model;lattice-rescoring","Training;Recurrent neural networks;Lattices;Speech recognition;Indexes;Task analysis;Context modeling","","3","","29","IEEE","1 Mar 2021","","","IEEE","IEEE Conferences"
"Audio-Visual Broadcast Transcription System in the Era of Covid-19","J. Chaloupka; K. Paleček","Institute of Information Technology and Electronics Faculty of Mechatronics, Informatics and Interdisciplinary Studies, Technical University of Liberec, Liberec, Czech Republic; Institute of Information Technology and Electronics Faculty of Mechatronics, Informatics and Interdisciplinary Studies, Technical University of Liberec, Liberec, Czech Republic",2023 46th International Conference on Telecommunications and Signal Processing (TSP),"4 Aug 2023","2023","","","276","279","This paper deals with the methods and algorithms for face (mask) detection and recognition in the system for automatic audio-visual TV broadcast transcription. In the era of Covid-19, traditional methods for face detection and recognition were failing as large numbers of people wearing face masks began to appear in television video recordings. Therefore, we focused our work on finding a sufficiently robust detector and classifier for face (masked/unmasked) detection and recognition. As part of this work, a face mask detector and a face recognizer were trained.","2768-3311","979-8-3503-0396-4","10.1109/TSP59544.2023.10197747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197747","Face mask detection;face recognition;deep learning;audio-visual broadcast transcription system","COVID-19;TV;Image recognition;Face recognition;Signal processing algorithms;Detectors;Signal processing","","","","25","IEEE","4 Aug 2023","","","IEEE","IEEE Conferences"
"Towards Measuring Fairness in Speech Recognition: Casual Conversations Dataset Transcriptions","C. Liu; M. Picheny; L. Sarı; P. Chitkara; A. Xiao; X. Zhang; M. Chou; A. Alvarado; C. Hazirbas; Y. Saraf","Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA; Meta AI, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6162","6166","The problem of machine learning systems demonstrating bias towards specific groups of individuals has been studied extensively, particularly in the Facial Recognition area, but much less so in Automatic Speech Recognition (ASR). This paper presents initial Speech Recognition results on “Casual Conversations” – a publicly released 846 hour corpus designed to help researchers evaluate their computer vision and audio models for accuracy across a diverse set of metadata, including age, gender, and skin tone. The entire corpus has been manually transcribed, allowing for detailed ASR evaluations across these metadata. Multiple ASR models are evaluated, including models trained on LibriSpeech, 14,000 hour transcribed, and over 2 million hour untranscribed social media videos. Significant differences in word error rate across gender and skin tone are observed at times for all models. We are releasing human transcripts from the Casual Conversations dataset to encourage the community to develop a variety of techniques to reduce these statistical biases.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747501","fairness;speech recognition;gender;age;skin tones","Visualization;Social networking (online);Error analysis;Computational modeling;Distributed databases;Training data;Speech recognition","","12","","35","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Contextual Learning for Missing Speech Automatic Speech Recognition","Y. Hong; M. Kim; W. -J. Chung; H. -G. Kang","dept. of electrical and electronic engineering, Yonsei University, Seoul, South Korea; dept. of electrical and electronic engineering, Yonsei University, Seoul, South Korea; dept. of electrical and electronic engineering, Yonsei University, Seoul, South Korea; dept. of electrical and electronic engineering, Yonsei University, Seoul, South Korea","2024 International Conference on Electronics, Information, and Communication (ICEIC)","19 Mar 2024","2024","","","1","3","In this paper, we present an automatic speech recognition (ASR) system that is capable of decoding complete transcriptions from speech even in cases where there are missing segments in the audio. To predict complete transcriptions from speech that may have missing segments, we utilize a contextual learning approach inspired by recent language model training approaches, in which our model leverages surrounding speech segments as cues for the prediction. Our model consists of two modules: a contextual feature extractor designed with the structure of wav2vec 2.0, and a projection layer. We further explore various masking lengths for model training so as to optimally benefit the ASR system without compromising its performance. Our proposed methodology demonstrates high-quality ASR performance on missing speech segments of various lengths, ranging from a word error rate (WER) of 4.7% on 0.25 seconds segments to 18.5% on 1 second segments.","2767-7699","979-8-3503-7188-8","10.1109/ICEIC61013.2024.10457193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10457193","wav2vec 2.0;Automatic Speech Recognition;Language Model","Training;Error analysis;Predictive models;Feature extraction;Distance measurement;Decoding;Reliability","","","","18","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"Indexing and Segmentation of Video Contents: A Review","G. S. Mishra; A. Raj; A. Kumar; A. K. Kasaudhan; P. Kumar Mishra; T. Maini","Department of Computer Science & Engineering, Sharda University, Greater Noida, India; Department of Computer Science & Engineering, Sharda University, Greater Noida, India; Department of Computer Science & Engineering, Sharda University, Greater Noida, India; Department of Computer Science & Engineering, Sharda University, Greater Noida, India; Department of Computer Science & Engineering, Sharda University, Greater Noida, India; Department of Computer Science & Engineering, Sharda University, Greater Noida, India",2023 4th International Conference for Emerging Technology (INCET),"10 Jul 2023","2023","","","1","9","Increasing video content on Internet day by day has increased the challenge of searching and finding the relevant topic for effective navigation inside video by users mostly in very long lecture video. One such way is the segmentation of specific video to get features like indexing, searching etc. In this paper we are going to discuss for indexing and segmentation inside a video. The model is based upon the extraction of text from the speech and uses them to segmentation and index the specific text inside video.","","979-8-3503-3575-0","10.1109/INCET57972.2023.10170589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10170589","Video Retrieval;Automated speech recognition (ASR);and optical character recognition (OCR);Speech recognition","Optical filters;Navigation;Image color analysis;Optical character recognition;Speech recognition;Streaming media;Feature extraction","","","","29","IEEE","10 Jul 2023","","","IEEE","IEEE Conferences"
"Speech Recognition Paradigms: A Comparative Evaluation of SpeechBrain, Whisper and Wav2Vec2 Models","D. Reddy Yerramreddy; J. Marasani; P. S. Venkata Gowtham; G. Harshit; Anjali","Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Computer Science and Engineering, Amrita School Of Computing, Amrita Vishwa Vidyapeetham, Amritapuri, India; Dept. of Electronics & Communication Engineering, School of Engineering, Amrita Vishwa Vidyapeetham, Amritapuri, India",2024 IEEE 9th International Conference for Convergence in Technology (I2CT),"10 Jun 2024","2024","","","1","6","Speech recognition plays a pivotal role in the realm of natural language processing that deals in converting the language into the written text, providing human-computer interaction and enables us to use it widely for applications starting with voice assistants and delving upto the transcription services. Due to the complexity present in performing the task of speech recognition has led to the development of various models to enhance accuracy and efficiency. Our project mainly delves into three prominent speech recognition models that are Whisper, Wav2Vec2 and Speechbrain each of them representing distinct approaches of transcribing spoken language. The significance of the models used lies in their potential to perform better for realtime applications by improving the accuracy and reliability of speech recognition. To evaluate the best model that performs effectively, an array of metrics are used including levenshtein distance and it’s similarity percentage, jaccard similarity along with semantic similarity that provides an additional layer of evaluation describing the model’s understanding of contextual meaning in spoken language. Out of the three models, Speech Brain model outperformed all the other models through the calculation of Word Error Rate (WER), Character Error rate (CER) and BLEU score. The results have shown the model’s efficiency in converting spoken language(audio files) into precise and contextually relevant text. The results shows that these models contribute to the field of speech recognition highlighting the strengths of each approach and among them considering speechbrain as an ideal solution for accurate and meaningful transcription.","","979-8-3503-9447-4","10.1109/I2CT61223.2024.10544133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10544133","Speech Recognition;Whsiper;Wav2Vec2;SpeechBrain;Jaccard Similarity;Levenshtein Distance","Measurement;Human computer interaction;Adaptation models;Technological innovation;Error analysis;Semantics;Speech recognition","","","","15","IEEE","10 Jun 2024","","","IEEE","IEEE Conferences"
"Combining TF-GridNet And Mixture Encoder For Continuous Speech Separation For Meeting Transcription","P. Vieting; S. Berger; T. v. Neumann; C. Boeddeker; R. Schlüter; R. Haeb-Umbach","Machine Learning and Human Language Technology Group, RWTH Aachen University, Germany; Machine Learning and Human Language Technology Group, RWTH Aachen University, Germany; Paderborn University, Germany; Paderborn University, Germany; Machine Learning and Human Language Technology Group, RWTH Aachen University, Germany; Paderborn University, Germany",2024 IEEE Spoken Language Technology Workshop (SLT),"16 Jan 2025","2024","","","155","162","Many real-life applications of automatic speech recognition (ASR) require processing of overlapped speech. A common method involves first separating the speech into overlap-free streams on which ASR is performed. Recently, TF-GridNet has shown impressive performance in speech separation in real reverberant conditions. Furthermore, a mixture encoder was proposed that leverages the mixed speech to mitigate the effect of separation artifacts. In this work, we extended the mixture encoder from a static two-speaker scenario to a natural meeting context featuring an arbitrary number of speakers and varying degrees of overlap. We further demonstrate its limits by the integration with separators of varying strength including TF-GridNet. Our experiments result in a new state-of-the-art performance on LibriCSS using a single microphone. They show that TF-GridNet largely closes the gap between previous methods and oracle separation independent of mixture encoding. We further investigate the remaining potential for improvement.","","979-8-3503-9225-8","10.1109/SLT61566.2024.10832307","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832307","speech separation;speech recognition;meeting transcription","Particle separators;Conferences;Encoding;Data models;Reverberation;Speech processing;Streams;Microphones;Automatic speech recognition","","","","32","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Using Character-Level Sequence-to-Sequence Model for Word Level Text Generation to Enhance Arabic Speech Recognition","M. A. Azim; W. Hussein; N. L. Badr","Information Systems Department, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt; Information Systems Department, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt; Information Systems Department, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt",IEEE Access,"31 Aug 2023","2023","11","","91173","91183","Owing to the linguistic richness of the Arabic language, which contains more than 6000 roots, building a reliable Arabic language model for Arabic speech recognition systems faces many challenges. This paper introduces a language model free Arabic automatic speech recognition system for Modern Standard Arabic based on an end-to-end-based Deep Speech architecture developed by Mozilla. The proposed model uses a character-level sequence-to-sequence model to map the character alignment produced by the recognizer model onto the corresponding words. The developed system outperformed recent studies on single-speaker and multi-speaker Arabic speech recognition using two different state-of-the-art datasets. The first was the Arabic Multi-Genre Broadcast (MGB2) corpus with 1200 h of audio data from multiple speakers. The system achieved a new milestone in the MGB2 challenge with a word error rate (WER) of 3.2, outperforming related work using the same corpus with a word error reduction of 17%. An additional experiment with a 7-hour Saudi Accent Single Speaker Corpus (SASSC) was used to build an additional model for single male speaker-based Arabic speech recognition using the same proposed network architecture. The single-speaker model outperformed related experiments with a WER of 4.25 with a relative improvement of 33.8%.","2169-3536","","10.1109/ACCESS.2023.3302257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10209196","Arabic speech recognition;CNN;CTC;RNN","Hidden Markov models;Speech recognition;Data models;Standards;Acoustics;Error analysis;Training","","2","","36","CCBYNCND","4 Aug 2023","","","IEEE","IEEE Journals"
"Multi-Turn RNN-T for Streaming Recognition of Multi-Party Speech","I. Sklyar; A. Piunova; X. Zheng; Y. Liu",Amazon Alexa; Amazon Alexa; University of Cambridge; Amazon Alexa,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","8402","8406","Automatic speech recognition (ASR) of single channel far-field recordings with an unknown number of speakers is traditionally tackled by cascaded modules. Recent research shows that end-to-end (E2E) multi-speaker ASR models can achieve superior recognition accuracy compared to modular systems. However, these models do not ensure real-time applicability due to their dependency on full audio context. This work takes real-time applicability as the first priority in model design and addresses a few challenges in previous work on multi-speaker recurrent neural network transducer (MS-RNN-T). First, we introduce on-the-fly overlapping speech simulation during training, yielding 14% relative word error rate (WER) improvement on LibriSpeechMix test set. Second, we propose a novel multi-turn RNN-T (MT-RNN-T) model with an overlap-based target arrangement strategy that generalizes to an arbitrary number of speakers without changes in the model architecture. We investigate the impact of the maximum number of speakers seen during training on MT-RNN-T performance on LibriCSS test set, and report 28% relative WER improvement over the two-speaker MS-RNN-T. Third, we experiment with a rich transcription strategy for joint recognition and segmentation of multi-party speech. Through an in-depth analysis, we discuss potential pitfalls of the proposed system as well as promising future research directions.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746074","streaming multi-speaker speech recognition;overlapped speech;recurrent neural network transducer;multi-turn","Training;Heart;Transducers;Recurrent neural networks;Error analysis;Speech recognition;Signal processing","","10","","35","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Decoupling Recognition and Transcription in Mandarin ASR","J. Yuan; X. Cai; D. Gao; R. Zheng; L. Huang; K. Church",Baidu Research USA; Baidu Research USA; Johns Hopkins University; Baidu Research USA; Baidu Research USA; Baidu Research USA,2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"3 Feb 2022","2021","","","1019","1025","Much of the recent literature on automatic speech recognition (ASR) is taking an end-to-end approach. Unlike English where the writing system is closely related to sound, Chinese characters (Hanzi) represent meaning, not sound. We propose factoring audio → Hanzi into two sub-tasks: (1) audio → Pinyin and (2) Pinyin → Hanzi, where Pinyin is a system of phonetic transcription of standard Chinese. Factoring the audio → Hanzi task in this way achieves 3.9% CER (character error rate) on the Aishell-1 corpus, the best result reported on this dataset so far.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9688053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688053","ASR;Wav2vec2.0;KenLM;Transformer","Error analysis;Conferences;Writing;Phonetics;Character recognition;Task analysis;Standards","","9","","53","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Hallucination of Speech Recognition Errors With Sequence to Sequence Learning","P. Serai; V. Sunder; E. Fosler-Lussier","Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","1 Mar 2022","2022","30","","890","900","Prior work in this domain has focused on modeling errors at the phonetic level, while using a lexicon to convert the phones to words, usually accompanied by an FST Language model. We present novel end-to-end models to directly predict hallucinated ASR word sequence outputs, conditioning on an input word sequence as well as a corresponding phoneme sequence. This improves prior published results for recall of errors from an in-domain ASR system’s transcription of unseen data, as well as an out-of-domain ASR system’s transcriptions of audio from an unrelated task, while additionally exploring an in-between scenario when limited characterization data from the test ASR system is obtainable. To verify the extrinsic validity of the method, we also use our hallucinated ASR errors to augment training for a spoken question classifier, finding that they enable robustness to real ASR errors in a downstream task, when scarce or even zero task-specific audio was available at train-time.","2329-9304","","10.1109/TASLP.2022.3145313","National Science Foundation(grant numbers:1618336); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693404","Error prediction;hallucinated asr errors;low resource;sequence to sequence neural networks;speech recognition","Predictive models;Decoding;Phonetics;Speech recognition;Task analysis;Context modeling;Text recognition","","5","","32","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Towards a Database For Detection of Multiple Speech Disfluencies in Indian English","S. Garg; U. Mehrotra; G. Krishna; A. K. Vuppala","Speech Processing Lab LTRC, KCIS, IIIT Hyderabad, Hyderabad, India; Speech Processing Lab LTRC, KCIS, IIIT Hyderabad, Hyderabad, India; Speech Processing Lab LTRC, KCIS, IIIT Hyderabad, Hyderabad, India; Speech Processing Lab LTRC, KCIS, IIIT Hyderabad, Hyderabad, India",2021 National Conference on Communications (NCC),"13 Sep 2021","2021","","","1","6","The detection and removal of disfluencies from speech is an important task since the presence of disfluencies can adversely affect the performance of speech-based applications such as Automatic Speech Recognition (ASR) systems and speech-to-speech translation systems. From the perspective of Indian languages, there is a lack of studies pertaining to speech disfluencies, their types and frequency of occurrence. Also, the resources available to perform such studies in an Indian context are limited. Through this paper, we attempt to address this issue by introducing the IIITH-Indian English Disfluency (IIITH-IED) Dataset. This dataset consists of 10-hours of lecture mode speech in Indian English. Five types of disfluencies - filled pause, prolongation, word repetition, part-word repetition and phrase repetition were identified in the speech signal and annotated in the corresponding transcription to prepare this dataset. The IIITH-IED dataset was then used to develop frame-level automatic disfluency detection systems. Two sets of features were extracted from the speech signal and then used to train classifiers for the task of disfluency detection. Amongst all the systems employed, Random Forest with MFCC features resulted in the highest average accuracy of 89.61% and F1-score of 0.89.","","978-1-6654-4177-3","10.1109/NCC52529.2021.9530043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530043","speech disfluencies;acoustic features;binary classification;recall","Databases;Filter banks;Feature extraction;Mel frequency cepstral coefficient;Task analysis;Automatic speech recognition","","5","","35","IEEE","13 Sep 2021","","","IEEE","IEEE Conferences"
"Advancing Search Engine Functionality Through AI-Powered Audio Transcription","S. Poply; A. K. Sharma; P. S. Rathore","Department of IoT and Intelligent, SystemsManipal University, Jaipur, India; Department of Computer and Communcation Engineering, Manipal University, Jaipur, India; Department of Computer and Communication Engineering, Manipal University Jaipur, Jaipur, India",2024 International Conference on Advances in Computing Research on Science Engineering and Technology (ACROSET),"12 Nov 2024","2024","","","1","4","Efficiently navigating through extensive datasets is fundamental for enhancing productivity, facilitating informed decision-making, and fostering innovation. To achieve this, the integration of state-of-the-art technologies such as artificial intelligence (AI) and advanced search engines has become imperative in maximizing the utilization of available information. This paper presents a novel approach to address the challenges associated with traditional search methodologies. Contrary to conventional methods focusing solely on optimizing speed and indexes, our proposed solution leverages transcript generation for video/audio content distributed across the internet. By integrating transcripts into the search process, our approach aims to significantly enhance search capabilities, allowing for more precise and efficient retrieval of relevant multimedia content. Through empirical evaluation and comparative analysis, we demonstrate the efficacy and superiority of our method in improving search efficiency and accuracy. This research contributes to advancing the field of information retrieval by introducing a smart solution tailored to meet the evolving demands of contemporary digital ecosystems.","","979-8-3503-8880-0","10.1109/ACROSET62108.2024.10743633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10743633","AI;search engine;NLP;automatic speech recognition","Productivity;Technological innovation;Navigation;Ecosystems;Focusing;Speech recognition;Search engines;Streaming media;Indexes;Artificial intelligence","","","","8","IEEE","12 Nov 2024","","","IEEE","IEEE Conferences"
"SQ-Whisper: Speaker-Querying Based Whisper Model for Target-Speaker ASR","P. Guo; X. Chang; H. Lv; S. Watanabe; L. Xie","Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China; Carnegie Mellon University, Pittsburgh, PA, USA; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China; Carnegie Mellon University, Pittsburgh, PA, USA; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China","IEEE Transactions on Audio, Speech and Language Processing","27 Jan 2025","2025","33","","175","185","Benefiting from massive and diverse data sources, speech foundation models exhibit strong generalization and knowledge transfer capabilities to a wide range of downstream tasks. However, a limitation arises from their exclusive handling of single-speaker speech input, making them ineffective in recognizing multi-speaker overlapped speech, a common occurrence in real-world scenarios. In this study, we delve into the adaptation of speech foundation models to eliminate interfering speakers from overlapping speech and perform target-speaker automatic speech recognition (TS-ASR). Initially, we utilize the Whisper model as the foundation for adaptation and conduct a thorough comparison of its integration with existing target-speaker adaptation techniques. We then propose an innovative model termed Speaker-Querying Whisper (SQ-Whisper), which employs a set number of trainable queries to capture speaker prompts from overlapping speech based on target-speaker enrollment. These prompts serve to steer the model in extracting speaker-specific features and accurately recognizing target-speaker transcriptions. Experimental results demonstrate that our approach effectively adapts the pre-trained speech foundation model to TS-ASR. Compared with the robust TS-HuBERT model, the proposed SQ-Whisper significantly improves performance, yielding up to 15% and 10% relative reductions in word error rates (WERs) on the Libri2Mix and WSJ0-2Mix datasets, respectively. With data augmentation, we establish new state-of-the-art WERs of 14.6% on the Libri2Mix Test set and 4.4% on the WSJ0-2Mix Test set. Furthermore, we evaluate our model on the real-world AMI meeting dataset, which shows consistent improvement over other adaptation methods.","2998-4173","","10.1109/TASLP.2024.3513835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10786377","Speech foundation model;Whisper;model fine-tuning;target-speaker ASR","Adaptation models;Feature extraction;Data models;Transformers;Training;Speech processing;Vectors;Predictive models;Decoding;Target recognition","","","","57","IEEE","9 Dec 2024","","","IEEE","IEEE Journals"
"Weakly Supervised Phonological Features for Pathological Speech Analysis","J. Thienpondt; G. Vanderreydt; A. Hammami; K. Demuynck","Department of Electronics and Information Systems, IDLab, Ghent University - Imec, Ghent, Belgium; Department of Electronics and Information Systems, IDLab, Ghent University - Imec, Ghent, Belgium; Department of Electronics and Information Systems, IDLab, Ghent University - Imec, Ghent, Belgium; Department of Electronics and Information Systems, IDLab, Ghent University - Imec, Ghent, Belgium","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Paralinguistic properties of speech are essential in analyzing and choosing optimal treatment options for patients with speech disorders. However, automatic modeling of these characteristics is difficult due to the lack of labeled speech datasets describing paralinguistic properties, especially at the frame-level. In this paper, we propose a weakly supervised training method which exploits the known acoustic properties of phonemes by training an ASR model with an interpretable frame-level phonological feature bottleneck layer. Subsequently, we assess the viability of these phonological features in speech pathology analysis by developing corresponding models for intelligibility prediction and speech pathology classification. Models using our proposed phonological features perform similar to other state-of-the-art acoustic features on both tasks with a classification accuracy of 75% and a 8.43 RMSE on speech intelligibility prediction. In contrast to others, our phonological features are text-independent and highly interpretable, providing potentially useful insights for speech therapists.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888038","phonological features;intelligibility;pathology","Training;Pathology;Analytical models;Speech analysis;Accuracy;Predictive models;Signal processing;Acoustics;Speech processing","","","","21","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"The Volcspeech System for the ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription Challenge","C. Shen; Y. Liu; W. Fan; B. Wang; S. Wen; Y. Tian; J. Zhang; J. Yang; Z. Ma",Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab; Bytedance AI Lab,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","9176","9180","This paper describes our submission to ICASSP 2022 Multi-channel Multi-party Meeting Transcription (M2MeT) Challenge. For Track 1, we propose several approaches to make the clustering-based speaker diarization system enable to handle overlapped speech. Front-end dereverberation and the direction-of-arrival (DOA) estimation are used to improve the accuracy of speaker diarization. Multi-channel combination and overlap detection are applied to reduce the missed speaker error. A modified DOVER-Lap is also proposed to fuse the results from different systems. We achieve the final DER of 5.79% on the Eval set and 7.23% on the Test set, which ranks 4th in the diarization challenge. For Track 2, we develop our system using the Conformer model in a joint CTC-attention architecture. Serialized output training (SOT) is adopted to multi-speaker overlapped speech recognition. We propose a neural front-end module to model multi-channel audio and train the model end-to-end. Various data augmentation methods are utilized to mitigate over-fitting in the multi-channel multi-speaker E2E system. Transformer language model fusion is developed to achieve better performance. The final CER is 19.2% on the Eval set and 20.8% on the Test set, which ranks 2nd in the ASR challenge.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747381","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747381","M2MeT;AliMeeting;speaker diarization;multi-channel multi-speaker speech recognition;data augmentation","Training;Direction-of-arrival estimation;Fuses;Conferences;Estimation;Speech recognition;Signal processing","","5","","27","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Khmer Speech Translation Corpus of the Extraordinary Chambers in the Courts of Cambodia (ECCC)","K. Soky; M. Mimura; T. Kawahara; S. Li; C. Ding; C. Chu; S. Sam","Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; National Institute of Information and Communications Technology (NICT), Kyoto, Japan; Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, Japan; Cambodia Academy of Digital Technology (CADT), Phnom Penh, Cambodia",2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"3 Jan 2022","2021","","","122","127","Speech translation (ST) is a subject of rapidly increasing interest in the area of speech processing research. This interest is apparent from the increasing tools and corpora for this task. However, the lack of sufficient datasets is still the biggest challenge for under-resourced languages. Specifically, ST requires a large corpus of parallel speech, transcription, and translation text. In this work, we construct a large corpus of the Extraordinary Chambers in the Courts of Cambodia (ECCC), including simultaneous translation from Khmer into English and French. We also address the problem of sentence segmentation of Khmer by conducting a bilingual sentence alignment from English to Khmer with a monotonic assumption. This corpus has approximately 155 hours of speech in length and 1.7M words of text. We also report the baseline results of automatic speech recognition (ASR), machine translation, and ST systems, which show reasonable performance.","2472-7695","978-1-6654-0870-7","10.1109/O-COCOSDA202152914.2021.9660421","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660421","Khmer language;low-resource language;spoken language translation corpus;court dataset","TV;Databases;Machine translation;Speech processing;Task analysis;Automatic speech recognition","","4","","24","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Development of a Bangla Speech to Text Conversion System Using Deep Learning","S. Saha; Asaduzzaman","Department of Computer Science & Engineering, Chittagong University of Engineering & Technology (CUET), Chattogram, Bangladesh; Department of Computer Science & Engineering, Chittagong University of Engineering & Technology (CUET), Chattogram, Bangladesh","2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","18 Oct 2021","2021","","","1","7","Bangla Speech-To-Text (STT) conversion is a technology that provides a means of converting spoken Bangla language to a written Bangla text form. The standard of speech recognition in different languages is rising step by step however Bangla speech recognition has drawn exceptionally little attention. Building up an STT framework is a bulky procedure and it requires a few stages. Deep neural network-based architecture replaces the stages with neural network components which makes the task simpler and removes the dependency on hand-engineered rules. CNN-RNN networks with CTC criterion are utilized in this undertaking to construct a Bangla STT system that generates text from speech. The architecture is trained solely on speech samples and text transcripts. We used 215.53 hours of speech data set for training, which includes a wide variety of speech samples collected by people of different ages and genders. This paper shows a comparison between genuine text transcripts with produced text transcripts for a similar sound example. Comparison of results between implemented architecture and already existing Bangla STT has also been presented in this paper. The paper is concluded with a discussion about the word error rate and implementation challenges. The key contribution of this paper is to develop a gender and speaker-independent continuous speech-to-text conversion system for the Bangla language using deep learning.","","978-1-6654-4923-6","10.1109/ICIEVicIVPR52578.2021.9564209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564209","Speech;Text;Recurrent Layers;Convolution Layers;Connectionist Temporal Classification;Word Error Rate;Bangla Language","Training;Deep learning;Neural networks;Computer architecture;Speech recognition;Linguistics;Convolutional neural networks","","1","","24","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Speech Recognition For Analysis of Police Radio Communication","T. Srivastava; J. -C. Chou; P. Shroff; K. Livescu; C. Graziul","University of Chicago, Chicago, IL; Toyota Technological Institute at Chicago, Chicago, IL; University of Chicago, Chicago, IL; Toyota Technological Institute at Chicago, Chicago, IL; University of Chicago, Chicago, IL",2024 IEEE Spoken Language Technology Workshop (SLT),"16 Jan 2025","2024","","","906","912","Police departments around the world use two-way radio for coordination. These broadcast police communications (BPC) are a unique source of information about everyday police activity and emergency response. Yet BPC are not transcribed, and their naturalistic audio properties make automatic transcription challenging. We collect a corpus of roughly 62,000 manually transcribed radio transmissions ($\sim 46$ hours of audio) to evaluate the feasibility of automatic speech recognition (ASR) using modern recognition models. We evaluate the performance of off-the-shelf spee ch recognizers, models fine-tuned on BPC data, and customized end-to-end models. We find that both human and machine transcription is challenging in this domain. Large off-the-shelf ASR models perform poorly, but fine-tuned models can reach the approximate range of human performance. Our work suggests directions for future work, including analysis of short utterances and potential miscommunication in police radio interactions. We make our corpus and data annotation pipeline available to other researchers, to enable further research on recognition and analysis of police communication.","","979-8-3503-9225-8","10.1109/SLT61566.2024.10832157","National Institutes of Health; Advanced Scientific Computing Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832157","speech recognition;police radio communication;naturalistic audio","Law enforcement;Annotations;Pipelines;Emergency services;Data models;Radio communication;Automatic speech recognition","","1","","39","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Improving Automatic Forced Alignment for Phoneme Segmentation in Quranic Recitation","A. M. A. Alqadasi; A. M. Zeki; M. S. Sunar; M. S. B. H. Salam; R. Abdulghafor; N. A. Khaled","Al-Saeed Faculty of Engineering and Information Technology, Taiz University, Taiz, Yemen; Faculty of Information and Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, Johor Bahru, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, Johor Bahru, Malaysia; Faculty of Information and Communication Technology, International Islamic University Malaysia, Kuala Lumpur, Malaysia; AbdulHamid A. AbuSulayman Kulliyyah of Islamic Revealed Knowledge and Human Sciences, International Islamic University Malaysia, Kuala Lumpur, Malaysia",IEEE Access,"3 Jan 2024","2024","12","","229","244","Segmentation plays a crucial role in speech processing applications, where high accuracy is essential. The quest for improved accuracy in automatic segmentation, particularly in the context of the Arabic language, has garnered substantial attention. However, the differences between Qur’an recitation and normal Arabic speech, especially with regard to intonation rules affecting the lengthening of long vowels, pose challenges in segmentation especially for Qur’an recitation. This research endeavors to address these challenges by delving into the domain of automatic segmentation for Qur’an recitation recognition. The proposed scheme employs a hidden Markov models (HMMs) forced alignment algorithm. To enhance the precision of segmentation, several refinements have been introduced, with a primary emphasis on the phonetic model of the Qur’an and Tajweed, particularly the intricate rules governing elongation. These enhancements encompass the adaptation of an acoustic model tailored for Qur’anic recitation as preprocessing and culminate in the development of an algorithm aimed at refining forced alignment based on the phonetic nuances of the Qur’an. These enhancements are seamlessly integrated as post-processing components for the classic HMM-based forced alignment. The research utilizes a comprehensive database featuring recordings from 100 renowned Qur’an reciters, encompassing the recitation of 21 Qur’anic verses (Ayat). Additionally, 30 reciters were asked to record the same verses, incorporating various recitation speed patterns. To facilitate the evaluation process, a Random sample of the Qur’anic database was manually segmented, comprised 21 Ayats, totaling 19,800 words, with 89 unique words (14 verses x 3 recitation levels: fast, slow and normal x 6 readers). The outcomes of this study manifest notable advancements in the alignment of long vowels within Qur’an recitation, all while maintaining the precise alignment of vowels and consonants. Objective comparisons between the proposed automatic methods and manual segmentation were conducted to ascertain the superior approach. The findings affirm that the classic forced alignment method produces satisfactory outcomes when employed on verses lacking long vowels. However, its performance diminishes when confronted with verses containing long vowels. Therefore, the test samples were categorized into three groups based on the presence of long vowels, resulting in a Correct Classification Rate (CCR) that ranged from 6% to 57%, contingent on whether the verse includes long vowels or not. The average CCR across all test samples was 23%. In contrast, the proposed algorithm significantly enhances audio segmentation. It achieved CCR values ranging from 16% to 70% within the same database categories, with an average CCR of 45% across all test samples. This marks a notable advancement of 22% in segmented speech accuracy, particularly within a 30 ms tolerance, for verses containing long vowels.","2169-3536","","10.1109/ACCESS.2023.3345843","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10371319","Phoneme alignment;forced alignment;phoneme segmentation;Arabic phoneme segmentation;phoneme duration;phoneme recognition;recitation recognition;Tajweed recognition","Hidden Markov models;Speech recognition;Phonetics;Speech processing;Manuals;Training;Task analysis","","1","","59","CCBYNCND","22 Dec 2023","","","IEEE","IEEE Journals"
"End-to-End Speech Recognition for Low Resource Language Sanskrit using Self-Supervised Learning","S. S. Holla; T. N. M. Kumar; J. R. Hiretanad; K. T. Deepak; A. V. Narasimhadhan","Department of Electronics and Communication Engineering, National Institute of Technology, Surathkal, Karnataka; Department of Electronics and Communication Engineering, National Institute of Technology, Surathkal, Karnataka; Department of Electronics and Communication Engineering, Indian Institute of Information Technology, Dharwad, Karnataka; Department of Electronics and Communication Engineering, Indian Institute of Information Technology, Dharwad, Karnataka; Department of Electronics and Communication Engineering, National Institute of Technology, Surathkal, Karnataka",2022 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET),"9 May 2022","2022","","","148","152","We are presenting the work on building a speaker independent, continuous speech recognition system for Samskruta (also called Sanskrit) using self-supervised learning. We have used a Pre-trained model from the Vakyansh team where the model is trained using 10,000 Hrs of data with 23 Indic languages and Fine-tuned it using a data-set containing nearly 78 Hrs of Samskruta audio along with their transcription taken from Vaksancaya - Sanskrit Speech Corpus from IIT Bombay. Acoustic representations are learned in an end-to-end deep learning approach using the wav2vec2.0 architecture from Fairseq. On top of this acoustic model, a language model is used to increase the overall performance. Our system provides a word error rate (WER) of 5.1 % on test data and 2.4% on train data. Meanwhile we built a graphical user interface in the form of a web page using the Flask framework, which provides an interactive platform for the user to record audio and see the transcription in real-time. To the best of our knowledge, our approach using self-supervised learning, gives better performance compared to the state of the art methods.","","978-1-6654-9648-3","10.1109/WiSPNET54241.2022.9767118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767118","Samskruta ASR;Self-Supervised Learning;WER;Pretraining;Finetuning","Wireless communication;Deep learning;Error analysis;Web pages;Speech recognition;Signal processing;Data models","","4","","15","IEEE","9 May 2022","","","IEEE","IEEE Conferences"
"Venomave: Targeted Poisoning Against Speech Recognition","H. Aghakhani; L. Schönherr; T. Eisenhofer; D. Kolossa; T. Holz; C. Kruegel; G. Vigna","University of California, Santa Barbara; CISPA Helmholtz Center for Information Security; Ruhr University Bochum; Technische Universität Berlin; CISPA Helmholtz Center for Information Security; University of California, Santa Barbara; University of California, Santa Barbara",2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),"1 Jun 2023","2023","","","404","417","Despite remarkable improvements, automatic speech recognition is susceptible to adversarial perturbations. Compared to standard machine learning architectures, these attacks are significantly more challenging, especially since the inputs to a speech recognition system are time series that contain both acoustic and linguistic properties of speech. Extracting all recognition-relevant information requires more complex pipelines and an ensemble of specialized components. Consequently, an attacker needs to consider the entire pipeline. In this paper, we present Venomave, the first training-time poisoning attack against speech recognition. Similar to the predominantly studied evasion attacks, we pursue the same goal: leading the system to an incorrect and attacker-chosen transcription of a target audio waveform. In contrast to evasion attacks, however, we assume that the attacker can only manipulate a small part of the training data without altering the target audio waveform at runtime. We evaluate our attack on two datasets: TIDIGITS and Speech Commands. When poisoning less than 0.17 % of the dataset, Venomave achieves attack success rates of more than 80.0 %, without access to the victim's network architecture or hyperparameters. In a more realistic scenario, when the target audio waveform is played over the air in different rooms, Venomave maintains a success rate of up to 73.3 %. Finally, Venomave achieves an attack transferability rate of 36.4 % between two different model architectures.","","978-1-6654-6299-0","10.1109/SaTML54575.2023.00035","NSF(grant numbers:CNS-2107101); Deutsche Forschungsgemeinschaft(grant numbers:390781972); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10136135","Data Poisoning;Automatic Speech Recognition","Toxicology;Runtime;Perturbation methods;Time series analysis;Pipelines;Training data;Speech recognition","","8","","50","IEEE","1 Jun 2023","","","IEEE","IEEE Conferences"
"Automatic Disfluency Detection From Untranscribed Speech","A. Romana; K. Koishida; E. M. Provost","University of Michigan, Ann Arbor, MI, USA; Microsoft Corporation, Redmond, WA, USA; University of Michigan, Ann Arbor, MI, USA","IEEE/ACM Transactions on Audio, Speech, and Language Processing","5 Nov 2024","2024","32","","4727","4740","Speech disfluencies, such as filled pauses or repetitions, are disruptions in the typical flow of speech. All speakers experience disfluencies at times, and the rate at which we produce disfluencies may be increased by certain speaker or environmental characteristics. Modeling disfluencies has been shown to be useful for a range of downstream tasks, and as a result, disfluency detection has many potential applications. In this work, we investigate language, acoustic, and multimodal methods for frame-level automatic disfluency detection and categorization. Each of these methods relies on audio as an input. First, we evaluate several automatic speech recognition (ASR) systems in terms of their ability to transcribe disfluencies, measured using disfluency error rates. We then use these ASR transcripts as input to a language-based disfluency detection model. We find that disfluency detection performance is largely limited by the quality of transcripts and alignments. We find that an acoustic-based approach that does not require transcription as an intermediate step outperforms the ASR language approach. Finally, we present multimodal architectures which we find improve disfluency detection performance over the unimodal approaches. Ultimately, this work introduces novel approaches for automatic frame-level disfluency and categorization. In the long term, this will help researchers incorporate automatic disfluency detection into a range of applications.","2329-9304","","10.1109/TASLP.2024.3485465","National Science Foundation(grant numbers:NSF IIS-RI 2006618); Advanced Research Computing; Information and Technology Services; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10731569","Automatic disfluency detection;multimodal disfluency detection;frame-level disfluency detection;disfluency categorization;automatic speech recognition","Acoustics;Feature extraction;Location awareness;Speech processing;Bidirectional control;Planning;Encoding;Transformers;Neural networks;Long short term memory","","1","","65","IEEE","23 Oct 2024","","","IEEE","IEEE Journals"
"The Second Multi-Channel Multi-Party Meeting Transcription Challenge (M2MeT 2.0): A Benchmark for Speaker-Attributed ASR","Y. Liang; M. Shi; F. Yu; Y. Li; S. Zhang; Z. Du; Q. Chen; L. Xie; Y. Qian; J. Wu; Z. Chen; K. A. Lee; Z. Yan; H. Bu","Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, China; NERC-SLIP, University of Science and Technology of China (USTC), China; Speech Lab of DAMO Academy, Alibaba Group, China; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, China; Speech Lab of DAMO Academy, Alibaba Group, China; Speech Lab of DAMO Academy, Alibaba Group, China; Speech Lab of DAMO Academy, Alibaba Group, China; Audio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, China; SpeechLab, Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; ICT Cluster, Singapore Institute of Technology, Singapore; ICT Cluster, Singapore Institute of Technology, Singapore; ICT Cluster, Singapore Institute of Technology, Singapore; Speech Lab of DAMO Academy, Alibaba Group, China; Beijing Shell Shell Technology Co., Ltd., Beijing, China",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","8","With the success of the first Multi-channel Multi-party Meeting Transcription challenge (M2MeT), the second M2MeT challenge (M2MeT 2.0) held in ASRU2023 particularly aims to tackle the complex task of speaker-attributed ASR (SAASR), which directly addresses the practical and challenging problem of “who spoke what at when” at typical meeting scenario. We particularly established two sub-tracks. The fixed training condition sub-track, where the training data is constrained to predetermined datasets, but participants can use any open-source pre-trained model. The open training condition sub-track, which allows for the use of all available data and models without limitation. In addition, we release a new 10-hour test set for challenge ranking. This paper provides an overview of the dataset, track settings, results, and analysis of submitted systems, as a benchmark to show the current state of speaker-attributed ASR.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389625","M2MeT 2.0;Alimeeting;Meeting Transcription;Multi-speaker ASR;Speaker-attributed ASR","Training;Conferences;Training data;Benchmark testing;Data models;Task analysis;Speech processing","","4","","39","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"Call Transcription Methodology for Contact Center Systems","M. Płaza; Ł. Pawlik; S. Deniziak","Faculty of Electrical Engineering, Automatics and Computer Science, Kielce University of Technology, Kielce, Poland; Altar Sp. z o.o., Kielce, Poland; Faculty of Electrical Engineering, Automatics and Computer Science, Kielce University of Technology, Kielce, Poland",IEEE Access,"12 Aug 2021","2021","9","","110975","110988","Nowadays, one of the key areas of research on contact centre systems is their automation. The main element that influences the possibility of automation of contact centre processes is the call transcription methods implemented by automatic speech recognition (ASR) systems. Such systems enable developing intention recognition methods and, consequently voice bots. The current solutions used in ASR systems for many less popular languages do not guarantee a fully satisfactory transcription quality for hotline voice calls. This is due to the unique characteristics of the sound signal generated there, whose quality parameters differ significantly from those of studio recordings. The paper presents a comparative study of selected speech recognition systems that were additionally supplemented with elements of preprocessing of sound recordings and postprocessing of originally produced transcriptions. As for preprocessing, the following methods were tested: separation of the client and agent channels into two independent signals, training of ASR systems, and audio signal correction. With regards to postprocessing, on the other hand, tests were performed for inarticulate sounds, normalization of standard phrases (e.g. numbers, dates, times, etc.), and identification of close-sounding phrases and foreign language phrases, and lemmatization. Based on the research conducted and the analyses performed, a new method of call transcription intended specifically for contact center systems was proposed. The research conducted for this paper was based on the Polish language model, for which major problems are observed with the quality of automatic contact center call transcriptions.","2169-3536","","10.1109/ACCESS.2021.3102502","European Union’s Smart Growth Operational Program 2014–2020(grant numbers:POIR.04.01.04-00-0079/19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9508438","Automatic speech recognition;call centre;contact centre;transcription;word error rate","Measurement;Automation;Automatic speech recognition;Error analysis;Tools;Task analysis","","8","","35","CCBY","6 Aug 2021","","","IEEE","IEEE Journals"
"Vararray Meets T-Sot: Advancing the State of the Art of Streaming Distant Conversational Speech Recognition","N. Kanda; J. Wu; X. Wang; Z. Chen; J. Li; T. Yoshioka","Microsoft, One Microsoft Way, Redmond, WA, USA; Microsoft, One Microsoft Way, Redmond, WA, USA; Microsoft, One Microsoft Way, Redmond, WA, USA; Microsoft, One Microsoft Way, Redmond, WA, USA; Microsoft, One Microsoft Way, Redmond, WA, USA; Microsoft, One Microsoft Way, Redmond, WA, USA","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","This paper presents a novel streaming automatic speech recognition (ASR) framework for multi-talker overlapping speech captured by a distant microphone array with an arbitrary geometry. Our framework, named t-SOT-VA, capitalizes on independently developed two recent technologies; array-geometry-agnostic continuous speech separation, or VarArray, and streaming multi-talker ASR based on token-level serialized output training (t-SOT). To combine the best of both technologies, we newly design a t-SOT-based ASR model that generates a serialized multi-talker transcription based on two separated speech signals from VarArray. We also propose a pre-training scheme for such an ASR model where we simulate VarArray’s output signals based on monaural single-talker ASR training data. Conversation transcription experiments using the AMI meeting corpus show that the system based on the proposed framework significantly outperforms conventional ones. Our system achieves the state-of-the-art word error rates of 13.7% and 15.5% for the AMI development and evaluation sets, respectively, in the multiple-distant-microphone setting while retaining the streaming inference capability.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095367","Multi-talker automatic speech recognition;conversation transcription;microphone array;streaming inference","Training;Geometry;Error analysis;Conferences;Training data;Oral communication;Microphone arrays","","10","","36","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"“Spanish Políglota”: an automatic Speech Recognition system based on HMM","J. A. Zea; J. Aguiar","Dept. de Informática y Ciencias de la Computación, Escuela Politécnica Nacional, Quito, Ecuador; Dept. de Informática y Ciencias de la Computación, Escuela Politécnica Nacional, Quito, Ecuador",2021 Second International Conference on Information Systems and Software Technologies (ICI2ST),"7 Jun 2021","2021","","","18","24","The goal of this ASR system is to be able to recognize audio queries that request static translation of a given Spanish word into a specified language. We call this ASR system as the Spanish Políglota. The pronunciation dictionary for the language model is obtained by applying grapheme to phoneme conversion. It was developed via Festival Speech Synthesis Scheme scripts and the SPPAS Spanish lexicon. The possible audio queries are restricted by a BNF grammar we designed for this project. A triphone acoustic model was generated from a set of 1621 words audio recordings. This acoustic model is based on a N-gram model that estimates its probabilities based on the maximum likelihood estimation MLE. We evaluated the prediction of individual words, as well as of synthetic phrases. We generated 1577 synthetic phrases concatenating the words of our audio set. The performance was also measured over a new set of audio recordings from a different speaker. Evaluation of isolated word recognition achieved 77.91% of correct predictions. Nevertheless, the performance dropped when evaluating the synthetic phrases as well as the second speaker’s speech. We consider it is an initial step towards the development of a fully functional automatic speech recognition system.","","978-1-6654-0411-2","10.1109/ICI2ST51859.2021.00011","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447349","Automatic Speech Recognition;ASR;HMM;HTK;Julius;Voxforge;Spanish;Grapheme-to-Phoneme Conversion","Training;Maximum likelihood estimation;Synthesizers;Hidden Markov models;Training data;Audio recording;Acoustics","","3","","10","IEEE","7 Jun 2021","","","IEEE","IEEE Conferences"
"Counterfactually Fair Automatic Speech Recognition","L. Sarı; M. Hasegawa-Johnson; C. D. Yoo","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; KAIST, Daejeon, Korea","IEEE/ACM Transactions on Audio, Speech, and Language Processing","3 Dec 2021","2021","29","","3515","3525","Widelyused automatic speech recognition (ASR) systems have been empirically demonstrated in various studies to be unfair, having higher error rates for some groups of users than others. One way to define fairness in ASR is to require that changing the demographic group affiliation of any individual (e.g., changing their gender, age, education or race) should not change the probability distribution across possible speech-to-text transcriptions. In the paradigm of counterfactual fairness, all variables independent of group affiliation (e.g., the text being read by the speaker) remain unchanged, while variables dependent on group affiliation (e.g., the speaker’s voice) are counterfactually modified. Hence, we approach the fairness of ASR by training the ASR to minimize change in its outcome probabilities despite a counterfactual change in the individual’s demographic attributes. Starting from the individualized counterfactual equal odds criterion, we provide relaxations to it and compare their performances for connectionist temporal classification (CTC) based end-to-end ASR systems. We perform our experiments on the Corpus of Regional African American Language (CORAAL) and the LibriSpeech dataset to accommodate for differences due to gender, age, education, and race. We show that with counterfactual training, we can reduce average character error rates while achieving lower performance gap between demographic groups, and lower error standard deviation among individuals.","2329-9304","","10.1109/TASLP.2021.3126949","Institute for Information & Communications Technology Planning & Evaluation(grant numbers:2019-0-01396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610166","Automatic speech recognition;speaker adaptation;fairness in machine learning;counterfactual fairness","Training;Machine learning;Speech processing;Error analysis;Machine learning algorithms;Automatic speech recognition","","10","","42","CCBY","10 Nov 2021","","","IEEE","IEEE Journals"
"Advancements in Speech-to-Text Systems for the Hearing Impaired","T. Y. G; N. B G; S. R. M; R. K; L. M. Mohamed","Dept. of Electronics and Communication Engineering, Nitte Meenakshi Institute of Technology, Bengaluru, Karnataka, India; Dept. of Electronics and Communication Engineering, Vidyavardhaka College of Engineering, Mysuru, Karnataka, India; Dept. of Electronics and Communication Engineering, Nitte Meenakshi Institute of Technology, Bengaluru, Karnataka, India; Dept. of Electronics and Communication Engineering, Nitte Meenakshi Institute of Technology, Bengaluru, Karnataka, India; Dept. of Electronics and Communication Engineering, Nitte Meenakshi Institute of Technology, Bengaluru, Karnataka, India",2024 IEEE North Karnataka Subsection Flagship International Conference (NKCon),"10 Dec 2024","2024","","","1","6","Speech is one of the most important types of communication among the human beings. Speech recognition is one of the most widely used applications of speech processing. Developing a automatic speech recognition (ASR) system is a stimulating task and Deaf people are prone to experience difficulties with tasks that involve communication. Wearable technology has played a significant role in evaluating and researching systems developed for hearing impaired community. This paper presents a system based on Google speech recognition library designed to assist Deaf people with live transcription of others voices, thereby using it as a communication assistant. The microphone embedded to the system (microcontroller) grasps the audio (voice) signals that are nearby the user wearing the spectacles, which is analyzed using the Google Speech-to-Text (Google speech recognition) library. The analyzed voice signals are displayed on an organic light-emitting diode (OLED) screen which mounted to the spectacles. The collaboration of all these technologies, along with AI, AR and IoT technologies, would assist hearing impaired people in finding solutions to their problems.","","979-8-3503-6456-9","10.1109/NKCon62728.2024.10775266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10775266","Speech to Text;Hearing impaired;smart glasses;OLED;Internet of Things;Bluetooth Communication","Text recognition;Neural networks;Auditory system;Organic light emitting diodes;Libraries;Real-time systems;Internet;Speech processing;Speech to text;Smart glasses","","","","27","IEEE","10 Dec 2024","","","IEEE","IEEE Conferences"
"Transformer-based Automatic Speech Recognition of Simultaneous Interpretation with Auxiliary Input of Source Language Text","S. Taniguchi; T. Kato; A. Tamura; K. Yasuda","Doshisha University, Japan; Doshisha University, Japan; Doshisha University, Japan; Mindword, Inc., Japan",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","1857","1861","Automatic speech recognition (ASR) of simultaneous interpretation is challenging due to disfluencies such as hesitations, filled pauses, interruptions, and self-repairs. Meanwhile, the simultaneous interpretation sometimes accompanies source language text. In particular, training materials for simultaneous interpreters have the source language text. The source language text can also be considered an ideal transcription of source language speech. Previously, we proposed a Transformer-based ASR with auxiliary input of source language text for transcribing simultaneous interpretation and verified the effectiveness of the model structure by using large-scale corpora of speech translation (ST). In this study, we fine-tuned and evaluated the proposed model using a corpus of simultaneous interpretation with transcription of the source language speech as the auxiliary input. We also added a filled pause model to the ASR and compared the effect on the reduction of a word error rate (WER) with the auxiliary input. The experimental results showed significant WER reductions by both the auxiliary input and the filled pause model, though the reduction made by the filled pause model was greater than that made by the auxiliary input.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317128","","Training;Error analysis;Asia;Information processing;Transformers;Automatic speech recognition","","1","","21","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Self-Supervised Adaptive AV Fusion Module for Pre-Trained ASR Models","C. Simic; T. Bocklet","Technische Hochschule Nürnberg Georg Simon Ohm, Germany; Technische Hochschule Nürnberg Georg Simon Ohm, Germany","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","12787","12791","Automatic speech recognition (ASR) has reached a level of accuracy in recent years, that even outperforms humans in transcribing speech to text. Nevertheless, all current ASR approaches show a certain weakness against ambient noise. To reduce this weakness, audio-visual speech recognition (AVSR) approaches additionally consider visual information from lip movements for transcription. This additional modality increases the computational cost for training models from scratch. We propose an approach, that builds on a pre-trained ASR model and extends it with an adaptive upstream module, that fuses audio and visual information. Since we do not need to train the transformer structure from scratch, our approach requires a fraction of the computational resources compared to traditional AVSR models. Compared to current SOTA systems like AV-HuBERT, our approach achieves an average improvement of 8.3 % in word error rate across different model sizes, noise categories and broad SNR range. The approach allows up to 21 % smaller models and requires only a fraction of the computational resources for training and inference compared to common AVSR approaches.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10448047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448047","audio-visual speech recogntion;audio-visual fusion;cross-attention;self-supervised","Training;Adaptation models;Visualization;Computational modeling;Signal processing;Transformers;Speech processing","","4","","28","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Design and Evaluation of a Voice-Controlled Elevator System to Improve the Safety and Accessibility","A. González-Docasal; J. Alonso; J. Olaizola; M. Mendicute; M. P. Franco; A. d. Pozo; D. Aguinaga; A. Álvarez; E. Lleida","Fundación Vicomtech, Basque Research and Technology Alliance, Donostia-San Sebastián, Spain; IKOR, Sistemas Electrónicos S.L., Parque Empresarial Zuatzu, Donostia-San Sebastian, Spain; Electronics & Computer Science Department, Mondragon University, Arrasate-Mondragon, Spain; Electronics & Computer Science Department, Mondragon University, Arrasate-Mondragon, Spain; Orona Corporación, Orona Ideo, Hernani, Spain; Fundación Vicomtech, Basque Research and Technology Alliance, Donostia-San Sebastián, Spain; IKOR, Sistemas Electrónicos S.L., Parque Empresarial Zuatzu, Donostia-San Sebastian, Spain; Fundación Vicomtech, Basque Research and Technology Alliance, Donostia-San Sebastián, Spain; Aragon Institute for Engineering Research, University of Zaragoza, Zaragoza, Spain",IEEE Open Journal of the Industrial Electronics Society,"20 Nov 2024","2024","5","","1239","1250","This work introduces the design and assessment of a voice-controlled elevator system aimed at facilitating touchless interaction between users and hardware, thereby minimizing contact and improving accessibility for individuals with disabilities. The research distinguishes three distinct deployment scenarios—on cloud, on edge, and embedded—with the ultimate goal of integrating the entire system into a low-resource environment on a custom carrier board. An objective evaluation measured acoustic conditions rigorously using a dataset of 2900 audio files recorded inside a laboratory elevator cabin featuring two internal coatings, five audio input devices, and under four distinct noise conditions. The study evaluated the performance of two Automatic Speech Recognition systems: Google's Speech-to-Text API and a Kaldi model adapted for this task, deployed using Vosk. In addition, latency times for these transcribers and two communication protocols were measured to enhance efficiency. Finally, two subjective evaluations on clean and noisy conditions were conducted simulating a real world scenario. The results, yielding 84.7 and 77.2 points, respectively, in a System Usability Scale questionnaire, affirm the reliability of the presented prototype for industrial deployment.","2644-1284","","10.1109/OJIES.2024.3483552","Basque Government's Elkartek research and innovation program; iVOZ; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721366","Automatic speech recognition;embedded systems;human–machine interaction","Elevators;Speech to text;Speech processing;Sensors;Safety;Noise measurement;Microphone arrays;Array signal processing;Automatic speech recognition;Human-machine systems","","","","33","CCBY","17 Oct 2024","","","IEEE","IEEE Journals"
"Transcription Free Filler Word Detection with Neural Semi-CRFs","G. Zhu; Y. Yan; J. -P. Caceres; Z. Duan",University of Rochester; University of Rochester; Adobe Research; University of Rochester,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Non-linguistic filler words, such as ""uh"" or ""um"", are prevalent in spontaneous speech and serve as indicators for expressing hesitation or uncertainty. Previous works for detecting certain non-linguistic filler words are highly dependent on transcriptions from a well-established commercial automatic speech recognition (ASR) system. However, certain ASR systems are not universally accessible from many aspects, e.g., budget, target languages, and computational power. In this work, we investigate filler word detection system1 that does not depend on ASR systems. We show that, by using the structured state space sequence model (S4) and neural semi-Markov conditional random fields (semi-CRFs), we achieve an absolute F1 improvement of 6.4% (segment level) and 3.1% (event level) on the PodcastFillers dataset. We also conduct a qualitative analysis on the detected results to analyze the limitations of our proposed system.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10097095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097095","filler word detection;speech disfluency;neural semi-CRFs","Uncertainty;Computational modeling;Signal processing;Acoustics;Calibration;Speech processing;Digital audio broadcasting","","1","","30","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Identifying False Content and Hate Speech in Sinhala YouTube Videos by Analyzing the Audio","W. A. K. M. Wickramaarachchi; S. S. Subasinghe; K. K. R. T. Wijerathna; A. S. U. Athukorala; L. Abeywardhana; A. Karunasena","Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka; Department of Information Technology, Sri Lanka Institute of Information Technology, Malabe, Sri Lanka",2023 5th International Conference on Advancements in Computing (ICAC),"12 Feb 2024","2023","","","364","369","YouTube faces a global crisis with the dissemination of false information and hate speech. To counter these issues, YouTube has implemented strict rules against uploading content that includes false information or promotes hate speech. While numerous studies have been conducted to reduce offensive English-language content, there's a significant lack of research on Sinhala content. This study aims to address the aforementioned gap by proposing a solution to minimize the spread of violence and misinformation in Sinhala YouTube videos. The approach involves developing a rating system that assesses whether a video contains false information by comparing the title and description with the audio content and evaluating whether the video includes hate speech. The methodology encompasses several steps, including audio extraction using the Pytube library, audio transcription via the fine-tuned Whisper model, hate speech detection employing the distilroberta-base model and a text classification LSTM model, and text summarization through the fine-tuned BART-Large-XSUM model. Notably, the Whisper model achieved a 48.99% word error rate, while the distilroberta-base model demonstrated an F1 score of 0.856 and a recall value of 0.861 in comparison to the LSTM model, which exhibited signs of overfitting.","2837-5424","979-8-3503-5813-1","10.1109/ICAC60630.2023.10417565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10417565","hate speech detection;automatic speech recognition;false content detection","Video on demand;Hate speech;Speech recognition;Transformers;Web sites;Fake news;Videos","","","","16","IEEE","12 Feb 2024","","","IEEE","IEEE Conferences"
"Low Resource Audio-To-Lyrics Alignment from Polyphonic Music Recordings","E. Demirel; S. Ahlbäck; S. Dixon","Centre for Digital Music, Queen Mary University of London, UK; Doremir Music Research AB, SE; Centre for Digital Music, Queen Mary University of London, UK","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","586","590","Lyrics alignment in long music recordings can be memory exhaustive when performed in a single pass. In this study, we present a novel method that performs audio-to-lyrics alignment with a low memory consumption footprint regardless of the duration of the music recording. The proposed system first spots the anchoring words within the audio signal. With respect to these anchors, the recording is then segmented and a second-pass alignment is performed to obtain the word timings. We show that our audio-to-lyrics alignment system performs competitively with the state-of-the-art, while requiring much less computational resources. In addition, we utilize our lyrics alignment system to segment the music recordings into sentence-level chunks. Notably on the segmented recordings, we report the lyrics transcription scores on a number of benchmark test sets. Finally, our experiments highlight the importance of the source separation step for good performance on the transcription and alignment tasks. For reproducibility, we publicly share our code with the research community.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414395","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414395","audio-to-lyrics alignment;music information retrieval;automatic lyrics transcription;long audio alignment;automatic speech recognition","Source separation;Web services;Memory management;Training data;Speech recognition;Benchmark testing;Reproducibility of results","","10","","25","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Implementation of Number Transcription Tool","A. Balvalli; A. Bathe; S. Dalvi; P. Erande; J. More","Department of Computer Engineering, Fr. C. Rodrigues Institute of Technology, Vashi, India; Department of Computer Engineering, Fr. C. Rodrigues Institute of Technology, Vashi, India; Department of Computer Engineering, Fr. C. Rodrigues Institute of Technology, Vashi, India; Department of Computer Engineering, Fr. C. Rodrigues Institute of Technology, Vashi, India; Department of Computer Engineering, Fr. C. Rodrigues Institute of Technology, Vashi, India",2023 6th International Conference on Advances in Science and Technology (ICAST),"4 Mar 2024","2023","","","563","568","The creation of a precise speech-based number transcription tool that accurately transforms spoken numbers (0–9) into their corresponding digits on a screen has become extremely important in today's digital world. In order to accomplish error-free and reliable transcription of numerical data, a system using advanced speech recognition technologies was designed and implemented, as revealed by this paper. The system uses a combination of machine learning models that have been combined and trained on large datasets to provide greater transcription accuracy and adaptability to different speech patterns. This paper thoroughly explains the system's technological architecture, highlighting its features and functionalities, with particular emphasis on its capacity to convert spoken numbers into their corresponding numerical representations on a visual interface. The paper also consists of a table which provides a brief summary of the benefits and drawbacks of several models for automatic speech recognition number transcription including hybrid DNN-HMM systems, noise-robust techniques, LAS models and a comparison of our system with these existing systems. The various applications of this system include finance, telecommunications, and data entry, where precise number transcription is of utmost importance.","","979-8-3503-5981-7","10.1109/ICAST59062.2023.10454912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10454912","Number-to-Digit Transcription;Speech Recognition;Machine Learning;CNN models.","Adaptation models;Visualization;Machine learning;Transforms;Numerical models;Telecommunications;Testing","","","","8","IEEE","4 Mar 2024","","","IEEE","IEEE Conferences"
"Exploring Sequence-to-Sequence Transformer-Transducer Models for Keyword Spotting","B. Labrador; G. Zhao; I. L. Moreno; A. Scorza Scarpati; L. Fowl; Q. Wang","AUDIAS (Audio, Data Intelligence and Speech), Universidad Autónoma de Madrid, Spain; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","In this paper, we present a novel approach to adapt a sequence-to-sequence Transformer-Transducer ASR system to the keyword spotting (KWS) task. We achieve this by replacing the keyword in the text transcription with a special token <kw> and training the system to detect the <kw> token in an audio stream. At inference time, we create a decision function inspired by conventional KWS approaches, to make our approach more suitable for the KWS task. Furthermore, we introduce a specific keyword spotting loss by adapting the sequence-discriminative Minimum Bayes-Risk training technique. We find that our approach significantly outperforms ASR based KWS systems. When compared with a conventional keyword spotting system, our proposal has similar performance while bringing the advantages and flexibility of sequence-to-sequence training. Additionally, when combined with the conventional KWS system, our approach can improve the performance at any operation point.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10095444","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10095444","Keyword spotting;sequence-to-sequence models;transformer transducer;speech recognition","Training;Performance evaluation;Computational modeling;Speech recognition;Signal processing;Transformers;Batteries","","1","","31","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"mmWave-Whisper: Phone Call Eavesdropping and Transcription Using Millimeter-Wave Radar","S. Basak; A. Padarthi; M. Gowda","Computer Science and Engineering, Pennsylvania State University State College, PA, USA; Computer Science and Engineering, Pennsylvania State University State College, PA, USA; Computer Science and Engineering, Pennsylvania State University State College, PA, USA","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","This paper introduces mmWave-Whisper, a system that demonstrates the feasibility of full-corpus automated speech recognition (ASR) on phone calls eavesdropped remotely using off-the-shelf frequency modulated continuous wave (FMCW) millimeter-wave radars. Operating in the 77-81 GHz range, mmWave-Whisper captures earpiece vibrations from smart-phones, converts them into audio, and processes the audio to produce speech transcriptions automatically. Unlike previous work that focused on loudspeakers or a limited vocabulary, this is the first to perform this kind of speech recognition by handling a large vocabulary and full sentences on earpiece vibrations from smartphones. This approach expands the potential for radar-audio eavesdropping. mmWave-Whisper addresses challenges such as the lack of large-scale training datasets, low SNR, and limited frequency information in radar data through a systematic data pipeline designed to leverage synthetic training data, domain adaptation, and inference by incorporating OpenAI’s Whisper automatic speech recognition model. The system achieves a word accuracy rate of 44.74% and a character accuracy rate of 62.52% over a range of 25 cm to 125 cm. The paper highlights emerging misuse modalities of AI as the technology evolves rapidly.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890670","side channel attacks;mmWave radars;speech privacy;speech recognition","Vibrations;Training;Vocabulary;Accuracy;Systematics;Training data;Millimeter wave radar;Millimeter wave communication;Speech processing;Eavesdropping","","","","24","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Mixed Precision Low-Bit Quantization of Neural Network Language Models for Speech Recognition","J. Xu; J. Yu; S. Hu; X. Liu; H. Meng","Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","10 Dec 2021","2021","29","","3679","3693","State-of-the-art language models (LMs) represented by long-short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming increasingly complex and expensive for practical applications. Low-bit neural network quantization provides a powerful solution to dramatically reduce their model size. Current quantization methods are based on uniform precision and fail to account for the varying performance sensitivity at different parts of LMs to quantization errors. To this end, novel mixed precision neural network LM quantization methods are proposed in this paper. The optimal local precision choices for LSTM-RNN and Transformer based neural LMs are automatically learned using three techniques. The first two approaches are based on quantization sensitivity metrics in the form of either the KL-divergence measured between full precision and quantized LMs, or Hessian trace weighted quantization perturbation that can be approximated efficiently using matrix free techniques. The third approach is based on mixed precision neural architecture search. In order to overcome the difficulty in using gradient descent methods to directly estimate discrete quantized weights, alternating direction methods of multipliers (ADMM) are used to efficiently train quantized LMs. Experiments were conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed perturbation, i-Vector and learning hidden unit contribution (LHUC) based speaker adaptation on two tasks: Switchboard telephone speech and AMI meeting transcription. The proposed mixed precision quantization techniques achieved “lossless” quantization on both tasks, by producing model size compression ratios of up to approximately 16 times over the full precision LSTM and Transformer baseline LMs, while incurring no statistically significant word error rate increase.","2329-9304","","10.1109/TASLP.2021.3129357","Hong Kong Research Grants Council GRF(grant numbers:14200218,14200220,14200021); Innovation and Technology Fund(grant numbers:ITS/254/19); Shun Hing Institute of Advanced Engineering(grant numbers:MMT-p1-19); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622190","Language models;speech recognition;LSTM-RNN;transformer;low-bit quantization;ADMM","Quantization (signal);Transformers;Recurrent neural networks;Speech recognition;Long short term memory","","7","","52","IEEE","19 Nov 2021","","","IEEE","IEEE Journals"
"AVIBRIEF: Automated Vocal Information Briefings Dataset","A. Johnson; V. Venkataraman; E. Reinholz","Cirrus Design Corporation, Duluth, MN, United States; Cirrus Design Corporation, Duluth, MN, United States; North Dakota State University, Fargo, ND, United States",2023 IEEE/AIAA 42nd Digital Avionics Systems Conference (DASC),"10 Nov 2023","2023","","","1","10","Automated briefings are an essential information source for pilots as they provide a comprehensive and up-to-date picture of meteorological and aeronautical conditions, ensuring safe and efficient flight operations. These briefings come in a variety of formats, a notable few being Automated Terminal Information Service (ATIS), Automated Surface/Weather Observation System (ASOS/AWOS). These systems provide an audio broadcast of the current weather, runway surface conditions, and other pertinent information to pilots without them having to reference the air traffic control tower or ground observers. While most aircraft are equipped with VHF radios for receiving information through audio transmissions, it poses challenges as pilots must accurately interpret the audio and manually execute necessary actions. Though digital data transmissions are the ideal solution to improve accuracy and allow automation, most airplanes lack the necessary equipment for this. As a result, general aviation (GA) will continue to rely on audio transmissions for the foreseeable future. However, recent advances in speech recognition and natural language processing can bridge this gap by converting analog audio information into digital data, thus enabling automation. These advanced technologies rely on vast amounts of data, particularly human-verified transcriptions, to be effective in aviation applications. To facilitate research in this area, we propose and publicly release an automated informational briefings voice dataset (AVIBRIEF) for Automatic Speech Recognition (ASR) and Natural Language Processing (NLP) research purposes.","2155-7209","979-8-3503-3357-2","10.1109/DASC58513.2023.10311162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10311162","Automatic speech recognition;Aviation speech;Weather briefing;Named entity recognition;ATIS;AWOS;ASOS","Bridges;Automation;Poles and towers;Information services;Observers;Natural language processing;Air traffic control","","","","29","IEEE","10 Nov 2023","","","IEEE","IEEE Conferences"
"Investigating End-to-End ASR Architectures for Long Form Audio Transcription","N. R. Koluguri; S. Kriman; G. Zelenfroind; S. Majumdar; D. Rekesh; V. Noroozi; J. Balam; B. Ginsburg",NA; NA; NA; NA; NA; NA; NA; NA,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","13366","13370","This paper presents an overview and evaluation of some of the end-to-end ASR models on long-form audio. We study three categories of Automatic Speech Recognition(ASR) models based on their core architecture: (1) convolutional, (2) convolutional with squeeze-and-excitation, and (3) convolutional models with attention. We selected one ASR model from each category and evaluated the Word Error Rate, maximum audio length and real-time factor for each model on a variety of long audio benchmarks: Earnings-21 and 22, CORAAL, and TED-LIUM3. The model from the category of self-attention with local attention and global token has the best accuracy compared to other architectures. We also compared models with CTC and RNNT decoders and showed that CTC-based models are more robust and efficient than RNNT on long form audio.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10448309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448309","Automatic Speech Recognition (ASR);Long-form Audio;Earnings-21;CORAAL;TED-LIUM","Convolution;Error analysis;Computational modeling;Computer architecture;Robustness;Real-time systems;Decoding","","1","","26","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Increasing Context for Estimating Confidence Scores in Automatic Speech Recognition","A. Ragni; M. J. F. Gales; O. Rose; K. M. Knill; A. Kastanos; Q. Li; P. M. Ness","Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","6 Apr 2022","2022","30","","1319","1329","Accurate confidence measures for predictions from machine learning techniques play a critical role in the deployment and training of many speech and language processing applications. For example, confidence scores are important when making use of automatically generated transcriptions in training automatic speech recognition (ASR) systems, as well as down-stream applications, such as information retrieval and conversational assistants. Previous work on improving confidence scores for these systems has focused on two main directions: designing features correlated with improved confidence prediction; and employing sequence models to account for the importance of contextual information. Few studies, however, have explored incorporating contextual information more broadly, such as from the future, in addition to the past, or making use of alternative multiple hypotheses in addition to the most likely one. This article introduces two general approaches for encapsulating contextual information from lattices. Experimental results illustrating the importance of increasing contextual information for estimating confidence scores are presented on a range of limited resource languages where word error rates range between 30% and 60%. The results show that the novel approaches provide significant gains in the accuracy of confidence estimation.","2329-9304","","10.1109/TASLP.2022.3161153","ALTA Institute; University of Cambridge; Office of the Director of National Intelligence; Intelligence Advanced Research Projects Activity; Air Force Research Laboratory(grant numbers:FA8650-17-C-9117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739834","Attention;confidence;graph structures;recurrent neural network;speech recognition","Lattices;Hidden Markov models;Speech processing;Estimation;History;Feature extraction;Stability criteria","","2","","74","IEEE","22 Mar 2022","","","IEEE","IEEE Journals"
"Multi-modal Streaming ASR in Cross-talk Scenario for Smart Glasses","Y. Jiang; H. Lan; Q. Wang; S. Niu","NERC-SLIP, University of Science and Technology of China (USTC), Hefei, China; NERC-SLIP, University of Science and Technology of China (USTC), Hefei, China; NERC-SLIP, University of Science and Technology of China (USTC), Hefei, China; NERC-SLIP, University of Science and Technology of China (USTC), Hefei, China","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","In the MMCSG task of the CHiME-8 Challenge, achieving real-time speaker-attributed transcriptions with limited multi-modal data presents significant challenges. To cope with the problem, we propose a novel ASR framework that leverages both audio-only and multi-modal inputs in a streaming fashion. For the audio-only modality, analyzing and emulating the characteristics of real audio, we utilize a multi-channel simulation to generate the augmented dataset, which efficiently reduces the model training deviation between real and simulated data. Additionally, we integrate the IMU data with audio data in the network structure, demonstrating that the functional filtered and encoded IMU data can assist audio information in achieving better real-time speech recognition performance with ablation experiments. Notably, our explorations based on the above schemes not only secured first place in the MMCSG sub-track but also represented the first investigation into the effectiveness of leveraging IMU data for this task.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10889243","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10889243","streaming ASR;multi-modal data;IMU data;cross-talk;smart glasses;MMCSG;CHiME","Training;Transducers;Noise;Speech recognition;Information filters;Real-time systems;Data models;Encoding;Speech processing;Smart glasses","","","","37","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Automation-Driven Dataset Preparation for Continuous Czech Sign Language Recognition","J. Snajder; J. Krejsa","Institute of Solid Mechanics, Mechatronics and Biomechanics, Brno University of Technology, Brno, Czech Republic; Institute of Solid Mechanics, Mechatronics and Biomechanics, Brno University of Technology, Brno, Czech Republic",2024 21st International Conference on Mechatronics - Mechatronika (ME),"17 Dec 2024","2024","","","1","5","This paper presents an automation-driven solution for preparing a continuous Czech Sign Language dataset, addressing the lack of resources in this area. Manual processing of daily sign language news recordings would be extremely time-consuming, as the videos vary in quality, use different overlays, and have no captions. To streamline this process, we use the Structural Similarity Index Measure (SSIM) to compare key frames and extract relevant parts of the recording, such as weather forecast segments. Automatic speech recognition (ASR) then processes the accompanying audio and generates textual transcriptions of the spoken content. The outcome is the highly automated preparation pipeline and the dataset containing 4699 annotated videos of weather forecast news in Czech Sign Language providing a foundation for future research in sign language recognition.","","979-8-3503-9490-0","10.1109/ME61309.2024.10789742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10789742","sign language;continuous;dataset;recognition;translation","Sign language;Vocabulary;Mechatronics;Pipelines;Weather forecasting;Manuals;Natural language processing;Recording;Videos;Automatic speech recognition","","","","11","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Crossmodal ASR Error Correction With Discrete Speech Units","Y. Li; P. Chen; P. Bell; C. Lai","University of Edinburgh, UK; University of Edinburgh, UK; University of Edinburgh, UK; University of Edinburgh, UK",2024 IEEE Spoken Language Technology Workshop (SLT),"16 Jan 2025","2024","","","431","438","ASR remains unsatisfactory in scenarios where the speaking style diverges from that used to train ASR systems, resulting in erroneous transcripts. To address this, ASR Error Correction (AEC), a post-ASR processing approach, is required. In this work, we tackle an understudied issue: the Low-Resource Out-of-Domain (LROOD) problem, by investigating crossmodal AEC on very limited downstream data with 1-best hypothesis transcription. We explore pretraining and fine-tuning strategies and uncover an ASR domain discrepancy phenomenon, shedding light on appropriate training schemes for LROOD data. Moreover, we propose the incorporation of discrete speech units to align with and enhance the word embeddings for improving AEC quality. Results from multiple corpora and several evaluation metrics demonstrate the feasibility and efficacy of our proposed AEC approach on LROOD data as well as its generalizability and superiority on large-scale data. Finally, a study on speech emotion recognition confirms that our model produces ASR error-robust transcripts suitable for downstream applications.","","979-8-3503-9225-8","10.1109/SLT61566.2024.10832240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832240","ASR Error Correction;Discrete Speech Units;Low-Resource Speech;Out-of-Domain Data","Training;Measurement;Emotion recognition;Conferences;Speech recognition;Speech enhancement;Error correction","","2","","38","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Efficient Architecture Integrating Wav2Vec 2.0 and Transformer Decoding for Low-Resource Environments","K. Narayanasamy; S. K. L; S. D; M. Maragatharajan","Department of Computer Science, J.J. College of Arts and Science (Autonomous), Affiliated to Bharathidasan University, Tiruchirappalli; School of Computing Science and Engineering (SCOPE), VIT Bhopal University, Sehore, Madhya Pradesh, India; School of Computing Science and Engineering (SCOPE), VIT Bhopal University, Sehore, Madhya Pradesh, India; School of Computing Science and Engineering (SCOPE), VIT Bhopal University, Sehore, Madhya Pradesh, India",2025 International Conference on Electronics and Renewable Systems (ICEARS),"2 Apr 2025","2025","","","105","108","This article introduces a cutting-edge Automatic Speech Recognition (ASR) architecture that combines a Transformer-based decoder optimised with weak supervision with Wav2Vec 2.0 for self-supervised feature extraction. By using high-level representations of raw audio and optimising with poorly tagged data, the architecture overcomes the difficulties posed by a lack of annotated datasets. Improved transcription accuracy is demonstrated by the model's notable decrease in Word Error Rate (WER), which goes from 15.6% with baseline RNN models to 7.2%. Furthermore, the system's computational efficiency is demonstrated by the 25% reduction in training time. The outcomes demonstrate that the suggested framework is a reliable option for ASR tasks in low-resource environments, attaining a fair compromise between computational cost, data needs, and performance.","","979-8-3315-0967-5","10.1109/ICEARS64219.2025.10940870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10940870","Automatic Speech Recognition (ASR);Wav2Vec 2.0;Transformer-based decoder;Weak supervision;Computational efficiency","Training;Weak supervision;Error analysis;Computational modeling;Computer architecture;Transformers;Feature extraction;Decoding;Computational efficiency;Automatic speech recognition","","","","13","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"Advanced Long-Content Speech Recognition With Factorized Neural Transducer","X. Gong; Y. Wu; J. Li; S. Liu; R. Zhao; X. Chen; Y. Qian","Auditory Cognition and Computational Acoustics Lab, the Department of Computer Science and Engineering and the MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Microsoft Corporation, Beijing, China; Auditory Cognition and Computational Acoustics Lab, the Department of Computer Science and Engineering and the MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Auditory Cognition and Computational Acoustics Lab, the Department of Computer Science and Engineering and the MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","14 Mar 2024","2024","32","","1803","1815","Long-content automatic speech recognition (ASR) has obtained increasing interest in recent years, as it captures the relationship among consecutive historical utterances while decoding the current utterance. In this paper, we propose two novel approaches, which integrate long-content information into the factorized neural transducer (FNT) based architecture in both non-streaming (referred to as LongFNT) and streaming (referred to as SLongFNT) scenarios. We first investigate whether long-content transcriptions can improve the vanilla conformer transducer (C-T) models. Our experiments indicate that the vanilla C-T models do not exhibit improved performance when utilizing long-content transcriptions, possibly due to the predictor network of C-T models not functioning as a pure language model. Instead, FNT shows its potential in utilizing long-content information, where we propose the LongFNT model and explore the impact of long-content information in both text (LongFNT-Text) and speech (LongFNT-Speech). The proposed LongFNT-Text and LongFNT-Speech models further complement each other to achieve better performance, with transcription history proving more valuable to the model. The effectiveness of our LongFNT approach is evaluated on LibriSpeech and GigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction, respectively. Furthermore, we extend the LongFNT model to the streaming scenario, which is named SLongFNT, consisting of SLongFNT-Text and SLongFNT-Speech approaches to utilize long-content text and speech information. Experiments show that the proposed SLongFNT model achieves relative 26% and 17% WER reduction on LibriSpeech and GigaSpeech respectively while keeping a good latency, compared to the FNT baseline. Overall, our proposed LongFNT and SLongFNT highlight the significance of considering long-content speech and transcription knowledge for improving both non-streaming and streaming speech recognition systems.","2329-9304","","10.1109/TASLP.2024.3350893","China NSFC Projects(grant numbers:62122050,62071288); Shanghai Municipal Science and Technology Commission Project(grant numbers:2021SHZDZX0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10398434","Factorized neural transducer;long-content speech recognition;RNN-T;streaming and non-streaming","Transducers;Predictive models;Vocabulary;Computer architecture;Acoustics;Context modeling;Transformers","","4","","61","IEEE","12 Jan 2024","","","IEEE","IEEE Journals"
"Algorithms To Increase Data Reliability In Video Transcription","F. M. Nazarov; Y. S. S. o’g’li; E. B. S. o’g’li","Samarkand State University, Samarkand, Uzbekistan; Samarkand State University, Samarkand, Uzbekistan; Samarkand State University, Samarkand, Uzbekistan",2022 IEEE 16th International Conference on Application of Information and Communication Technologies (AICT),"16 Jan 2023","2022","","","1","6","Currently, the transmission of data in the form of video and audio on computer networks is growing rapidly. Increasing the flow of media data makes it the task of qualitative data processing and analysis. The growth of video data is one of the most pressing issues in the intellectual analysis of the content of this video data and transcription to extract the texts contained in the video files. In the process of video transcription, the process of transferring the data in the video file to the text without errors is followed. The process of video transcription is important for the intellectual analysis of data in hearing-impaired and large- scale video networks. To date, a lot of research has been done on video transcription. Nevertheless, shortcomings in transcription remain sufficient for arbitrary language. This research includes theoretical research on the study of algorithms and models of video transcription, as well as a theoretical analysis of experiments based on them.","2472-8586","978-1-6654-5162-8","10.1109/AICT55583.2022.10013558","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013558","Transcription;video transcription;phonetic transcription;intellectual transcription;Hidden Markov Model;Gaussian Mixture Model;K-Means Clustering Algorithm","Analytical models;Machine learning algorithms;Pressing;Reliability theory;Media;Markov processes;Data processing","","1","","14","IEEE","16 Jan 2023","","","IEEE","IEEE Conferences"
"Named Entity Recognition for Audio De-Identification","G. Baril; P. Cardinal; A. L. Koerich","Department of Software and IT Engineering, École de Technologie Supérieure, Université du Québec, Montréal, Canada; Department of Software and IT Engineering, École de Technologie Supérieure, Université du Québec, Montréal, Canada; Department of Software and IT Engineering, École de Technologie Supérieure, Université du Québec, Montréal, Canada",2022 International Joint Conference on Neural Networks (IJCNN),"30 Sep 2022","2022","","","1","8","Data anonymization is often a task carried out by humans. Automating it would reduce the cost and time required to complete this task. This paper presents a pipeline to automate the anonymization of audio data in French. We propose a pipeline, which takes audio files with their transcriptions and removes the named entities (NEs) present in the audio. Our pipeline is made up of a forced aligner, which aligns words in an audio transcript with speech and a model that performs named entity recognition (NER). Then, the audio segments that correspond to NEs are substituted with silence to anonymize audio. We compared forced aligners and NER models to find the best ones for our scenario. We evaluated our pipeline on a small hand-annotated dataset, achieving an F1 score of 0.769. This result shows that automating this task is feasible.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892285","MITACS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892285","Spoken language understanding;Automatic speech recognition;Audio de-identification;Audio redaction","Training;Data privacy;Pipelines;Neural networks;Speech recognition;Syntactics;Transformers","","4","","40","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Multimodal Speaker Recognition: Combining FFT, CNN, Speech-to-Text, BERT-Based Punctuation Restoration and Sentence Correction","K. P. Sai; K. Ajay; M. Ramesh","Department of Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College, Vijayawada, Andhra Pradesh, India; Department of Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College, Vijayawada, Andhra Pradesh, India; Department of Information Technology, Velagapudi Ramakrishna Siddhartha Engineering College, Vijayawada, Andhra Pradesh, India","2023 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)","19 Mar 2024","2023","","","1","7","In this paper, we look at technological solutions to problems with speech recognition, text-to-speech conversion, sentence correction, and punctuation restoration. Convolutional neural networks (CNNs) for model development, the Fast Fourier transform (FFT) for feature extraction from data, and the potent Bidirectional Encoder Representations from Transformers (BERT) model for context-aware sentence correction are all included into our methodology. Our study intends to enhance the precision, context sensitivity, and usability of language processing systems by merging CNNs, FFT, and BERT. By providing excellent transcription and precise sentence correction, this holistic approach pushes the limits of audio-based applications.","","979-8-3503-1920-0","10.1109/ICSES60034.2023.10465391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10465391","Speaker Recognition;Speech-to-Text Conversion;Convolutional Neural Networks (CNNs);Fast Fourier Transform (FFT);Bidirectional Encoder Representations from Transformers (BERT);Sentence Correction;Punctuation Restoration","Neural networks;Bidirectional control;Speech recognition;Transformers;Feature extraction;Encoding;Data models","","","","16","IEEE","19 Mar 2024","","","IEEE","IEEE Conferences"
"DENOASR: Debiasing ASRs through Selective Denoising","A. K. Rai; S. D. Jaiswal; S. Prakash; B. P. Sree; A. Mukherjee","IIT Kharagpur, India; IIT Kharagpur, India; IIT Kharagpur, India; IIT Kharagpur, India; IIT Kharagpur, India",2024 IEEE International Conference on Knowledge Graph (ICKG),"19 Feb 2025","2024","","","283","290","Automatic Speech Recognition (ASR) systems have been examined and shown to exhibit biases toward particular groups of individuals, influenced by factors such as demographic traits, accents, and speech styles. Noise can disproportionately impact speakers with certain accents, dialects, or speaking styles, leading to biased error rates. In this work we introduce a novel framework ${\mathcal{D}}{\mathcal{E}}{\mathcal{N}}{\mathcal{O}}{\mathcal{A}}{\mathcal{S}}{\mathcal{R}}$ which is a selective denoising technique to reduce the disparity in the word error rates between the two gender groups male and female. We find that a combination of two popular speech denoising techniques viz. DEmucs and Le can be effectively used to mitigate ASR disparity without compromising their overall performance. Experiments using two SOTA open-source ASRs - OpenAI Whisper and NVIDIA Nemo on multiple benchmark datasets - TIE, Vox-Populi, TedLium and Fleurs show that there is a promising reduction in the average word error rate gap across the two gender groups. For a given dataset, the denoising is selectively applied on speech samples having speech intelligibility below a certain threshold estimated using a small validation sample thus ameliorating the need for large-scale human written ground-truth transcripts. Our findings suggest that selective denoising can be an elegant approach to mitigate biases in present-day ASR systems.","","979-8-3315-0882-1","10.1109/ICKG63256.2024.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10884312","debiasing;selective denoising;ASR;word error rate","Deep learning;Accuracy;Error analysis;Noise reduction;Noise;Knowledge graphs;Benchmark testing;Robustness;Real-time systems;Automatic speech recognition","","","","65","IEEE","19 Feb 2025","","","IEEE","IEEE Conferences"
"A Survey on Spoken Italian Datasets and Corpora","M. Giordano; C. Rinaldi","Department of Information Engineering, Computer Science and Mathematics (DISIM), Università degli Studi dell’Aquila, L’Aquila, Italy; National Inter-University Consortium for Telecommunications (CNIT), Parma, Italy",IEEE Access,"18 Feb 2025","2025","13","","29190","29205","Spoken Italian datasets are curated collections of audio recordings featuring native Italian speech in various contexts (e.g., spontaneous dialogues, read text, telephone conversations), often accompanied by transcriptions or linguistic annotations. They serve as foundational resources for a wide range of applications, including Automatic Speech Recognition (ASR), Text-To-Speech (TTS) synthesis, emotion detection, and broader linguistic research. Despite Italian’s status as a richly diverse Romance language—marked by significant dialectal variation—publicly available large-scale corpora have remained comparatively underrepresented when contrasted with those of major world languages such as English or Mandarin. In this survey, we present a comprehensive examination of 66 spoken Italian datasets, highlighting their key characteristics, data collection methodologies, and annotation frameworks. We categorize the datasets by speech type (e.g., conversational, monologic, spontaneous), by source (e.g., broadcast media, telephone calls, field recordings), and by demographic or linguistic attributes (including dialects and sociolinguistic features). Our analysis uncovers critical issues around dataset scarcity, demographic underrepresentation, and restricted accessibility, limiting broader research and development efforts. To address these gaps, we propose best practices and future directions—such as expanding demographic coverage, promoting open-access models, and standardizing annotation protocols—to enrich Italian speech data resources. The complete dataset inventory is publicly available on GitHub and archived on Zenodo, offering researchers and developers a valuable reference. By highlighting both the achievements and the shortcomings in existing resources, this work ultimately aims to foster collaboration and to spur further advancements in Italian speech technologies and linguistic research.","2169-3536","","10.1109/ACCESS.2025.3538952","European Union (EU) under Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU, Partnership on “Telecommunications of the Future” (Program “RESTART”)(grant numbers:PE00000001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10872911","Spoken Italian datasets;speech technology;natural language processing (NLP);dataset annotation;computational linguistics;deep learning models","Linguistics;Recording;Surveys;Speech recognition;Training;Telephone sets;Oral communication;Media;Digital audio broadcasting;Cultural differences","","","","83","CCBY","4 Feb 2025","","","IEEE","IEEE Journals"
"Pronunciation Dictionary-Free Multilingual Speech Synthesis Using Learned Phonetic Representations","C. Liu; Z. -H. Ling; L. -H. Chen","National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; National Engineering Research Center of Speech and Language Information Processing, University of Science and Technology of China, Hefei, China; iFLYTEK Research, iFLYTEK Company Ltd., Hefei, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","23 Oct 2023","2023","31","","3706","3716","This article presents a multilingual speech synthesis approach that leverages learned phonetic representations to eliminate the need for pronunciation dictionaries in target languages. The learned phonetic representations consist of unsupervised phonetic representations (UPR) and supervised phonetic representations (SPR). To extract UPRs, a pre-trained wav2vec 2.0 model is utilized, while a language-independent automatic speech recognition (LI-ASR) model with a connectionist temporal classification (CTC) loss is employed to derive segment-level SPRs from the speech data of target languages. An acoustic model using UPRs and SPRs as intermediate representations is then designed, comprising a UPR predictor, an SPR predictor, and a representation-to-mel-spectrogram (RTM) converter. The two predictors generate UPRs and SPRs from texts, respectively. The RTM converter first combines UPRs with SPRs using a Transformer-based encoder, and then feeds the merged representations into a decoder to produce mel-spectrograms. Considering the difficulty of collecting large training corpora for all languages in multilingual speech synthesis, the parameters of both the two predictors and the RTM converter can be pre-trained on non-target languages to further improve model performance. Experimental results on six target languages demonstrate that our method outperformed the approaches directly predicting mel-spectrograms from character or phoneme sequences, and pre-training the acoustic model using a multilingual corpus further improved the performance of synthetic speech.","2329-9304","","10.1109/TASLP.2023.3313424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10244053","Speech synthesis;multilingual;wav2vec 2.0;phonetic representations","Phonetics;Speech synthesis;Acoustics;Feature extraction;Training;Spectrogram;Dictionaries","","3","","50","IEEE","8 Sep 2023","","","IEEE","IEEE Journals"
"Towards Application of Speech Analysis in Predicting Learners’ Performance","D. Chowdary Attota; N. Dehbozorgi","Department of Computer Science, Kennesaw State University, Marietta, GA, USA; Department of Software Engineering, Kennesaw State University, Marietta, GA, USA",2022 IEEE Frontiers in Education Conference (FIE),"29 Nov 2022","2022","","","1","5","In this work in progress, we propose a model for analysis of students’ verbal conversation during teamwork to predict their academic performance based on expressed emotions. Our previous studies support the link between an individual’s attitude and emotional states during the cognitive process with their performance in the given context [1], [2]. Traditionally the learners’ affective states were assessed by having them fill out standard surveys. More recently the researchers have been using advanced methods to extract students’ emotions from their writings by using Natural Language Processing (NLP) models. These models are applied to data collected from different sources such as discussion forums, team chats, students’ reflective surveys, and journals. In this research, we take one step further by recording students audio in class as they converse about the course topic in low-stake teams and extract emotions from their conversations by NLP methods. The main contributions of the proposed model are 1) the audio transcription component 2) the multi-class emotion analysis unit and 3) the performance prediction model based on input data. SpeechBrain pre-trained models with transformer language models were applied for automated transcription of audio data and converting them to embedding vectors. NLP methods were applied for sentiment analysis. Next, we formed the feature set by combining the extracted emotions with students’ formative assessment grades during the semester to implement a prediction model. We further analyzed which features in the feature set have a higher impact on the students’ academic performance. The early result of this research is promising as we found high accuracy in the predicted scores of the students.","2377-634X","978-1-6654-6244-0","10.1109/FIE56618.2022.9962701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9962701","Automatic Speech Recognition (ASR);emotion analysis;predictive model;academic performance;NLP","Analytical models;Emotion recognition;Speech analysis;Predictive models;Writing;Transformers;Feature extraction","","4","","43","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Identifying and Mitigating Mismatched Language Code in Multilingual ASR","J. Kim; S. Mavandadi; K. Audhkhasi; S. Bharadwaj; B. Farris; T. Chen; B. Ramabhadran; S. Ganapathy","Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA; Google LLC, USA","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Multilingual speech recognition systems often use an input language code in order to prompt the transcription in the target language. However, the spoken language in the input audio may not always match the language code, as often prevalent in multilingual societies. This language mismatch can significantly reduce ASR quality. We present a technique to identify and mitigate this issue. We combine off-the-shelf language-ID and language verification models to determine the language code input to the ASR model. The language verification model acts as a gate that decides when to trust the provided language code or use the output of the language-ID model. We compare these approaches with baselines that include vanilla language-ID based and language-independent ASR models. Our experiments on YouTube, SPRING-INX and FLEURS datasets shows the efficacy of the proposed model especially in the mismatched language code setting.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10887912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10887912","multilingual speech recognition;language verification identification;language conditioning","Codes;Video on demand;Speech coding;Speech recognition;Predictive models;Signal processing;Logic gates;Multilingual;Web sites;Speech processing","","","","36","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"All-Neural Beamformer for Continuous Speech Separation","Z. Zhang; T. Yoshioka; N. Kanda; Z. Chen; X. Wang; D. Wang; S. E. Eskimez","Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6032","6036","Continuous speech separation (CSS) aims to separate overlapping voices from a continuous influx of conversational audio containing an unknown number of utterances spoken by an unknown number of speakers. A common application scenario is transcribing a meeting conversation recorded by a microphone array. Prior studies explored various deep learning models for time-frequency mask estimation, followed by a minimum variance distortionless response (MVDR) filter to improve the automatic speech recognition (ASR) accuracy. The performance of these methods is fundamentally upper-bounded by MVDR’s spatial selectivity. Recently, the all deep learning MVDR (ADL-MVDR) model was proposed for neural beamforming and demonstrated superior performance in a target speech extraction task using pre-segmented input. In this paper, we further adapt ADL-MVDR to the CSS task with several enhancements to enable end-to-end neural beamforming. The proposed system achieves significant word error rate reduction over a baseline spectral masking system on the LibriCSS dataset. Moreover, the proposed neural beamformer is shown to be comparable to a state-of-the-art MVDR-based system in real meeting transcription tasks, including AMI, while showing potentials to further simplify the run-time implementation and reduce the system latency with frame-wise processing.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747261","Continuous speech separation;LibriCSS;AMI;automatic speech recognition;ADL-MVDR","Deep learning;Time-frequency analysis;Runtime;Array signal processing;Error analysis;Estimation;Microphone arrays","","6","","34","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Diffusion-Based Mel-Spectrogram Enhancement for Personalized Speech Synthesis with Found Data","Y. Tian; W. Liu; T. Lee","Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR; Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong SAR",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","7","Creating synthetic voices with found data is challenging, as real-world recordings often contain various types of audio degradation. One way to address this problem is to pre-enhance the speech with an enhancement model and then use the enhanced data for text-to-speech (TTS) model training. This paper investigates the use of conditional diffusion models for generalized speech enhancement, which aims at addressing multiple types of audio degradation simultaneously. The enhancement is performed on the log Mel-spectrogram domain to align with the TTS training objective. Text information is introduced as an additional condition to improve the model robustness. Experiments on real-world recordings demonstrate that the synthetic voice built on data enhanced by the proposed model produces higher-quality synthetic speech, compared to those trained on data enhanced by strong baselines. Code and check-points of the proposed enhancement model are available at https://github.com/dmse4tts/DMSE4TTS.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389748","Personalized speech synthesis;found data;generalized speech enhancement;text-informed speech enhancement;conditional diffusion models","Training;Degradation;Codes;Conferences;Noise reduction;Speech enhancement;Data models","","1","","35","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"VoxMM: Rich Transcription of Conversations in the Wild","D. Kwak; J. Jung; K. Nam; Y. Jang; J. -W. Jung; S. Watanabe; J. S. Chung","Korea Advanced Institute of Science and Technology, South Korea; Korea Advanced Institute of Science and Technology, South Korea; Korea Advanced Institute of Science and Technology, South Korea; Korea Advanced Institute of Science and Technology, South Korea; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Korea Advanced Institute of Science and Technology, South Korea","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","12551","12555","This paper presents a multi-modal dataset that contains rich transcriptions of spoken conversations. As diverse multi-modal and multi-task models emerge, there is a growing need for multi-modal training and evaluation datasets accompanied by rich metadata. However, there is no universal dataset that addresses these requirements for the diverse tasks partially due to the cost of annotation. To overcome this limitation, we develop a semi-automatic pipeline that makes the annotation more feasible. The resulting dataset is VoxMM, a multi-modal, multi-domain dataset. VoxMM incorporates video, audio, and text modalities. In terms of labels, it offers a wide array of metadata such as speaker labels, transcriptions, gender, and more. VoxMM supports both the training and the evaluation of any-to-any modality mapping models. It also offers a more accurate representation of real-world scenarios, bridging the gap between controlled laboratory experiments and the varying performances in the real-world. We present initial benchmarks on automatic speech recognition and speaker diarisation. The VoxMM dataset can be downloaded from https://mm.kaist.ac.kr/projects/voxmm","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446300","Audio-Visual;Dataset;Speech Recognition;Speaker Diarisation;Speaker Recognition","Training;Costs;Annotations;Pipelines;Oral communication;Task analysis;Speech processing","","1","","42","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Whisper in Medusa’s Ear: Multi-head Efficient Decoding for Transformer-based ASR","Y. Segal-Feldman; A. Shamsian; A. Navon; G. Hetz; J. Keshet","aiOla Research, Israel; aiOla Research, Israel; aiOla Research, Israel; aiOla Research, Israel; aiOla Research, Israel","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","Large transformer-based models have significant potential for speech transcription and translation. Their self-attention mechanisms and parallel processing enable them to capture complex patterns and dependencies in audio sequences. However, this potential comes with challenges, as these large and computationally intensive models lead to slow inference speeds. Various optimization strategies have been proposed to improve performance, including efficient hardware utilization and algorithmic enhancements. In this paper, we introduce Whisper-Medusa, a novel approach designed to enhance processing speed with minimal impact on Word Error Rate (WER). The proposed model extends the OpenAI’s Whisper architecture by predicting multiple tokens per iteration, resulting in a 50% reduction in latency. We showcase the effectiveness of Whisper-Medusa across different learning setups and datasets.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10888140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10888140","Automatic speech recognition;Transformers;Speculative decoding;Efficient decoding","Translation;Computational modeling;Signal processing algorithms;Speech recognition;Computer architecture;Signal processing;Predictive models;Transformers;Speech processing;Iterative decoding","","","","20","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset","S. Gautam; M. H. Sarkhoosh; J. Held; C. Midoglu; A. Cioppa; S. Giancola; V. Thambawita; M. A. Riegler; P. Halvorsen; M. Shah","SimulaMet, Norway; OsloMet, Norway; University of Liege, Belgium; SimulaMet, Norway; University of Liege, Belgium; KAUST, Saudi Arabia; SimulaMet, Norway; SimulaMet, Norway; SimulaMet, Norway; University of Central Florida, USA",2024 International Symposium on Multimedia (ISM),"27 Mar 2025","2024","","","71","78","The application of Automatic Speech Recognition (ASR) technology in soccer enables sports analytics by extracting audio commentaries to provide insights into game events and facilitate automatic game understanding. This paper presents SoccerNet-Echoes, an extension of the SoccerNet dataset with automatically generated transcriptions of soccer game broadcasts. Generated using the Whisper model and translated with Google Translate into English when needed, these transcriptions enhance video content with textual information derived from game audio. SoccerNet-Echoes serves as a comprehensive resource for developing algorithms in action spotting, caption generation, and game summarization. Through a series of experiments, we demonstrate that combining modalities—audio, video, and text—yields mixed results on classification tasks. The combination of audio and video shows improved performance over individual modalities, while the addition of ASR text does not significantly enhance results. Additionally, our baseline summarization tasks indicate that ASR content enriches summaries, offering insights beyond event information. This multimodal dataset supports diverse applications, broadening the scope of research in sports analytics. The dataset is available at: https://github.com/SoccerNet/sn-echoes.","","979-8-3315-1111-1","10.1109/ISM63611.2024.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10935951","Automatic Speech Recognition;Multimodal Learning;Sports Analytics;SoccerNet","Translation;Games;Internet;Classification algorithms;Sports;Automatic speech recognition","","","","41","IEEE","27 Mar 2025","","","IEEE","IEEE Conferences"
"Protection Method based on Multiple Sub-Detectors against Audio Adversarial Examples","K. Tamura; H. Ito","Graduate School of Information Sciences, Hiroshima City University, Asa-Minami-Ku, Hiroshima 731-3194, Japan; Faculty of Information Sciences, Hiroshima City University, Asa-Minami-Ku, Hiroshima 731-3194, Japan",2021 IEEE 12th International Workshop on Computational Intelligence and Applications (IWCIA),"30 Nov 2021","2021","","","1","7","Applications with audio speech recognition usually involve personal and authentication information; therefore, security measurement for audio speech recognition is one of the most important issues. Audio adversarial examples are a safety threat in the real world such that automatic speech recognition systems convert an input voice sound to an incorrect text message. Audio adversarial examples deceive the automatic speech recognition systems and they are created by exploiting the vulnerabilities of the deep-learning-based automatic speech recognition systems with the speech-to-text transcription neural network technique. To protect applications with automatic speech recognition, a new method based on multiple sub-detectors against audio adversarial examples is proposed in this study. The proposed protection method has a detector that is composed of three sub-detectors: dynamic-sampling-based, denoising-based, and temporal-dependency-based sub-detectors. In the experiments, we created 1,000 audio adversarial examples for evaluation. These audio adversarial examples are created on Mozilla-implemented Deep Speech. Deep Speech is a deep-learning-based end-to-end ASR system and it is the main attack target of this study. The experimental results of detecting audio adversarial examples show that the protection performance of the new method is higher than those of conventional methods.","1883-3977","978-1-6654-4425-5","10.1109/IWCIA52852.2021.9626023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626023","Adversarial Example;Audio Adversarial Example;Automatic Speech Recognition;Computer Security;Deep Learning","Deep learning;Conferences;Neural networks;Authentication;Detectors;Safety;Computational intelligence","","2","","20","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Confidence Score Based Speaker Adaptation of Conformer Speech Recognition Systems","J. Deng; X. Xie; T. Wang; M. Cui; B. Xue; Z. Jin; G. Li; S. Hu; X. Liu","Chinese University of Hong Kong, Central Ave, Hong Kong; Institute of Software, Chinese Academy of Sciences, Beijing, China; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong; Chinese University of Hong Kong, Central Ave, Hong Kong","IEEE/ACM Transactions on Audio, Speech, and Language Processing","15 Mar 2023","2023","31","","1175","1190","Speaker adaptation techniques provide a powerful solution to customise automatic speech recognition (ASR) systems for individual users. Practical application of unsupervised model-based speaker adaptation techniques to data intensive end-to-end ASR systems is hindered by the scarcity of speaker-level data and performance sensitivity to transcription errors. To address these issues, a set of compact and data efficient speaker-dependent (SD) parameter representations are used to facilitate both speaker adaptive training and test-time unsupervised speaker adaptation of state-of-the-art Conformer ASR systems. The sensitivity to supervision quality is reduced using a confidence score-based selection of the less erroneous subset of speaker-level adaptation data. Two lightweight confidence score estimation modules are proposed to produce more reliable confidence scores. The data sparsity issue, which is exacerbated by data selection, is addressed by modelling the SD parameter uncertainty using Bayesian learning. Experiments on the benchmark 300-hour Switchboard and the 233-hour AMI datasets suggest that the proposed confidence score-based adaptation schemes consistently outperformed the baseline speaker-independent (SI) Conformer model and conventional non-Bayesian, point estimate-based adaptation using no speaker data selection. Similar consistent performance improvements were retained after external Transformer and LSTM language model rescoring. In particular, on the 300-hour Switchboard corpus, statistically significant WER reductions of 1.0%, 1.3%, and 1.4% absolute (9.5%, 10.9%, and 11.3% relative) were obtained over the baseline SI Conformer on the NIST Hub5’00, RT02, and RT03 evaluation sets respectively. Similar WER reductions of 2.7% and 3.3% absolute (8.9% and 10.2% relative) were also obtained on the AMI development and evaluation sets.","2329-9304","","10.1109/TASLP.2023.3250842","Hong Kong RGC GRF(grant numbers:14200021,14200220); Innovation & Technology Fund(grant numbers:ITS/254/19,ITS/218/21); National Key R&D Program of China(grant numbers:2020YFC2004100); National Natural Science Foundation of China(grant numbers:62106255); Guangzhou Civil Affairs Science and Technology Foundation(grant numbers:2022MZK02); Open Research Fund of Guangxi Key Lab of Human-machine Interaction and Intelligent Decision(grant numbers:GXHIID2202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057052","Speech recognition;speaker adaptation;confidence score estimation;bayesian learning;conformer","Adaptation models;Hidden Markov models;Data models;Acoustics;Transformers;Switches;Task analysis","","5","","109","IEEE","1 Mar 2023","","","IEEE","IEEE Journals"
"Summarization of Video into Text and Text to Braille Script","B. Sridhar; G. Saivishnu; V. ManiShanker; D. D. Lakshmi; S. Hariharan","Department of CSE, Vardhaman College of Engineering, Hyderabad, India; Department of CSE, Vardhaman College of Engineering, Hyderabad, India; Department of CSE, Vardhaman College of Engineering, Hyderabad, India; Department of CSE, Vardhaman College of Engineering, Hyderabad, India; Department of Computer Science and Engineering, Vardhaman College of Engineering, Hyderabad, India",2024 International Conference on Knowledge Engineering and Communication Systems (ICKECS),"7 Aug 2024","2024","1","","1","5","In an increasingly digitised world, accessibility to multimedia content is vital for fostering inclusivity and empowering individuals with disabilities. The study presented deals with innovative approach to enhancing accessibility for blind individuals by summarising video content into text and further converting it into Braille script. This dual process seeks to bridge the informational gap that often exists for the blind community, enabling them to access and comprehend a wide array of video content. This dual process aims to bridge the informational gap for the blind community, enabling them to access and comprehend a wide array of video content. The first phase involves video summarization using advanced technologies like automatic speech recognition (ASR) and computer vision. This process captures spoken words, dialogues, and other auditory elements while analysing the visual aspects of the video. The second phase converts the text-based video summary into Braille script, a tactile writing system with raised dots that blind individuals can read by touch. It presents a comprehensive framework for making video content accessible to blind individuals through text summarization and Braille conversion. This approach offers a transformative solution which could tremendously improve the quality of life, independence, and opportunities for the blind community.","","979-8-3503-5968-8","10.1109/ICKECS61492.2024.10617121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10617121","Video Summarization;Braille Script;Video to Text;Text to Braille;Deep Learning;Python;LSTM;Speech Recognition","Bridges;Knowledge engineering;Braille;Visualization;Computer vision;Text recognition;Communication systems","","","","15","IEEE","7 Aug 2024","","","IEEE","IEEE Conferences"
"Intent Recognition and Unsupervised Slot Identification for Low-Resourced Spoken Dialog Systems","A. Gupta; O. Deng; A. Kushwaha; S. Mittal; W. Zeng; S. K. Rallabandi; A. W. Black",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"3 Feb 2022","2021","","","853","860","Intent Recognition and Slot Identification are crucial components in spoken language understanding (SLU) systems. In this paper, we present a novel approach towards both these tasks in the context of low-resourced and unwritten languages. We use an acoustic based SLU system that converts speech to its phonetic transcription using a universal phone recognition system. We build a word-free natural language understanding module that does intent recognition and slot identification from these phonetic transcription. Our proposed SLU system performs competitively for resource rich scenarios and significantly outperforms existing approaches as the amount of available data reduces. We train both recurrent and transformer based neural networks and test our system on five natural speech datasets in five different languages. We observe more than 10% improvement for intent classification in Tamil and more than 5% improvement for intent classification in Sinhala. Additionally, we present a novel approach towards unsupervised slot identification using normalized attention scores. This approach can be used for unsupervised slot labelling, data augmentation and to generate data for a new slot in a one-shot way with only one speech recording.","","978-1-6654-3739-4","10.1109/ASRU51503.2021.9688264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688264","Intent Recognition;Spoken Language Understanding;Transformers;low-resourced;Multilingual","Conferences;Natural languages;Neural networks;Phonetics;Transformers;Acoustics;Labeling","","","","28","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Advanced Video Transcription And Summarization A Synergy of Langchain, Language Models, And VectorDB with Mozilla Deep Speech","K. Lavanya; A. K; V. Dixit; A. S. A","Vellore Institute of Technology-vellore, Vellore, India; Vellore Institute of Technology-vellore, Vellore, India; Vellore Institute of Technology-vellore, Vellore, India; Vellore Institute of Technology-vellore, Vellore, India",2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE),"18 Apr 2024","2024","","","1","9","This paper introduces an advanced automated information management system, addressing the critical need for effective transcription and summarization in the face of the burgeoning volume of video data. The study's objective is to overcome the limitations of existing methods, primarily in handling vast video transcriptions and enhancing information retrieval. We propose an innovative integration of Mozilla Deep Speech, LangChain, VectorDB, and Large Language Models (LLMs), aiming to significantly improve the efficiency and accuracy of document-oriented processes in various sectors. Our approach leverages generative AI to transform the way organizations process video content, offering a solution to the lengthy and often challenging transcription tasks that current technologies struggle with. We highlight the potential economic impact, as underscored by a McKinsey study, indicating a substantial contribution of these technologies to various industries, particularly in customer service, sales, marketing, and software development. The methodology encompasses a comprehensive system architecture, utilizing NLP techniques and models like BERT, GPT, and spaCy, addressing the shortcomings of existing approaches with enhanced precision and adaptability. Initial findings demonstrate a transformative improvement in video summarization, providing significant benefits in education, journalism, and accessibility for the hearing impaired. This research not only offers a novel solution to existing challenges in data management but also sets a new standard for the application of generative AI and LLMs in automated information processing.","","979-8-3503-2820-2","10.1109/ic-ETITE58242.2024.10493791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10493791","Document-Oriented Agents;Workflow Transformation;Video Transcription Summarization;LangChain;Large Language Models;VectorDB;Mozilla Deep Speech;Context-Aware Summarization;Audio-to-Text Translation;Video Analysis;Audio Content Transcription","Analytical models;Generative AI;Biological system modeling;Standards organizations;Systems architecture;Transforms;Organizations","","","","16","IEEE","18 Apr 2024","","","IEEE","IEEE Conferences"
"aaeCAPTCHA: The Design and Implementation of Audio Adversarial CAPTCHA","I. Hossen; X. Hei","School of Computing & Informatics, University of Louisiana at Lafayette, Lafayette, USA; School of Computing & Informatics, University of Louisiana at Lafayette, Lafayette, USA",2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P),"23 Jun 2022","2022","","","430","447","CAPTCHAs are designed to prevent malicious bot programs from abusing websites. Most online service providers deploy audio CAPTCHAs as an alternative to text and image CAPTCHAs for visually impaired users. However, prior research investigating the security of audio CAPTCHAs found them highly vulnerable to automated attacks using Automatic Speech Recognition (ASR) systems. To improve the robustness of audio CAPTCHAs against automated abuses, we present the design and implementation of an audio adversarial CAPTCHA (aaeCAPTCHA) system in this paper. The aaeCAPTCHA system exploits audio adversarial examples as CAPTCHAs to prevent the ASR systems from automatically solving them. Furthermore, we conducted a rigorous security evaluation of our new audio CAPTCHA design against five state-of-the-art DNN-based ASR systems and three commercial Speech-to-Text (STT) services. Our experimental evaluations demonstrate that aaeCAPTCHA is highly secure against these speech recognition technologies, even when the attacker has complete knowledge of the current attacks against audio adversarial examples. We also conducted a usability evaluation of the proof-of-concept implementation of the aaeCAPTCHA scheme. Our results show that it achieves high robustness at a moderate usability cost compared to normal audio CAPTCHAs. Finally, our extensive analysis highlights that aaeCAPTCHA can significantly enhance the security and robustness of traditional audio CAPTCHA systems while maintaining similar usability.","","978-1-6654-1614-6","10.1109/EuroSP53844.2022.00034","US NSF(grant numbers:OIA-1946231,CNS-2117785); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797373","CAPTCHA;security;audio adversarial CAPTCHA;ASR system;Speech-to-Text service","Costs;Robustness;Security;Usability;CAPTCHAs;Automatic speech recognition","","3","","97","IEEE","23 Jun 2022","","","IEEE","IEEE Conferences"
"Romanian Speech-to-Text Transcription for Medical Applications","N. Nitu; A. Catruna; E. Radoi","National Univ. of Science and Technology Politehnica, Bucharest; National Univ. of Science and Technology Politehnica, Bucharest; National Univ. of Science and Technology Politehnica, Bucharest",2024 IEEE 20th International Conference on Intelligent Computer Communication and Processing (ICCP),"17 Dec 2024","2024","","","1","7","Speech recognition models have an important role in improving efficiency and accessibility across various industries. Due to its advantages and benefits, this technology has transformed sectors such as healthcare, automotive, and customer service. However, the speech recognition in low-resource languages such as Romanian remains relatively underdeveloped due to the lack of data for training deep learning models. For this reason it cannot be utilized in the healthcare industry where Romanian specialized terms are unknown to the speech-to-text models. To address this problem, we propose to fine-tune large speech recognition models on a specialized dataset with Romanian audio and medical discourse. We obtain this dataset in a semi-automatic manner by lever-aging speech recognition and large language models. By adapting state-of-the-art fine-tuning approaches that are used for LLMs, we improve the performance of the model on Romanian medical discourse by 19.9% in substitution errors, 28% in deletion errors and 9 % in word error rate. This approach paves the way for speech recognition technology usage across many Romanian industries.","2766-8495","979-8-3315-3997-9","10.1109/ICCP63557.2024.10793032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10793032","","Industries;Training;Adaptation models;Analytical models;Error analysis;Statistical analysis;Computational modeling;Large language models;Medical services;Speech to text","","","","22","IEEE","17 Dec 2024","","","IEEE","IEEE Conferences"
"Automatic Lyrics Transcription of Polyphonic Music With Lyrics-Chord Multi-Task Learning","X. Gao; C. Gupta; H. Li","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; The Chinese University of Hong Kong, Shenzhen, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","26 Jul 2022","2022","30","","2280","2294","Lyrics are the words that make up a song, while chords are harmonic sets of multiple notes in music. Lyrics and chords are generally essential information in music, i.e. unaccompanied singing vocals mixed with instrumental music, representing important components in polyphonic music. In a traditional lyrics transcription task, we first extract the singing vocals from the polyphonic music and then transcribe the resulting singing vocals, where the two steps are optimized independently. In this paper, we propose novel end-to-end network architectures that are designed to disentangle lyrics from chords in polyphonic music for effective lyrics transcription in a single step, where we consider chords as musical words, analogously to lexical words as lyrics intuitively. We start by studying a single-task lyrics transcriber as the reference baseline and the initial model to develop the multi-task lyrics transcription solutions. The main idea is to take advantage of chord transcription available in the training data through multi-task training to improve lyrics transcription. The experiments show that the proposed multitask lyrics transcriber significantly outperforms other competing solutions, with a word error rate (WER) of 31.82% on a standard test dataset.","2329-9304","","10.1109/TASLP.2022.3190742","Academic Research Council; Ministry of Education - Singapore(grant numbers:MOE2018-T2-2-127); Guangdong Provincial Key Laboratory of Big Data Computing; The Chinese University of Hong Kong, Shenzhen, China(grant numbers:B10120210117-KP02); Agency for Science, Technology and Research; AME Programmatic Funding Scheme(grant numbers:A18A2b0046); Agency for Science, Technology and Research; RIE2020 Advanced Manufacturing and Engineering Domain(grant numbers:A1687b0033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833328","Automatic lyrics transcription in polyphonic music;multi-task learning;singing voice separation;music information retrieval","Speech recognition;Decoding;Instruments;Training;Task analysis;Multitasking;Hidden Markov models","","17","","96","CCBYNCND","19 Jul 2022","","","IEEE","IEEE Journals"
"Improved Data Selection for Domain Adaptation in ASR","S. Wotherspoon; W. Hartmann; M. Snover; O. Kimball","Raytheon BBN, Cambridge, MA, USA; Raytheon BBN, Cambridge, MA, USA; Raytheon BBN, Cambridge, MA, USA; Raytheon BBN, Cambridge, MA, USA","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7018","7022","Automatic speech recognition (ASR) systems are highly sensitive to train-test domain mismatch. However, because transcription is often prohibitively expensive, it is important to be able to make use of available transcribed out-of-domain data. We address the problem of domain adaptation with semi-supervised training (SST). Contrary to work in in-domain SST, we find significant performance improvement even with just one hour of target-domain data—though, the selection of the data is critical. We show that minimum phone error rate is a good oracle measure for selection, and we approximate this measure by using the average phone confidence of an utterance. With larger domain shifts, we also find that deletions and low lexical diversity are a serious issue, which we address by incorporating phone rate into our selection metric. With our proposed selection criterion, we see up to 57% relative improvements over the out-of-domain baseline model. Furthermore, this selection method generalizes well, and matches or outperforms word-level confidence selection across six separate domain shift conditions.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413869","Domain adaptation;data selection;semisupervised training","Training;Adaptation models;Error analysis;Conferences;Measurement uncertainty;Signal processing;Acoustics","","4","","26","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Disha: A Bilingual Humanoid Virtual Assistant","M. R. Ullah; M. N. Mahbub; M. A. Hakim; Y. Sultana; I. Alam","Indiana University-Purdue University, Indianapolis, Indiana, USA; Genuity Systems Limited, Dhaka, Bangladesh; Genuity Systems Limited, Dhaka, Bangladesh; Dhaka University, Dhaka, Bangladesh; Genuity Systems Limited, Dhaka, Bangladesh",2024 6th International Conference on Electrical Engineering and Information & Communication Technology (ICEEICT),"23 May 2024","2024","","","457","462","Researchers and tech giants have developed AI-based virtual assistants that can answer basic questions, largely in English. As People are consumed by smart devices in modern life, a virtual assistant can be a game-changer. Surprisingly, no work has been done to adapt AI-based virtual assistants for the Bengali language. In this paper, we introduce a humanoid virtual assistant named “Disha” designed to cater primarily to the banking sector in Bangladesh. Disha is capable of answering customers' financial service-related questions and conducting real-time transactions. Initially, we developed a web-chat system using RASA and trained models for automatic speech recognition (ASR) and text-to-speech (TTS) that operate in both Bengali and English separately. This project not only serves the current banking needs but also contributes to future research, potentially reducing the reliance on human agents and replacing traditional IVR and chatbots in the banking sector.","2769-5700","979-8-3503-8577-9","10.1109/ICEEICT62016.2024.10534321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10534321","Automatic Speech Recognition;RASA;Natural Language Processing;Natural Language Understanding;Human-Computer Interaction;Text-to-Speech;Language Model;Virtual Assistant","Virtual assistants;Humanoid robots;Banking;Chatbots;Real-time systems;Communications technology;Speech processing","","","","39","IEEE","23 May 2024","","","IEEE","IEEE Conferences"
"Unsupervised Model Adaptation for End-to-End ASR","G. Sivaraman; R. Casal; M. Garland; E. Khoury","Pindrop, Atlanta, GA, USA; Pindrop, Atlanta, GA, USA; Pindrop, Atlanta, GA, USA; Pindrop, Atlanta, GA, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6987","6991","End-to-end (E2E) Automatic Speech Recognition (ASR) systems are widely applied in various devices and communication domains. However, state-of-the-art ASR systems are known to underperform when there is a mismatch in the training and test domains. As a result, acoustic models deployed in production are often adapted to the target domain to improve accuracy. This paper proposes a method to perform unsupervised model adaptation for E2E ASR using first-pass transcriptions of adaptation data produced by the baseline ASR model itself. The paper proposes two transcription confidence measures that can be used to select an optimal in-domain adaptation set. Experiments were performed using the Quartznet ASR architecture on the HarperValleyBank corpus. Results show that the unsupervised adaptation technique with the confidence measure based data selection results in a 8% absolute reduction in word error rate on the HarperValleyBank test set. The proposed method can be applied to any E2E ASR system and is suitable for model adaptation on call center audio with little to no manual transcription.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746188","IndexTerms;End-to-end;speech recognition;unsupervised adaptation;confidence measure;call centers;telephony audio","Training;Adaptation models;Vocabulary;Manuals;Acoustic measurements;Loss measurement;Data models","","2","","21","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Using AI to help Preserve Indigenous Oral Histories","G. W. Dueck","Faculty of Computer Science, University of New Brunswick, Fredericton, Canada",2024 IEEE International Humanitarian Technologies Conference (IHTC),"29 Jan 2025","2024","","","1","5","The preservation of Indigenous oral histories is crucial for maintaining cultural heritage. This paper explores the use of artificial intelligence (AI) to enhance the accessibility and accuracy of oral history recordings. By implementing AI-driven tools for speech segmentation, transcription, and translation, we aim to support the documentation and revitalization of endangered Indigenous languages. Our focus includes developing automatic speech recognition (ASR) systems for the Enlhet language and it will be applicable to other indigenous languages. This interdisciplinary approach fosters collaboration between technical experts and Indigenous communities, ensuring the cultural and linguistic knowledge is preserved for future generations.","2837-4800","979-8-3503-5464-5","10.1109/IHTC61819.2024.10855026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10855026","indigenous languages;oral history;transcript generation;language translation;artificial intelligence (AI)","Translation;Collaboration;Documentation;Linguistics;Recording;History;Cultural differences;Global communication;Artificial intelligence;Automatic speech recognition","","","","25","IEEE","29 Jan 2025","","","IEEE","IEEE Conferences"
"Call Translator with Voice Cloning Using Transformers","N. F. Khan; N. Hemanth; N. Goyal; P. KR; P. Agarwal","Computer Science Engineering Dept., PES University, Bangalore, India; Computer Science Engineering Dept., PES University, Bangalore, India; Computer Science Engineering Dept., PES University, Bangalore, India; Computer Science Engineering Dept., PES University, Bangalore, India; Computer Science Engineering Dept., PES University, Bangalore, India",2024 IEEE 9th International Conference for Convergence in Technology (I2CT),"10 Jun 2024","2024","","","1","6","Voice Cloning employs technology and algorithms to create an artificial or synthetic reproduction of a person’s voice. To understand and mimic the distinct vocal qualities of the target speaker, including tone, pitch, cadence, and pronunciation, a machine learning model must be trained on audio samples of the speaker. This project presents itself as a revolutionary approach to the enduring problem of successful cross-lingual interactions, the paper’s novel approach to speech-to-speech machine translation that incorporates voice cloning. This method allows us to synthesize speech in the target language while maintaining the original speaker’s vocal characteristics. The model algorithms such as Whisper AI(WER 5% for transcription, No-Language-Left-Behind-200(44% more accurate than other models) for translation, Tacotron for speech synthesis and multiple transformers for voice cloning will be incorporated. The Voice Cloning model operates effectively on unseen voices, transcending the limitations of voice cloning will be incorporated. The voice cloning model operates effectively on unseen voices, transcending the limitations of relying solely on pre-trained models. The model successfully clones users’ voices in a different language with an approximate latency of 10 seconds.","","979-8-3503-9447-4","10.1109/I2CT61223.2024.10543304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10543304","Voice Cloning;Automatic Speech Recognition Text-To-Speech;Speech-To-Text","Machine learning algorithms;Navigation;Cloning;Speech recognition;Machine learning;Linguistics;Transformers","","1","","25","IEEE","10 Jun 2024","","","IEEE","IEEE Conferences"
"Explanations for Automatic Speech Recognition","X. Wu; P. Bell; A. Rajan","School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh; School of Informatics, University of Edinburgh","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","We address quality assessment for neural network based ASR by providing explanations that help increase our understanding of the system and ultimately help build trust in the system. Compared to simple classification labels, explaining transcriptions is more challenging as judging their correctness is not straightforward and transcriptions as a variable-length sequence is not handled by existing interpretable machine learning models.We provide an explanation for an ASR transcription as a subset of audio frames that is both a minimal and sufficient cause of the transcription. To do this, we adapt existing explainable AI (XAI) techniques from image classification - (1) Statistical Fault Localisation(SFL) [1] and (2) Causal [2]. Additionally, we use an adapted version of Local Interpretable Model-Agnostic Explanations (LIME) [3] for ASR as a baseline in our experiments. We evaluate the quality of the explanations generated by the proposed techniques over three different ASR – Google API [4], the baseline model of Sphinx [5], Deepspeech [6] – and 100 audio samples from the Commonvoice dataset [7].","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094635","Explanation;Automatic Speech Recognition","Adaptation models;Neural networks;Machine learning;Signal processing;Quality assessment;Internet;Speech processing","","3","","23","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Live Transcription and Closed Caption","P. Jaswanth; C. H. Priya; K. Thota; M. Khanna","Dept.of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidhyapeetham, Bangalore, India; Dept.of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidhyapeetham, Bangalore, India; Dept.of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidhyapeetham, Bangalore, India; Dept.of Computer Science and Engineering, Amrita School of Computing, Amrita Vishwa Vidhyapeetham, Bangalore, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","7","This innovative study proposes a novel tool targeted at addressing communication issues for people with hearing impairments. This improvement focuses on smart glasses with real-time captioning for improved accessibility.The main components are a powerful microphone array, a cloud computing platform driven by Raspberry pi zero w, and an advanced Automatic Speech Recognition (ASR) system. These components work together to enable the smart glasses to detect spoken language, process it using cloud-based algorithms, and display accurate captions in real time inside the wearer’s field of view.The primary purpose is to improve communication accessibility for those with hearing impairments by providing timely, accurate, and prominently displayed captions. Beyond personal relationships, the impact extends to a variety of settings, including educational institutions, workplaces, and public spaces, where good communication is essential for involvement.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10724562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10724562","Hearing Impairments;Smart Glasses;Real-Time Captioning;Automatic Speech Recognition;Accessibility Technology","Cloud computing;Accuracy;Prototypes;Auditory system;Glass;Hearing aids;Real-time systems;User experience;Microphone arrays;Smart glasses","","","","14","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"A Multimodal Approach to Multispeaker Summarization and Mind Mapping for Audio Data","H. Modi; A. Patel; I. Joshi; P. Kanani","Department of Computer Engineering, D.J. Sanghvi College of Engineering, Mumbai, India; Department of Computer Engineering, D.J. Sanghvi College of Engineering, Mumbai, India; Department of Computer Engineering, D.J. Sanghvi College of Engineering, Mumbai, India; Department of Computer Engineering, D.J. Sanghvi College of Engineering, Mumbai, India",2023 International Conference on Advanced Computing Technologies and Applications (ICACTA),"23 Jan 2024","2023","","","1","6","With recent global events resulting in a shift in the work setting and way of life worldwide, most educational institutes and corporate organizations have been forced to conduct their day-to-day operations online. Lectures and meetings that are conducted online can be downloaded and users can store these recorded meetings. Owing to the latest technological advancements, storage capacities are no longer a hindrance when storing and using this data. However, storing these recorded meetings will only be helpful when users can access and browse them quickly. This paper proposes implementing a multi-speaker speech summarization model to benefit from the content captured in meeting recordings. Browsing through audio-visual data for information can be a time-consuming task for humans since manual summary generation would require a person to sit through the whole meeting. This can be overcome by combining Natural Language Processing techniques with text summarization to generate the transcript of a meeting. By performing speaker diarization, the various speakers in a multi-speaker meeting are identified, and accordingly, labels are assigned to every participant in the transcript. The model then performs summarization for individual speakers as well as the whole meeting and creates a mind map to represent the meeting minutes visually. Thus, this paper provides an efficient manner of reviewing meetings and labelling them for future reference.","","979-8-3503-4834-7","10.1109/ICACTA58201.2023.10393859","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10393859","Diarization;Transcription;Text Summarization;Mind-Mapping;Natural Language Processing;Transformers;Neural Networks","Training;Computational modeling;Neural networks;Speech recognition;Organizations;Manuals;Predictive models","","","","20","IEEE","23 Jan 2024","","","IEEE","IEEE Conferences"
"Toward Automatic Generation of Transcript From Spoken Lectures: The “Dream of the Red Chamber” Series","T. -H. Lin; K. -L. Lee; H. -Y. Chung; F. -H. F. Wu; J. -C. Li; T. -L. Li; S. -L. Lo; Y. -W. Liu; J. S. Chang","National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan; National Tsing Hua University, Hsinchu, Taiwan",2022 25th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"28 Dec 2022","2022","","","1","6","We present a corpus of Mandarin literature lectures. The recording device, environment, and the topics covered in the lectures are described briefly; then, we developed speech recognition and text simplification approaches for automatic transcription of the lectures. Given that the size of this corpus is relatively small, we applied transfer learning on AISHELL-1 by varying the number of transferred layers and fine-tuning the learning rates to improve the accuracy of speech recognition. Experimental results showed that a character error rate (CER) of 15.83% could be obtained. Additionally, by reducing the perplexity of the language model and the number of out-of-vocabulary words, the CER improved by another 0.29%. Further, to improve the fluency of transcription, we chose to deal with punctuation and pleonasm. Accuracy of 81% and 91.5% were reported by professional judges on adding punctuation and deleting pleonasm, respectively.","2472-7695","979-8-3503-9856-4","10.1109/O-COCOSDA202257103.2022.9997890","Taiwan Semiconductor Manufacturing Company; Office of Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9997890","speech recognition;transfer learning;lecture corpus;text simplification","Vocabulary;Error analysis;Databases;Transfer learning;Speech recognition;Acoustics;Recording","","","","22","IEEE","28 Dec 2022","","","IEEE","IEEE Conferences"
"Transcribe-to-Diarize: Neural Speaker Diarization for Unlimited Number of Speakers Using End-to-End Speaker-Attributed ASR","N. Kanda; X. Xiao; Y. Gaur; X. Wang; Z. Meng; Z. Chen; T. Yoshioka","Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","8082","8086","This paper presents Transcribe-to-Diarize, a new approach for neural speaker diarization that uses an end-to-end (E2E) speaker-attributed automatic speech recognition (SA-ASR). The E2E SA-ASR is a joint model that was recently proposed for speaker counting, multi-talker speech recognition, and speaker identification from monaural audio that contains overlapping speech. Although the E2E SA-ASR model originally does not estimate any time-related information, we show that the start and end times of each word can be estimated with sufficient accuracy from the internal state of the E2E SA-ASR by adding a small number of learnable parameters. Similar to the target-speaker voice activity detection (TS-VAD)-based diarization method, the E2E SA-ASR model is applied to estimate speech activity of each speaker while it has the advantages of (i) handling unlimited number of speakers, (ii) leveraging linguistic information for speaker diarization, and (iii) simultaneously generating speaker-attributed transcriptions. Experimental results on the LibriCSS and AMI corpora show that the proposed method achieves significantly better diarization error rate than various existing speaker diarization methods when the number of speakers is unknown, and achieves a comparable performance to TS-VAD when the number of speakers is given in advance. The proposed method simultaneously generates speaker-attributed transcription with state-of-the-art accuracy.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746225","Speaker diarization;rich transcription;speech recognition;speaker counting","Voice activity detection;Error analysis;Conferences;Signal processing;Linguistics;Acoustics;Automatic speech recognition","","18","","38","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Automatic Subtitle Synchronization and Positioning System Dedicated to Deaf and Hearing Impaired People","B. Mocanu; R. Tapu","Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Bucharest, Romania; Telecommunication Department, Faculty of ETTI, University Politehnica of Bucharest, Bucharest, Romania",IEEE Access,"19 Oct 2021","2021","9","","139544","139555","In this paper, we introduce a subtitle synchronization and positioning system designed to increase the accessibility of deaf and hearing impaired people to multimedia documents. The main contributions of the paper concern: a novel synchronization algorithm able to robustly align, without any human intervention, the closed caption with the audio transcript and a timestamp refinement technique that adjusts the subtitle segments duration with respect to the audiovisual recommendations. Finally, we introduce a novel method that performs a high level understanding of the multimedia content, in order to determine the subtitle optimal positions, within the video frame, such that they do not overlap with other relevant textual information. The experimental evaluation performed on a large dataset of 30 videos taken from the French national television validates the approach with average accuracy scores superior to 90% regardless on the video genre. The subjective evaluation of the proposed subtitle synchronization and positioning system, performed with actual hearing impaired people, demonstrates the effectiveness of our approach.","2169-3536","","10.1109/ACCESS.2021.3119201","Romanian Ministry of Education and Research, CNCS - UEFISCDI(grant numbers:PN-III-P1-1.1-TE-2019-0420,PNCDI III); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565923","Subtitle/closed caption synchronization;audiovisual recommendations;anchor words;tokens;subtitle positioning","Synchronization;Speech recognition;TV;Closed captioning;Hidden Markov models;Auditory system;Training","","2","","31","CCBY","8 Oct 2021","","","IEEE","IEEE Journals"
"Simulating Realistic Speech Overlaps Improves Multi-Talker ASR","M. Yang; N. Kanda; X. Wang; J. Wu; S. Sivasankaran; Z. Chen; J. Li; T. Yoshioka","Carnegie Mellon University, Pittsburgh, PA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA","ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","5 May 2023","2023","","","1","5","Multi-talker automatic speech recognition (ASR) has been studied to generate transcriptions of natural conversation including over-lapping speech of multiple speakers. Due to the difficulty in acquiring real conversation data with high-quality human transcriptions, a naïve simulation of multi-talker speech by randomly mixing multiple utterances was conventionally used for model training. In this work, we propose an improved technique to simulate multi-talker overlap-ping speech with realistic speech overlaps, where an arbitrary pattern of speech overlaps is represented by a sequence of discrete tokens. With this representation, speech overlapping patterns can be learned from real conversations based on a statistical language model, such as N-gram, which can be then used to generate multi-talker speech for training. In our experiments, multi-talker ASR models trained with the proposed method show consistent improvement on the word error rates across multiple datasets.","2379-190X","978-1-7281-6327-7","10.1109/ICASSP49357.2023.10094928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094928","Multi-talker automatic speech recognition;conversation analysis;data simulation","Training;Analytical models;Error analysis;Conferences;Signal processing algorithms;Oral communication;Signal processing","","3","","27","IEEE","5 May 2023","","","IEEE","IEEE Conferences"
"Benchmarking Transformer-Based Transcription on Embedded GPUs for Space Applications","M. E. Schubert; A. D. George","NSF Center for Space, High-Performance, and Resilient Computing (SHREC), University of Pittsburgh, Pittsburgh, PA; NSF Center for Space, High-Performance, and Resilient Computing (SHREC), University of Pittsburgh, Pittsburgh, PA","2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)","7 Dec 2021","2021","","","01","06","Speech transcription is a necessary tool for backend applications commonly found in voice assistants. Transcription is typically performed using cloud-based servers or custom hardware, but those resources are not always amenable to space environments due to size, weight, power, and cost constraints. Therefore, it is important to determine the performance of and optimal conditions for running transcription on hardware that is feasible for deployment in a space application. This research investigates and evaluates the performance of the wav2vec2 speech transcription engine, the current state-of-the-art model for this domain with and without optimizations. The target hardware, the NVIDIA Xavier NX Jetson embedded GPU, was chosen for its modern GPU architecture and small form factor. In addition to examining the input scaling behavior, we evaluate the hyperparameters of the clustered attention optimization, and average power and energy for inference relative to the operating power mode of the device. The clustered attention model outperformed the improved-clustered model for large input sizes, but the wav2vec2 model without clustering performed better for small input sizes. The clustered model energy per inference (13.90 J) was less than energy per inference of the improved-cluster model (15.03 J) and the vanilla softmax model (15.85 J). All models meet real-time speech processing requirements necessary to perform onboard inference entirely on a space system.","2766-2101","978-1-6654-2849-1","10.1109/CONECCT52877.2021.9622586","National Science Foundation(grant numbers:CNS-1738783); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622586","Automatic speech recognition;benchmarking;GPU;machine learning;optimization;parallel processing","Performance evaluation;Adaptation models;Tensors;Runtime;Graphics processing units;Speech recognition;Hardware","","2","","23","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"Hypothesis Stitcher for End-to-End Speaker-Attributed ASR on Long-Form Multi-Talker Recordings","X. Chang; N. Kanda; Y. Gaur; X. Wang; Z. Meng; T. Yoshioka","Center for Language and Speech Processing, Johns Hopkins University, USA; Microsoft Corp., USA; Microsoft Corp., USA; Microsoft Corp., USA; Microsoft Corp., USA; Microsoft Corp., USA","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","6763","6767","An end-to-end (E2E) speaker-attributed automatic speech recognition (SA-ASR) model was proposed recently to jointly perform speaker counting, speech recognition and speaker identification. The model achieved a low speaker-attributed word error rate (SA-WER) for monaural overlapped speech comprising an unknown number of speakers. However, the E2E modeling approach is susceptible to the mismatch between the training and testing conditions. It has yet to be investigated whether the E2E SA-ASR model works well for recordings that are much longer than samples seen during training. In this work, we first apply a known decoding technique that was developed to perform single-speaker ASR for long-form audio to our E2E SA-ASR task. Then, we propose a novel method using a sequence-to-sequence model, called hypothesis stitcher. The model takes multiple hypotheses obtained from short audio segments that are extracted from the original long-form input, and it then outputs a fused single hypothesis. We propose several architectural variations of the hypothesis stitcher model and compare them with the conventional decoding methods. Experiments using LibriSpeech and LibriCSS corpora show that the proposed method significantly improves SA-WER especially for long-form multi-talker recordings.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414432","Hypothesis stitcher;speech recognition;speaker identification;rich transcription","Training;Error analysis;Computational modeling;Conferences;Computer architecture;Signal processing;Decoding","","3","","21","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Automated Youtube Video Transcription To Summarized Text Using Natural Language Processing","P. Nagaraj; V. Muneeswaran.; B. Rohith; B. Sai Vasanth; G. Veda Varshith Reddy; A. Koushik Teja","Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Virudhunagar, India; Department of Electronics and Communication Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Virudhunagar, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Virudhunagar, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Virudhunagar, India; Department of Electronics and Communication Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Virudhunagar, India; Department of Computer Science and Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Virudhunagar, India",2023 International Conference on Computer Communication and Informatics (ICCCI),"24 May 2023","2023","","","1","6","This paper proposes video transcription using python language. With the adding quantum of videotape data generated every day, it has come important to epitomize the videos for faster reclamation and quick surfing of vids so that druggies can select the more applicable videotape for viewing as per their demand. The significance of videotape summarization lies in the fact that it helps in effective storage and allows quick browsing through a large number of videos. We propose a system to induce summaries for videos by utilizing the audio element. Our videotape summarization approach involves generating the audio paraphrase, using speech recognition, if it isn’t readily available. It’s grounded on assigning scores to rulings in the reiterations and opting for the bones with the loftiest scores. Also from the original videotape, parts corresponding to the named rulings are uprooted and intermingled to gain the final summary.","2473-7577","979-8-3503-4821-7","10.1109/ICCCI56745.2023.10128375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128375","Summarization;Speech Recognition;Transcription;Captions;NLP","Video on demand;Text recognition;Speech recognition;Bones;Natural language processing;Informatics","","1","","37","IEEE","24 May 2023","","","IEEE","IEEE Conferences"
"Optimizing Direct Speech-to-Text Translation for un-orthographic low-resource tribal languages using source transliterations","T. Rajkhowa; A. Roy Chowdhury; P. Bannulmath; D. K.T.; S. R. M. Prasanna","Dept. of Electrical Engineering, IIT Dharwad, Dharwad, India; Dept. of Electrical Engineering, IIT Dharwad, Dharwad, India; Dept. of Electronics and Communication, IIIT Dharwad, Dharwad, India; Dept. of Electronics and Communication, IIIT Dharwad, Dharwad, India; Dept. of Electrical Engineering, IIT Dharwad, Dharwad, India",2023 26th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"2 Apr 2024","2023","","","1","6","Spoken language technologies such as Direct Speech-to-Text Translation systems prove instrumental for documenting high-resourced languages in textual format in other languages. They may not require transcriptions as these systems can provide good quality translation. However, for low-resourced languages that do not have a standardized written form, it becomes a challenge. The purpose of this study is to investigate the role of source language transcription in low-resource settings for Direct Speech-to-Text Translation(DS2TT) and to optimize the performance using only the source audio and transliterated transcription and target text of the given dataset. To begin the work, corpora containing audios from two scriptless tribal languages viz. Lambani and Soliga were recorded and transliterated into Kannada which were then translated into English manually. The utilization of transliterated source language text during training was effective in improving the performance when compared to system where source language transcription was not used. Additionally, this method of transliterating could be useful for documenting languages that do not have a standard written form.","2472-7695","979-8-3503-4402-8","10.1109/O-COCOSDA60357.2023.10482960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10482960","Spoken language technology;Direct Speech-to-Text Translation;transliteration","Training;Databases;Instruments;Machine translation;Task analysis;Standards;Automatic speech recognition","","","","45","IEEE","2 Apr 2024","","","IEEE","IEEE Conferences"
"AI-enabled Audio and Chat Collaboration Services","E. P. Andersen; J. R. Goksør; S. E. Halleraker; F. T. Johnsen; S. Kvalø; O. P. Myhre; T. S. Omdal; H. H. Seternes; L. S. Thorstad","Norwegian Defence Research Establishment (FFI), Kjeller, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian Defence Research Establishment (FFI), Kjeller, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway; Norwegian University of Science and Technology (NTNU), Trondheim, Norway",MILCOM 2024 - 2024 IEEE Military Communications Conference (MILCOM),"6 Dec 2024","2024","","","728","733","In this paper we investigate an approach to improve audio services for use at the tactical edge where networks can be characterized as Disconnected, Intermittent and Limited (DIL). We look at using newer artificial intelligence speech recognition systems, namely Vosk and OpenAI’s Whisper, to bring transcription functionality to the services. Allowing services to convert voice audio to text will reduce the strain on the network, which is an important aspect to consider in DIL environments.To demonstrate our approach to improve audio services, we introduce a speech-to-text (STT) application that implements both Vosk and Whisper as transcriber modules. The application builds on a technology stack with three parts that includes transcription, messaging and Voice over IP. In addition to having STT functionality, we also implement the reverse: a text-to-speech module that translates a text message back to audio for the recipient.The paper discusses the design and architecture of the application, detailing how the technology stack is built using a set of technologies that benefit audio services that are used in DIL networks. The application needs to work at the tactical edge where resources are sparse, and we therefore evaluate the implemented transcribers with regard to resource use. Finally, we investigate the accuracy of both transcribers to assess the quality they deliver.","2155-7586","979-8-3503-7423-0","10.1109/MILCOM61039.2024.10773645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10773645","","Military communication;Accuracy;Collaboration;Text to speech;Internet telephony;Artificial intelligence;Electronics packaging;Strain;Speech to text","","","","31","IEEE","6 Dec 2024","","","IEEE","IEEE Conferences"
"Acoustics Based Intent Recognition Using Discovered Phonetic Units for Low Resource Languages","A. Gupta; X. Li; S. K. Rallabandi; A. W. Black",Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7453","7457","With recent advancements in language technologies, humans are now speaking to devices. Increasing the reach of spoken language technologies requires building systems in local languages. A major bottleneck here are the underlying data-intensive parts that make up such systems, including automatic speech recognition (ASR) systems that require large amounts of labelled data. With the aim of aiding development of spoken dialog systems in low resourced languages, we propose a novel acoustics based intent recognition system that uses discovered phonetic units for intent classification. The system is made up of two blocks - the first block is a universal phone recognition system that generates a transcript of discovered phonetic units for the input audio, and the second block performs intent classification from the generated phonetic transcripts. We propose a CNN+LSTM based architecture and present results for two languages families - Indic languages and Romance languages, for two different intent recognition tasks. We also perform multilingual training of our intent classifier and show improved cross-lingual transfer and zero-shot performance on an unknown language within the same language family.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9415112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415112","Intent;Low Resource;Cross Lingual;Multilingual;Long short term Memory","Training;Conferences;Buildings;Phonetics;Signal processing;Acoustics;Task analysis","","5","","16","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"COCO-NUT: Corpus of Japanese Utterance and Voice Characteristics Description for Prompt-Based Control","A. Watanabe; S. Takamichi; Y. Saito; W. Nakata; D. Xin; H. Saruwatari","The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","8","In text-to-speech, controlling voice characteristics is important in achieving various-purpose speech synthesis. Considering the success of text-conditioned generation, such as text-to-image, free-form text instruction should be useful for intuitive and complicated control of voice characteristics. A sufficiently large corpus of high-quality and diverse voice samples with corresponding free-form descriptions can advance such control research. However, neither an open corpus nor a scalable method is currently available. To this end, we develop Coco-Nut, a new corpus including diverse Japanese utterances, along with text transcriptions and free-form voice characteristics descriptions. Our methodology to construct this corpus consists of 1) automatic collection of voice-related audio data from the Internet, 2) quality assurance, and 3) manual annotation using crowdsourcing. Additionally, we benchmark our corpus on the prompt embedding model trained by contrastive speech-text learning.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389693","University of Tokyo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389693","Speech synthesis;speech dataset;voice characteristics;text prompt;crowdsourcing","Crowdsourcing;Quality assurance;Annotations;Conferences;Manuals;Benchmark testing;Speech synthesis","","3","","42","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"Content-Aware Speaker Embeddings for Speaker Diarisation","G. Sun; D. Liu; C. Zhang; P. C. Woodland","Cambridge University Engineering Dept., Cambridge, U.K.; Cambridge University Engineering Dept., Cambridge, U.K.; Cambridge University Engineering Dept., Cambridge, U.K.; Cambridge University Engineering Dept., Cambridge, U.K.","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7168","7172","Recent speaker diarisation systems often convert variable length speech segments into fixed-length vector representations for speaker clustering, which are known as speaker embeddings. In this paper, the content-aware speaker embeddings (CASE) approach is proposed, which extends the input of the speaker classifier to include not only acoustic features but also their corresponding speech content, via phone, character, and word embeddings. Compared to alternative methods that leverage similar information, such as multitask or adversarial training, CASE factorises automatic speech recognition (ASR) from speaker recognition to focus on modelling speaker characteristics and correlations with the corresponding content units to derive more expressive representations. CASE is evaluated for speaker re-clustering with a realistic speaker diarisation setup using the AMI meeting transcription dataset, where the content information is obtained by performing ASR based on an automatic segmentation. Experimental results showed that CASE achieved a 17.8% relative speaker error rate reduction over conventional methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414390","Cambridge Trust; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414390","content-aware speaker embedding;diarisation;d-vector;speech recognition;distributed representation","Training;Computer aided software engineering;Error analysis;Manuals;Signal processing;Feature extraction;Acoustics","","2","","38","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Transcription, Translation and Summarization to Improve Educational Understanding","R. V. Kulkarni; A. Shilimkar; S. Bandi; S. Bhegade; J. Dadmal","Department of Computer Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Computer Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Computer Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Computer Engineering, Vishwakarma Institute of Technology, Pune, India; Department of Computer Engineering, Vishwakarma Institute of Technology, Pune, India",2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG),"13 Mar 2025","2024","","","1","7","In-depth research on improving educational comprehension with the use of summarization, translation, and transcription technology is presented in this work. With an emphasis on accuracy and efficiency, a comparative analysis of the different models employed in these processes is carried out. In particular, we investigate the use of Whisper AI for transcription tasks and a Python interface for text summarization when implementing BART (Bidirectional and Auto-Regressive Transformers). Our results show that Whisper AI greatly increases transcription speed and accuracy, whereas BART excels at producing clear and succinct summaries. Better educational content distribution can be enabled by incorporating these modern techniques, which will make the material easier for teachers and students to understand. This research has many kinds of practical applications, from translating instructional materials between languages to producing succinct summarized notes and then providing flashcards and quizzes based on that.","","979-8-3315-1898-1","10.1109/ICTBIG64922.2024.10911052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10911052","Translation;Transcription;Bart;Whisper;Automatic speech recognition","Technological innovation;Translation;Accuracy;Text summarization;Media;User interfaces;Transformers;Multilingual;Noise measurement;Artificial intelligence","","","","20","IEEE","13 Mar 2025","","","IEEE","IEEE Conferences"
"Phone-to-Audio Alignment without Text: A Semi-Supervised Approach","J. Zhu; C. Zhang; D. Jurgens","Department of Linguistics, University of Michigan, Ann Arbor, USA; Center for Language Studies, Radboud University, Nijmegen, Netherlands; School of Information, University of Michigan, Ann Arbor, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","8167","8171","The task of phone-to-audio alignment has many applications in speech research. Here we introduce two Wav2Vec2-based models for both text-dependent and text-independent phone-to-audio alignment. The proposed Wav2Vec2-FS, a semi-supervised model, directly learns phone-to-audio alignment through contrastive learning and a forward sum loss, and can be coupled with a pretrained phone recognizer to achieve text-independent alignment. The other model, Wav2Vec2-FC, is a frame classification model trained on forced aligned labels that can both perform forced alignment and text-independent segmentation. Evaluation results suggest that both proposed methods, even when transcriptions are not available, generate highly close results to existing forced alignment tools. Our work presents a neural pipeline of fully automated phone-to-audio alignment. Code and pretrained models are available at https://github.com/lingjzhu/charsiu.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746112","forced alignment;phone segmentation;deep learning;automatic speech recognition","Codes;Conferences;Pipelines;Speech recognition;Signal processing;Acoustics;Task analysis","","15","","32","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Adapting Pretrained Speech Model for Mandarin Lyrics Transcription and Alignment","J. -Y. Wang; C. -I. Leong; Y. -C. Lin; L. Su; J. -S. R. Jang","National Taiwan University, Taiwan; National Taiwan University, Taiwan; National Taiwan University, Taiwan; Academia Sinica, Institute of Information Science, Taiwan; National Taiwan University, Taiwan",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","8","The tasks of automatic lyrics transcription and lyrics alignment have witnessed significant performance improvements in the past few years. However, most of the previous works only focus on English in which large-scale datasets are available. In this paper, we address lyrics transcription and alignment of polyphonic Mandarin pop music in a low-resource setting. To deal with the data scarcity issue, we adapt pretrained Whisper model and fine-tune it on a monophonic Mandarin singing dataset. With the use of data augmentation and source separation model, results show that the proposed method achieves a character error rate of less than 18% on a Mandarin polyphonic dataset for lyrics transcription, and a mean absolute error of 0.071 seconds for lyrics alignment. Our results demonstrate the potential of adapting a pretrained speech model for lyrics transcription and alignment in low-resource scenarios.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389800","Automatic lyrics transcription;automatic lyrics alignment;data augmentation;model adaptation","Industries;Adaptation models;Source separation;Error analysis;Conferences;Data augmentation;Data models","","2","","27","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"Ear Assist for Hearing Impaired","A. K. J; A. Xavier; H. Mubarak; K. K. Ashfaq; S. M. Abdulla","Dept. of ECE, KMEA Engineering College, Ernakulam, India; Dept. of ECE, KMEA Engineering College, Ernakulam, India; Dept. of ECE, KMEA Engineering College, Ernakulam, India; Dept. of ECE, KMEA Engineering College, Ernakulam, India; Dept of ECE, KMEA Engineering College, Ernakulam, India",2024 International Conference on Futuristic Technologies in Control Systems & Renewable Energy (ICFCR),"28 Nov 2024","2024","","","1","6","Hearing impairment, affecting over 5 percent of the global population, presents challenges in communication, relationships, and overall well-being. Ear Assist(EA), a smart glass integrating automatic voice recognition and transcription technologies, addresses these challenges by enhancing accessibility and communication for individuals with hearing impairment. Leveraging Automatic Speech Recognition (ASR) technology, Ear Assist offers real-time transcription of spoken words, facilitating smoother interactions in social, educational, and professional settings. This paper examines the impact of hearing impairment on daily life and the significance of overcoming accessibility barriers. It explores the technological advancements of Ear Assist and its potential to transform the lives of hearing impaired individuals. Through empirical evidence and user testimonials, this paper underscores Ear Assist's efficacy in bridging communication gaps and improving the quality of life for users. Moreover, it discusses the broader implications of adopting such assistive technologies in promoting inclusivity and social equity. By offering a multidisciplinary perspective, this paper reveals innovative solutions to support individuals with hearing impairment.","","979-8-3315-0460-1","10.1109/ICFCR64128.2024.10762968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10762968","Hearing impairment;Communication;Ear Assist;Smart glass;Automatic Voice Recognition;Transcription;Automatic Speech Recognition Technology","Visualization;Auditory system;Ear;Transforms;Organic light emitting diodes;Assistive technologies;User interfaces;Usability;Microphones;Testing","","","","18","IEEE","28 Nov 2024","","","IEEE","IEEE Conferences"
"Universal Cross-Lingual Data Generation for Low Resource ASR","W. Wang; Y. Qian","Auditory Cognition and Computational Acoustics Lab, the Department of Computer Science and Engineering and the MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China; Auditory Cognition and Computational Acoustics Lab, the Department of Computer Science and Engineering and the MoE Key Laboratory of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","5 Jan 2024","2024","32","","973","983","Significant advances in end-to-end (E2E) automatic speech recognition (ASR) have primarily been concentrated on languages rich in annotated data. Nevertheless, a large proportion of languages worldwide, which are typically low-resource, continue to pose significant challenges. To address this issue, this study presents a novel speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. In our framework, the SSL phonetic units serve as crucial bridges between speech and text across different languages. By leveraging these units, we successfully splice speech fragments from high-resource languages into synthesized speech that maintains acoustic coherence with text from low-resource languages. To further enhance the practicality of the framework, we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. The application of this confidence sampling strategy in data splicing significantly accelerates ASR model convergence and enhances overall ASR performance. Experimental results on the CommonVoice dataset show 25-35% relative improvement for four Indo-European languages and about 20% for Turkish using a 4-gram language model for rescoring, under a 10-hour low-resource setup. Furthermore, we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing, resulting in an additional 10% relative improvement.","2329-9304","","10.1109/TASLP.2023.3345150","China NSFC projects(grant numbers:62122050,62071288); Shanghai Municipal Science and Technology Commission Project(grant numbers:2021SHZDZX0102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10372076","Low-resource speech recognition;text-to-seech;data splicing;self-supervised learning","Splicing;Data models;Phonetics;Training;Speech synthesis;Dictionaries;Data mining","","","","56","IEEE","22 Dec 2023","","","IEEE","IEEE Journals"
"Who Said what? An Automated Approach to Analyzing Speech in Preschool Classrooms","A. Sun; J. J. Londono; B. Elbaum; L. Estrada; R. J. Lazo; L. Vitale; H. G. Villasanti; R. Fusaroli; L. K. Perry; D. S. Messinger","Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA; Department of Psychology, University of Miami, Coral Gables, FL, USA; Department of Teaching and Learning, University of Miami, Coral Gables, FL, USA; Department of Psychology, University of Miami, Coral Gables, FL, USA; Department of Psychology, University of Miami, Coral Gables, FL, USA; Department of Psychology, University of Miami, Coral Gables, FL, USA; Department of Mechanical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Linguistics, Cognitive Science and Semiotics, Aarhus University, Aarhus C, Denmark; Department of Psychology, University of Miami, Coral Gables, FL, USA; Department of Psychology, University of Miami, Coral Gables, FL, USA",2024 IEEE International Conference on Development and Learning (ICDL),"27 Aug 2024","2024","","","1","8","Young children spend substantial portions of their waking hours in noisy preschool classrooms. In these environments, children's vocal interactions with teachers are critical contributors to their language outcomes, but manually transcribing these interactions is prohibitive. Using audio from child- and teacher-worn recorders, we propose an automated framework that uses open source software both to classify speakers (ALICE) and to transcribe their utterances (Whisper). We compare results from our framework to those from a human expert for 110 minutes of classroom recordings, including 85 minutes from child-word microphones ($\mathrm{n}=4$ children) and 25 minutes from teacher-worn microphones ($\mathrm{n}=2$ teachers). The overall proportion of agreement, that is, the proportion of correctly classified teacher and child utterances, was. 76, with an error-corrected kappa of. 50 and a weighted F1 of. 76. The word error rate for both teacher and child transcriptions was. 15, meaning that 15% of words would need to be deleted, added, or changed to equate the Whisper and expert transcriptions. Moreover, speech features such as the mean length of utterances in words, the proportion of teacher and child utterances that were questions, and the proportion of utterances that were responded to within 2.5 seconds were similar when calculated separately from expert and automated transcriptions. The results suggest substantial progress in analyzing classroom speech that may support children's language development. Future research using natural language processing is under way to improve speaker classification and to analyze results from the application of the automated framework to a larger dataset containing classroom recordings from 13 children and 3 teachers observed on 17 occasions over one year.","","979-8-3503-4855-2","10.1109/ICDL61372.2024.10644508","National Science Foundation(grant numbers:2150830); Simons Foundation Autism Research Initiative(grant numbers:SFI-AR-HUMAN-00004115-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10644508","Natural Language Processing;Preschool;Speech Features;Machine Learning;Automatic Transcription","Error analysis;Current measurement;Pipelines;Natural language processing;Recording;Reliability;Noise measurement","","1","","38","IEEE","27 Aug 2024","","","IEEE","IEEE Conferences"
"Feature Engineering and Stacked Echo State Networks for Musical Onset Detection","P. Steiner; A. Jalalvand; S. Stone; P. Birkholz","Institute for Acoustics and Speech Communication, Technische Universität Dresden, Dresden, Germany; Ghent University - imec, IDLab, Ghent, Belgium; Institute for Acoustics and Speech Communication, Technische Universität Dresden, Dresden, Germany; Institute for Acoustics and Speech Communication, Technische Universität Dresden, Dresden, Germany",2020 25th International Conference on Pattern Recognition (ICPR),"5 May 2021","2021","","","9537","9544","In music analysis, one of the most fundamental tasks is note onset detection - detecting the beginning of new note events. As the target function of onset detection is related to other tasks, such as beat tracking or tempo estimation, onset detection is the basis for such related tasks. Furthermore, it can help to improve Automatic Music Transcription (AMT). Typically, different approaches for onset detection follow a similar outline: An audio signal is transformed into an Onset Detection Function (ODF), which should have rather low values (i.e. close to zero) for most of the time but with pronounced peaks at onset times, which can then be extracted by applying peak picking algorithms on the ODF. In the recent years, several kinds of neural networks were used successfully to compute the ODF from feature vectors. Currently, Convolutional Neural Networks (CNNs) define the state of the art. In this paper, we build up on an alternative approach to obtain a ODF by Echo State Networks (ESNs), which have achieved comparable results to CNNs in several tasks, such as speech and image recognition. In contrast to the typical iterative training procedures of deep learning architectures, such as CNNs or networks consisting of Long-Short-Term Memory Cells (LSTMs), in ESNs only a very small part of the weights is easily trained in one shot using linear regression. By comparing the performance of several feature extraction methods, pre-processing steps and introducing a new way to stack ESNs, we expand our previous approach to achieve results that fall between a bidirectional LSTM network and a CNN with relative improvements of 1.8 % and -1.4 %, respectively. For the evaluation, we used exactly the same 8-fold cross validation setup as for the reference results.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9413205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413205","Reservoir computing;echo state networks;note onset detection","Training;Target tracking;Image recognition;Neural networks;Linear regression;Music;Speech recognition","","5","","14","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Any-to-Any Voice Conversion With Multi-Layer Speaker Adaptation and Content Supervision","X. Xu; L. Shi; X. Chen; P. Lin; J. Lian; J. Chen; Z. Zhang; E. R. Hancock","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; Graduate School of System Informatics, Kobe University, Kobe, Japan; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; Faculty of Systems Engineering, Wakayama University, Wakayama, Japan; School of Informatics, Xiamen University, Xiamen, China; Department of Computer Science, University of York, York, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","23 Oct 2023","2023","31","","3431","3445","Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.","2329-9304","","10.1109/TASLP.2023.3306716","National Natural Science Foundation of China(grant numbers:62176227,U2066213); Fundamental Research Funds for the Central Universities(grant numbers:20720210047); JSPS KAKENHI(grant numbers:19H00597); Research Support Fund of Wakayama University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10224343","Voice conversion;attention mechanism;contrastive learning;feature disentanglement","Timbre;Semantics;Linguistics;Feature extraction;Training;Mutual information;Automatic speech recognition","","1","","58","IEEE","18 Aug 2023","","","IEEE","IEEE Journals"
"ASR Model Adaptation for Rare Words Using Synthetic Data Generated by Multiple Text-To-Speech Systems","K. C. Yuen; L. Haoyang; C. E. Siong","Nangyang Technological University, Singapore; Nangyang Technological University, Singapore; Nangyang Technological University, Singapore",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","1771","1778","Automatic speech recognition (ASR) for rare words is difficult as there are little relevant text-audio data pairs to train an ASR model. To obtain more text-audio pairs, text-only data are fed to Text-To-Speech (TTS) systems to generate synthetic audio. Previous works use a single TTS system conditioned on multiple speakers to produce different speaker voices to improve the output data’s speaker diversity, and they show that training an ASR model on the more diverse data can avoid overfitting and improve the model’s robustness. As an alternative way to improve the diversity, we study the speaker embedding distribution of audios synthesized by different TTS systems and found that the audios synthesized by different TTS systems have different speaker distributions even when they are conditioned on the same speaker. Inspired by this, this paper proposes to condition multiple TTS systems repeatedly on a single speaker to synthesize more diverse speaker data, so ASR models can be trained more robustly. When we apply our method to a rare word dataset partitioned from National Speech Corpus SG, which contains mostly road names and addresses in its text transcripts, experiments show that a pretrained ASR model adapted to our multi-TTS-same-SPK data gives relatively 9.8% lower word error rate (WER) compared to the ASR models adapted to same-TTS-multi-SPK data of the same data size, and our overall adaptation improves the model’s WER from 57.6% to 16.5% without using any real audio as training data.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317116","","Training;Adaptation models;Error analysis;Roads;Training data;Information processing;Data models","","1","","51","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Speaker Diarization in Multispeaker and Multilingual Scenarios","P. Rudrresh; P. V. T. Raj; K. Hariharan; S. Sivaprasath; C. S. Shibi; J. Jeyasudha; C. M. Vikram; G. K. Agarwal; R. P. Gohil","Department of Computational Intelligence, SRM Institute of Science and Technology, Chennai, India; Department of Computational Intelligence, SRM Institute of Science and Technology, Chennai, India; Department of Computational Intelligence, SRM Institute of Science and Technology, Chennai, India; Department of Computational Intelligence, SRM Institute of Science and Technology, Chennai, India; Department of Computational Intelligence, SRM Institute of Science and Technology, Chennai, India; Department of Computational Intelligence, SRM Institute of Science and Technology, Chennai, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India; Samsung R&D Institute, Bangalore, India",2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT),"4 Nov 2024","2024","","","1","7","Speaker diarization is a process in automatic speech processing that involves segmenting and labeling an audio recording based on distinct speakers. The goal is to identify and differentiate speakers within the audio. A comprehensive approach to multilingual speaker diarization, crucial for understanding and processing speech across diverse languages, was implemented. The Pyannote library has been utilized to develop a custom speaker diarization model and finetune it for the in-house multi lingual conversation dataset. Using PyanNet segmentation model, segmentation of audio recordings was done to identify speech regions. Speaker embeddings are extracted using the ECAPA-TDNN architecture, pretrained on Vox-Celeb data and fine-tuned on a unique multilingual dataset featuring six languages and 100 audio samples, split into training, development and testing sets for rigorous evaluation. Agglomerative clustering was employed and compared the system’s output with manually generated Rich Transcription Time Marked (RTTM) with ground truth speaker segments and labels, alongside ECAPA-TDNN, X-vector MFCC and X-Vector SincNet embedding models. This study advances the understanding of multilingual speaker diarization, offering insights into its potential applications in transcription, voice biometrics, and multilingual voice assistants.","2473-7674","979-8-3503-7024-9","10.1109/ICCCNT61001.2024.10725901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10725901","Speaker Diarization;Automatic Speaker Recognition (ASR);Multilingual Speech Diarization;ECAPA-TDNN;x-Vector SincNet;x-vector MFCC","Training;Analytical models;Biological system modeling;Feature extraction;Audio recording;Speaker recognition;Mel frequency cepstral coefficient;Speech processing;Research and development;Testing","","","","18","IEEE","4 Nov 2024","","","IEEE","IEEE Conferences"
"Confides: A Visual Analytics Solution for Automated Speech Recognition Analysis and Exploration","S. Ha; C. Lim; R. J. Crouser; A. Ottley",NA; NA; NA; NA,2024 IEEE Visualization and Visual Analytics (VIS),"2 Dec 2024","2024","","","271","275","Confidence scores of automatic speech recognition (ASR) outputs are often inadequately communicated, preventing its seamless integration into analytical workflows. In this paper, we introduce Confides, a visual analytic system developed in collaboration with intelligence analysts to address this issue. Confides aims to aid exploration and post-AI-transcription editing by visually representing the confidence associated with the transcription. We demonstrate how our tool can assist intelligence analysts who use ASR outputs in their analytical and exploratory tasks and how it can help mitigate misinterpretation of crucial information. We also discuss opportunities for improving textual data cleaning and model transparency for human-machine collaboration.","2771-9553","979-8-3503-5485-0","10.1109/VIS55277.2024.00062","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10771110","Visual analytics;confidence visualization;automatic speech recognition","Analytical models;Uncertainty;Visual analytics;Human-machine systems;Collaboration;Data visualization;Cleaning;Data models;Speech to text;Automatic speech recognition","","","","32","IEEE","2 Dec 2024","","","IEEE","IEEE Conferences"
"Hear ""No Evil"", See ""Kenansville"": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems","H. Abdullah; M. S. Rahman; W. Garcia; K. Warren; A. S. Yadav; T. Shrimpton; P. Traynor",University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida; University of Florida,2021 IEEE Symposium on Security and Privacy (SP),"26 Aug 2021","2021","","","712","729","Automatic speech recognition and voice identification systems are being deployed in a wide array of applications, from providing control mechanisms to devices lacking traditional interfaces, to the automatic transcription of conversations and authentication of users. Many of these applications have significant security and privacy considerations. We develop attacks that force mistranscription and misidentification in state of the art systems, with minimal impact on human comprehension. Processing pipelines for modern systems are comprised of signal preprocessing and feature extraction steps, whose output is fed to a machine-learned model. Prior work has focused on the models, using white-box knowledge to tailor model-specific attacks. We focus on the pipeline stages before the models, which (unlike the models) are quite similar across systems. As such, our attacks are black-box, transferable, can be tuned to require zero queries to the target, and demonstrably achieve mistranscription and misidentification rates as high as 100% by modifying only a few frames of audio. We perform a study via Amazon Mechanical Turk demonstrating that there is no statistically significant difference between human perception of regular and perturbed audio. Our findings suggest that models may learn aspects of speech that are generally not perceived by human subjects, but that are crucial for model accuracy.","2375-1207","978-1-7281-8934-5","10.1109/SP40001.2021.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519472","","Training;Privacy;Perturbation methods;Pipelines;Force;Feature extraction;Robustness","","37","","85","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"Automatic Diagnosis and Prediction of Cognitive Decline Associated with Alzheimer’s Dementia through Spontaneous Speech","Z. Liu; L. Proctor; P. N. Collier; X. Zhao","Department of Mechanical, Aerospace, and Biomedical Engineering, University of Tennessee, Knoxville, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; Department of Mechanical, Aerospace, and Biomedical Engineering, University of Tennessee, Knoxville, USA",2021 IEEE International Conference on Signal and Image Processing Applications (ICSIPA),"20 Oct 2021","2021","","","39","43","With the increasing prevalence of Alzheimer’s disease (AD), it is important to develop detectable biomarkers to reliably identify AD in the early stage. Language deficit is one of the common signs that appear in the early stage of mild Alzheimer’s disease. Therefore, using natural language processing and related machine learning algorithms for AD diagnosis using patients’ speech recordings has drawn more attention in recent years. In this study, three approaches are proposed to extract features through speech recording in this model: (1) using fine-tuning pre-trained encoder model (BERT) for transcripts from automatic transcription, (2) hand-crafted linguistic features for transcripts from automatic transcription, and (3) selected acoustic features for denoised speech recordings. The three designed approaches are applied to three tasks: AD diagnosis, MMSE score prediction, and cognitive decline inference. The approach using BERT yields the best performance in all three challenge tasks based on cross-validation results using the training dataset. Specifically, in the AD diagnosis task, 5-fold cross-validation using encoded features based on transcripts generated from Deep Speech yields an average classification accuracy of 97.18%. In the MMSE score prediction task, 5-fold cross-validation using BERT encoded features based on transcripts generated from Deep Speech yields an average Root Mean Squared Error (RMSE) of 3.76. In the cognitive decline inference task, the leave-one-out cross-validation using BERT encoded features based on transcripts generated from Sphinx or Deep Speech yields an average classification accuracy of 100%. The analyses suggest that the combination of automatic transcription and BERT may produce a significant performance in AD related detection and prediction problems.","2642-6471","978-1-6654-3592-5","10.1109/ICSIPA52582.2021.9576784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576784","automatic speech recognition;detection;prediction;Alzheimer’s dementia","Training;Machine learning algorithms;Bit error rate;Linguistics;Feature extraction;Natural language processing;Reliability","","6","","23","IEEE","20 Oct 2021","","","IEEE","IEEE Conferences"
"Bayesian Neural Network Language Modeling for Speech Recognition","B. Xue; S. Hu; J. Xu; M. Geng; X. Liu; H. Meng","Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, SAR, China; Nanyang Technological University, Singapore; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, SAR, China; Department of Systems Engineering and Engineering Management, The Chinese University of Hong Kong, Hong Kong, SAR, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","14 Sep 2022","2022","30","","2900","2917","State-of-the-art neural network language models (NNLMs) represented by long short term memory recurrent neural networks (LSTM-RNNs) and Transformers are becoming highly complex. They are prone to overfitting and poor generalization when given limited training data. To this end, an overarching full Bayesian learning framework encompassing three methods is proposed in this paper to account for the underlying uncertainty in LSTM-RNN and Transformer LMs. The uncertainty over their model parameters, choice of neural activations and hidden output representations are modeled using Bayesian, Gaussian Process and variational LSTM-RNN or Transformer LMs respectively. Efficient inference approaches were used to automatically select the optimal network internal components to be Bayesian learned using neural architecture search. A minimal number of Monte Carlo parameter samples as low as one was also used. These allow the computational costs incurred in Bayesian NNLM training and evaluation to be minimized. Experiments are conducted on two tasks: AMI meeting transcription and Oxford-BBC LipReading Sentences 2 (LRS2) overlapped speech recognition using state-of-the-art LF-MMI trained factored TDNN systems featuring data augmentation, speaker adaptation and audio-visual multi-channel beamforming for overlapped speech. Consistent performance improvements over the baseline LSTM-RNN and Transformer LMs with point estimated model parameters and drop-out regularization were obtained across both tasks in terms of perplexity and word error rate (WER). In particular, on the LRS2 data, statistically significant WER reductions up to 1.3% and 1.2% absolute (12.1% and 11.3% relative) were obtained over the baseline LSTM-RNN and Transformer LMs respectively after model combination between Bayesian NNLMs and their respective baselines.","2329-9304","","10.1109/TASLP.2022.3203891","Hong Kong Research Council GRF(grant numbers:14200218,14200220,14200021); Innovation and Technology Fund(grant numbers:ITS/254/19,InP/057/21); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874985","Bayesian learning;model uncertainty;neural architecture search;neural language models;speech recognition","Bayes methods;Transformers;Uncertainty;Computational modeling;Artificial neural networks;Computer architecture;Task analysis","","11","","103","IEEE","2 Sep 2022","","","IEEE","IEEE Journals"
"Thai-Dialect: Low Resource Thai Dialectal Speech to Text Corpora","A. Suwanbandit; J. Chitiyaphol; S. Chuenchom; K. Kwiecien; H. Sawal; R. Uthai; O. Sangpetch; E. Chuangsuwanich","Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Thailand; Khon Kaen University, Thailand; Chiang Mai Rajabhat University, Thailand; Khon Kaen University, Thailand; Prince of Songkla University, Thailand; Prince of Songkla University, Thailand; CMKL University, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Thailand",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","8","We release a speech-to-text benchmark dataset containing 10 Thai dialects that cover different regions of Thailand. Our corpora consists of the standard dialect, Thai-central (THA); the northern dialects (Khummuang (NOD), Nan (KHB) and Yno (YNO)); the northeastern dialects (Korat (TTS), Khmer (KXM) and Laos (TTS)); and the southern dialects (Krabi (SOU), Pattani (MFA) and Phangnga (SOU)). All transcriptions are based on the Thai writing standard. We constructed baseline models by fine-tuning from self-supervised pre-trained models. Results show that multilingual/multidialectal systems outperform monolingual ones, and different dialect combinations can affect the performance of multilingual/multidialectal training.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389792","speech recognition;Thai dialect;low-resource language;multilingual ASR","Training;Conferences;Process control;Quality control;Writing;Data collection;Benchmark testing","","1","","27","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"Parody Detection Using Source-Target Attention with Teacher-Forced Lyrics","T. Ariga; Y. Higuchi; K. Hayasaka; N. Okamoto; T. Ogawa","Department of Communications and Computer Engineering, Waseda University, Tokyo, Japan; Department of Communications and Computer Engineering, Waseda University, Tokyo, Japan; DAIICHIKOSHO CO., LTD., Tokyo, Japan; DAIICHIKOSHO CO., LTD., Tokyo, Japan; Department of Communications and Computer Engineering, Waseda University, Tokyo, Japan","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","18 Mar 2024","2024","","","1151","1155","We propose an approach to detect parodies in singing voices, analyzing attention weights derived from an encoder-decoder-based automatic speech recognition (ASR) model. Here, parodies involve modifying and singing existing lyrics written for songs. Sharing such modified singing voices on the internet carries the potential risk of copyright infringement, posing the need of an automatic parody detection system. Given that songs typically comprise fixed lyrics, the pair of speech and its corresponding transcription can be used to analyze singing voices. In this work, we feed singing voices into an encoder-decoder-based ASR system and perform the decoding process using the corresponding lyrics in a teacher-forcing manner. Here, when the ASR model encounters a singing voice that includes a parody segment, there is a potential for the attention weights between the singing voice and the correct lyrics become collapsed. By identifying such misalignments in the attention weights, we attempt to detect parodies in singing voices. Experimental comparisons using real karaoke singing voice data demonstrate that the developed system achieves highly accurate parody detection performance by effectively identifying misalignments.","2379-190X","979-8-3503-4485-1","10.1109/ICASSP48485.2024.10446577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446577","Source-target attention;parody detection;singing voice;karaoke","Analytical models;Signal processing;Copyright protection;Acoustics;Decoding;Feeds;Speech processing","","","","18","IEEE","18 Mar 2024","","","IEEE","IEEE Conferences"
"Noisy Student Teacher Training with Self Supervised Learning for Children ASR","S. S. Chaturvedi; H. B. Sailor; H. A. Patil","Speech Research Lab, DA-IICT, Gandhinagar; Institute for Infocomm Research (I2R), A*STAR, Singapore; Speech Research Lab, DA-IICT, Gandhinagar",2022 IEEE International Conference on Signal Processing and Communications (SPCOM),"1 Aug 2022","2022","","","1","5","Automatic Speech Recognition (ASR) is a fast-growing field, where reliable systems are made for high resource languages and for adult’s speech. However, performance of such ASR system is inefficient for children speech, due to numerous acoustic variability in children speech and scarcity of resources. In this paper, we propose to use the unlabeled data extensively to develop ASR system for low resourced children speech. State-of-the-art wav2vec 2.0 is the baseline ASR technique used here. The baseline’s performance is further enhanced with the intuition of Noisy Student Teacher (NST) learning. The proposed technique is not only limited to introducing the use of soft labels (i.e., word-level transcription) of unlabeled data, but also adapts the learning of teacher model or preceding student model, which results in reduction of the redundant training significantly. To that effect, a detailed analysis is reported in this paper, as there is a difference in teacher and student learning. In ASR experiments, character-level tokenization was used and hence, Connectionist Temporal Classification (CTC) loss was used for fine-tuning. Due to computational limitations, experiments are performed with approximately 12 hours of training, and 5 hours of development and test data was used from standard My Science Tutor (MyST) corpus. The baseline wav2vec 2.0 achieves 34% WER, while relatively 10% of performance was improved using the proposed approach. Further, the analysis of performance loss and effect of language model is discussed in details.","2474-915X","978-1-6654-8250-9","10.1109/SPCOM55316.2022.9840763","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840763","","Training;Adaptation models;Conferences;Self-supervised learning;Signal processing;Data models;Tokenization","","1","","25","IEEE","1 Aug 2022","","","IEEE","IEEE Conferences"
"ASR Error Correction Using Large Language Models","R. Ma; M. Qian; M. Gales; K. Knill","ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.; ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.; ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.; ALTA Institute, Machine Intelligence Lab, Department of Engineering, Cambridge University, Cambridge, U.K.","IEEE Transactions on Audio, Speech and Language Processing","1 Apr 2025","2025","33","","1389","1401","Error correction (EC) models play a crucial role in refining Automatic Speech Recognition (ASR) transcriptions, enhancing the readability and quality of transcriptions. Without requiring access to the underlying code or model weights, EC can improve performance and provide domain adaptation for black-box ASR systems. This work investigates the use of large language models (LLMs) for error correction across diverse scenarios. 1-best ASR hypotheses are commonly used as the input to EC models. We propose building high-performance EC models using ASR N-best lists which should provide more contextual information for the correction process. Additionally, the generation process of a standard EC model is unrestricted in the sense that any output sequence can be generated. For some scenarios, such as unseen domains, this flexibility may impact performance. To address this, we introduce a constrained decoding approach based on the N-best list or an ASR lattice. Finally, most EC models are trained for a specific ASR system requiring retraining whenever the underlying ASR system is changed. This paper explores the ability of EC models to operate on the output of different ASR systems. This concept is further extended to zero-shot error correction using LLMs, such as ChatGPT. Experiments on three standard datasets demonstrate the efficacy of our proposed methods for both Transducer and attention-based encoder-decoder ASR systems. In addition, the proposed method can serve as an effective method for model ensembling.","2998-4173","","10.1109/TASLPRO.2025.3551083","Engineering and Physical Sciences Research Council(grant numbers:EP/V006223/1); Cambridge University Press & Assessment; University of Cambridge; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10930744","Automatic speech recognition;error correction;large language model;supervised training;zero-shot prompting","Error correction;Decoding;Hidden Markov models;Biological system modeling;Adaptation models;Training;Data models;Context modeling;Chatbots;Training data","","","","52","IEEE","18 Mar 2025","","","IEEE","IEEE Journals"
"DO as I Mean, Not as I Say: Sequence Loss Training for Spoken Language Understanding","M. Rao; P. Dheram; G. Tiwari; A. Raju; J. Droppo; A. Rastrow; A. Stolcke","Amazon Alexa, USA; Amazon Alexa, USA; Amazon Alexa, USA; Amazon Alexa, USA; Amazon Alexa, USA; Amazon Alexa, USA; Amazon Alexa, USA","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","7473","7477","Spoken language understanding (SLU) systems extract transcriptions, as well as semantics of intent or named entities from speech, and are essential components of voice activated systems. SLU models, which either directly extract semantics from audio or are composed of pipelined automatic speech recognition (ASR) and natural language understanding (NLU) models, are typically trained via differentiable cross-entropy losses, even when the relevant performance metrics of interest are word or semantic error rates. In this work, we propose non-differentiable sequence losses based on SLU metrics as a proxy for semantic error and use the REINFORCE trick to train ASR and SLU models with this loss. We show that custom sequence loss training is the state-of-the-art on open SLU datasets and leads to 6% relative improvement in both ASR and NLU performance metrics on large proprietary datasets. We also demonstrate how the semantic sequence loss training paradigm can be used to update ASR and SLU models without transcripts, using semantic feedback alone.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414566","speech recognition;spoken language understanding;REINFORCE;multitask training;neural interfaces","Measurement;Training;Error analysis;Conferences;Semantics;Natural languages;Signal processing","","7","","30","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Predominant Jazz Instrument Recognition: Empirical Studies on Neural Network Architectures","J. Abeßer; J. Chauhan; P. P. Pillai; M. Taenzer; S. I. Mimilakis","Semantic Music Technologies, Fraunhofer IDMT, Ilmenau, Germany; Semantic Music Technologies, Fraunhofer IDMT, Ilmenau, Germany; Semantic Music Technologies, Fraunhofer IDMT, Ilmenau, Germany; Semantic Music Technologies, Fraunhofer IDMT, Ilmenau, Germany; Semantic Music Technologies, Fraunhofer IDMT, Ilmenau, Germany",2021 29th European Signal Processing Conference (EUSIPCO),"8 Dec 2021","2021","","","361","365","Musicological studies on jazz performance analysis commonly require a manual selection and transcription of improvised solo parts, both of which can be time-consuming. In order to expand these studies to larger corpora of jazz recordings, algorithms for automatic content analysis can accelerate these processes. In this study, we aim to detect the presence of predominant music instruments in jazz ensemble recordings. This information can guide a structural analysis in order to detect improvised solo parts. As the main contribution, we perform a comparative study on predominant automatic instrument recognition (AIR) in jazz ensembles using a taxonomy of 11 common instruments including singing voice. We compare the performance of three state-of-the-art convolutional neural networks (CNNs) including a recurrent variant and one with an attention mechanism. Our main finding is that while all networks perform comparably, the attention-based model learns the most compact feature representation as it is by orders of magnitude smaller than the other models.","2076-1465","978-9-0827-9706-0","10.23919/EUSIPCO54536.2021.9616031","German Research Foundation(grant numbers:AB 675/2-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616031","automatic instrument recognition;convolutional neural networks;deep learning;attention;jazz analysis","Analytical models;Instruments;Taxonomy;Neural networks;Speech recognition;Manuals;Signal processing","","1","","24","","8 Dec 2021","","","IEEE","IEEE Conferences"
"Hybrid Speech and Text Analysis Methods for Speaker Change Detection","O. H. Anidjar; I. Lapidot; C. Hajaj; A. Dvir; I. Gilad","Department of Computer Science, Ariel University, Ariel, IL, Israel; Afeka Tel-Aviv Academic College of Engineering, ACLP, Tel-Aviv, IL, Israel; Data Science Artificial Intelligence Research Center, Ariel University, Ramat HaGolan, Israel; Department of Computer Science, Ariel University, Ariel, IL, Israel; Department of Industrial Engineering, Management, Ariel University, Ariel, IL, Israel","IEEE/ACM Transactions on Audio, Speech, and Language Processing","20 Jul 2021","2021","29","","2324","2338","Speaker Change Detection (SCD) is the task of segmenting an input audio-recording according to speaker interchanges. Nowadays, many applications, such as Speaker Diarization (SD) or automatic vocal transcription, depend on this segmentation task. In this paper, we focus on the essential task of the SD problem, the audio segmenting process, and suggest a solution for the SCD problem, as well as the assignment of clustered speaker labels for the extracted segments, and applying the solution over two datasets: a commercial dataset in Hebrew and the ICSI Meeting Corpus. As such, we propose a hybrid framework for the SCD problem that is learned by textual information and speech signals and the meta-data features that can be extracted from them. Moreover, we demonstrate the negative correlation between an increase in the number of speakers in the training dataset and the influence on the overall diarization system's performance, which is improved using our efficient SCD component. Finally, we show how our proposed hybrid framework remains robust compared to the ICSI Meeting Corpus, as the experimental evaluation's training and testing is based on two languages.","2329-9304","","10.1109/TASLP.2021.3093817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468954","Speaker change detection;speaker diarization;speaker verification;speech analysis;D-vectors","Feature extraction;Task analysis;Speech processing;Mel frequency cepstral coefficient;Clustering algorithms;Encoding;Training","","8","","76","IEEE","30 Jun 2021","","","IEEE","IEEE Journals"
"Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling","Y. Li; Z. Zhang; J. Han; P. Bell; C. Lai","University of Edinburgh, UK; Hunan University, China; University of Cambridge, UK; University of Edinburgh, UK; University of Edinburgh, UK","ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","7 Mar 2025","2025","","","1","5","The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Fréchet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.","2379-190X","979-8-3503-6874-1","10.1109/ICASSP49660.2025.10890660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10890660","Emotion Recognition;Dementia Detection;Semi-Supervised Learning;Fréchet Audio Distance;LLMs","Training;Emotion recognition;Large language models;Supervised learning;Semisupervised learning;Linguistics;Signal processing;Acoustics;Speech processing;Dementia","","","","33","IEEE","7 Mar 2025","","","IEEE","IEEE Conferences"
"Unsupervised Forced Alignment on Syllable and Phoneme with Universal Phonetics Transcriptions","L. Sun","Faculty of Linguistic Sciences, Beijing Language and Culture University, Beijing, China","2023 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","29 Jan 2024","2023","","","1","5","Forced alignment inputs the sound recordings and transcription texts, then outputs the temporal boundaries for each speech unit. Instead of the special phonemes belong to specific language, this work uses the basic syllable types and the broad phonetic classes which can be generalized to any language, so called as the universal phonetics transcriptions. We adopt the Deep Neural Networks with the Connectionist Temporal Classification cost function as the solution of achieving alignment. The evaluations are carried out on six different languages and the results demonstrate that the alignment system can be applied to a wide range of languages once training successful. Compared to existing state-of-the-art supervised and unsupervised alignment system, the proposed system provides competitive performance in the task of forced alignment.","2837-116X","979-8-3503-1672-8","10.1109/ICSPCC59353.2023.10400287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10400287","unsupervised;forced alignment;syllable types;phonetic classes;connectionist temporal classification","Training;Phonetics;Network architecture;Cost function;Recording;Synchronization;Task analysis","","1","","24","IEEE","29 Jan 2024","","","IEEE","IEEE Conferences"
"Automatic Silence Detection Employing Artificial Intelligence for Clinical Context Analyses","C. A.; P. O.; L. V.; U. C.; A. M.; A. M.; A. D. Orjuela-Cañón; Á. M. R.; M. F. Jiménez","School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Medicine and Health Sciences, Universidad del Rosario, Bogota, D.C., Colombia; School of Engineering, Science and Technology Universidad del Rosario, Bogota, D.C., Colombia",2024 3rd International Congress of Biomedical Engineering and Bioengineering (CIIBBI),"13 Dec 2024","2024","","","1","6","Automated speech and pause/silence detection is a crucial task in clinical and pathological environments, supporting diagnostic processes and providing essential information for treatment planning. This study evaluates three methods for automatic silence detection in clinical speech analysis: (1) a traditional energy-based method using zero-crossing detection, (2) a pretrained neural network model for voice activity detection (Silero-VAD), and (3) NVIDIA's speaker diarization and transcription tool. All methods demonstrated effective pause/silence detection with comparable error rates, though Silero-VAD exhibited superior precision and performance. Key metrics included a Dice coefficient of 0.917, an onset error of 500 ms, and an endpoint error of 370 ms, highlighting the importance of audio preprocessing.","","979-8-3315-3235-2","10.1109/CIIBBI63846.2024.10785143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10785143","Silence Detection;Artificial Intelligence;Voice Activity Detection;Clinical Audios","Measurement;Voice activity detection;Pathology;Runtime;Error analysis;Neural networks;Planning;Delays;Artificial intelligence;Biomedical engineering","","","","19","IEEE","13 Dec 2024","","","IEEE","IEEE Conferences"
"0.01 Cent per Second: Developing a Cloud-based Cost-effective Audio Transcription System for an Online Video Learning Platform","N. Pinyo; P. Lokaphadhana; P. Saengow; S. Siangsanoh; T. Wonnaparhown; E. Chuangsuwanich; P. Punyabukkana; A. Suchato","Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand",2023 20th International Joint Conference on Computer Science and Software Engineering (JCSSE),"10 Aug 2023","2023","","","432","437","Using automatic speech recognition (ASR) to transcribe videos in an online video learning platform can benefit learners in multiple ways. However, existing speech-to-text APIs can be costly to use, especially for long lecture videos commonly found in such platform. In this work, we developed a cloud-based ASR system that is cost-optimized for the workload of online learning platforms. We characterized such workload and applied a combination of techniques from system architecture, including: (1) serverless, (2) preemptible instance, and (3) batching and audio transcription optimization, including: (1) audio segmentation, (2) cost-based segment merging, and (3) locally hosted transcription model. All of which work together to provide a low transcription cost per minute of audio. We experimented and calculated the processing cost, time, and accuracy and showed that our system offers accuracy on par with existing speech-to-text services at a significantly lower cost. We have also integrated this system into an online video learning platform.","2642-6579","979-8-3503-0050-5","10.1109/JCSSE58229.2023.10201942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10201942","","Measurement;Computer science;Vocabulary;Costs;Merging;Systems architecture;Gain control","","1","","17","IEEE","10 Aug 2023","","","IEEE","IEEE Conferences"
"A Study of Audio-to-Text Conversion Software Using Whispers Model","A. L. Haz; E. D. Fajrianti; N. Funabiki; S. Sukaridhoto","Department of Information and Communication Systems, Okayama University, Okayama, Japan; Department of Information and Communication Systems, Okayama University, Okayama, Japan; Department of Information and Communication Systems, Okayama University, Okayama, Japan; Informatics and Computer Department, Politeknik Elektro Negeri Surabaya, Surabaya, Indonesia",2023 Sixth International Conference on Vocational Education and Electrical Engineering (ICVEE),"14 Dec 2023","2023","","","268","273","This paper explores the potential of utilizing the Whispers model to create unified interfaces for audio-to-text in the context of Natural Language Processing (NLP). It offers possibilities for accurately converting spoken language into written texts. Whispers model by OpenAI is a state-of-the-art model in the field of NLP and is employe. In this study, various metrics and criteria are considered to evaluate the performance of the developed audio-to-text conversion software, including loading time, stress test, and transcription accuracy and speed. The proposal is capable of handling up to 180 concurrent users with an average response time of 309 ms and 471.5 requests per second. The results and findings of this study provide valuable insights into the effectiveness and limitations of the Whispers model in the context of audio-to-text conversion.","","979-8-3503-2664-2","10.1109/ICVEE59738.2023.10348186","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10348186","Audio-to-text;Streamlit;Web Application;Whisper Model","Loading;Natural language processing;Software;Data models;Time factors;Task analysis;Unsupervised learning","","6","","26","IEEE","14 Dec 2023","","","IEEE","IEEE Conferences"
"Note-Level Singing Melody Transcription for Time-Aligned Musical Score Generation","L. Kim; S. Jeon; W. Heo; J. Park","Department of Industrial Engineering and Institute for Industrial Systems Innovation, Seoul National University, Seoul, South Korea; Department of Industrial Engineering and Institute for Industrial Systems Innovation, Seoul National University, Seoul, South Korea; Department of Industrial Engineering and Institute for Industrial Systems Innovation, Seoul National University, Seoul, South Korea; Department of Industrial Engineering and Institute for Industrial Systems Innovation, Seoul National University, Seoul, South Korea","IEEE Transactions on Audio, Speech and Language Processing","11 Mar 2025","2025","33","","1088","1102","Automatic music transcription converts audio recordings into symbolic representations, facilitating music analysis, retrieval, and generation. A musical note is characterized by pitch, onset, and offset in an audio domain, whereas it is defined in terms of pitch and note value in a musical score domain. A time-aligned score, derived from timing information along with pitch and note value, allows matching a part of the score with the corresponding part of the music audio, enabling various applications. In this paper, we consider an extended version of the traditional note-level transcription task that recognizes onset, offset, and pitch, through including extraction of additional note value to generate a time-aligned score from an audio input. To address this new challenge, we propose an end-to-end framework that integrates recognition of the note value, pitch, and temporal information. This approach avoids error accumulation inherent in multi-stage methods and enhances accuracy through mutual reinforcement. Our framework employs tokenized representations specifically targeted for this task, through incorporating note value information. Furthermore, we introduce a pseudo-labeling technique to address a scarcity problem of annotated note value data. This technique produces approximate note value labels from existing datasets for the traditional note-level transcription. Experimental results demonstrate the superior performance of the proposed model in note-level transcription tasks when compared to existing state-of-the-art approaches. We also introduce new evaluation metrics that assess both temporal and note value aspects to demonstrate the robustness of the model. Moreover, qualitative assessments via visualized musical scores confirmed the effectiveness of our model in capturing the note values.","2998-4173","","10.1109/TASLPRO.2025.3544064","National Research Foundation of Korea; Korea government(grant numbers:NRF-2019R1F1A1053366); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10897320","Automatic music transcription;music information retrieval;note value detection;singing melody transcription","Music;Training;Predictive models;Measurement;Data models;Speech processing;Quantization (signal);Adaptation models;Data mining;Symbols","","","","55","IEEE","20 Feb 2025","","","IEEE","IEEE Journals"
"Vocalsound: A Dataset for Improving Human Vocal Sounds Recognition","Y. Gong; J. Yu; J. Glass","MIT CSAIL, Cambridge, MA, USA; Signify Research, Cambridge, MA, USA; MIT CSAIL, Cambridge, MA, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","151","155","Recognizing human non-speech vocalizations is an important task and has broad applications such as automatic sound transcription and health condition monitoring. However, existing datasets have a relatively small number of vocal sound samples or noisy labels. As a consequence, state-of-the-art audio event classification models may not perform well in detecting human vocal sounds. To support research on building robust and accurate vocal sound recognition, we have created a VocalSound dataset consisting of over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. Experiments show that the vocal sound recognition performance of a model can be significantly improved by 41.9% by adding VocalSound dataset to an existing dataset as training material. In addition, different from previous datasets, the VocalSound dataset contains meta information such as speaker age, gender, native language, country, and health condition.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746828","vocal sounds;audio classification;corpus","Training;Condition monitoring;Conferences;Buildings;Signal processing;Audio recording;Acoustics","","18","","23","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Catch Me If You Can: Blackbox Adversarial Attacks on Automatic Speech Recognition using Frequency Masking","X. Wu; A. Rajan","School of Informatics, University of Edinburgh, Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom",2022 29th Asia-Pacific Software Engineering Conference (APSEC),"16 Feb 2023","2022","","","169","178","Automatic speech recognition (ASR) models are used widely in applications for voice navigation and voice control of domestic appliances. ASRs have been misused by attackers to generate malicious outputs by attacking the deep learning component within ASRs. To assess the security and robustnesss of ASRs, we propose techniques within our framework SPAT that generate blackbox (agnostic to the DNN) adversarial attacks that are portable across ASRs. This is in contrast to existing work that focuses on whitebox attacks that are time consuming and lack portability. Our techniques generate adversarial attacks that have no human audible difference by manipulating the input speech signal using a psychoacoustic model that maintains the audio perturbations below the thresholds of human perception. We propose a framework SPAT with three attack generation techniques based on the psychoacoustic concept and frame selection techniques to selectively target the attack. We evaluate portability and effectiveness of our techniques using three popular ASRs and two input audio datasets using the metrics- Word Error Rate (WER) of output transcription, Similarity to original audio, attack Success Rate on different ASRs and Detection score by a defense system. We found our adversarial attacks were portable across ASRs, not easily detected by a state-of the-art defense system, and had significant difference in output transcriptions while sounding similar to original audio.","2640-0715","978-1-6654-5537-4","10.1109/APSEC57359.2022.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043254","Automatic Speech Recognition;Adversarial Attack;Blackbox;Frequency Masking","Psychoacoustics;Measurement;Target recognition;Navigation;Perturbation methods;Closed box;Psychoacoustic models","","2","","35","IEEE","16 Feb 2023","","","IEEE","IEEE Conferences"
"Identifying Sponsored Content in YouTube using Information Extraction","J. P. Santos Rodrigues; A. C. Munaro; E. C. Paraiso","Computer Science at Pontifícia Universidade Católica do Paraná (PUCPR, Brazil) in Natural Language Processing (NLP) and Machine Learning; Computer Science at Pontifícia Universidade Católica do Paraná (PUCPR, Brazil) in Natural Language Processing (NLP) and Machine Learning; Computer Science at Pontifícia Universidade Católica do Paraná (PUCPR, Brazil) in Natural Language Processing (NLP) and Machine Learning","2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","6 Jan 2022","2021","","","3075","3080","Information Extraction techniques can retrieve useful information from unstructured data that improve data analytics’ effectiveness and play a key role in the consumer decision-making process. The growth of sponsored content videos on social media increases the demand on knowing the effectiveness of the sponsored investment in the engagement results obtained. This study aims to analyze an approach to infer sponsored content in videos from top digital influencers on YouTube using knowledge acquisition techniques in audio transcriptions. A dataset with 34,563 videos, among 103 different YouTubers channels, was used in a model comprising six stages: data acquisition, preprocessing of documents, identification of candidate videos, manual transcriptions processing, automatic transcriptions processing, and tuple filtering. Despite we perceived several difficulties during the recognition of speech from YouTube videos, such as the absence of clear boundaries between words, the presence of two or more people talking in the video, and colloquial expressions, as a result, the approach identifies sponsored videos from their audio transcripts in a feasible process by identifying keywords, obtaining knowledge from tuples, and recognizing named entities.","2577-1655","978-1-6654-4207-7","10.1109/SMC52423.2021.9659291","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659291","","Filtering;Social networking (online);Semantics;Speech recognition;Manuals;Machine learning;Data mining","","1","","27","IEEE","6 Jan 2022","","","IEEE","IEEE Conferences"
"PipaSet and TEAS: A Multimodal Dataset and Annotation Platform for Automatic Music Transcription and Expressive Analysis Dedicated to Chinese Traditional Plucked String Instrument Pipa","Y. Wang; Y. Jing; W. Wei; D. Cazau; O. Adam; Q. Wang","School of Information Science and Engineering, Southeast University, Nanjing, China; Nanjing University of the Art, Nanjing, China; Conservatory of Music, XiaoZhuang University, Nanjing, China; Laboratory-STICC, UMR 6285, Brest, CNRS, Institute of Mines-Télécom Atlantique, Palaiseau, France; UMR7190, CNRS, Institute of Jean Le Rond d’Alembert, Sorbonne Univerisity, Paris, France; School of Information Science and Engineering, Southeast University, Nanjing, China",IEEE Access,"8 Nov 2022","2022","10","","113850","113864","Music information retrieval (MIR) is developing these years rapidly. As the fundamental MIR tasks, automatic music transcription (AMT) and expressive analysis (EA) are gaining momentum in both Western and non-European music. However, the annotated datasets for non-Eurogenic instruments remain scarce in terms of quantity and feature diversity, so that general evaluations and data-driven models on various tasks cannot be well explored. As one of the most popular traditional plucked string instruments in Asia, which is barely studied in the MIR community, pipa has lots of distinctive national and local characteristics, including the fake nails, intrinsic pitch deviation, rubato, as well as sophisticated playing techniques, that greatly enhance the music expressiveness. Our work aims to systematically clarify a complete creation procedure of a pipa solo dataset with audio, musical notation and multiview video modalities. The use of 4-track string vibration signals captured by optical sensors paves a path to the high quality annotations. Furthermore, a transcription and expressiveness annotation system (TEAS) was transparently implemented to ensure the scalability of dataset. Three expressive analysis approaches in this system were newly proposed and evaluated in the paper. Finally, a series of the existing and emerging MIR tasks enabled by this dataset were enumerated and two AMT models were simply investigated for future exploration.","2169-3536","","10.1109/ACCESS.2022.3216282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926100","Multimodal dataset creation;automatic music transcription;playing technique analysis;optical sensoring;annotation system","Music information retrieval;Harmonic analysis;Annotations;Acoustics;Data models;Optical sensors;Multisensor systems;Sensors","","3","","119","CCBYNCND","21 Oct 2022","","","IEEE","IEEE Journals"
"Audio Embedding-Aware Dialogue Policy Learning","A. L. Zorrilla; M. I. Torres; H. Cuayáhuitl","University of the Basque Country UPV/EHU, Leioa, Spain; University of the Basque Country UPV/EHU, Leioa, Spain; University of Lincoln, Lincoln, U.K.","IEEE/ACM Transactions on Audio, Speech, and Language Processing","16 Dec 2022","2023","31","","525","538","Following the success of Natural Language Processing (NLP) transformers pretrained via self-supervised learning, similar models have been proposed recently for speech processing such as Wav2Vec2, HuBERT and UniSpeech-SAT. An interesting yet unexplored area of application of these models is Spoken Dialogue Systems, where the users' audio signals are typically just mapped to word-level features derived from an Automatic Speech Recogniser (ASR), and then processed using NLP techniques to generate system responses. This paper reports a comprehensive comparison of dialogue policies trained using ASR-based transcriptions and extended with the aforementioned audio processing transformers in the DSTC2 task. Whilst our dialogue policies are trained with supervised and policy-based deep reinforcement learning, they are assessed using both automatic task completion metrics and a human evaluation. Our results reveal that using audio embeddings is more beneficial than detrimental in most of our trained dialogue policies, and that the benefits are stronger for supervised learning than reinforcement learning.","2329-9304","","10.1109/TASLP.2022.3225658","Eusko Jaurlaritza; Ministerio Ciencia e Innovación; Next generation EU(grant numbers:PRE_2017_1_0357,PLEC2021-008171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966819","Audio embeddings;deep reinforcement learning;spoken dialogue systems;transformer neural networks","Transformers;Speech processing;Task analysis;History;Reinforcement learning;Supervised learning;Noise measurement","","","","66","IEEE","30 Nov 2022","","","IEEE","IEEE Journals"
"ALIF: Low-Cost Adversarial Audio Attacks on Black-Box Speech Platforms using Linguistic Features","P. Cheng; Y. Wang; P. Huang; Z. Ba; X. Lin; F. Lin; L. Lu; K. Ren","Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; University of Guelph, Guelph, Canada; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China",2024 IEEE Symposium on Security and Privacy (SP),"5 Sep 2024","2024","","","1628","1645","Extensive research has revealed that adversarial examples (AE) pose a significant threat to voice-controllable smart devices. Recent studies have proposed black-box adversarial attacks that require only the final transcription from an automatic speech recognition (ASR) system. However, these attacks typically involve many queries to the ASR, resulting in substantial costs. Moreover, AE-based adversarial audio samples are susceptible to ASR updates. In this paper, we identify the root cause of these limitations, namely the inability to construct AE attack samples directly around the decision boundary of deep learning (DL) models. Building on this observation, we propose ALIF, the first black-box adversarial linguistic feature-based attack pipeline. We leverage the reciprocal process of text-to-speech (TTS) and ASR models to generate perturbations in the linguistic embedding space where the decision boundary resides. Based on the ALIF pipeline, we present the ALIF-OTL and ALIF-OTA schemes for launching attacks in both the digital domain and the physical playback environment on four commercial ASRs and voice assistants. Extensive evaluations demonstrate that ALIF-OTL and -OTA significantly improve query efficiency by 97.7% and 73.3%, respectively, while achieving competitive performance compared to existing methods. Notably, ALIF-OTL can generate an attack sample with only one query. Furthermore, our test-of-time experiment validates the robustness of our approach against ASR updates.","2375-1207","979-8-3503-3130-1","10.1109/SP54263.2024.00056","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10646791","","Training;Perturbation methods;Pipelines;Closed box;Symbols;Linguistics;Robustness","","1","","47","IEEE","5 Sep 2024","","","IEEE","IEEE Conferences"
"An Automatic Teacher Activity Recognition System based on Speech Transcriptions","P. Uribe; D. Schlotterbeck; A. Jiménez; R. Araya; D. Caballero; J. V. d. M. Moris","Institute of Education University of Chile, Santiago, Chile; Institute of Education University of Chile, Santiago, Chile; Institute of Education University of Chile, Santiago, Chile; Institute of Education University of Chile, Santiago, Chile; Institute of Education University of Chile, Santiago, Chile; MRC Biostatistics Unit, University of Cambridge, Great Britain",2021 XVI Latin American Conference on Learning Technologies (LACLO),"8 Mar 2022","2021","","","216","223","The importance of classroom observation as a tool to improve the quality of education has motivated the use of Machine Learning methods for automatic detection of classroom activities. However, most of these efforts require expensive sensors to register video and/or audio signals from lessons or do not follow a validated observation protocol. In contrast, in this work we present a cost-effective, easy to use, non-intrusive scalable method designed for the needs and practical conditions of teachers. We recorded teacher’s talk relying only on a smartphone with a budget lavalier microphone and followed a validated observation protocol: the Classroom Observation Protocol for Undergraduate STEM (COPUS). Using teachers’ talk transcriptions of 41 fourth grade lessons, we trained Machine Learning models to recognize three collapsed categories from COPUS’ teacher dimension throughout a lesson (Presenting, Guiding, and Administration). In particular, we propose a deep network model which outperformed a powerful model like BERT for this task. With this method we contribute with a useful tool to provide fast and effective feedback for teachers, as well as the possibility for researchers to quickly analyze in a thoughtful way a large amount of lessons.","","978-1-6654-2358-8","10.1109/LACLO54177.2021.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725142","Teaching Practices Detection;Transfer Learning;BERT","Protocols;Education;Bit error rate;Machine learning;Speech recognition;Activity recognition;Sensors","","","","47","IEEE","8 Mar 2022","","","IEEE","IEEE Conferences"
"Audio Summarization for Podcasts","A. Vartakavi; A. Garg; Z. Rafii","Gracenote, Emeryville, CA, USA; Gracenote, Emeryville, CA, USA; Gracenote, Emeryville, CA, USA",2021 29th European Signal Processing Conference (EUSIPCO),"8 Dec 2021","2021","","","431","435","We propose a novel system to automatically generate audio summaries for podcasts, allowing listeners to quickly preview podcast episodes. The proposed system first transcribes the audio from a podcast using automatic speech recognition (ASR), then summarizes the transcript using extractive text summarization, and finally returns the audio associated with the text summary. Motivated by a lack of relevant datasets for this task, we created our own by transcribing the audio from various podcasts and generating summaries for these transcripts using a manual annotation tool. Using these text summaries, we fine-tuned a recent Transformer-based summarization model to specifically handle podcast summaries. Our system achieves ROUGE-(1/2/L) F-scores of 0.63/0.53/0.63, respectively, showing good performance for podcast summarization. We present some examples of podcast audio summaries here: https://github.com/aneeshvartakavi/podsumm.","2076-1465","978-9-0827-9706-0","10.23919/EUSIPCO54536.2021.9615948","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615948","Podcasts;audio summarization;automatic speech recognition;automatic summarization","Annotations;Manuals;Production;Tools;Signal processing;Transformers;Robustness","","10","","19","","8 Dec 2021","","","IEEE","IEEE Conferences"
"WhisperSum: Unified Audio-to-Text Summarization","S. Ganguly; S. Mandal; N. Das; B. Sadhukhan; S. Sarkar; S. Paul","Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India; Dept of CSE, TINT, Kolkata, India",2024 International Conference on Intelligent Algorithms for Computational Intelligence Systems (IACIS),"24 Oct 2024","2024","","","1","7","In an era overwhelmed by information, efficiently extracting relevant data from audio sources is crucial. This study introduces WhisperSum, a solution combining OpenAI's Whisper for accurate voice transcription with spaCy's advanced natural language processing (NLP) for extractive text summarization. WhisperSum transcribes audio files into text, analyzes the content, and produces concise summaries to help users quickly understand key information. The web application, built using Flask, offers a user-friendly interface for easy audio uploads, real-time transcription, and summary retrieval. The system integrates Whisper's transcription capabilities with the NLP tools of spaCy, ensuring accuracy and efficiency. WhisperSum achieved high performance, with precision, recall, and F1-scores of 91.98%, 92.12%, and 92.04%, respectively, highlighting its effectiveness in accurately transcribing and summarizing audio content while minimizing errors and maintaining comprehensive coverage. The application is valuable for content management, education, and journalism, providing a streamlined solution to information overload. WhisperSum enhances decision-making by ensuring the accessible availability of crucial information, demonstrating the significant potential of integrating advanced NLP and transcription technologies.","","979-8-3503-6066-0","10.1109/IACIS61494.2024.10721926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10721926","extractive summarization;NLP;spacy;whisper;text;flask;audio-to-text","Accuracy;Education;Decision making;Text summarization;Speech recognition;Journalism;Feature extraction;Natural language processing;Real-time systems;Data mining","","","","22","IEEE","24 Oct 2024","","","IEEE","IEEE Conferences"
"AI Chrome Extension for Automated Meeting Summary","I. Perumal; K. P; V. R; J. S; N. K; N. T","Department of Information Technology, Sona College of Technology, Salem, India; Department of CSE, Kongu Engineering College, Perundurai, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India; Department of Information Technology, Sona College of Technology, Salem, India",2025 International Conference on Electronics and Renewable Systems (ICEARS),"2 Apr 2025","2025","","","1080","1085","The increasing volume of textual information in the digital era poses considerable hurdles for efficient data retrieval and summarization. Conventional note-taking techniques can be laborious and ineffective, frequently leading to the omission of essential information during meetings. Automatic Text Summarization (ATS) provides a mechanism for producing succinct summaries that encapsulate essential ideas and conclusions from dialogues. This project presents an AI-driven Chrome addon intended to automate the summarizing of conference discussions. The addon includes an intuitive interface developed using HTML and CSS, which records audio and video inputs and transcribes them into text with Rev.ai for precise transcription. The backend is constructed with Python and Flask, offering a resilient foundation for data processing. The system utilizes the T5 model for summarization, providing coherent and contextually pertinent summaries that enable users to concentrate on vital content rather than navigating extensive transcripts. Essential attributes comprise real-time transcribing, guaranteeing the retention of significant facts, and automated summarizing, which condenses crucial elements into succinct outputs. The solution enhances user experience, fosters cooperation, and improves decision-making, while minimizing time allocated to manual notetaking. The initiative utilizes innovative technology to overcome the shortcomings of traditional methods, providing professionals with an enhanced and efficient meeting experience.","","979-8-3315-0967-5","10.1109/ICEARS64219.2025.10940198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10940198","Automatic Text Summarization;AI-powered Chrome extension;meeting transcription;real-time summarization;Rev.ai;T5 model;Python/Flask backend;productivity;user experience;information management","Productivity;Visualization;Renewable energy sources;Navigation;Decision making;Text summarization;Manuals;Real-time systems;User experience;Context modeling","","","","20","IEEE","2 Apr 2025","","","IEEE","IEEE Conferences"
"An Extensive Survey on Audio-to-Text and Text Summarization for Video Content","N. B. Raut; A. S. Pranesh; B. Nagulan; S. Pranesh; R. Vasantharajan","Dept. of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Dept. of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Dept. of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Dept. of Computer Science and Engineering, KPR Institute of Engineering and Technology, Coimbatore, India; Grids and Guides, Chennai, India",2023 3rd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA),"18 Apr 2024","2023","","","1251","1257","This literature review examines evolving trends in audio-to-text and text summarization techniques applied to YouTube video content. With the rapid expansion of online video platforms, the demand for efficient transcription and summarization of multimedia data has grown significantly. This study offers a comprehensive analysis of current methodologies, algorithms, and tools for converting audio content to text, highlighting the intricacies of speech recognition technologies and their applicability in various contexts. Additionally, the survey explores text summarization techniques specifically designed for YouTube videos, including both extractive and abstractive methods. It assesses their effectiveness in condensing lengthy video transcripts into concise and coherent textual representations. Special emphasis is given to NLP (Natural Language Processing) algorithms and ML (Machine Learning) models that aid in extracting essential information while maintaining contextual relevance.","","979-8-3503-4363-2","10.1109/ICIMIA60377.2023.10426376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426376","Supervised Learning;Unsupervised Learning;Reinforcement Learning;Training Data;Testing Data;Overfitting","Surveys;Video on demand;Training data;Natural language processing;Real-time systems;Acoustics;Web sites","","","","16","IEEE","18 Apr 2024","","","IEEE","IEEE Conferences"
"Multimodal Punctuation Prediction with Contextual Dropout","A. Silva; B. -J. Theobald; N. Apostoloff",Georgia Institute of Technology; Apple; Apple,"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","3980","3984","Automatic speech recognition (ASR) is widely used in consumer electronics. ASR greatly improves the utility and accessibility of technology, but usually the output is only word sequences without punctuation. This can result in ambiguity in inferring user-intent. We first present a transformer-based approach for punctuation prediction that achieves 8% improvement on the IWSLT 2012 TED Task, beating the previous state of the art [1]. We next describe our multimodal model that learns from both text and audio, which achieves 8% improvement over the text-only algorithm on an internal dataset for which we have both the audio and transcriptions. Finally, we present an approach to learning a model using contextual dropout that allows us to handle variable amounts of future context at test time.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414979","Punctuation;speech;language;text;multimodal","Training;Conferences;Prediction algorithms;Robustness;Task analysis;Speech processing;Spectrogram","","4","","24","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"A Genetic-based Fusion Approach of Persian and Universal Phonetic Results for Spoken Language Identification","A. Moradi; Y. Shekofteh; S. Zarei","Faculty of Computer Science and Engineering, Shahid Beheshti University, Tehran, Iran; Faculty of Computer Science and Engineering, Shahid Beheshti University, Tehran, Iran; Faculty of Computer Science and Engineering, Shahid Beheshti University, Tehran, Iran",2021 11th International Conference on Computer Engineering and Knowledge (ICCKE),"28 Feb 2022","2021","","","175","181","Automatic Spoken language identification (LID) refers to the automatic process of identifying languages spoken in the audio files. Pure acoustic approaches have shown great potential in LID. Since acoustic approaches have become more and more popular, phonetic information has been largely overlooked. In this paper, we present a genetic-based fusion approach based on the score probabilities of two phonetic LID systems. There are two SVM classifiers trained on perplexities as their feature vectors which are obtained from phone language models of different phone recognizers. Two phone recognizers are here utilized; one decodes the speech file to a sequence of IPA alphabet, as a universal phone recognizer, and the other is a Farsi phone recognizer which is trained on FARSDAT databases. With the help of the genetic-based fusion approach, we will extract 54 weights. We have 27 languages in our database and 2 individual phonetic LID systems; therefore, we will achieve 54 weights for our fusion. The first 27 weights correspond to our system using a universal phone recognizer and the second 27 weights are related to our system with the Farsi phone recognizer. In the end, we use these weights to combine the results of each of our individual phonetic LID systems. The experimental results conducted on 27 languages within the NIST-LRE09 corpus demonstrated that the proposed fusion approach could greatly increase the classification accuracy of target languages. It should also be noted that we separate the files of each speaker and place them only in one set (train set, development set, or test set) to prevent speaker-related biases.","2643-279X","978-1-6654-0208-8","10.1109/ICCKE54056.2021.9721511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721511","Spoken language identification;Phonetic-based approach;perplexity;Classifier fusion;Genetic Algorithm","Support vector machines;Knowledge engineering;Databases;Speech recognition;Phonetics;Acoustics","","","","17","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Automatic Assessment of Language Developmental Disorders in Non-English Contexts","M. Nishio; A. Koyanagi; A. Takamori; K. Hasuike; Y. Shimoura; H. Yakura; S. Shi","National Center for Child Health and Development, Tokyo, Japan; Fvital inc., Tokyo, Japan; Fvital inc., Tokyo, Japan; Fvital inc., Tokyo, Japan; Fvital inc., Tokyo, Japan; Fvital inc., Tokyo, Japan; Fvital inc., Tokyo, Japan","2024 IEEE International Conference on E-health Networking, Application & Services (HealthCom)","18 Feb 2025","2024","","","1","6","Assessing the language development of children is crucial for early detection and intervention in language developmental disorders. However, it poses challenges due to the difficulty in gathering sufficient data to accurately evaluate children's language skills within limited medical consultation hours and in unfamiliar environments for children. While machine learning (ML) offers potential for efficient data collection in natural environments for children, ML models designed for specific clinical purposes are often trained on English and perform poorly in non-English contexts. In this study, we used widely available ML tools such as OpenAI's GPT and Azure's Speech-to-Text, which are trained with large datasets including diverse languages, to develop an automated pipeline for speaker identification, speech transcription, and speech content analysis tailored for assessing Japanese language development. We have demonstrated the effectiveness of our pipeline in everyday settings for children, including special educational centers and public nursery schools. Furthermore, we found that the vocabulary size of children as-sessed by our pipeline significantly correlates with developmental scales evaluated by teachers, indicating the potential value of our pipeline as a reliable assessment tool for children's language development in non-English contexts.","","979-8-3503-5054-8","10.1109/HealthCom60970.2024.10880728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10880728","Natural language processing;language development;pediatrics","Pediatrics;Vocabulary;Pipelines;Natural languages;Machine learning;Data collection;Data models;Reliability;Speech to text;Context modeling","","","","19","IEEE","18 Feb 2025","","","IEEE","IEEE Conferences"
"Automatic Audio-based Screening System for Alzheimer’s Disease Detection","S. -Y. Lin; H. -L. Chang; J. -J. Hwang; T. Wai; Y. -L. Chang; L. -C. Fu","G.I. of Networking and Muiltimedia, National Taiwan University, Taipei, Taiwan; G.I. of Networking and Muiltimedia, National Taiwan University, Taipei, Taiwan; Department of CSIE, National Taiwan University, Taipei, Taiwan; Department of CSIE, National Taiwan University, Taipei, Taiwan; Department of Psychology, National Taiwan University, Taipei, Taiwan; Department of CSIE, National Taiwan University, Taipei, Taiwan","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","2770","2775","Alzheimer’s disease (AD) and other types of dementia have become a public health priority worldwide. To lessen the burden of AD diagnosis, an automatic screening system that can be deployed in large-scale and cost-efficient screening methods will be needed. This paper presents a speech assessment system for cognitive impairment detection, detecting whether elders have AD or suffer from mild cognitive impairment (MCI) based on their audio recordings taken from neuropsychological tests. The audio waveform first is transformed to Mel-spectrogram and done the downsampling. With the combination of Transformer and convolutional neural network (CNN) architecture, we can do the feature extraction and get a better representation for the classifier. We conducted experiments on 120 subjects with a balanced distribution of ordinary aging, MCI, and AD patients to validate our study. Our experiments achieve an accuracy of 91% and 79% for classifying groups of AD and MCI from ordinary aging people, respectively.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945127","Ministry of Science and Technology; National Taiwan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945127","Alzheimer’s disease;mild cognitive impairment;speech assessment system;Transformer;convolutional neural network","Deep learning;Text recognition;Training data;Aging;Feature extraction;Transformers;Data models","","2","","36","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"LibriWASN: A Data Set for Meeting Separation, Diarization, and Recognition with Asynchronous Recording Devices","J. Schmalenstroeer; T. Gburrek; R. Haeb-Umbach",NA; NA; NA,Speech Communication; 15th ITG Conference,"18 Dec 2023","2023","","","86","90","We present LibriWASN, a data set whose design follows closely the LibriCSS meeting recognition data set, with the marked difference that the data is recorded with devices that are randomly positioned on a meeting table and whose sampling clocks are not synchronized. Nine different devices, five smartphones with a single recording channel and four microphone arrays, are used to record a total of 29 channels. Other than that, the data set follows closely the LibriCSS design: the same LibriSpeech sentences are played back from eight loudspeakers arranged around a meeting table and the data is organized in subsets with different percentages of speech overlap. LibriWASN is meant as a test set for clock synchronization algorithms, meeting separation, diarization and transcription systems on ad-hoc wireless acoustic sensor networks. Due to its similarity to LibriCSS, meeting transcription systems developed for the former can readily be tested on LibriWASN. The data set is recorded in two different rooms and is complemented with ground-truth diarization information of who speaks when.","","978-3-8007-6164-7","10.30420/456164016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363004","","","","1","","","","18 Dec 2023","","","VDE","VDE Conferences"
"Multi-Instrument Polyphonic Melody Transcription Based on Deep Learning","R. Zheng; X. Liu; M. D. Butala","ZJU-UIUC Institute, Zhejiang University, Haining, China; ZJU-UIUC Institute, Zhejiang University, Haining, China; ZJU-UIUC Institute, Zhejiang University, Haining, China",2023 3rd International Conference on Consumer Electronics and Computer Engineering (ICCECE),"2 Jun 2023","2023","","","762","767","Automatic music transcription (AMT) is an important task in the field of music information retrieval (MIR). Polyphonic melody transcription remains a challenging problem. Existing neural network models for music transcription are generally tailored to a single instrument, such as a piano or a guitar. In this work, we develop a methodology for polyphonic melody transcription. It is a neural network with high resolution output and high performance which can not only to transcribe polyphonic melodies, but also label the corresponding instrument. We investigate its performance on several datasets as well robustness in a simulated noise experiment.","","979-8-3503-3157-8","10.1109/ICCECE58074.2023.10135397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10135397","Deep learning;neural network;pitch detection;melody recognition;instrument classification","Sensitivity;Instruments;Neural networks;Music;Speech recognition;Predictive models;Robustness","","","","25","IEEE","2 Jun 2023","","","IEEE","IEEE Conferences"
"Converting Vocal Performances into Sheet Music Leveraging Large Language Models","J. Jiang; N. Teo; H. Pen; S. -B. Ho; Z. Wang","Research and Development, Mastercard, Singapore, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore, Singapore; School of Electrical and Information Engineering, Tianjin University, China, China; AI Institute Global, Singapore; School of Computing and Information Systems, Singapore Management University, Singapore, Singapore",2024 IEEE International Conference on Data Mining Workshops (ICDMW),"18 Mar 2025","2024","","","445","452","Advanced natural language processing (NLP) models are increasingly applied in music composition and performance, particularly in generating vocal melodies and simulating singing voices. While NLP techniques have been successfully used to analyze vocal performance data, providing insights into performance quality and style, the automatic transcription of vocal performances into sheet music remains a complex challenge. Existing tools for manual transcription are often insufficient due to the intricate dynamics of vocal expression. This study addresses the challenge of automating the conversion of vocal performances into sheet music using a combination of novel techniques, including large language models (LLMs). We present a method that successfully translates vocal audio input into display-ready sheet music. Our findings highlight the strengths and limitations of various approaches, particularly in the transcription of a cappella performances into notes and lyrics. This research contributes to the expanding field of NLP-driven music analysis and showcases the potential of these models to revolutionize vocal transcription in the future.","2375-9259","979-8-3315-3063-1","10.1109/ICDMW65004.2024.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10917973","Natural Language Processing;Vocal Performance;Automatic Music Transcription (AMT);Large Language Models;Machine Learning;A Cappella;Lyric Transcription;Sheet Music","Hands;Technological innovation;Translation;Accuracy;Large language models;Music;Manuals;Machine learning;Natural language processing;Data mining","","","","54","IEEE","18 Mar 2025","","","IEEE","IEEE Conferences"
"Unified Multimodal Punctuation Restoration Framework for Mixed-Modality Corpus","Y. Zhu; L. Wu; S. Cheng; M. Wang",ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab; ByteDance AI Lab,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","7272","7276","The punctuation restoration task aims to correctly punctuate the output transcriptions of automatic speech recognition systems. Previous punctuation models, either using text only or demanding the corresponding audio, tend to be constrained by real scenes, where unpunctuated sentences are a mixture of those with and without audio. This paper proposes a unified multimodal punctuation restoration framework, named UniPunc, to punctuate the mixed sentences with a single model. UniPunc jointly represents audio and non-audio samples in a shared latent space, based on which the model learns a hybrid representation and punctuates both kinds of samples. We validate the effectiveness of the UniPunc on real-world datasets, which outperforms various strong baselines (e.g. BERT, MuSe) by at least 0.8 overall F1 scores, making a new state-of-the-art. Extensive experiments show that UniPunc’s design is a pervasive solution: by grafting onto previous models, UniPunc enables them to punctuate on the mixed corpus. Our code is available at github.com/Yaoming95/UniPunc","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747131","Speech;Punctuation Restoration;Multimodal","Codes;Conferences;Bit error rate;Signal processing;Acoustics;Task analysis;Speech processing","","6","","23","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Improving Noise Robustness for Spoken Content Retrieval Using Semi-Supervised ASR and N-Best Transcripts for BERT-Based Ranking Models","Y. Moriya; G. J. F. Jones","ADAPT Centre, School of Computing, Dublin City University, Dublin 9, Ireland; ADAPT Centre, School of Computing, Dublin City University, Dublin 9, Ireland",2022 IEEE Spoken Language Technology Workshop (SLT),"27 Jan 2023","2023","","","398","405","BERT-based re-ranking and dense retrieval (DR) systems have been shown to improve search effectiveness for spoken content retrieval (SCR). However, both methods can still show a reduction in effectiveness when using ASR transcripts in comparison to accurate manual transcripts. We find that a known-item search task on the How2 dataset of spoken instruction videos shows a reduction in mean reciprocal rank (MRR) scores of 10-14%. As a potential method to reduce this disparity, we investigate the use of semi-supervised ASR transcripts and N-best ASR transcripts to mitigate ASR errors for spoken search using BERT-based ranking. Semi-supervised ASR transcripts brought 2-5.5% MRR improvements over standard ASR transcripts and our N-best early fusion methods for BERT DR systems improved MRR by 3-4%. Combining semi-supervised transcripts with N-best early fusion for BERT DR reduced the MRR gap in search effectiveness between manual and ASR transcripts by more than 50% from 14.32% to 6.58%.","","979-8-3503-9690-4","10.1109/SLT54892.2023.10023197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023197","spoken content retrieval;BERT re-ranking;BERT dense retrieval;N-best BERT-based retrieval","Conferences;Bit error rate;Manuals;Noise robustness;Task analysis;Standards;Videos","","1","","28","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"YouTube Video Summarizer Using ASR","R. Patil; A. Buchade; G. Yadav; N. Sharma; S. Joshi; A. Bhokare","Department of Artificial Intelligence and Data Science, Vishwakarma Institute of Information Technology, Pune, Maharashtra, India; Department of Artificial Intelligence and Data Science, Vishwakarma Institute of Information Technology, Pune, Maharashtra, India; Department of Artificial Intelligence and Data Science, Vishwakarma Institute of Information Technology, Pune, Maharashtra, India; Department of Artificial Intelligence and Data Science, Vishwakarma Institute of Information Technology, Pune, Maharashtra, India; Department of Artificial Intelligence and Data Science, Vishwakarma Institute of Information Technology, Pune, Maharashtra, India; Department of Artificial Intelligence and Data Science, Vishwakarma Institute of Information Technology, Pune, Maharashtra, India",2024 IEEE International Conference on Blockchain and Distributed Systems Security (ICBDS),"17 Jan 2025","2024","","","1","5","Platforms like YouTube have revolutionized the way people distribute and consume information in the video-dominated digital age. However, viewers who are trying to get pertinent information quickly may find it difficult due to the sheer volume of videos. This research describes a system for summarizing YouTube videos that effectively extracts information using Term Frequency-Inverse Document Frequency (TF-IDF) and Automatic Speech Recognition (ASR) analysis. ASR translates audio to text, and TF-IDF finds the most important information for brief summaries. Empirical validation supports the optimization of usability through automation and personalization. The technology facilitates navigation through large amounts of video footage by supporting Natural Language processing (NLP) and information retrieval.","","979-8-3503-5434-8","10.1109/ICBDS61829.2024.10837316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10837316","Video Summarization;Automatic Speech Recognition (ASR);Speech-to-Text (STT);Term Frequency-Inverse Document Frequency (TF-IDF);Information Retrieval;Multimedia Analysis;Extractive Summarization","Video on demand;Translation;Text summarization;Natural language processing;User experience;Web sites;Data mining;Security;Usability;Videos","","","","20","IEEE","17 Jan 2025","","","IEEE","IEEE Conferences"
"Joint Multi-Pitch Detection and Score Transcription for Polyphonic Piano Music","L. Liu; V. Morfi; E. Benetos","Centre for Digital Music, Queen Mary University of London, UK; Centre for Digital Music, Queen Mary University of London, UK; Centre for Digital Music, Queen Mary University of London, UK","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","281","285","Research on automatic music transcription has largely focused on multi-pitch detection; there is limited discussion on how to obtain a machine- or human-readable score transcription. In this paper, we propose a method for joint multi-pitch detection and score transcription for polyphonic piano music. The outputs of our system include both a piano-roll representation (a descriptive transcription) and a symbolic musical notation (a prescriptive transcription). Unlike traditional methods that further convert MIDI transcriptions into musical scores, we use a multitask model combined with a Convolutional Recurrent Neural Network and Sequence-to-sequence models with attention mechanisms. We propose a Reshaped score representation that outperforms a LilyPond representation in terms of both prediction accuracy and time/memory resources, and compare different input audio spectrograms. We also create a new synthesized dataset for score transcription research. Experimental results show that the joint model outperforms a single-task model in score transcription.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413601","Automatic music transcription;sequence-tosequence models;score transcription","Recurrent neural networks;Convolution;Conferences;Music;Multiple signal classification;Task analysis;Speech processing","","7","","24","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Infographics Generator: A Smart Application for Visual Summarization","M. Nath; L. Ethirajan","IT & DA Boeing India Private Limited, Bangalore, India; IT & DA Boeing India Private Limited, Bangalore, India",2023 16th International Conference on Developments in eSystems Engineering (DeSE),"21 Mar 2024","2023","","","630","635","Visuals help in quickly interpreting complex information. Infographics are visual representations of information, that use charts, icons, or graphical illustrations with minimal text. Manually summarizing information into an infographic representation is not only a time-consuming activity but also requires additional skills in terms of tools and domain knowledge. Here we present a novel idea to perform a smart info-graphical summarization of any audio, video, voice, text input, using generative AI techniques. The input contents are fed into a preprocessor, which uses video converters and speech-to-text algorithms, to consolidate the input into a single text document. From this text, a summary is generated by using pre-trained language models. Along with it, the keywords associated with each of the summary entries are extracted. Based on the keywords, the most relevant icons and graphic assets are picked from the image library. A suitable infographic template is identified based on the topic and an intuitive infographic is generated. An infographic not only provides a quick overview of the contents but also reduces the cognitive load or the mental energy required to interpret the contents otherwise. This solution can thus provide a crisp summary in near real-time.","","979-8-3503-8134-4","10.1109/DeSE60595.2023.10469225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10469225","infographics;large language model;LLM summarizer;NLP;pre-trained language model;visual summary","Visualization;Generative AI;Data preprocessing;Cognitive load;Real-time systems;Libraries;Generators","","","","19","IEEE","21 Mar 2024","","","IEEE","IEEE Conferences"
"An Integrated Platform for Lie Detection and Plagiarism analysis in Interviews","S. Snehalakshmi; S. A. Lakshmanan; B. Devanathan","Department of CCE, Amrita school of Engineering Amrita Vishwa Vidyapeetham, Chennai, India; Department of CCE, Amrita school of Engineering Amrita Vishwa Vidyapeetham, Chennai, India; Department of CCE, Amrita school of Engineering Amrita Vishwa Vidyapeetham, Chennai, India",2024 IEEE Conference on Engineering Informatics (ICEI),"12 Mar 2025","2024","","","1","6","This paper discusses an advanced interview platform that integrates AI-driven technologies for interview integrity and authenticity. It is possible to record video-based interviews with built-in functionalities including identifying lying, plagiarism analysis, and stress evaluation. Video recording is carried out simultaneously while concurrently recording the audio; transcripts are also generated. The transcripts are consequently passed through the Winston AI plagiarism checker to search for signs of internet usage during the interview. Facial analysis is used to measure stress and detect deception in real-time. The tool generates a comprehensive integrity report and provides an innovative tool to carry out safe and reliable interviews.","","979-8-3315-0577-6","10.1109/ICEI64305.2024.10912205","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10912205","Video evaluation;audio transcription;Stress analysis;Fair interviews;Lie and Plagiarism detection","Technological innovation;Plagiarism;Manuals;Real-time systems;Reliability;Interviews;Stress;Video recording;Stress measurement;Lenses","","","","16","IEEE","12 Mar 2025","","","IEEE","IEEE Conferences"
"Exploring the Role of Audio in Video Captioning","Y. Shen; L. Yang; L. Wen; H. Yu; E. Elhamifar; H. Wang",Northeastern University; ByteDance; ByteDance; ByteDance; Northeastern University; ByteDance,2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"27 Sep 2024","2024","","","2090","2100","Recent focus in video captioning has been on designing architectures that can consume both video and text modalities, and using large-scale video datasets with text transcripts for pre-training, such as HowTo100M. Though these approaches have achieved significant improvement, the audio modality is often ignored in video captioning. In this work, we present an audio-visual framework, which aims to fully exploit the potential of the audio modality for captioning. Instead of relying on text transcripts extracted via automatic speech recognition (ASR), we argue that learning with raw audio signals can be more beneficial, as audio has additional information including acoustic events, speaker identity, etc. Our contributions are twofold. First, we observed that the model overspecializes to the audio modality when pre-training with both video and audio modality, since the ground truth (i.e., text transcripts) can be solely predicted using audio. We proposed a Modality Balanced Pre-training (MBP) loss to mitigate this issue and significantly improve the performance on downstream tasks. Second, we slice and dice different design choices of the cross-modal module, which may become an information bottleneck and generate inferior results. We proposed new local-global fusion mechanisms to improve information exchange across audio and video. We demonstrate significant improvements by leveraging the audio modality on four datasets, and even outperform the state of the art on some metrics without relying on the text modality as the input.","2160-7516","979-8-3503-6547-4","10.1109/CVPRW63382.2024.00214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10678632","","Measurement;Computer vision;Computational modeling;Conferences;Predictive models;Acoustics;Data mining","","","","71","IEEE","27 Sep 2024","","","IEEE","IEEE Conferences"
"An Exploration of Hubert with Large Number of Cluster Units and Model Assessment Using Bayesian Information Criterion","T. Maekaku; X. Chang; Y. Fujita; S. Watanabe","Yahoo Japan Corporation, Tokyo, Japan; Carnegie Mellon University, PA, USA; Yahoo Japan Corporation, Tokyo, Japan; Carnegie Mellon University, PA, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","7107","7111","Self-supervised learning (SSL) has become one of the most important technologies to realize spoken dialogue systems for languages that do not have much audio data and its transcription available. Speech representation models are one of the keys to achieving this, and have been actively studied in recent years. Among them, Hidden-Unit BERT (HuBERT) has shown promising results in automatic speech recognition (ASR) tasks. However, previous studies have investigated with limited iterations and cluster units. We explore HuBERT with larger numbers of clusters and iterations in order to obtain better speech representation. Furthermore, we introduce the Bayesian Information Criterion (BIC) as the performance measure of the model. Experimental results show that our model achieves the best performance in 5 out of 8 scores in the 4 metrics for the Zero Resource Speech 2021 task. It also outperforms the HuBERT BASE model trained with 960-hour LibriSpeech (LS) even though our model is only trained with 100-hour LS. In addition, we report that BIC is useful as a clue for determining the appropriate number of clusters to improve performance on phonetic, lexical, and syntactic metrics. Finally, we show that these findings are also effective for the ASR task.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746097","HuBERT;unit-based language model;self-supervised learning;acoustic unit discovery;BIC","Measurement;Training;Conferences;Bit error rate;Syntactics;Phonetics;Signal processing","","2","","30","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"QueryMintAI: Multipurpose Multimodal Large Language Models for Personal Data","A. Ghosh; K. Deepa","School of Computer Science and Engineering, Vellore Institute of Technology (VIT), Vellore, India; School of Computer Science and Engineering, Vellore Institute of Technology (VIT), Vellore, India",IEEE Access,"10 Oct 2024","2024","12","","144631","144651","QueryMintAI, a versatile multimodal Language Learning Model (LLM) designed to address the complex challenges associated with processing various types of user inputs and generating corresponding outputs across different modalities. The proliferation of diverse data formats, including text, images, videos, documents, URLs, and audio recordings, necessitates an intelligent system capable of understanding and responding to user queries effectively. Existing models often exhibit limitations in handling multimodal inputs and generating coherent outputs across different modalities. The proposed QueryMintAI framework leverages state-of-the-art language models such as GPT-3.5 Turbo, DALL-E-2, TTS-1 and Whisper v2 among others, to enable seamless interaction with users across multiple modalities. By integrating advanced natural language processing (NLP) techniques with domain-specific models, QueryMintAI offers a comprehensive solution for text-to-text, text-to-image, text-to-video, and text-to-audio conversions. Additionally, the system supports document processing, URL analysis, image description, video summarization, audio transcription, and database querying, catering to diverse user needs and preferences. The proposed model addresses several limitations observed in existing approaches, including restricted modality support, lack of adaptability to various data formats, and limited response generation capabilities. QueryMintAI overcomes these challenges by employing a combination of advanced NLP algorithms, deep learning architectures, and multimodal fusion techniques.","2169-3536","","10.1109/ACCESS.2024.3468996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10695061","Multimodal large language models;generative AI;private database;Langchain;OpenAI","Context modeling;Accuracy;Videos;Natural language processing;Computational modeling;Adaptation models;Deep learning;Large language models;Generative AI;Open source software","","","","48","CCBY","26 Sep 2024","","","IEEE","IEEE Journals"
"Effective Text Adaptation For LLM-Based ASR Through Soft Prompt Fine-Tuning","Y. Ma; Z. Liu; O. Kalinli","Meta AI, Menlo Park, CA, USA; Meta AI, Menlo Park, CA, USA; Meta AI, Menlo Park, CA, USA",2024 IEEE Spoken Language Technology Workshop (SLT),"16 Jan 2025","2024","","","64","69","The advent of Large Language Models (LLM) has reformed the Automatic Speech Recognition (ASR). Prompting LLM with audio embeddings to generate transcriptions becomes the new state-of-the-art ASR. Despite LLMs being trained with an extensive amount of text corpora, high-quality domain-specific text data can still significantly enhance ASR performance on domain adaptation tasks. Although LLM-based ASR can naturally incorporate more text corpora by fine-tuning the LLM decoder, fine-tuning such ASR on text-only data without paired prompts may diminish the effectiveness of domain-specific knowledge. To mitigate this issue, we propose a two-step soft prompt fine-tuning strategy that enhances domain-specific text adaptation. Experimental results show that text adaptation with our proposed method achieved a relative up to 9% Word Error Rate (WER) reduction and up to 18% Entity Error Rate (EER) reduction on the target domain compared to the baseline ASR. Combining this with domain-specific Language Model (LM) fusion can further improve the EER by a relative 2-5%.","","979-8-3503-9225-8","10.1109/SLT61566.2024.10832227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10832227","Text adaptation;LLM-based ASR;soft prompt","Adaptation models;Error analysis;Large language models;Conferences;Decoding;Domain specific languages;Automatic speech recognition","","","","32","IEEE","16 Jan 2025","","","IEEE","IEEE Conferences"
"Bertraffic: Bert-Based Joint Speaker Role and Speaker Change Detection for Air Traffic Control Communications","J. Zuluaga-Gomez; S. S. Sarfjoo; A. Prasad; I. Nigmatulina; P. Motlicek; K. Ondrej; O. Ohneiser; H. Helmke","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Brno University of Technology, Brno, Czech Republic; German Aerospace Center (DLR), Institute of Flight Guidance, Braunschweig, Germany; German Aerospace Center (DLR), Institute of Flight Guidance, Braunschweig, Germany",2022 IEEE Spoken Language Technology Workshop (SLT),"27 Jan 2023","2023","","","633","640","Automatic speech recognition (ASR) allows transcribing the communications between air traffic controllers (ATCOs) and aircraft pilots. The transcriptions are used later to extract ATC named entities, e.g., aircraft callsigns. One common challenge is speech activity detection (SAD) and speaker diarization (SD). In the failure condition, two or more segments remain in the same recording, jeopardizing the overall performance. We propose a system that combines SAD and a BERT model to perform speaker change detection and speaker role detection (SRD) by chunking ASR transcripts, i.e., SD with a defined number of speakers together with SRD. The proposed model is evaluated on real-life public ATC databases. Our BERT SD model baseline reaches up to 10% and 20% token-based Jaccard error rate (JER) in public and private ATC databases. We also achieved relative improvements of 32% and 7.7% in JERs and SD error rate (DER), respectively, compared to VBx, a well-known SD system.11Our code is stored in the following public GitHub repository: https://github.com/idiap/bert-text-diarization-atc","","979-8-3503-9690-4","10.1109/SLT54892.2023.10022718","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10022718","Text-based speaker diarization;speaker change detection;speaker role detection;air traffic control communications;chunking","Voice activity detection;Databases;Bit error rate;Pipelines;Traffic control;Recording;Air traffic control","","7","","49","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"Deep Learning Approaches in Topics of Singing Information Processing","C. Gupta; H. Li; M. Goto","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; Chinese University of Hong Kong, Shenzhen, China; National Institute of Advanced Industrial Science and Technology (AIST), Ibaraki, Japan","IEEE/ACM Transactions on Audio, Speech, and Language Processing","29 Jul 2022","2022","30","","2422","2451","Singing, the vocal productionof musical tones, is one of the most important elements of music. Addressing the needs of real-world applications, the study of technologies related to singing voices has become an increasingly active area of research. In this paper, we provide a comprehensive overview of the recent developments in the field of singing information processing, specifically in the topics of singing skill evaluation, singing voice synthesis, singing voice separation, and lyrics synchronization and transcription. We will especially focus on deep learning approaches including modern representation learning techniques for singing voices. We will also provide an overview of contributions in public datasets for singing voice research.","2329-9304","","10.1109/TASLP.2022.3190732","Academic Research Council; Ministry of Education - Singapore(grant numbers:MOE2018-T2-2-127); Guangdong Provincial Key Laboratory of Big Data Computing; The Chinese University of Hong Kong, Shenzhen, China(grant numbers:B10120210117-KP02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829265","Singing information processing;singing voice;singing skill evaluation;singing voice synthesis;singing voice separation;lyrics synchronization;lyrics transcription","Music;Information processing;Synchronization;Speech processing;Rhythm;Instruments;Deep learning","","10","","242","CCBY","13 Jul 2022","","","IEEE","IEEE Journals"
"Toward Leveraging Pre-Trained Self-Supervised Frontends for Automatic Singing Voice Understanding Tasks: Three Case Studies","Y. Yamamoto","University of Tsukuba, Tsukuba, Japan",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","1745","1752","Automatic singing voice understanding tasks, such as singer identification, singing voice transcription, and singing technique classification, benefit from data-driven approaches that utilize deep learning techniques. These approaches work well even under the rich diversity of vocal and noisy samples owing to their representation ability. However, the limited availability of labeled data remains a significant obstacle to achieving satisfactory performance. In recent years, self-supervised learning models (SSL models) have been trained using large amounts of unlabeled data in the field of speech processing and music classification. By fine-tuning these models for the target tasks, comparable performance to conventional supervised learning can be achieved with limited training data. Therefore, in this paper, we investigate the effectiveness of SSL models for various singing voice recognition tasks. We report the results of experiments comparing SSL models for three different tasks (i.e., singer identification, singing voice transcription, and singing technique classification) as initial exploration and aim to discuss these findings. Experimental results show that each SSL model achieves comparable performance and sometimes outperforms compared to state-of-the-art methods on each task. We also conducted a layer-wise analysis to further understand the behavior of the SSL models.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317286","National Taiwan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317286","","Training;Transfer learning;Supervised learning;Training data;Speech recognition;Self-supervised learning;Data models","","1","","60","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Movie Genre Classification by Language Augmentation and Shot Sampling","Z. Zhang; Y. Gu; B. A. Plummer; X. Miao; J. Liu; H. Wang",Boston University; Boston University; Boston University; Kuaishou Technology; Kuaishou Technology; Kuaishou Technology,2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),"9 Apr 2024","2024","","","7260","7270","Video-based movie genre classification has garnered considerable attention due to its various applications in recommendation systems. Prior work has typically addressed this task by adapting models from traditional video classification tasks, such as action recognition or event detection. However, these models often neglect language elements (e.g., narrations or conversations) present in videos, which can implicitly convey high-level semantics of movie genres, like storylines or background context. Additionally, existing approaches are primarily designed to encode the entire content of the input video, leading to inefficiencies in predicting movie genres. Movie genre prediction may require only a few shots1 to accurately determine the genres, rendering a comprehensive understanding of the entire video unnecessary. To address these challenges, we propose a Movie genre Classification method based on Language augmentatIon and shot samPling (Movie-CLIP). MovieCLIP mainly consists of two parts: a language augmentation module to recognize language elements from the input audio, and a shot sampling module to select representative shots from the entire video. We evaluate our method on MovieNet and Condensed Movies datasets, achieving approximate 6 − 9% improvement in mean Average Precision (mAP) over the baselines. We also generalize Movie-CLIP to the scene boundary detection task, achieving 1.1% improvement in Average Precision (AP) over the state-of-the-art. We release our implementation at this http URL.","2642-9381","979-8-3503-1892-0","10.1109/WACV57701.2024.00711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483588","Applications;Arts / games / social media;Algorithms;Video recognition and understanding","Uniform resource locators;Adaptation models;Event detection;Computational modeling;Semantics;Oral communication;Motion pictures","","3","","64","IEEE","9 Apr 2024","","","IEEE","IEEE Conferences"
"CTC2: End-to-End Drum Transcription Based on Connectionist Temporal Classification With Constant Tempo Constraint","D. Kamakura; E. Nakamura; K. Yoshii","Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","158","164","This paper describes end-to-end automatic drum transcription for directly estimating a drum score from an audio signal of popular music using non-aligned paired data. We aim to convert a sequence of frame-level acoustic features into a sequence of tatum-level score fragments (three-dimensional multi-hot vectors) representing the presence or absence of the onsets of the bass and snare drums and the hi-hats. The main challenge of this task lies in estimating the correct number of inactive tatums having no onset between active tatums. One may use the connectionist temporal classification (CTC) for end-to-end training of a deep neural network (DNN) that infers a frame-level state sequence (alignment path) including the special ""blank"" states representing the tatum boundaries. At run-time, a drum score is obtained by annexing repeated states and removing all blank states from the most likely frame-level state sequence. This approach, however, tends to yield a shortened drum score in which repeated inactive tatums are annexed mistakenly because the blank state (tatum boundary) cannot be distinguished acoustically from the inactive state (onset absence) at the frame level. In this paper, we propose a sophisticated version of the CTC with constant tempo constraint, CTC2 in short, that encourages each tatum to be aligned with almost the same number of frames. Although the loss function can be computed efficiently as in the basic CTC, the backpropagation over the huge computation graph made through the forward algorithm is computationally prohibitive. To solve this problem, we propose to perform the backpropagation with only an alignment path stochastically drawn with Gibbs sampling. The experiment showed that the proposed method worked well as expected.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317515","","Training;Backpropagation;Viterbi algorithm;Artificial neural networks;Information processing;Computational efficiency;Multiple signal classification","","","","33","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Deception Detection using a Multimodal Stacked Bi-LSTM Model","P. K. Sehrawat; R. Kumar; N. Kumar; D. K. Vishwakarma","Department of Information Technology, Delhi Technological University, Delhi, India; Department of Information Technology, Delhi Technological University, Delhi, India; Department of Information Technology, Delhi Technological University, Delhi, India; Department of Information Technology, Delhi Technological University, Delhi, India",2023 International Conference on Innovative Data Communication Technologies and Application (ICIDCA),"20 Apr 2023","2023","","","318","326","In the scientific community, researchers have recently become interested in the automatic identification of deceptive actions because of the range of fields where it might be advantageous, including criminology or security. Deception detection in conversational speech has drawn much attention in recent years. Given the significant risks associated with trial outcomes, using precise and efficient computational methods to assess the accuracy of court evidence may be very beneficial throughout the decision-making process. This study discusses about spotting deception in trial data from actual cases. Doing this has some challenges associated with creating a robust model that can accurately classify the deception and perform this action as fast as possible. Due to the limited number of videos and datasets available, some models overfit the data. A model should exist which can classify the data using various modalities, i.e., video, audio and text, and be able to work on multiple different datasets with excellent accuracy. This study has used videos from actual trials that were collected from open court proceedings and some videos from other datasets. To design a robust deception detection system that discriminates between witnesses and defendants, genuine and fraudulent testimony, this study investigates the utilization of text, audio and video modalities. By extracting and integrating information about the spoken words from audio, this study can achieve an accuracy of 80% approximately. The proposed model results with a classification accuracy of 96% approximately in an extended approach to perform video transcriptions. The Bag-of-lies dataset, a multimodal database captured in real-world settings has achieved an accuracy of 85%. The Miami University Deception Detection Dataset focuses on people telling truths and lies about their social relationships, achieved an accuracy of 98.1 % on the presented model. The proposed model employs LSTM (Long-Short Term Memory), Bidirectional LSTM (Long-Short Term Memory), CNN (Convolution Neural Network), and RestNet50. The results demonstrate that the proposed algorithm performs better at detecting deception than humans.","","979-8-3503-9720-8","10.1109/ICIDCA56705.2023.10099779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10099779","Deception Detection;Criminology;CNN;Long Short-Term Memory;RestNet50 (Residual Network 50);Multimodal","Analytical models;Neural networks;Decision making;Interference;Data models;Real-time systems;Data mining","","5","","22","IEEE","20 Apr 2023","","","IEEE","IEEE Conferences"
"Audio-to-Score Singing Transcription Based on Joint Estimation of Pitches, Onsets, and Metrical Positions With Tatum-Level CTC Loss","T. Deng; E. Nakamura; K. Yoshii","Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan; Graduate School of Informatics, Kyoto University, Japan",2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"20 Nov 2023","2023","","","583","590","This paper describes an end-to-end singing transcription method that directly estimates a musical score (a sequence of sung notes with metrical positions) from a music audio signal. The monotonicity of audio-to-score mapping naturally calls for the use of connectionist temporal classification (CTC). Inspired by the success of character-level automatic speech recognition, previous studies on CTC-based music transcription represent a musical score as a sequence of various kinds of symbols (e.g., note pitches and values and barlines) defined in some music notation. Such a naive notation-respecting representation, however, does not fit the non-overlapping monotonic audio-to-symbol alignment and the positions of barlines and the durations of beats tend to be incoherent in the estimated score. To solve this problem, we propose a tatum-level singing transcription method that jointly estimates the pitch (including rest), onset flag, and metrical position at each tatum. Our approach enables the tatums to be monotonically aligned with regularly-spanned intervals of the music signal and the estimated notes are located on the estimated metrical positions that are encouraged to be periodic. Experimental results clearly showed that our proposed model reached comparable accuracy on score-level singing transcription with only unaligned training data, and the proposed tatum-level representation significantly improved the stability of the metrical structures in the estimated scores.","2640-0103","979-8-3503-0067-3","10.1109/APSIPAASC58517.2023.10317419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317419","","Meters;Training;Estimation;Training data;Symbols;Rhythm;Loss measurement","","1","","30","IEEE","20 Nov 2023","","","IEEE","IEEE Conferences"
"Mandarin Speakers' Acquisitions and Representations of Flapping in American English in An ESL Context: A Perception and Production Study","C. -W. Chuang","Department of English, National Chengchi University",2021 24th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA),"3 Jan 2022","2021","","","106","110","Previous discussion about perception and production of flapping in American English mostly centers on native speakers' exploration. The present study offers a new insight into flapping in American English learners. To investigate American-English-trained Taiwan ESL learners' perception and production of [ſ], two experiments were conducted, with completely different subjects. English major (Linguistically-trained) and non-English major college students were invited to the experiments: (1) In the production experiment, both subject groups would read 14 high-frequency words (7 with /t/ and 7 with /d/), whereby word-medial flapping was acceptable, two times in random order. (2) The perception test explored the sensitivity of [ſ] identification. English major students were asked to present phonetic transcriptions while those non-English majors judged that the recordings sounded more [t]-alike or [d]-alike. Results showed more occurrence of flapping laid on English major students but they could have perceptual confusion over [d, ſ] contrast. Hypercorrection, where [ſ] might be pronounced as [d] to seek audio resemblance, was also found in English maj ors. The study generally implicated [ſ] could be well pronounced by Mandarin speakers under more exposure to English and linguistic training. The overall findings can enlighten English pronunciation education in ESL contexts and potentially help ameliorate Automatic Speech Recognition (ASR) on non-English-native users' interfaces.","2472-7695","978-1-6654-0870-7","10.1109/O-COCOSDA202152914.2021.9660593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660593","Intervocalic flapping;Taiwan ESL;American English;Perception and production","Training;Sensitivity;Text recognition;Databases;Production;User interfaces;Phonetics","","","","10","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Abstractive Summarization for Video: A Revisit in Multistage Fusion Network With Forget Gate","N. Liu; X. Sun; H. Yu; F. Yao; G. Xu; K. Fu","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Multimedia,"8 Aug 2023","2023","25","","3296","3310","Multimodal abstractive summarization for videos is an emerging task that aims to generate a summary from multi-source information (i.e., video, audio transcript). The challenge is how to merge multimodal long sequences to capture rich semantic information without allowing possible noise from either lengthy modal sequence to degrade the other modality and thus hurt the entire model. To address the issues, we propose a multistage fusion network with forget gate (MFFG), which selectively integrates multi-source information through the cross-fusion in encoding and hierarchical fusion in decoding between modalities, and design a fusion forget gate module to suppress the potential multimodal noise flow of multi-source long sequence. Meanwhile, considering that the source text in this task is lengthy and has the same distribution as the output summary text, we inherit the partial structure of the MFFG model and again propose its variant, single-stage fusion network with forget gate (SFFG), which simplifies the fusion schema, and leverages the long source text to enhance the representation of the target summary. Experimental results on How2 dataset and How2-300 dataset demonstrate the superiority of the two multimodal fusion methods. Further, we provide a version of ASR transcription data of How2 dataset to evaluate model performance under noisy scenarios, and experimental results show obvious advantages of our proposed models over prior systems.","1941-0077","","10.1109/TMM.2022.3157993","National Science Fund for Distinguished Young Scholars of China(grant numbers:61725105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732216","Abstractive summarization;abstractive summarization for videos;multimodal;multimodal fusion;forget gate","Logic gates;Task analysis;Noise measurement;Decoding;Data models;Periodic structures;Correlation","","2","","49","IEEE","9 Mar 2022","","","IEEE","IEEE Journals"
"SynthoTranslate: Multimodal Knowledge Assimilation Tool for Overcoming Auditory Barriers and Bridging Language Barriers","P. Dhabe; O. Joshi; J. Barhate; B. Kundu; A. Nandeshwar; M. Gujar","Information Technology, Vishwakarma Institute of Technology, Pune; Information Technology, Vishwakarma Institute of Technology, Pune; Information Technology, Vishwakarma Institute of Technology, Pune; Information Technology, Vishwakarma Institute of Technology, Pune; Information Technology, Vishwakarma Institute of Technology, Pune; Information Technology, Vishwakarma Institute of Technology, Pune",2024 6th International Conference on Computational Intelligence and Networks (CINE),"18 Feb 2025","2024","","","1","6","This paper introduces an automated system to improve educational accessibility through multilingual video translation and dubbing. It uses advanced speech-to-text (STT) technologies, natural language processing (NLP), and voice conversion models for accurate subtitles, translations, and dubbed audio in Indian languages. Key components include Whisper and Wav2vec for subtitle generation, GoogleTrans for translation, and Silero models for natural-sounding voice cloning. Indian Sign Language (ISL) transcription is integrated to support hearing-impaired students, with Wav2Lip ensuring precise lip-syncing of dubbed audio. Evaluation shows a Word Error Rate (WER) of 1.9% and cosine similarities over $\mathbf{9 5 \%}$. The system also offers content summarization and Q&A to enhance engagement. Future work will focus on optimizing speech recognition and improving ISL transcription for greater accessibility.","","979-8-3315-1679-6","10.1109/CINE63708.2024.10881585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10881585","Multilingual Translation;Voice Cloning;Indian Sign Language (ISL);Video Dubbing and Subtitling","Sign language;Solid modeling;Translation;Three-dimensional displays;Error analysis;Cloning;Natural language processing;Multilingual;Human voice;Speech to text","","","","20","IEEE","18 Feb 2025","","","IEEE","IEEE Conferences"
"Zero-Shot Singing Voice Synthesis from Musical Score","J. -Y. Wang; H. -Y. Lee; J. -S. R. Jang; L. Su","National Taiwan University, Taiwan; National Taiwan University, Taiwan; National Taiwan University, Taiwan; Academia Sinica, Institute of Information Science, Taiwan",2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),"19 Jan 2024","2023","","","1","8","Zero-shot singing voice synthesis (SVS), the task to synthesize the singing voice of an arbitrary target singer, has gained increasing attentions in the past few years. Several recently proposed systems have demonstrated promising results on this task. However, these systems require detailed musical features at the frame level as the musical content. To deal with this issue, we propose a model that performs zero-shot SVS with only musical score as the musical content condition. To help model training, we build an acoustic encoder that extracts linguistic features from audio, and train it with the lyrics transcription objective. The output of the acoustic encoder serves as an alternative to the musical score, allowing the SVS model to learn from weakly labeled data. Results suggest that the proposed method outperforms baseline semi-supervised method in both subjective and objective tests.","","979-8-3503-0689-7","10.1109/ASRU57964.2023.10389711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10389711","Singing voice synthesis;zero-shot;semi-weakly-supervised learning","Training;Conferences;Linguistics;Feature extraction;Acoustics;Data models;Encoding","","1","","30","IEEE","19 Jan 2024","","","IEEE","IEEE Conferences"
"Teager Energy Cepstral Coefficients for Spoken Language Identification","A. J. Shah; S. H. Yadav; H. A. Patil","Dhirubhai Ambani Institute of Information and Communication Technology, Gujarat, India; Uka Tarsadia University, Bardoli, Gujarat, India; Dhirubhai Ambani Institute of Information and Communication Technology, Gujarat, India",2024 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),"27 Jan 2025","2024","","","1","6","Spoken Language Identification (SLID) is a key component in audio processing that facilitates the recognition and understanding of audio clips of spoken languages. Various applications, such as automatic speech recognition, multilingual voice assistants, and real-time translation services need SLID capabilities. Effective SLID enhances the transcription and processing of spoken language data and improves user experiences by providing personalized and relevant responses in the correct language. This study propose Teager Energy Cepstral Coefficients (TECC) features to capture the characteristics of spoken language. To evaluate how well TECC performs for SLID, we conducted a comparative analysis of its performance across various spectral features, namely, Mel Frequency Cepstral Coefficients (MFCC), and Linear Frequency Cepstral Coefficients (LFCC). Two classifiers, deep residual networks (ResNet-50) and Time Delay Neural Networks (TDNN), were employed in this comparison. To maximize the performance of SLID system, we applied feature-level fusion and score-level Fusion techniques to advance the state-of-the-art. Additionally, latency analysis assesses time optimization to ensure the system operates efficiently. We obtained 98.25 % accuracy in this study for two languages, and 84.25 % on 10 different languages using TECC features.","2640-0103","979-8-3503-6733-1","10.1109/APSIPAASC63619.2025.10849047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10849047","Spoken Language Identification;Teager Energy Cepstral Coefficients;ResNet-50;Feature-Level Fusion;Score-Level Fusion","Translation;Delay effects;Neural networks;Personal voice assistants;Information processing;Real-time systems;Multilingual;Mel frequency cepstral coefficient;Optimization;Residual neural networks","","","","28","IEEE","27 Jan 2025","","","IEEE","IEEE Conferences"
"Multimodal Depression Classification using Articulatory Coordination Features and Hierarchical Attention Based text Embeddings","N. Seneviratne; C. Espy-Wilson","University of Maryland, College Park, USA; University of Maryland, College Park, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","6252","6256","Multimodal depression classification has gained immense popularity over the recent years. We develop a multimodal depression classification system using articulatory coordination features extracted from vocal tract variables and text transcriptions obtained from an automatic speech recognition tool that yields improvements of area under the receiver operating characteristics curve compared to unimodal classifiers (7.5% and 13.7% for audio and text respectively). We show that in the case of limited training data, a segment-level classifier can first be trained to then obtain a session-wise prediction without hindering the performance, using a multi-stage convolutional recurrent neural network. A text model is trained using a Hierarchical Attention Network (HAN). The multimodal system is developed by combining embeddings from the session-level audio model and the HAN text model.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747462","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747462","depression detection;multimodal;vocal tract variables;articulatory coordination features;hierarchical attention","Recurrent neural networks;Training data;Receivers;Predictive models;Linguistics;Depression;Feature extraction","","5","","30","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
