@article{10.1109/TASLP.2024.3363414,
author = {Zhou, Xuehao and Zhang, Mingyang and Zhou, Yi and Wu, Zhizheng and Li, Haizhou},
title = {Accented Text-to-Speech Synthesis With Limited Data},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3363414},
doi = {10.1109/TASLP.2024.3363414},
abstract = {This paper presents an accented text-to-speech (TTS) synthesis framework with limited training data. We study two aspects concerning accent rendering: phonetic (phoneme difference) and prosodic (pitch pattern and phoneme duration) variations. The proposed accented TTS framework consists of two models: an accented front-end for grapheme-to-phoneme (G2P) conversion and an accented acoustic model with integrated pitch and duration predictors for phoneme-to-Mel-spectrogram prediction. The accented front-end directly models the phonetic variation, while the accented acoustic model explicitly controls the prosodic variation. Specifically, both models are first pre-trained on a large amount of data, then only the accent-related layers are fine-tuned on a limited amount of data for the target accent. In the experiments, speech data of three English accents, i.e., General American English, Irish English, and British English Received Pronunciation, are used for pre-training. The pre-trained models are then fine-tuned with Scottish and General Australian English accents, respectively. Both objective and subjective evaluation results show that the accented TTS front-end fine-tuned with a small accented phonetic lexicon (&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$text{5},k$&lt;/tex-math&gt;&lt;/inline-formula&gt; words) effectively handles the phonetic variation of accents, while the accented TTS acoustic model fine-tuned with a limited amount of accented speech data (approximately 3 minutes) effectively improves the prosodic rendering including pitch and duration. The overall accent modeling contributes to improved speech quality and accent similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1699–1711},
numpages = {13}
}

@inproceedings{10.1145/3664647.3681348,
author = {Xiao, Yujia and Wang, Xi and Tan, Xu and He, Lei and Zhu, Xinfa and Zhao, Sheng and Lee, Tan},
title = {Contrastive Context-Speech Pretraining for Expressive Text-to-Speech Synthesis},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681348},
doi = {10.1145/3664647.3681348},
abstract = {The latest Text-to-Speech (TTS) systems can produce speech with voice quality and naturalness comparable to human speech. Yet the demand for large amount of high-quality data from target speakers remains a significant challenge. Particularly for long-form expressive reading, target speaker's training speech that covers rich contextual information are needed. In this paper a novel design of context-aware speech pre-trained model is developed for expressive TTS based on contrastive learning. The model can be trained with abundant speech data without explicitly labelled speaker identities. It captures the intricate relationship between the speech expression of a spoken sentence and the contextual text information. By incorporating cross-modal text and speech features into the TTS model, it enables the generation of coherent and expressive speech, which is especially beneficial when there is a scarcity of target speaker data. The pre-trained model is evaluated first in the task of Context-Speech retrieval and then as the integral part of a zero-shot TTS system. Experimental results demonstrate that the pretraining framework effectively learns Context-Speech representations and significantly enhances the expressiveness of synthesized speech. Audio demos are available at: https://ccsp2024.github.io/demo/.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {2099–2107},
numpages = {9},
keywords = {contextual modeling, contrastive learning, cross-modal learning, pretraining model, speech synthesis},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1109/TASLP.2024.3350893,
author = {Gong, Xun and Wu, Yu and Li, Jinyu and Liu, Shujie and Zhao, Rui and Chen, Xie and Qian, Yanmin},
title = {Advanced Long-Content Speech Recognition With Factorized Neural Transducer},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3350893},
doi = {10.1109/TASLP.2024.3350893},
abstract = {Long-content automatic speech recognition (ASR) has obtained increasing interest in recent years, as it captures the relationship among consecutive historical utterances while decoding the current utterance. In this paper, we propose two novel approaches, which integrate long-content information into the factorized neural transducer (FNT) based architecture in both non-streaming (referred to as &lt;italic&gt;LongFNT&lt;/italic&gt;) and streaming (referred to as &lt;italic&gt;SLongFNT&lt;/italic&gt;) scenarios. We first investigate whether long-content transcriptions can improve the vanilla conformer transducer (C-T) models. Our experiments indicate that the vanilla C-T models do not exhibit improved performance when utilizing long-content transcriptions, possibly due to the predictor network of C-T models not functioning as a pure language model. Instead, FNT shows its potential in utilizing long-content information, where we propose the &lt;italic&gt;LongFNT&lt;/italic&gt; model and explore the impact of long-content information in both text (LongFNT-Text) and speech (LongFNT-Speech). The proposed LongFNT-Text and LongFNT-Speech models further complement each other to achieve better performance, with transcription history proving more valuable to the model. The effectiveness of our LongFNT approach is evaluated on LibriSpeech and GigaSpeech corpora, and obtains relative 19% and 12% word error rate reduction, respectively. Furthermore, we extend the LongFNT model to the streaming scenario, which is named &lt;italic&gt;SLongFNT&lt;/italic&gt;, consisting of SLongFNT-Text and SLongFNT-Speech approaches to utilize long-content text and speech information. Experiments show that the proposed SLongFNT model achieves relative 26% and 17% WER reduction on LibriSpeech and GigaSpeech respectively while keeping a good latency, compared to the FNT baseline. Overall, our proposed &lt;italic&gt;LongFNT&lt;/italic&gt; and &lt;italic&gt;SLongFNT&lt;/italic&gt; highlight the significance of considering long-content speech and transcription knowledge for improving both non-streaming and streaming speech recognition systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1803–1815},
numpages = {13}
}

@article{10.1109/TASLP.2023.3308374,
author = {Du, Chenpeng and Guo, Yiwei and Chen, Xie and Yu, Kai},
title = {Speaker Adaptive Text-to-Speech With Timbre-Normalized Vector-Quantized Feature},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3308374},
doi = {10.1109/TASLP.2023.3308374},
abstract = {Achieving high fidelity and speaker similarity in text-to-speech speaker adaptation with limited amount of data is a challenging task. Most existing methods only consider adapting to the timbre of the target speakers but fail to capture their speaking styles from little data. In this work, we propose a novel TTS system, TN-VQTTS, which leverages timbre-normalized vector-quantized (TN-VQ) acoustic feature for speaker adaptation with little data. With the TN-VQ feature, speaking style and timbre can be effectively decomposed and controlled by the acoustic model and the vocoder separately of VQTTS. Such decomposition enables us to finely mimic both the two characteristics of the target speaker in adaptation with little data. Specifically, we first reduce the dimensionality of self-supervised VQ acoustic feature via PCA and normalize its timbre with a normalizing flow model. The feature is then quantized with k-means and used as the TN-VQ feature for a multi-speaker VQ-TTS system. Furthermore, we optimize timbre-independent style embeddings of the training speakers jointly with the acoustic model and store them in a lookup table. The embedding table later serves as a selectable codebook or a group of basis for representing the style of unseen speakers. Our experiments on LibriTTS dataset first show that the proposed model architecture for VQ feature achieves better performance in multi-speaker text-to-speech synthesis than several existing methods. We also find that the reconstruction performance and the naturalness are almost unchanged after applying timbre normalization and k-means quantization. Finally, we show that TN-VQTTS achieves better performance on speaker similarity in adaptation than both speaker embedding based adaptation method and fine-tuning based baseline AdaSpeech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3446–3456},
numpages = {11}
}

@inproceedings{10.5555/3721488.3721749,
author = {Verhelst, Eva and Belpaeme, Tony},
title = {Large Language Models Cover for Speech Recognition Mistakes: Evaluating Conversational AI for Second Language Learners},
year = {2025},
publisher = {IEEE Press},
abstract = {Automatic Speech Recognition (ASR) technology has been reported to reach near-human performance in recent years, yet it continues to struggle with atypical speakers, particularly second language learners. This limitation has hindered progress in leveraging social robots for second language education, a field with significant promise. Recent advancements in Large Language Models (LLMs), which demonstrate capabilities in context understanding, common sense reasoning, and pragmatics, offer a potential solution by compensating for transcription errors introduced by ASR. This study examines whether ASR combined with an LLM can produce flowing conversation. Particularly, we look at its application in learning French as a second language by Dutch-speaking students. Through task-based interactions, where successful task completion depends on the accurate interpretation of user speech, the study evaluates the impact of LLMs on conversational outcomes. Results confirm that the performance of ASR degrades significantly for both speakers with limited proficiency and a non-English language. Nonetheless, LLMs demonstrate the ability to interpret context and sustain meaningful conversations despite suboptimal ASR outputs, highlighting a promising path forward for the integration of these technologies in second-language education.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1705–1709},
numpages = {5},
keywords = {l2 speakers, large language models, pragmatics, speech recognition},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3658644.3670309,
author = {Fang, Zheng and Wang, Tao and Zhao, Lingchen and Zhang, Shenyi and Li, Bowen and Ge, Yunjie and Li, Qi and Shen, Chao and Wang, Qian},
title = {Zero-Query Adversarial Attack on Black-box Automatic Speech Recognition Systems},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670309},
doi = {10.1145/3658644.3670309},
abstract = {In recent years, extensive research has been conducted on the vulnerability of ASR systems, revealing that black-box adversarial example attacks pose significant threats to real-world ASR systems. However, most existing black-box attacks rely on queries to the target ASRs, which is impractical when queries are not permitted. In this paper, we propose ZQ-Attack, a transfer-based adversarial attack on ASR systems in the zero-query black-box setting. Through a comprehensive review and categorization of modern ASR technologies, we first meticulously select surrogate ASRs of diverse types to generate adversarial examples. Following this, ZQ-Attack initializes the adversarial perturbation with a scaled target command audio, rendering it relatively imperceptible while maintaining effectiveness. Subsequently, to achieve high transferability of adversarial perturbations, we propose a sequential ensemble optimization algorithm, which iteratively optimizes the adversarial perturbation on each surrogate model, leveraging collaborative information from other models. We conduct extensive experiments to evaluate ZQ-Attack. In the over-the-line setting, ZQ-Attack achieves a 100% success rate of attack (SRoA) with an average signal-to-noise ratio (SNR) of 21.91dB on 4 online speech recognition services, and attains an average SRoA of 100% and SNR of 19.67dB on 16 open-source ASRs. In the over-the-air setting, ZQ-Attack also achieves a 100% SRoA with an average SNR of 15.77dB on 2 commercial intelligent voice control devices.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {630–644},
numpages = {15},
keywords = {adversarial example attacks, speech recognition},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@article{10.1109/TASLP.2024.3369528,
author = {Miao, Chenfeng and Zhu, Qingying and Chen, Minchuan and Ma, Jun and Wang, Shaojun and Xiao, Jing},
title = {EfficientTTS 2: Variational End-to-End Text-to-Speech Synthesis and Voice Conversion},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3369528},
doi = {10.1109/TASLP.2024.3369528},
abstract = {Recently, the field of Text-to-Speech (TTS) has been dominated by one-stage text-to-waveform models which have significantly improved speech quality compared to two-stage models. In this work, we propose EfficientTTS 2 (EFTS2), a one-stage high-quality end-to-end TTS framework that is fully differentiable and highly efficient. Our method adopts an adversarial training process, with a differentiable aligner and a hierarchical-VAE-based waveform generator. These design choices free the model from the use of external aligners, invertible structures, and complex training procedures as most previous TTS works have. Moreover, we extend EFTS2 to the voice conversion (VC) task and propose EFTS2-VC, an end-to-end VC model that allows high-quality speech-to-speech conversion. Experimental results suggest that the two proposed models achieve better or at least comparable speech quality compared to baseline models, while also providing faster inference speeds and smaller model sizes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1650–1661},
numpages = {12}
}

@article{10.1145/3501397,
author = {P, Jasir M. and Balakrishnan, Kannan},
title = {Text-to-Speech Synthesis: Literature Review with an Emphasis on Malayalam Language},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3501397},
doi = {10.1145/3501397},
abstract = {Text-to-Speech Synthesis (TTS) is an active area of research to generate synthetic speech from underlying text. The identified syllables are uttered with proper duration and prosody characteristics to emulate natural speech. It falls under the category of Natural Language Processing (NLP), which aims to bridge the gap in communication between human and machine. So far as Western languages like English are concerned, the research to produce intelligent and natural synthetic speech has advanced considerably. But in a multilingual state like India, many regional languages viz. Malayalam is underexplored when it comes to NLP. In this article, we try to amalgamate the major research works performed in the area of TTS in English and the prominent Indian languages, with a special emphasis on the South Indian language, Malayalam. This review intends to provide right direction to the research activities in the language, in the area of TTS.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {76},
numpages = {56},
keywords = {Malayalam TTS, Indian language TTS, TTS literature review, Text to speech synthesis}
}

@article{10.1145/3557894,
author = {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng, Michael},
title = {Improving Readability for Automatic Speech Recognition Transcription},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3557894},
doi = {10.1145/3557894},
abstract = {Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {142},
numpages = {23},
keywords = {pre-trained model, data synthesis, post-processing for readability, Automatic speech recognition}
}

@inproceedings{10.1145/3700706.3700730,
author = {Samonte, Mary Jane C. and Viera, Rafaello Jose M. and Tupas, Jan Edgar E. and Sabilala, Allen Kyle D. and Tejada, Ervin C.},
title = {An In-Depth Analysis of Artificial Intelligence on Service Capabilities of Humanoid Robots},
year = {2025},
isbn = {9798400717567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700706.3700730},
doi = {10.1145/3700706.3700730},
abstract = {Artificial intelligence (AI) emulates human intelligence in digital devices and machines. A humanoid robot is designed to imitate the look and form of a human body and is primarily used for humanitarian assistance and interaction. AI is applied to humanoid robots to allow intelligent behavior and decision-making, resulting in the ability to execute service activities and achieve goals intelligently in the human-robot interaction environment. The paper is a systematic literature review of studies published in the last four years that presents and assesses how AI techniques and methods have been applied to humanoid robots to implement and enhance service capabilities. The research papers used for the systematic literature review were carefully selected under the directives of the PRISMA method. Eighty studies were screened, and eighteen were used for the review. The results show that the primary AI technique applied in humanoid robots is the Convolutional Neural Network (CNN). The results show that the effective AI techniques used in humanoid robots based on a specific area are Hybrid Convolutional Neural Network (CNN) – Long-Short Term Memory (LSTM), Artificial Neural Network (ANN), Convolutional Neural Network (CNN), Deep Convolutional Neural Network DCNN), Recurrent Neural Network (RNN), and Grow-when-required Network (GWR). The study provides AI techniques and methods utilized in modern studies that can be used as a reference for future studies, research, and innovation.},
booktitle = {Proceedings of the 2024 7th International Conference on Information Science and Systems},
pages = {140–150},
numpages = {11},
keywords = {Artificial Intelligence, Human-Robot Interaction, Humanoid Robot, Intelligent Behavior},
location = {
},
series = {ICISS '24}
}

@article{10.1109/TASLP.2023.3328283,
author = {Prabhavalkar, Rohit and Hori, Takaaki and Sainath, Tara N. and Schl\"{u}ter, Ralf and Watanabe, Shinji},
title = {End-to-End Speech Recognition: A Survey},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3328283},
doi = {10.1109/TASLP.2023.3328283},
abstract = {In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called &lt;italic&gt;end-to-end&lt;/italic&gt; (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {325–351},
numpages = {27}
}

@article{10.1109/TASLP.2024.3363444,
author = {Zhu, Xinfa and Lei, Yi and Li, Tao and Zhang, Yongmao and Zhou, Hongbin and Lu, Heng and Xie, Lei},
title = {METTS: Multilingual Emotional Text-to-Speech by Cross-Speaker and Cross-Lingual Emotion Transfer},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3363444},
doi = {10.1109/TASLP.2024.3363444},
abstract = {Previous multilingual text-to-speech (TTS) approaches have considered leveraging monolingual speaker data to enable cross-lingual speech synthesis. However, such data-efficient approaches have ignored synthesizing emotional aspects of speech due to the challenges of cross-speaker cross-lingual emotion transfer – the heavy entanglement of &lt;italic&gt;speaker timbre&lt;/italic&gt;, &lt;italic&gt;emotion&lt;/italic&gt; and &lt;italic&gt;language&lt;/italic&gt; factors in the speech signal will make a system to produce cross-lingual synthetic speech with an undesired foreign accent and weak emotion expressiveness. This paper proposes a Multilingual Emotional TTS (METTS) model to mitigate these problems, realizing both cross-speaker and cross-lingual emotion transfer. Specifically, METTS takes DelightfulTTS as the backbone model and proposes the following designs. First, to alleviate the foreign accent problem, METTS introduces &lt;italic&gt;multi-scale emotion modeling&lt;/italic&gt; to disentangle speech prosody into coarse-grained and fine-grained scales, producing language-agnostic and language-specific emotion representations, respectively. Second, as a pre-processing step, formant shift based &lt;italic&gt;information perturbation&lt;/italic&gt; is applied to the reference signal for better disentanglement of speaker timbre in the speech. Third, a vector quantization based &lt;italic&gt;emotion matcher&lt;/italic&gt; is designed for reference selection, leading to decent naturalness and emotion diversity in cross-lingual synthetic speech. Experiments demonstrate the good design of METTS.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1506–1518},
numpages = {13}
}

@article{10.1145/3701725,
author = {Yuan, Xuejing and Zhang, Jiangshan and Chen, Kai and Wei, Cheng'an and Li, Ruiyuan and Ma, Zhenkun and Ling, Xinqi},
title = {Adversarial Attack and Defense for Commercial Black-box Chinese-English Speech Recognition Systems},
year = {2024},
issue_date = {February 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {1},
issn = {2471-2566},
url = {https://doi.org/10.1145/3701725},
doi = {10.1145/3701725},
abstract = {The attacker can generate adversarial examples (AEs) to stealthily mislead automatic speech recognition (ASR) models, raising significant concerns about the security of intelligent voice control (IVC) devices. Existing adversarial attacks mainly generate AEs to mislead ASR models to output specific target English commands (e.g., open the door). However, it remains unknown whether AEs can be used to issue commands in other languages to attack commercial black-box ASR models.In this article, taking Chinese phrases (e.g., 支付宝付款) and “Chinese–English code-switching” phrases (e.g., 关闭GPS) as the target commands, we propose adversarial attacks for commercial multilingual ASR models. In particular, if a multilingual speech recognition model can recognize Chinese and English, we call it a Chinese–English speech recognition model. In English, the meaning of “支付宝付款” and “关闭GPS” are “Alipay payment” and “turn off GPS”, respectively. In detail, we generate transferable AEs based on the open-sourced conventional DataTang Mandarin ASR model. Given 55 target commands, the success rate for generating AEs of them is up to 96% and 80% for Aliyun ASR API and Tencentyun ASR API, respectively. Our AEs can trigger actual attack actions on voice assistants (e.g., Apple Siri, Xiaomi Xiaoaitongxue) or spread malicious messages through ASR API services, while the target commands in the AEs are inaudible to human beings.1 Finally, by analyzing the spectrum differences between benign audio clips and AEs, we propose a general defense against adversarial audio attacks.},
journal = {ACM Trans. Priv. Secur.},
month = dec,
articleno = {10},
numpages = {27},
keywords = {Adversarial example, Chinese-English automatic speech recognition, black-box model}
}

@article{10.1109/TASLP.2022.3171974,
author = {Liu, Rui and Sisman, Berrak and Gao, Guanglai and Li, Haizhou},
title = {Decoding Knowledge Transfer for Neural Text-to-Speech Training},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3171974},
doi = {10.1109/TASLP.2022.3171974},
abstract = {Neural end-to-end text-to-speech (TTS) is superior to conventional statistical methods in many ways. However, the exposure bias problem, that arises from the mismatch between the training and inference process in autoregressive models, remains an issue. It often leads to performance degradation in face of out-of-domain test data. To address this problem, we study a novel decoding knowledge transfer strategy, and propose a multi-teacher knowledge distillation (MT-KD) network for Tacotron2 TTS model. The idea is to pre-train two Tacotron2 TTS teacher models in teacher forcing and scheduled sampling modes, and transfer the pre-trained knowledge to a student model that performs free running decoding. We show that the MT-KD network provides an adequate platform for neural TTS training, where the student model learns to emulate the behaviors of the two teachers, at the same time, minimizing the mismatch between training and run-time inference. Experiments on both Chinese and English data show that MT-KD system consistently outperforms the competitive baselines in terms of naturalness, robustness and expressiveness for in-domain and out-of-domain test data. Furthermore, we show that knowledge distillation outperforms adversarial learning and data augmentation in addressing the exposure bias problem.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1789–1802},
numpages = {14}
}

@inproceedings{10.1145/3641825.3687716,
author = {Christiansen, Frederik Roland and Hollensberg, Linus N\o{}rgaard and Jensen, Niko Bach and Julsgaard, Kristian and Jespersen, Kristian Nyborg and Nikolov, Ivan},
title = {Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options},
year = {2024},
isbn = {9798400705359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641825.3687716},
doi = {10.1145/3641825.3687716},
abstract = {Combining modern technologies like large-language models (LLMs), speech-to-text, and text-to-speech can enhance immersion in virtual reality (VR) environments. However, challenges exist in effectively implementing LLMs and educating users. This paper explores implementing LLM-powered virtual social actors and facilitating user communication. We developed a murder mystery game where users interact with LLM-based non-playable characters (NPCs) through interrogation, clue-gathering, and exploration. Two versions were tested: one using speech recognition and another with traditional dialog boxes. While both provided similar social presence, users felt more immersed with speech recognition but found it overwhelming, while the dialog version was more challenging. Slow NPC response times were a source of frustration, highlighting the need for faster generation or better masking for a seamless experience.},
booktitle = {Proceedings of the 30th ACM Symposium on Virtual Reality Software and Technology},
articleno = {6},
numpages = {11},
keywords = {Immersive systems, Large Language Models (LLM), NPC, Presence, Social Actors, Speech Recognition, VR},
location = {Trier, Germany},
series = {VRST '24}
}

@article{10.1109/TASLP.2022.3140552,
author = {Qian, Yanmin and Zhou, Zhikai},
title = {Optimizing Data Usage for Low-Resource Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3140552},
doi = {10.1109/TASLP.2022.3140552},
abstract = {Automatic speech recognition has made huge progress recently. However, the current modeling strategy still suffers a large performance degradation when facing the low-resource languages with limited training data. In this paper, we propose a series of methods to optimize the data usage for low-resource speech recognition. Multilingual speech recognition helps a lot in low-resource scenarios. The correlation and similarity between languages are further exploited for multilingual pretraining in our work. We utilize the posterior of the target language extracted from a language classifier to perform data weighing on training samples, which assists the model in being more biased towards the target language during pretraining. Furthermore, dynamic curriculum learning for data allocation and length perturbation for data augmentation are also designed. All these three methods form the new strategy on optimized data usage for low-resource languages. We evaluate the proposed method using rich resource languages for pretraining (PT) and finetuning (FT) the model on the target language with limited data. Experimental results show that the proposed data usage method obtains a 15 to 25% relative word error rate reduction for different target languages compared with the commonly adopted multilingual PT+FT method on CommonVoice dataset. The same improvement and conclusion are also observed on Babel dataset with conversational telephone speech, and &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$sim$&lt;/tex-math&gt;&lt;/inline-formula&gt;40% relative character error rate reduction can be obtained for the target low-resource language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {394–403},
numpages = {10}
}

@inproceedings{10.1145/3649217.3653542,
author = {Kasinidou, Maria and Kleanthous, Styliani and Busso, Matteo and Rodas, Marcelo and Otterbacher, Jahna and Giunchiglia, Fausto},
title = {Artificial Intelligence in Everyday Life 2.0: Educating University Students from Different Majors},
year = {2024},
isbn = {9798400706004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649217.3653542},
doi = {10.1145/3649217.3653542},
abstract = {With the surge in data-centric AI and its increasing capabilities, AI applications have become a part of our everyday lives. However, misunderstandings regarding their capabilities, limitations, and associated advantages and disadvantages are widespread. Consequently, in the university setting, there is a crucial need to educate not only computer science majors but also students from various disciplines about AI. In this experience report, we present an overview of an introductory course that we offered to students coming from different majors. Moreover, we discuss the assignments and quizzes of the course, which provided students with a firsthand experience of AI processes and insights into their learning patterns. Additionally, we provide a summary of the course evaluation, as well as students' performance. Finally, we present insights gained from teaching this course and elaborate on our future plans.},
booktitle = {Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1},
pages = {24–30},
numpages = {7},
keywords = {AI education, AI literacy, artificial intelligence, university students},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3587716.3587760,
author = {Usami, Yoshiyuki and Kitaoka, Kosuke and Shindo, Koichi},
title = {Integrated Artificial Intelligence for Making Digital Human},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587760},
doi = {10.1145/3587716.3587760},
abstract = {Artificial intelligence is actively researched in various fields such as image recognition, image detection, audio recognition, natural language processing, face expression recognition, and facial expression generation. If we want to create artificial intelligence in the original sense, it will be necessary to integrate these many research results and create a system that can exactly imitate the functions of the human brain. Commercially, the current situation is that integrated AI such as Ameria [2], Uneeq [3], Neon [13], LaMDA [29] and the system using GPT-3 [9] have entered the market. However, there is no research that creates integrated AI with open source in the academic field. This work is an attempt to construct such an integrated AI as an academic research which is in an form of open source. Furthermore, this work is described in a form of multi-processing job with socket connection. Then, execution of the program can be accomplished by multiple computers. For the visual input, object detection is performed by Redman’s YOLO [14]. Next, the system accomplishes Image2text which generates sentences describing the image [34]. The system recognizes the meaning of visual input. As for speech recognition, the question and answering task is activated, and it is possible to give an accurate answer to the question through the microphone [7]. In addition, text generation enables this system to respond to human chattering [5]. This work combines four different sources: visual, text, audio, and scraping outworld news sources. We believe that attempts like this work will become more common in future AI studies.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {267–273},
numpages = {7},
keywords = {mage2text, question and answering, text generation, visual object detection, visual object recognition},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@article{10.1109/TASLP.2023.3348762,
author = {Liu, Rui and Hu, Yifan and Zuo, Haolin and Luo, Zhaojie and Wang, Longbiao and Gao, Guanglai},
title = {Text-to-Speech for Low-Resource Agglutinative Language With Morphology-Aware Language Model Pre-Training},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3348762},
doi = {10.1109/TASLP.2023.3348762},
abstract = {Text-to-Speech (TTS) aims to convert the input text to a human-like voice. With the development of deep learning, encoder-decoder based TTS models perform superior performance, in terms of naturalness, in mainstream languages such as Chinese, English, etc. Note that the linguistic information learning capability of the text encoder is the key. However, for TTS of low-resource agglutinative languages, the scale of the &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$&lt; $&lt;/tex-math&gt;&lt;/inline-formula&gt;text, speech&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$&gt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; paired data is limited. Therefore, how to extract rich linguistic information from small-scale text data to enhance the naturalness of the synthesized speech, is an urgent issue that needs to be addressed. In this paper, we first collect a large unsupervised text data for BERT-like language model pre-training, and then adopt the trained language model to extract deep linguistic information for the input text of the TTS model to improve the naturalness of the final synthesized speech. It should be emphasized that in order to fully exploit the prosody-related linguistic information in agglutinative languages, we incorporated morphological information into the language model training and constructed a morphology-aware masking based BERT model (MAM-BERT). Experimental results based on various advanced TTS models validate the effectiveness of our approach. Further comparison of the various data scales also validates the effectiveness of our approach in low-resource scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1075–1087},
numpages = {13}
}

@article{10.1109/TASLP.2022.3169634,
author = {Kumar, Neeraj and Narang, Ankur and Lall, Brejesh},
title = {Zero-Shot Normalization Driven Multi-Speaker Text to Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},ż
url = {https://doi.org/10.1109/TASLP.2022.3169634},
doi = {10.1109/TASLP.2022.3169634},
abstract = {Text-to-speech (TTS) systems are designed to synthesize natural and expressive speech, adapt to an unseen voice, and capture the speaking style of an unseen speaker by converting text into speech. The introduction of an unseen speaker’s speaking style into a TTS system offers a wide range of application scenarios, including personal assistant, news broadcast, and audio navigation, among others. The style of the speech varies from person to person and every person exhibits his or her style of speaking that is determined by the language, demography, culture and other factors. Style is best captured by the prosody of a signal. It is an ongoing research area with numerous real-world applications that produces high-quality multi-speaker voice synthesis while taking into account prosody and in a zero-shot manner. Despite the fact that several efforts have been made in this area, it continues to be an interesting and difficult topic to solve. In this paper, we present a novel zero-shot multi-speaker speech synthesis approach (ZSM-SS) that leverages the normalization architecture and speaker encoder with non-autoregressive multi-head attention driven encoder-decoder architecture. Given an input text and a reference speech sample of an unseen person, ZSM-SS can generate speech in that person’s style in a zero-shot manner. Additionally, we demonstrate how the affine parameters of normalization help in capturing the prosodic features such as energy and fundamental frequency in a disentangled fashion and can be used to generate morphed speech output. We generate the 256 dimensional speaker embedding using a speaker encoder based on wav2vec2.0 based architecture. We demonstrate the efficacy of our proposed architecture on multi-speaker VCTK [1] and LibriTTS [2] datasets, using visualization of hessian of proposed model, multiple quantitative metrics that measure generated speech distortion and MOS, along with speaker embedding analysis of the proposed speaker encoder model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1679–1693},
numpages = {15}
}

@article{10.1109/TASLP.2024.3364085,
author = {Jeong, Myeonghun and Kim, Minchan and Choi, Byoung Jin and Yoon, Jaesam and Jang, Won and Kim, Nam Soo},
title = {Transfer Learning for Low-Resource, Multi-Lingual, and Zero-Shot Multi-Speaker Text-to-Speech},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3364085},
doi = {10.1109/TASLP.2024.3364085},
abstract = {Though neural text-to-speech (TTS) models show remarkable performance, they still require a large amount of &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$&lt; speech, text&gt;$&lt;/tex-math&gt;&lt;/inline-formula&gt; paired dataset, which is expensive to collect. The heavy demand for collecting paired datasets makes the TTS models support only a small number of speakers and languages. To address this problem, we introduce a transfer learning framework for multi-lingual, zero-shot multi-speaker, and low-resource TTS. Firstly, we pretrain our model in an unsupervised manner with a multi-lingual multi-speaker speech-only dataset by leveraging the self-supervised speech representations as intermediate linguistic representations. Given this pretrained linguistic information, we then apply a supervised learning technique to the TTS model with a small amount of paired dataset. The pretrained linguistic representations extracted from the large-scale speech-only dataset facilitate phoneme-to-linguistic feature matching, which provides good guidance for supervised learning with a limited amount of labeled data. We evaluate the performance of our proposed model in low-resource, multi-lingual, and zero-shot multi-speaker TTS tasks. The experimental results demonstrate that our proposed method outperforms the baseline in terms of naturalness, intelligibility, and speaker similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1519–1530},
numpages = {12}
}

@inproceedings{10.1145/3647444.3647841,
author = {Sinha, Shweta and Agarwal, Tushar and Pandey, Pratiyush},
title = {Artificial intelligence in healthcare: medical named entity recognition based audio prescription generator},
year = {2024},
isbn = {9798400709418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647444.3647841},
doi = {10.1145/3647444.3647841},
abstract = {Artificial Intelligence has made significant contributions in various aspects and domains of human life, permeating fields such as Education, Healthcare, Economy, Socialization, Security, Agriculture, and Privacy. This research specifically delves into the promising possibilities and applications of AI in healthcare and medical domains, focusing on developing a Medical Named Entity Recognition (NER) model. The proposed approach involves using different pre-trained NER models and an in-house-generated corpora to train a personalized Spacy model. The research then conducts a thorough comparison of these models to understand their strengths and shortcomings. Widening the scope of the proposed research, we introduce an inventive mechanism to extract medical information from doctors' audio in real-time saved on buffer memory, deleted after use, which is then used to create patient medical prescriptions. The potential benefits of this mechanism are vast, expanding beyond reducing patient wait times, enhancing the clarity of prescriptions, maintains comprehensive patient histories, and finally introducing automation into the prescription drug ordering chain. Additionally, the proposed model eases the scheduling and management of medical tests and procedures based on doctor and patient availability. In jist, the proposed paper aims to present an innovative human-centric approach to automating interactions between patients and doctors and optimizing the delivery of healthcare services. Through the integration of modern-day AI services, The proposed mechanism has the potential to bring about a potential change in medical practices, providing a more efficient and effective breakthrough to patient care.},
booktitle = {Proceedings of the 5th International Conference on Information Management &amp; Machine Intelligence},
articleno = {15},
numpages = {5},
keywords = {AI in Healthcare, Artificial Intelligence, Named Entity Recognition, Natural Language Processing},
location = {Jaipur, India},
series = {ICIMMI '23}
}

@inproceedings{10.1145/3654446.3654484,
author = {Jia, Li},
title = {Design of Lora Communication System Based on Speech Recognition in Emergency Rescue Environment},
year = {2024},
isbn = {9798400716430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654446.3654484},
doi = {10.1145/3654446.3654484},
abstract = {In order to provide a more timely communication environment and achieve long-distance, low-cost voice information transmission in emergency rescue tasks after disasters, this article describes an intelligent voice processing system for narrowband environments. It uses Lora spread spectrum technology as the communication protocol and speech recognition technology as the compression method, supporting outdoor rescue workers to communicate and use it on time even in environments without public networks, Can be deployed in disaster relief missions to provide emergency communication.},
booktitle = {Proceedings of the 2nd International Conference on Signal Processing, Computer Networks and Communications},
pages = {215–219},
numpages = {5},
location = {Xiamen, China},
series = {SPCNC '23}
}

@article{10.1145/3614426,
author = {Qayyum, Adnan and Butt, Muhammad Atif and Ali, Hassan and Usman, Muhammad and Halabi, Osama and Al-Fuqaha, Ala and Abbasi, Qammer H. and Imran, Muhammad Ali and Qadir, Junaid},
title = {Secure and Trustworthy Artificial Intelligence-extended Reality (AI-XR) for Metaverses},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3614426},
doi = {10.1145/3614426},
abstract = {Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users’ privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {170},
numpages = {38},
keywords = {Metaverse, AR, VR, XR, MR, secure ML, robust ML, trustworthy ML}
}

@article{10.1109/TASLP.2024.3451982,
author = {Ueno, Sei and Lee, Akinobu and Kawahara, Tatsuya},
title = {Refining Synthesized Speech Using Speaker Information and Phone Masking for Data Augmentation of Speech Recognition},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3451982},
doi = {10.1109/TASLP.2024.3451982},
abstract = {While end-to-end automatic speech recognition (ASR) has shown impressive performance, it requires a huge amount of speech and transcription data. The conversion of domain-matched text to speech (TTS) has been investigated as one approach to data augmentation. The quality and diversity of the synthesized speech are critical in this approach. To ensure quality, a neural vocoder is widely used to generate speech waveforms in conventional studies, but it requires a huge amount of computation and another conversion to spectral-domain features such as the log-Mel filterbank (lmfb) output typically used for ASR. In this study, we explore the direct refinement of these features. Unlike conventional speech enhancement, we can use information on the ground-truth phone sequences of the speech and designated speaker to improve the quality and diversity. This process is realized as a Mel-to-Mel network, which can be placed after a text-to-Mel synthesis system such as FastSpeech 2. These two networks can be trained jointly. Moreover, semantic masking is applied to the lmfb features for robust training. Experimental evaluations demonstrate the effect of phone information, speaker information, and semantic masking. For speaker information, x-vector performs better than the simple speaker embedding. The proposed method achieves even better ASR performance with a much shorter computation time than the conventional method using a vocoder.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3924–3933},
numpages = {10}
}

@article{10.1109/TASLP.2022.3167258,
author = {Huang, Sung-Feng and Lin, Chyi-Jiunn and Liu, Da-Rong and Chen, Yi-Chen and Lee, Hung-yi},
title = {Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3167258},
doi = {10.1109/TASLP.2022.3167258},
abstract = {Personalizing a speech synthesis system is a highly desired application, where the system can generate speech with the user’s voice with rare enrolled recordings. There are two main approaches to build such a system in recent works: speaker adaptation and speaker encoding. On the one hand, speaker adaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model with few enrolled samples. However, they require at least thousands of fine-tuning steps for high-quality adaptation, making it hard to apply on devices. On the other hand, speaker encoding methods encode enrollment utterances into a speaker embedding. The trained TTS model can synthesize the user’s speech conditioned on the corresponding speaker embedding. Nevertheless, the speaker encoder suffers from the generalization gap between the seen and unseen speakers. In this paper, we propose applying a meta-learning algorithm to the speaker adaptation method. More specifically, we use Model Agnostic Meta-Learning (MAML) as the training algorithm of a multi-speaker TTS model, which aims to find a great meta-initialization to adapt the model to any few-shot speaker adaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS model to unseen speakers efficiently. Our experiments compare the proposed method (Meta-TTS) with two baselines: a speaker adaptation method baseline and a speaker encoding method baseline. The evaluation results show that Meta-TTS can synthesize high speaker-similarity speech from few enrollment samples with fewer adaptation steps than the speaker adaptation baseline and outperforms the speaker encoding baseline under the same training scheme. When the speaker encoder of the baseline is pre-trained with extra 8371 speakers of data, Meta-TTS can still outperform the baseline on LibriTTS dataset and achieve comparable results on VCTK dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1558–1571},
numpages = {14}
}

@article{10.1109/TASLP.2022.3198546,
author = {Qian, Yanmin and Gong, Xun and Huang, Houjun},
title = {Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3198546},
doi = {10.1109/TASLP.2022.3198546},
abstract = {The variety and complexity of accents pose a huge challenge to robust Automatic Speech Recognition (ASR). Some previous work has attempted to address such problems, however most of the current approaches either require prior knowledge about the target accent, or cannot handle unseen accents and accent-unspecific standard speech. In this work, we aim to improve multi-accent speech recognition in the end-to-end (E2E) framework with a novel layer-wise adaptation architecture. Firstly, we propose a robust deep accent representation learning architecture to obtain accurate accent embedding, and some advanced schemes are designed to further boost the quality of accent embeddings, including phone posteriorgram (PPG) feature, TTS based data augmentation in the training stage, test-time augmentation and multi-embedding fusion in the testing stage. Then, the layer-wise adaptation with accent embeddings is developed for fast accent adaptation in ASR, and two types of adapter layers are designed, including the gated adapter layer and multi-basis adapter layer. Compared to the usual two-pass adaptation, these adapter layers are injected between the ASR encoder layers to encode the accent information in ASR flexibly, and perform fast adaption on the corresponding speech accent. The experiments on Accent AESRC corpus show that the proposed deep accent representation learning can capture accurate accent knowledge, and get high performance on accent classification. The new layer-wise adaptation architecture with the accurate accent embedding outperforms the other traditional methods, and obtains consistent &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$sim$&lt;/tex-math&gt;&lt;/inline-formula&gt;15% relative word error rate (WER) reduction on all kinds of testing scenarios, including seen accents, unseen accents and accent-unspecific standard speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2842–2853},
numpages = {12}
}

@inproceedings{10.1145/3534678.3539268,
author = {Qu, Xinghua and Wei, Pengfei and Gao, Mingyong and Sun, Zhu and Ong, Yew Soon and Ma, Zejun},
title = {Synthesising Audio Adversarial Examples for Automatic Speech Recognition},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539268},
doi = {10.1145/3534678.3539268},
abstract = {Adversarial examples in automatic speech recognition (ASR) are naturally sounded by humans yet capable of fooling well trained ASR models to transcribe incorrectly. Existing audio adversarial examples are typically constructed by adding constrained perturbations on benign audio inputs. Such attacks are therefore generated with an audio dependent assumption. For the first time, we propose the Speech Synthesising based Attack (SSA), a novel threat model that constructs audio adversarial examples entirely from scratch, i.e., without depending on any existing audio to fool cutting-edge ASR models. To this end, we introduce a conditional variational auto-encoder (CVAE) as the speech synthesiser. Meanwhile, an adaptive sign gradient descent algorithm is proposed to solve the adversarial audio synthesis task. Experiments on three datasets (i.e., Audio Mnist, Common Voice, and Librispeech) show that our method could synthesise naturally sounded audio adversarial examples to mislead the start-of-the-art ASR models. Our web-page containing generated audio demos is at https://sites.google.com/view/ssa-asr/home.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1430–1440},
numpages = {11},
keywords = {adversarial attack, automatic speech recognition, speech synthesis},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1109/TASLP.2023.3250842,
author = {Deng, Jiajun and Xie, Xurong and Wang, Tianzi and Cui, Mingyu and Xue, Boyang and Jin, Zengrui and Li, Guinan and Hu, Shujie and Liu, Xunying},
title = {Confidence Score Based Speaker Adaptation of Conformer Speech Recognition Systems},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3250842},
doi = {10.1109/TASLP.2023.3250842},
abstract = {Speaker adaptation techniques provide a powerful solution to customise automatic speech recognition (ASR) systems for individual users. Practical application of unsupervised model-based speaker adaptation techniques to data intensive end-to-end ASR systems is hindered by the scarcity of speaker-level data and performance sensitivity to transcription errors. To address these issues, a set of compact and data efficient speaker-dependent (SD) parameter representations are used to facilitate both speaker adaptive training and test-time unsupervised speaker adaptation of state-of-the-art Conformer ASR systems. The sensitivity to supervision quality is reduced using a confidence score-based selection of the less erroneous subset of speaker-level adaptation data. Two lightweight confidence score estimation modules are proposed to produce more reliable confidence scores. The data sparsity issue, which is exacerbated by data selection, is addressed by modelling the SD parameter uncertainty using Bayesian learning. Experiments on the benchmark 300-hour Switchboard and the 233-hour AMI datasets suggest that the proposed confidence score-based adaptation schemes consistently outperformed the baseline speaker-independent (SI) Conformer model and conventional non-Bayesian, point estimate-based adaptation using no speaker data selection. Similar consistent performance improvements were retained after external Transformer and LSTM language model rescoring. In particular, on the 300-hour Switchboard corpus, statistically significant WER reductions of 1.0%, 1.3%, and 1.4% absolute (9.5%, 10.9%, and 11.3% relative) were obtained over the baseline SI Conformer on the NIST Hub5’00, RT02, and RT03 evaluation sets respectively. Similar WER reductions of 2.7% and 3.3% absolute (8.9% and 10.2% relative) were also obtained on the AMI development and evaluation sets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1175–1190},
numpages = {16}
}

@inproceedings{10.1145/3501247.3539017,
author = {Stan, George Vlad and Baart, Andr\'{e} and Dittoh, Francis and Akkermans, Hans and Bon, Anna},
title = {A Lightweight Downscaled Approach to Automatic Speech Recognition for Small Indigenous Languages},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3539017},
doi = {10.1145/3501247.3539017},
abstract = {Development of fully featured Automatic Speech Recognition (ASR) systems for a complete language vocabulary generally requires large data repositories, massive computing power, and a stable digital network infrastructure. These conditions are not met in the case of many indigenous languages. Based on our research for over a decade in West Africa, we present a lightweight and downscaled approach to AI-based ASR and describe a set of associated experiments. The aim is to produce a variety of limited-vocabulary ASRs as a basis for the development of practically useful (mobile and radio) voice-based information services that fit needs, preferences and knowledge of local rural communities.},
booktitle = {Proceedings of the 14th ACM Web Science Conference 2022},
pages = {451–458},
numpages = {8},
keywords = {automatic speech recognition, low resource environments, machine learning, neural networks, under-resourced/indigenous languages, voice-based technologies},
location = {Barcelona, Spain},
series = {WebSci '22}
}

@article{10.1109/TASLP.2023.3313434,
author = {Du, Ye-Qian and Zhang, Jie and Fang, Xin and Wu, Ming-Hui and Yang, Zhou-Wang},
title = {A Semi-Supervised Complementary Joint Training Approach for Low-Resource Speech Recognition},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3313434},
doi = {10.1109/TASLP.2023.3313434},
abstract = {Both unpaired speech and text have shown to be beneficial for low-resource automatic speech recognition (ASR), which, however were either separately used for pre-training, self-training and language model (LM) training, or jointly used for designing hybrid models in literature. In this work, we leverage both unpaired speech and text to train a general ASR model, which are used in the form of data pairs by generating the missing parts in prior to model training. We propose to train a model alternatively using the prepared speech-PseudoLabel and SynthesizedAudio-text pairs and reveal the complementary property in both acoustic and linguistic features. The proposed method is thus called complementary joint training (CJT). Based on the basic CJT, label masking for pseudo-labels and parallel layers for synthesized audio are then proposed for re-training to further cope with the deviations from real data, termed as CJT++. In addition, the proposed CJT is extended to the scenario with zero paired data by considering an iterative CJT for the training of seed ASR model. Experimental results on Libri-light show the efficacy of joint training as well as two second-round training strategies, and the superiority over recent models is validated, particularly in extreme low-resource cases.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3908–3921},
numpages = {14}
}

@article{10.1109/TASLP.2023.3313413,
author = {Li, Tao and Hu, Chenxu and Cong, Jian and Zhu, Xinfa and Li, Jingbei and Tian, Qiao and Wang, Yuping and Xie, Lei},
title = {DiCLET-TTS: Diffusion Model Based Cross-Lingual Emotion Transfer for Text-to-Speech — A Study Between English and Mandarin},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3313413},
doi = {10.1109/TASLP.2023.3313413},
abstract = {While the performance of cross-lingual TTS based on monolingual corpora has been significantly improved recently, generating cross-lingual speech still suffers from the foreign accent problem, leading to limited naturalness. Besides, current cross-lingual methods ignore modeling emotion, which is indispensable paralinguistic information in speech delivery. In this article, we propose DiCLET-TTS, a Diffusion model based Cross-Lingual Emotion Transfer method that can transfer emotion from a source speaker to the intra- and cross-lingual target speakers. Specifically, to relieve the foreign accent problem while improving the emotion expressiveness, the terminal distribution of the forward diffusion process is parameterized into a speaker-irrelevant but emotion-related linguistic prior by a prior text encoder with the emotion embedding as a condition. To address the weaker emotional expressiveness problem caused by speaker disentanglement in emotion embedding, a novel orthogonal projection based emotion disentangling module (OP-EDM) is proposed to learn the speaker-irrelevant but emotion-discriminative embedding. Moreover, a condition-enhanced DPM decoder is introduced to strengthen the modeling ability of the speaker and the emotion in the reverse diffusion process to further improve emotion expressiveness in speech delivery. Cross-lingual emotion transfer experiments show the superiority of DiCLET-TTS over various competitive models and the good design of OP-EDM in learning speaker-irrelevant but emotion-discriminative embedding.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3418–3430},
numpages = {13}
}

@inproceedings{10.1145/3587102.3588784,
author = {Kasinidou, Maria and Kleanthous, Styliani and Otterbacher, Jahna},
title = {Artificial Intelligence in Everyday Life: Educating the Public Through an Open, Distance-learning Course},
year = {2023},
isbn = {9798400701382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587102.3588784},
doi = {10.1145/3587102.3588784},
abstract = {With the rise of data-driven AI and its "democratization", we are all interacting with AI-enabled technologies in everyday life. However, not everyone understands how these technologies work. There are many misconceptions surrounding what they can and cannot do, and what are the risks and benefits. Thus, there is a need to educate the general public about everyday AI, upscaling their algorithmic literacy and helping them become more informed, responsible users. We describe the development and evaluation of an eight-week course open to the public, AI in Everyday Life. Pre- and post-course questionnaires were used to evaluate the impact of the course and participants' attitudes towards AI applications. The results suggest that more targeted educational approaches are needed for the general public to fully understand the strengths and weaknesses of the use of AI-enabled technologies.},
booktitle = {Proceedings of the 2023 Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {306–312},
numpages = {7},
keywords = {AI education, AI literacy, artificial intelligence, distance learning},
location = {Turku, Finland},
series = {ITiCSE 2023}
}

@inproceedings{10.1145/3453800.3453826,
author = {Thanh Diep, Hai and Thi My Nguyen, Thanh and Ngoc Le, Bich and Xuan Dao, Quy},
title = {Evaluation of Vietnamese Speech Recognition Platforms},
year = {2021},
isbn = {9781450387613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453800.3453826},
doi = {10.1145/3453800.3453826},
booktitle = {Proceedings of the 2021 5th International Conference on Machine Learning and Soft Computing},
pages = {141–146},
numpages = {6},
keywords = {word error rate, speech recognition, natural langue processing, API},
location = {Da Nang, Viet Nam},
series = {ICMLSC '21}
}

@article{10.1145/3446243.3446250,
author = {Martiniello, Natalina and Asuncion, Jennison and Fichten, Catherine and Jorgensen, Mary and Havel, Alice and Harvison, Maegan and Legault, Anick and Lussier, Alex and Vo, Christine},
title = {Artificial intelligence for students in postsecondary education: a world of opportunity},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {3},
url = {https://doi.org/10.1145/3446243.3446250},
doi = {10.1145/3446243.3446250},
abstract = {AI-based apps can facilitate learning for all post-secondary students and may also be useful for students with disabilities. Here we share some reflections from discussions that took place during two advisory board meetings on the use of such apps for students with disabilities at the post-secondary level.},
journal = {AI Matters},
month = feb,
pages = {17–29},
numpages = {13},
keywords = {mobile AI apps, college and university students with disabilities, artificial intelligence apps}
}

@inproceedings{10.1145/3503162.3503173,
author = {Joshi, Raviraj and Kannan, Venkateshan},
title = {Attention based end to end Speech Recognition for Voice Search in Hindi and English},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503173},
doi = {10.1145/3503162.3503173},
abstract = {We describe here our work with automatic speech recognition (ASR) in the context of voice search functionality on the Flipkart e-Commerce platform. Starting with the deep learning architecture of Listen-Attend-Spell (LAS), we build upon and expand the model design and attention mechanisms to incorporate innovative approaches including multi-objective training, multi-pass training, and external rescoring using language models and phoneme based losses. We report a relative WER improvement of 15.7% on top of state-of-the-art LAS models using these modifications. Overall, we report an improvement of 36.9% over the phoneme-CTC system on the Flipkart Voice Search dataset. The paper also provides an overview of different components that can be tuned in a LAS based system.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {107–113},
numpages = {7},
keywords = {listen attend spell, encoder-decoder models, automatic speech recognition, attention},
location = {Virtual Event, India},
series = {FIRE '21}
}

@article{10.1109/TASLP.2022.3145313,
author = {Serai, Prashant and Sunder, Vishal and Fosler-Lussier, Eric},
title = {Hallucination of Speech Recognition Errors With Sequence to Sequence Learning},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3145313},
doi = {10.1109/TASLP.2022.3145313},
abstract = {Prior work in this domain has focused on modeling errors at the phonetic level, while using a lexicon to convert the phones to words, usually accompanied by an FST Language model. We present novel end-to-end models to directly predict hallucinated ASR word sequence outputs, conditioning on an input word sequence as well as a corresponding phoneme sequence. This improves prior published results for recall of errors from an in-domain ASR system’s transcription of unseen data, as well as an out-of-domain ASR system’s transcriptions of audio from an unrelated task, while additionally exploring an in-between scenario when limited characterization data from the test ASR system is obtainable. To verify the extrinsic validity of the method, we also use our hallucinated ASR errors to augment training for a spoken question classifier, finding that they enable robustness to real ASR errors in a downstream task, when scarce or even zero task-specific audio was available at train-time.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {890–900},
numpages = {11}
}

@inproceedings{10.1145/3544548.3581463,
author = {Lewicki, Kornel and Lee, Michelle Seng Ah and Cobbe, Jennifer and Singh, Jatinder},
title = {Out of Context: Investigating the Bias and Fairness Concerns of “Artificial Intelligence as a Service”},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581463},
doi = {10.1145/3544548.3581463},
abstract = {“AI as a Service” (AIaaS) is a rapidly growing market, offering various plug-and-play AI services and tools. AIaaS enables its customers (users)—who may lack the expertise, data, and/or resources to develop their own systems—to easily build and integrate AI capabilities into their applications. Yet, it is known that AI systems can encapsulate biases and inequalities that can have societal impact. This paper argues that the context-sensitive nature of fairness is often incompatible with AIaaS’ ‘one-size-fits-all’ approach, leading to issues and tensions. Specifically, we review and systematise the AIaaS space by proposing a taxonomy of AI services based on the levels of autonomy afforded to the user. We then critically examine the different categories of AIaaS, outlining how these services can lead to biases or be otherwise harmful in the context of end-user applications. In doing so, we seek to draw research attention to the challenges of this emerging area.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {135},
numpages = {17},
keywords = {AIaaS, MLaaS, accountability, algorithmic supply chains, artificial intelligence, bias, cloud, data-driven, fairness, machine learning},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3531146.3533117,
author = {Markl, Nina},
title = {Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533117},
doi = {10.1145/3531146.3533117},
abstract = {All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {521–534},
numpages = {14},
keywords = {algorithmic bias, language variation, speech and language technologies, speech recognition},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.1109/TASLP.2022.3205753,
author = {Wang, Xiaoqiang and Liu, Yanqing and Li, Jinyu and Miljanic, Veljko and Zhao, Sheng and Khalil, Hosam},
title = {Towards Contextual Spelling Correction for Customization of End-to-End Speech Recognition Systems},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3205753},
doi = {10.1109/TASLP.2022.3205753},
abstract = {Contextual biasing is an important and challenging task for end-to-end automatic speech recognition (ASR) systems, which aims to achieve better recognition performance by biasing the ASR system to particular context phrases such as person names, music list, proper nouns, etc. Existing methods mainly include contextual LM biasing and adding bias encoder into end-to-end ASR models. In this work, we introduce a novel approach to do contextual biasing by adding a contextual spelling correction model on top of the end-to-end ASR system. We incorporate contextual information into a sequence-to-sequence spelling correction model with a shared context encoder. The proposed model includes two different mechanisms: autoregressive (AR) and non-autoregressive (NAR). We also propose filtering algorithms to handle large-size context lists, and performance balancing mechanisms to control the biasing degree of the model. The proposed model is a general biasing solution which is domain-insensitive and can be adopted in different scenarios. Experiments show that the proposed method achieves as much as 51% relative word error rate (WER) reduction over ASR system and outperforms traditional biasing methods. Compared to the AR solution, the NAR model reduces model size by 43.2% and speeds up inference by 2.1 times.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3089–3097},
numpages = {9}
}

@article{10.1109/TASLP.2021.3066274,
author = {Bai, Ye and Yi, Jiangyan and Tao, Jianhua and Wen, Zhengqi and Tian, Zhengkun and Zhang, Shuai},
title = {Integrating Knowledge Into End-to-End Speech Recognition From External Text-Only Data},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3066274},
doi = {10.1109/TASLP.2021.3066274},
abstract = {Attention-based encoder-decoder (AED) models have achieved promising performance in speech recognition. However, because of the end-to-end training, an AED model is usually trained with speech-text paired data. It is challenging to incorporate external text-only data into AED models. Another issue of the AED model is that it does not use the right context of a text token while predicting the token. To alleviate the above two issues, we propose a unified method called LST (Learn Spelling from Teachers) to integrate knowledge into an AED model from the external text-only data and leverage the whole context in a sentence. The method is divided into two stages. First, in the representation stage, a language model is trained on the text. It can be seen as that the knowledge in the text is compressed into the LM. Then, at the transferring stage, the knowledge is transferred to the AED model via teacher-student learning. To further use the whole context of the text sentence, we propose an LM called causal cloze completer (COR), which estimates the probability of a token, given both the left context and the right context of it. Therefore, with LST training, the AED model can leverage the whole context in the sentence. Different from fusion based methods, which use LM during decoding, the proposed method does not increase any extra complexity at the inference stage. We conduct experiments on two scales of public Chinese datasets AISHELL-1 and AISHELL-2. The experimental results demonstrate the effectiveness of leveraging external text-only data and the whole context in a sentence with our proposed method, compared with baseline hybrid systems and AED model based systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1340–1351},
numpages = {12}
}

@article{10.1145/3674968,
author = {Salhab, Mahmoud and Harmanani, Haidar},
title = {AraSpot: Arabic Spoken Command Spotting},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3674968},
doi = {10.1145/3674968},
abstract = {Spoken keyword spotting is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a state-of-the-art 99.59% result outperforming previous approaches.1},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {105},
numpages = {12},
keywords = {Arabic command spotting, speech recognition, conformer, synthetic data generation}
}

@article{10.1109/TASLP.2024.3485466,
author = {Liang, Zheng and Ma, Ziyang and Du, Chenpeng and Yu, Kai and Chen, Xie},
title = {E&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{3}$&lt;/tex-math&gt;&lt;/inline-formula&gt;TTS: End-to-End Text-Based Speech Editing TTS System and Its Applications},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3485466},
doi = {10.1109/TASLP.2024.3485466},
abstract = {Text-based speech editing aims at manipulating part of real audio by modifying the corresponding transcribed text, without being discernible by human auditory system. With the enhanced capability of neural Text-to-speech (TTS), researchers try to tackle speech editing problems with TTS methods. In this paper, we propose E&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{3}$&lt;/tex-math&gt;&lt;/inline-formula&gt;TTS, a.k.a. end-to-end text-based speech editing TTS system, which combines a text encoder, a speech encoder, and a joint net for speech synthesis and speech editing. E&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{3}$&lt;/tex-math&gt;&lt;/inline-formula&gt;TTS can insert, replace, and delete speech content at will, by manipulating the given text. Experiments show that our speech editing outperforms strong baselines on HiFiTTS and LibriTTS datasets, speakers of which are seen or unseen, respectively. Further, we introduce E&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{3}$&lt;/tex-math&gt;&lt;/inline-formula&gt;TTS into data augmentation for automatic speech recognition (ASR) to mitigate the data insufficiency problem in code-switching and named entity recognition scenarios&lt;sup&gt;1&lt;/sup&gt;. E&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{3}$&lt;/tex-math&gt;&lt;/inline-formula&gt;TTS retains the coherence and reality of the recorded audio compared to past data augmentation methods. The experimental results show significant performance improvements over baseline systems with traditional TTS-based data augmentation. The code and samples of the proposed speech editing model are available at this repository.&lt;sup&gt;2&lt;/sup&gt;},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {4810–4821},
numpages = {12}
}

@inproceedings{10.1145/3674912.3674919,
author = {Andreeva, Anna and Lekova, Anna and Tsvetkova, Paulina and Simonska, Miglena},
title = {Expanding the Capabilities of Robot NAO to Enable Human-Like Communication with Children with Speech and Language Disorders},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674919},
doi = {10.1145/3674912.3674919},
abstract = {The humanoid robot NAO is widely used in therapy scenarios for children with neurodevelopmental disorders, however it has poor speech recognition and dialog based on a predefined lexicon that results in limited vocabulary and limited number of predefined dialog scenarios. The integration of Conversational Artificial Intelligence (AI) in NAO can significantly enhance and expand the robot's capabilities for intensive speech and listening exercises for children with speech and language disorders. Applying design-based research, the authors present the ongoing effective iteration in the building-testing cycles of a software architecture for Conversational AI in the robot NAO by integrating different AI cloud services for NLP into NAO's native software. Examining the aims and methods of stakeholders interested in integrating Conversational AI in NAO for speech and language therapy, we revealed several technical and ethical challenges. These challenges were successfully addressed with solutions implemented in the Node-RED platform, such as achieving more accurate and speech recognition, generating human-like text based on a context, and implementing multilingual text-to-speech synthesis. Additionally, implementation raises ethical considerations for both developers and therapists, especially regarding the assessment of risks linked with AI systems. We followed the guidelines set in the European AI Act, which categorizes AI systems according to their associated risk levels.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {63–68},
numpages = {6},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@inproceedings{10.1145/3630106.3658969,
author = {Prinos, Kerri and Patwari, Neal and Power, Cathleen A.},
title = {Speaking of accent: A content analysis of accent misconceptions in ASR research},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658969},
doi = {10.1145/3630106.3658969},
abstract = {Automatic speech recognition (ASR) researchers are working to address the differing transcription performance of ASR by accent or dialect. However, research often has a limited view of accent in ways that reproduce discrimination and limit the scope of potential solutions. In this paper we present a content analysis of 22 papers published in 2022 in top conferences and journals on the topic of accent and ASR. We report on how accent is sometimes mistakenly viewed as something some people don’t have; as having a default; and being an attribute only of the speaker, and not of the listener. We discuss the implications on research and provide recommendations to researchers who hope to reduce ASR biases by accent.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1245–1254},
numpages = {10},
keywords = {AI fairness, accent, discrimination, speech recognition},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@article{10.1109/TASLP.2023.3345150,
author = {Wang, Wei and Qian, Yanmin},
title = {Universal Cross-Lingual Data Generation for Low Resource ASR},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3345150},
doi = {10.1109/TASLP.2023.3345150},
abstract = {Significant advances in end-to-end (E2E) automatic speech recognition (ASR) have primarily been concentrated on languages rich in annotated data. Nevertheless, a large proportion of languages worldwide, which are typically low-resource, continue to pose significant challenges. To address this issue, this study presents a novel speech synthesis framework based on data splicing that leverages self-supervised learning (SSL) units from Hidden Unit BERT (HuBERT) as universal phonetic units. In our framework, the SSL phonetic units serve as crucial bridges between speech and text across different languages. By leveraging these units, we successfully splice speech fragments from high-resource languages into synthesized speech that maintains acoustic coherence with text from low-resource languages. To further enhance the practicality of the framework, we introduce a sampling strategy based on confidence scores assigned to the speech segments used in data splicing. The application of this confidence sampling strategy in data splicing significantly accelerates ASR model convergence and enhances overall ASR performance. Experimental results on the &lt;sc&gt;CommonVoice&lt;/sc&gt; dataset show 25-35% relative improvement for four Indo-European languages and about 20% for Turkish using a 4-gram language model for rescoring, under a 10-hour low-resource setup. Furthermore, we showcase the scalability of our framework by incorporating a larger unsupervised speech corpus for generating speech fragments in data splicing, resulting in an additional 10% relative improvement.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {973–983},
numpages = {11}
}

@article{10.1109/TASLP.2023.3320864,
author = {Qu, Leyuan and Li, Taihao and Weber, Cornelius and Pekarek-Rosin, Theresa and Ren, Fuji and Wermter, Stefan},
title = {Disentangling Prosody Representations With Unsupervised Speech Reconstruction},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3320864},
doi = {10.1109/TASLP.2023.3320864},
abstract = {Human speech can be characterized by different components, including semantic content, speaker identity and prosodic information. Significant progress has been made in disentangling representations for semantic content and speaker identity in speech recognition and speaker verification tasks respectively. However, it is still an open challenging question to extract prosodic information because of the intrinsic association of different attributes, such as timbre and rhythm, and because of the need for supervised training schemes to achieve robust speech recognition. The aim of this article is to address the disentanglement of emotional prosody based on unsupervised reconstruction. Specifically, we identify, design, implement and integrate three crucial components in our proposed model Prosody2Vec: (1) a unit encoder that transforms speech signals into discrete units for semantic content, (2) a pretrained speaker verification model to generate speaker identity embeddings, and (3) a trainable prosody encoder to learn prosody representations. We first pretrain Prosody2Vec on unlabelled emotional speech corpora, then fine-tune the model on specific datasets to perform Speech Emotion Recognition (SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective evaluations on the EVC task suggest that Prosody2Vec effectively captures general prosodic features that can be smoothly transferred to other emotional speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the prosody features learned by Prosody2Vec are complementary and beneficial for the performance of widely used speech pretraining models and surpass the state-of-the-art methods when combining Prosody2Vec with HuBERT representations. Audio samples can be found on our demo website.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {39–54},
numpages = {16}
}

@inproceedings{10.1145/3664647.3681345,
author = {Zhang, Qishan and Wen, Shuangbing and Hu, Tao},
title = {Audio Deepfake Detection with Self-Supervised XLS-R and SLS Classifier},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681345},
doi = {10.1145/3664647.3681345},
abstract = {Generative AI technologies, including text-to-speech (TTS) and voice conversion (VC), frequently become indistinguishable from genuine samples, posing challenges for individuals in discerning between real and synthetic content. This indistinguishability undermines trust in media, and the arbitrary cloning of personal voice signals presents significant challenges to privacy and security. In the field of deepfake audio detection, the majority of models achieving higher detection accuracy currently employ self-supervised pre-trained models. However, with the ongoing development of deepfake audio generation algorithms, maintaining high discrimination accuracy against new algorithms grows more challenging. To enhance the sensitivity of deepfake audio features, we propose a deepfake audio detection model that incorporates an SLS (Sensitive Layer Selection) module. Specifically, utilizing the pre-trained XLS-R enables our model to extract diverse audio features from its various layers, each providing distinct discriminative information. Utilizing the SLS classifier, our model captures sensitive contextual information across different layer levels of audio features, effectively employing this information for fake audio detection. Experimental results show that our method achieves state-of-the-art (SOTA) performance on both the ASVspoof 2021 DF and In-the-Wild datasets, with a specific Equal Error Rate (EER) of 1.92% on the ASVspoof 2021 DF dataset and 7.46% on the In-the-Wild dataset. Codes and data can be found at https://github.com/QiShanZhang/SLSforADD.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6765–6773},
numpages = {9},
keywords = {aigc, anti spoofing, audio deepfake detection, countermeasures, text to speech, voice conversion},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3707450,
author = {Yang, Zhou and Shi, Jieke and Asyrofi, Muhammad Hilmi and Xu, Bowen and Zhou, Xin and Han, DongGyun and Lo, David},
title = {Prioritizing Speech Test Cases},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3707450},
doi = {10.1145/3707450},
abstract = {As automated speech recognition (ASR) systems gain widespread acceptance, there is a pressing need to rigorously test and enhance their performance. Nonetheless, the process of collecting and executing speech test cases is typically both costly and time-consuming. This presents a compelling case for the strategic prioritization of speech test cases, which consist of a piece of audio and the corresponding reference text. The central question we address is: In what sequence should speech test cases be collected and executed to identify the maximum number of errors at the earliest stage? In this study, we introduce Prophet (PRiOritising sPeecH tEsT cases), a tool designed to predict the likelihood that speech test cases will identify errors. Consequently, Prophet can assess and prioritize these test cases without having to run the ASR system, facilitating large-scale analysis. Our evaluation encompasses  (6)  distinct prioritization techniques across  (3)  ASR systems and  (12)  datasets. When constrained by the same test budget, our approach identified  (15.44%)  more misrecognized words than the leading the state-of-the-art method. We select top-ranked speech test cases from the prioritized list to fine-tune ASR systems and analyze how our approach can improve the ASR system performance. Statistical evaluations show that our method delivers a considerably higher performance boost for ASR systems compared to established baseline techniques. Moreover, our correlation analysis confirms that fine-tuning an ASR system with a dataset where the model initially underperforms tends to yield greater performance improvements.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = dec,
keywords = {Automated Speech Recognition, Test Case prioritization, DNN Model Quality}
}

@inproceedings{10.1145/3643491.3660286,
author = {Choi, Jeong-Eun and Sch\"{a}fer, Karla and Zmudzinski, Sascha},
title = {Introduction to Audio Deepfake Generation: Academic Insights for Non-Experts},
year = {2024},
isbn = {9798400705526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643491.3660286},
doi = {10.1145/3643491.3660286},
abstract = {With the advancement of artificial intelligence, the methods for generating audio deepfakes have improved, but the technology behind it has become more complex. Despite this, non-expert users are able to generate audio deepfakes due to the increased accessibility of the latest technologies. These technologies can be used to support content creators, singers, and businesses such as the advertisement or entertainment industries. However, they can also be misused to create disinformation, CEO fraud, and voice scams. Therefore, with the increasing demand for countermeasures against their misuse, continuous interdisciplinary exchange is required. This work introduces recent techniques for generating audio deepfakes, with a focus on Text-to-Speech Synthesis and Voice Conversion for non-experts. It covers background knowledge, the latest trends and models, as well as open-source and closed-source software to explore both technological and practical aspects of audio deepfakes.},
booktitle = {Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation},
pages = {3–12},
numpages = {10},
keywords = {Attacks, Audio Deepfakes, Disinformation, Text-to-Speech Synthesis, Voice Conversion},
location = {Phuket, Thailand},
series = {MAD '24}
}

@article{10.1109/TASLP.2023.3349053,
author = {Kim, Seung-Bin and Lee, Sang-Hoon and Choi, Ha-Yeong and Lee, Seong-Whan},
title = {Audio Super-Resolution With Robust Speech Representation Learning of Masked Autoencoder},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3349053},
doi = {10.1109/TASLP.2023.3349053},
abstract = {This paper proposes Fre-Painter, a high-fidelity audio super-resolution system that utilizes robust speech representation learning with various masking strategies. Recently, masked autoencoders have been found to be beneficial in learning robust representations of audio for speech classification tasks. Following these studies, we leverage these representations and investigate several masking strategies for neural audio super-resolution. In this paper, we propose an upper-band masking strategy with the initialization of the mask token, which is simple but efficient for audio super-resolution. Furthermore, we propose a mix-ratio masking strategy that makes the model robust for input speech with various sampling rates. For practical applicability, we extend Fre-Painter to a text-to-speech system, which synthesizes high-resolution speech using low-resolution speech data. The experimental results demonstrate that Fre-Painter outperforms other neural audio super-resolution models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1012–1022},
numpages = {11}
}

@inproceedings{10.1145/3653876.3653893,
author = {Jiang, Yuan and Bao, Shun and Hu, Yajun and Liu, Lijuan and Hu, Guoping and Ai, Yang and Ling, Zhenhua},
title = {DeepGAN: A fast and high-quality time-domain-based neural vocoder for low-resource scenarios},
year = {2024},
isbn = {9798400709029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3653876.3653893},
doi = {10.1145/3653876.3653893},
abstract = {Recent advancements in neural vocoders have primarily relied on generative adversarial networks (GANs) operating in the time domain. However, these vocoders are parameter-heavy and computationally expensive, limiting their use in resource-constrained environments such as embedded devices. Depthwise separable convolution, known for its lower parameter count and reduced computational costs, can be employed to construct lightweight networks. In this paper, we introduce an extension to HiFi-GAN, named DeepGAN, which utilizes depthwise separable convolution as the primary unit within the network, introduces a novel upsample module, and incorporates a lightweight excitation generation network to enhance the quality of the generated speech. Both objective and subjective evaluations demonstrate that our proposed DeepGAN achieves comparable results to competing vocoders for both seen and unseen speakers. Notably, the parameter count of DeepGAN is only 1/7 of that of HiFi-GAN, resulting in an approximately sixfold improvement in generation speed, while maintaining the synthesized speech quality.},
booktitle = {Proceedings of the 2024 8th International Conference on Digital Signal Processing},
pages = {112–117},
numpages = {6},
keywords = {Neural vocoder, depthwise separable convolution, generative adversarial networks, text-to-speech},
location = {Hangzhou, China},
series = {ICDSP '24}
}

@article{10.1145/3451150,
author = {Ali, Ahmed and Chowdhury, Shammur and Afify, Mohamed and El-Hajj, Wassim and Hajj, Hazem and Abbas, Mourad and Hussein, Amir and Ghneim, Nada and Abushariah, Mohammad and Alqudah, Assal},
title = {Connecting Arabs: bridging the gap in dialectal speech recognition},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3451150},
doi = {10.1145/3451150},
journal = {Commun. ACM},
month = mar,
pages = {124–129},
numpages = {6}
}

@inproceedings{10.1145/3594806.3596536,
author = {Langlois, Quentin and Jodogne, S\'{e}bastien},
title = {Practical Study of Deep Learning Models for Speech Synthesis},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596536},
doi = {10.1145/3594806.3596536},
abstract = {Speech synthesis systems, also known as Text-To-Speech (TTS) systems, are increasingly frequent nowadays, with multiple applications such as voice assistants and screen readers for visually impaired or blind people. These applications require strong real-time capabilities to be usable in practice, which can be at the cost of a reduced quality in the synthesized voices. Deep Learning models, which have shown impressive results in the task of audio generation, are hardly ever used for everyday TTS because of their high demand in computational resources. Training such models also requires a large amount of good quality data, which is not available for most languages. This paper explores the benefits of cross-lingual transfer learning, both in terms of training time and amount of data that is needed to obtain good quality models. Our contributions are evaluated with respect to other TTS systems available for the French language. The main observation is that good quality single-speaker models can be trained within half a week on a single GPU, with a limited number of good quality data, by combining transfer learning with few-shot learning.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {700–706},
numpages = {7},
keywords = {Deep Learning, Speech Synthesis, Text-to-Speech, Transfer Learning, Voice Cloning},
location = {Corfu, Greece},
series = {PETRA '23}
}

@inproceedings{10.1145/3704137.3704179,
author = {Arora, Rakshit and Heron, Marcus and Nagar, Abhishek},
title = {Application of Speech to Text Solutions for Manufacturing Environment with Industry Specific Lexicon},
year = {2025},
isbn = {9798400718014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704137.3704179},
doi = {10.1145/3704137.3704179},
abstract = {This study focuses on developing a custom Speech-to-Text (STT) application to improve data collection from technicians working in a cleanroom environment. Comment data is traditionally captured via keyboard input. However, within a cleanroom, employees wear gloves making typing difficult, they may not have free hands and workstations can be located far from the tools requiring travel. As a result comments are often short and lack information, the length for a one hour job and a six hour job are often comparable so, there is significant information loss. Industries often have their own specific language and existing STT systems are not able to accurately capture this, so a custom model is required. The small.en Whisper AI Model was taken as a starting point and refined using a training dataset of recordings taken from the application environment. A PoC implementation of the STT capability was developed using an Android application deployed on a voice controlled Smart Helmet with Private 5G Network and Wifi connectivity that enables hands free capture of comments at the source. The study also explores the potential applications of Automated Speech Recognition systems for transcriptions and translations, and contributes to the ongoing efforts in leveraging STT technology to drive continuous improvement and innovation in industrial operations.},
booktitle = {Proceedings of the 2024 8th International Conference on Advances in Artificial Intelligence},
pages = {148–155},
numpages = {8},
keywords = {ASR, STT, WhisperAI, Finetuning, Dataset, Android, GenAI},
location = {
},
series = {ICAAI '24}
}

@inproceedings{10.1145/3633598.3633600,
author = {Sanapala, Divija and Choudhary, Kabita and Shetty, Sujala},
title = {Multi-Person Speech Curator for Minutes of Meetings along with Meeting Summarization and Language Translation},
year = {2024},
isbn = {9798400708985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3633598.3633600},
doi = {10.1145/3633598.3633600},
abstract = {Multi-Person Speech Curator for Minutes of Meetings refers to a system where we can generate the summary of a meeting from the audio recording of the meeting using automatic speech recognition (ASR) and NLP algorithms. The system can identify and differentiate speakers and generate transcript with time stamp. ASR is used to transcribe the audio of the meeting and then NLP algorithm used to for speaker diarization and then extract the summary from the transcript. In this paper we use Whisper API from OpenAI for speech to text conversion. The model facebook/bart-large-cnn is used to generate the summary from the transcript. The model is fine-tuned with Samsum dataset that contains about 16k messenger-like conversations. mBART model is used for summary translation. The objective of the paper is to create minutes of meeting which has concise information about the complete meeting discussions, decisions, and actions. The system has the potential to significantly save the time and resources as it eliminates manual transcript and captures all important points of the meeting.},
booktitle = {Proceedings of the 2023 7th International Conference on Advances in Artificial Intelligence},
pages = {6–12},
numpages = {7},
keywords = {BART(Bidirectional Auto-Regressive Transformers), BERT (Bidirectional Encoder Representations from Transformers), NLP(Natural Language Processing), OpenAI, Speech-text-summarization, Whisper API, automatic speech recognition, mBART(multilingual Bidirectional Encoder Representations from Transformers), machine learning, speaker diarization, text summarization},
location = {Istanbul, Turkiye},
series = {ICAAI '23}
}

@inproceedings{10.1145/3675888.3676106,
author = {Garg, Kashish and Alam, Taj},
title = {Desktop Voice Assistant for Elderly People using Feed-forward Neural Network for Intent Recognition},
year = {2024},
isbn = {9798400709722},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675888.3676106},
doi = {10.1145/3675888.3676106},
abstract = {Technology has deeply embedded itself in our daily lives, making it easier to carry out tasks. However, the use of technology can be complicated for some like elderly people, cognitively and visually impaired individuals, illiterate, or people with limited education. A proposed desktop voice assistant for elderly people using a feed-forward neural network for intent recognition to simplify complex tasks and make technology more accessible for those who find it difficult. The assistant is a companion to assist, automate, and make it easier for older people to use computers. It aids them in understanding, responding to user needs, and assisting with task management and data retrieval. The Feed-forward Neural Network trains the pre-processed data for voice-to-intent recognition. The proposed and developed desktop assistant does voice recognition with an accuracy of around 78.5 percent, external APIs are used to enhance user experience simplify interactions, and boost productivity. The assistant's configurable personality, which is tailored to the user's preferences, makes it stand out. It provides a high level of interactivity, enabling it to be used quickly and is very flexible regarding individual requirements.},
booktitle = {Proceedings of the 2024 Sixteenth International Conference on Contemporary Computing},
pages = {515–518},
numpages = {4},
keywords = {Accessibility, Elderly Care, Independent Living, Speech Recognition, Technology Adoption, Voice-enabled Assistants},
location = {Noida, India},
series = {IC3-2024}
}

@article{10.1109/TASLP.2024.3444470,
author = {Kim, Minsu and Choi, Jeongsoo and Kim, Dahun and Ro, Yong Man},
title = {Textless Unit-to-Unit Training for Many-to-Many Multilingual Speech-to-Speech Translation},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3444470},
doi = {10.1109/TASLP.2024.3444470},
abstract = {This paper proposes a textless training method for many-to-many multilingual speech-to-speech translation that can also benefit the transfer of pre-trained knowledge to text-based systems, text-to-speech synthesis and text-to-speech translation. To this end, we represent multilingual speech with speech units that are the discretized representations of speech features derived from a self-supervised speech model. By treating the speech units as pseudo-text, we can focus on the linguistic content of the speech, which can be easily associated with both speech and text modalities at the phonetic level information. By setting both the inputs and outputs of our learning problem as speech units, we propose to train an encoder-decoder model in a many-to-many spoken language translation setting, namely Unit-to-Unit Translation (UTUT). Specifically, the encoder is conditioned on the source language token to correctly understand the input spoken language, while the decoder is conditioned on the target language token to generate the translated speech in the target language. Therefore, during the training, the model can build the knowledge of how languages are comprehended and how to relate them to different languages. Since speech units can be easily associated from both audio and text by quantization and phonemization respectively, the trained model can easily transferred to text-related tasks, even if it is trained in a textless manner. We demonstrate that the proposed UTUT model can be effectively utilized not only for Speech-to-Speech Translation (S2ST) but also for multilingual Text-to-Speech Synthesis (T2S) and Text-to-Speech Translation (T2ST), requiring only minimal fine-tuning steps on text inputs. By conducting comprehensive experiments encompassing various languages, we validate the efficacy of the proposed method across diverse multilingual tasks. Moreover, thanks to the many-to-many language training, we show that the UTUT can also perform language translations for novel language pairs that are not present during training as pairs, which has not well been explored in the previous literature.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3934–3946},
numpages = {13}
}

@article{10.5555/3722577.3722910,
author = {Ravanelli, Mirco and Parcollet, Titouan and Moumen, Adel and De Langen, Sylvain and Subakan, Cem and Plantinga, Peter and Wang, Yingzhi and Mousavi, Pooneh and Libera, Luca Della and Ploujnikov, Artem and Paissan, Francesco and Borra, Davide and Zaiem, Salah and Zhao, Zeyu and Zhang, Shucong and Karakasidis, Georgios and Yeh, Sung-Lin and Champion, Pierre and Rouhe, Aku and Braun, Rudolf and Mai, Florian and Zuluaga-Gomez, Juan and Mousavi, Seyed Mahed and Nautsch, Andreas and Nguyen, Ha and Liu, Xuechen and Sagar, Sangeet and Duret, Jarod and Mdhaffar, Salima and Laperri\`{e}re, Ga\"{e}lle and Rouvier, Mickael and De Mori, Renato and Est\`{e}ve, Yannick},
title = {Open-source conversational AI with SpeechBrain 1.0},
year = {2025},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on speech processing tasks such as speech recognition, speech enhancement, speaker recognition, text-to-speech, and much more. It promotes transparency and replicability by releasing both the pre-trained models and the complete "recipes" of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {333},
numpages = {11},
keywords = {conversational AI, open-source, speech processing, deep learning}
}

@inproceedings{10.1145/3652583.3658086,
author = {Kang, Zuheng and He, Yayun and Zhao, Botao and Qu, Xiaoyang and Peng, Junqing and Xiao, Jing and Wang, Jianzong},
title = {Retrieval-Augmented Audio Deepfake Detection},
year = {2024},
isbn = {9798400706196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652583.3658086},
doi = {10.1145/3652583.3658086},
abstract = {With recent advances in speech synthesis including text-to-speech (TTS) and voice conversion (VC) systems enabling the generation of ultra-realistic audio deepfakes, there is growing concern about their potential misuse. However, most deepfake (DF) detection methods rely solely on the fuzzy knowledge learned by a single model, resulting in performance bottlenecks and transparency issues. Inspired by retrieval-augmented generation (RAG), we propose a retrieval-augmented detection (RAD) framework that augments test samples with similar retrieved samples for enhanced detection. We also extend the multi-fusion attentive classifier to integrate it with our proposed RAD framework. Extensive experiments show the superior performance of the proposed RAD framework over baseline methods, achieving state-of-the-art results on the ASVspoof 2021 DF set and competitive results on the 2019 and 2021 LA sets. Further sample analysis indicates that the retriever consistently retrieves samples mostly from the same speaker with acoustic characteristics highly consistent with the query audio, thereby improving detection performance.},
booktitle = {Proceedings of the 2024 International Conference on Multimedia Retrieval},
pages = {376–384},
numpages = {9},
keywords = {audio deepfake, deepfake detection, llm, retrieval-augmented detection, retrieval-augmented generation, text-to-speech, voice conversion},
location = {Phuket, Thailand},
series = {ICMR '24}
}

@inproceedings{10.1145/3664647.3681465,
author = {Zhu, Xinfa and Tian, Wenjie and Wang, Xinsheng and He, Lei and Xiao, Yujia and Wang, Xi and Tan, Xu and Zhao, Sheng and Xie, Lei},
title = {UniStyle: Unified Style Modeling for Speaking Style Captioning and Stylistic Speech Synthesis},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681465},
doi = {10.1145/3664647.3681465},
abstract = {Understanding the speaking style, such as the emotion of the interlocutor's speech, and responding with speech in an appropriate style is a natural occurrence in human conversations. However, technically, existing research on speech synthesis and speaking style captioning typically proceeds independently. In this work, an innovative framework, referred to as UniStyle, is proposed to incorporate both the capabilities of speaking style captioning and style-controllable speech synthesizing. Specifically, UniStyle consists of a UniConnector and a style prompt-based speech generator. The role of the UniConnector is to bridge the gap between different modalities, namely speech audio and text descriptions. It enables the generation of text descriptions with speech as input and the creation of style representations from text descriptions for speech synthesis with the speech generator. Besides, to overcome the issue of data scarcity, we propose a two-stage and semi-supervised training strategy, which reduces data requirements while boosting performance. Extensive experiments conducted on open-source corpora demonstrate that UniStyle achieves state-of-the-art performance in speaking style captioning and synthesizes expressive speech with various speaker timbres and speaking styles in a zero-shot manner.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {7513–7522},
numpages = {10},
keywords = {data scarcity, speaking style captioning, style modeling, text-to-speech},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3459212.3459227,
author = {Dao, Xuan-Quy and Le, Ngoc-Bich and Nguyen, Thi-My-Thanh},
title = {AI-Powered MOOCs: Video Lecture Generation},
year = {2021},
isbn = {9781450388917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459212.3459227},
doi = {10.1145/3459212.3459227},
abstract = {This paper focuses on the application of artificial intelligence in education that artificial intelligence technology was utilized to reduce the instructor's workload as well as enhance learner engagement in online learning platforms. The idea of this paper is to use text-to-speech and speech-driven-face to automatically create a video lecture with the instructor's voice and face without recording the video. This approach not only gives the best display of lesson content of video lectures which contains slides (pdf format), instructor's voice, and face but also enables the modification of video lectures without recapture video. Furthermore, this approach allows learners to choose a video lecture with their desired instructor's voice and face. The experimental results validated our approach.},
booktitle = {Proceedings of the 2021 3rd International Conference on Image, Video and Signal Processing},
pages = {95–102},
numpages = {8},
keywords = {Video Lecture, Text-to-Speech, Speech-to-Text, Speech-driven-Face, Online Learning, AI},
location = {Singapore, Singapore},
series = {IVSP '21}
}

@article{10.1145/3701031,
author = {Al-Zoghby, Aya M. and Al-Awadly, Esraa Mohamed K. and Ebada, Ahmed Ismail and Awad, Wael A.},
title = {Overview of Multimodal Machine Learning},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3701031},
doi = {10.1145/3701031},
abstract = {Human nature is fundamentally driven by the need for interaction and attention, which are fulfilled through various sensory modalities, including hearing, sight, touch, taste, and smell. These senses enable us to perceive, understand, and engage with the world around us. The quality and depth of our interactions change considerably when we use multiple senses simultaneously, highlighting the importance of multimodal interactions in our daily lives. In the realm of technology, multimodal integration offers immense value, as it aims to create systems that can replicate or complement these natural human abilities for enhanced interaction.This article explores the significance of spatial multimodalities in machine learning, highlighting their role in improving model performance in applications such as autonomous driving, healthcare, and virtual assistants. It addresses challenges like the complexity of fusing diverse sensory data types and proposes solutions such as advanced data fusion techniques, adaptive learning algorithms, and transformer architectures. The goal is to provide an overview of state-of-the-art research and future directions for advancing human–computer interaction.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {10},
numpages = {20},
keywords = {Multimodal Machine Learning(MML), Natural Language Processing(NLP), Deep Learning(DL), Fusion Techniques, Cross-Modal}
}

@article{10.1145/3600228,
author = {Mirishkar, Ganesh S. and Raju V, Vishnu Vidyadhara and Naroju, Meher Dinesh and Maity, Sudhamay and Yalla, Prakash and Vuppala, Anil Kumar},
title = {IIITH-CSTD Corpus: Crowdsourced Strategies for the Collection of a Large-scale Telugu Speech Corpus},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3600228},
doi = {10.1145/3600228},
abstract = {Due to the lack of a large annotated speech corpus, many low-resource Indian languages struggle to utilize recent advancements in deep neural network architectures for Automatic Speech Recognition (ASR) tasks. Collecting large-scale databases is an expensive and time-consuming task. Current approaches lack extensive traditional expert-based data acquisition guidelines, as they are tedious and complex. In this work, we present the International Institute of Information Technology Hyderabad-Crowd Sourced Telugu Database (IIITH-CSTD), a Telugu corpus collected through crowdsourcing. In particular, our main objective is to mitigate the low-resource problem for Telugu. We also present the sources, crowdsourcing pipeline, and the protocols used to collect the corpus for a low-resource language, namely, Telugu. Data of approximately 2,000 hours of transcribed audio is presented and released in this article, covering three major regional dialects of the Telugu language in three different (i.e., read, conversational and spontaneous) speaking styles on topics such as politics, sports, and arts, science, and so on.1 We also present the experimental results of the collected corpus on ASR tasks. We hope this work will motivate researchers to curate large-scale annotated speech data for other low-resource Indic languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {195},
numpages = {26},
keywords = {low-resource languages, resource creation, End-to-End, TDNN, dialects, Speech recognition}
}

@inproceedings{10.1145/3639856.3639876,
author = {Sasindran, Zitha and Yelchuri, Harsha and Tamma, Prabhakar Venkata and Rao, Pooja},
title = {MobileASR: A resource-aware on-device learning framework for user voice personalization applications on mobile phones},
year = {2024},
isbn = {9798400716492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639856.3639876},
doi = {10.1145/3639856.3639876},
abstract = {We describe a comprehensive methodology for developing user-voice personalized automatic speech recognition (ASR) models by effectively training models on mobile phones, allowing user data and models to be stored and used locally. To achieve this, we propose a resource-aware sub-model-based training approach that considers the RAM, and battery capabilities of mobile phones. By considering the evaluation metric and resource constraints of the mobile phones, we are able to perform efficient training and halt the process accordingly. To simulate real users, we use speakers with various accents. The entire on-device training and evaluation framework was then tested on various mobile phones across brands. We show that fine-tuning the models and selecting the right hyperparameter values is a trade-off between the lowest achievable performance metric, on-device training time, and memory consumption. Overall, our methodology offers a comprehensive solution for developing personalized ASR models while leveraging the capabilities of mobile phones, and balancing the need for accuracy with resource constraints.},
booktitle = {Proceedings of the Third International Conference on AI-ML Systems},
articleno = {20},
numpages = {11},
keywords = {model adaptation, on-device personalization, on-device training, speech recognition, stopping criteria.},
location = {Bangalore, India},
series = {AIMLSystems '23}
}

@inproceedings{10.1145/3613904.3642817,
author = {Han, Chaeeun and Mitra, Prasenjit and Billah, Syed Masum},
title = {Uncovering Human Traits in Determining Real and Spoofed Audio: Insights from Blind and Sighted Individuals},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642817},
doi = {10.1145/3613904.3642817},
abstract = {This paper explores how blind and sighted individuals perceive real and spoofed audio, highlighting differences and similarities between the groups. Through two studies, we find that both groups focus on specific human traits in audio–such as accents, vocal inflections, breathing patterns, and emotions–to assess audio authenticity. We further reveal that humans, irrespective of visual ability, can still outperform current state-of-the-art machine learning models in discerning audio authenticity; however, the task proves psychologically demanding. Moreover, detection accuracy scores between blind and sighted individuals are comparable, but each group exhibits unique strengths: the sighted group excels at detecting deepfake-generated audio, while the blind group excels at detecting text-to-speech (TTS) generated audio. These findings not only deepen our understanding of machine-manipulated and neural-renderer audio but also have implications for developing countermeasures, such as perceptible watermarks and human-AI collaboration strategies for spoofing detection.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {949},
numpages = {14},
keywords = {Audio perception, and audio watermarking, and human-AI collaboration., audio, blind, bona fide audio, deep fake audio, generative AI, neural speech, replay attack, sighted, speech, spoofed audio, text-to-speech (TTS), vision impairments, voice},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3727341,
author = {Deng, Jiacheng and Ye, Dengpan and Li, Jizhi and Zhang, Yunming and Liu, Ziyi and Tang, Long},
title = {The Interpretable and Transferable Adversarial Attack Against Synthetic Speech Detectors},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3727341},
doi = {10.1145/3727341},
abstract = {Existing work finds it challenging for adversarial examples to transfer among different synthetic speech detectors because of cross-feature and cross-model. To enhance the transferability of adversarial examples, we propose a spectral saliency analysis method and gain insight into the underlying detection mechanisms of existing detectors for the first time. These insights offer an interpretable basis for why adversarial examples are challenging to transfer between synthetic speech detection models. Then we further propose a two-stage adversarial attack framework. Specifically, the first stage leverages insights into the model detection mechanism to design a random time-frequency masking module, the random offset module, and one-dimensional convolution to generate transferable and robust adversarial examples. In the second stage, to mitigate the problem of obvious noise in the low-energy frames of the carrier in existing adversarial attacks, we perform secondary optimization on frames below the Signal-Noise-Rate threshold to enhance its auditory quality. Extensive experimental results demonstrate that the proposed method significantly enhances the transferability and robustness of adversarial examples, while simultaneously preserving the acoustic quality compared to typical approaches.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = apr,
keywords = {Adversarial Example, Transferability, Synthetic Speech Detection}
}

@inproceedings{10.5555/3507788.3507830,
author = {Packowski, Sarah and Kulkarni, Rashmi and Richard, Sharyn and Faircloth, Gary},
title = {Using IBM watson services to process video to streamline business processes and improve customer experience},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Artificial intelligence (AI) technologies such as speech to text, text to speech, and natural language processing are now available through software-as-a-service offerings from multiple cloud service providers. The performance and accuracy of these technologies continue to improve. However, it can still be a struggle to build end-to-end infrastructure and automation using these technologies to support business processes and goals. In this paper, we describe our team's experience using IBM Watson services on IBM Cloud to process internal meeting recordings and external product videos. We create accurate, well-formatted text transcripts and captions files, implement video transcript search, and generate translated audio. These solutions improve our internal processes and provide a better experience for our customers.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {262–267},
numpages = {6},
keywords = {video search, text to speech, speech to text, IBM watson},
location = {Toronto, Canada},
series = {CASCON '21}
}

@article{10.5555/3722577.3722674,
author = {Pratap, Vineel and Tjandra, Andros and Shi, Bowen and Tomasello, Paden and Babu, Arun and Kundu, Sayani and Elkahky, Ali and Ni, Zhaoheng and Vyas, Apoorv and Fazel-Zarandi, Maryam and Baevski, Alexei and Adi, Yossi and Zhang, Xiaohui and Hsu, Wei-Ning and Conneau, Alexis and Auli, Michael},
title = {Scaling speech technology to 1,000+ languages},
year = {2025},
issue_date = {January 2024},
publisher = {JMLR.org},
volume = {25},
number = {1},
issn = {1532-4435},
abstract = {Expanding the language coverage of speech technology has the potential to improve access to information for many more people. However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000 languages spoken around the world. The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task while providing improved accuracy compared to prior work. The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models for the same number of languages, as well as a language identification model for 4,017 languages. Experiments show that our multilingual speech recognition model more than halves the word error rate of Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data. The MMS models and tooling for data pre-processing are available at https://github.com/pytorch/fairseq/tree/master/examples/mms.},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {97},
numpages = {52},
keywords = {multilingual speech processing, self-supervised learning, language expansion, neural networks}
}

@inproceedings{10.1145/3640543.3645154,
author = {Kim, Suhyun and Lee, Semin and Kim, Kyungok and Oh, Uran},
title = {Utilizing a Dense Video Captioning Technique for Generating Image Descriptions of Comics for People with Visual Impairments},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645154},
doi = {10.1145/3640543.3645154},
abstract = {To improve the accessibility of visual figures, auto-generation of text description of individual images has been studied. However, it cannot be directly applied to comics as the descriptions can be redundant as similar scenes appear in a row. To address this issue, we propose generating the descriptions per group of related images and demonstrate how an dense captioning technique for videos can be utilized for this purpose and ways to improve its performance. To assess the effectiveness of our approach and to identify factors affecting the quality of text descriptions of comics, we conducted a preliminary study with 3 sighted evaluators and a main user study with 12 participants with visual impairments. The results show that text descriptions generated per group of images are perceived to be better than those generated per image in terms of accuracy, clarity, understandability, length, informativeness and preference for sighted groups, when annotator is human. In the same conditions, when the annotator is AI, it exhibited better performance in terms of length. Also, people with visual impairments prefer group descriptions because of conciseness, smooth connectivity of sentences, and non-repetitive features. Based on the findings, we provide design recommendations for generating accessible comic descriptions at a scale for blind users.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {750–760},
numpages = {11},
keywords = {comics, dense video captioning, image description, people with visual impairment},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3508230.3508239,
author = {Ambrocio Sagum, Ria and A. Lopez, Kier Bryan and D. Bonzol, Steven John and L. Pidlaoan, Wendell Vryan},
title = {TAGSYNTA: Tagalog Speech Synthesizer using Part of Speech in Prosody Analyzation},
year = {2022},
isbn = {9781450387354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508230.3508239},
doi = {10.1145/3508230.3508239},
abstract = {In this paper, we developed a system called, TagSynta: Tagalog Text to Speech Synthesizer using Part of Speech in Prosody Analyzation, which is a Tagalog Text-to-Speech Synthesizer with proper intonation of the word given a Tagalog sentence. It used Part of Speech as its identifier to reduce the reuse of audio processor for faster output which aims to solve the problem of accuracy in an automatic Text-to-Speech conversion of a Tagalog language. The system obtained 92.50% in converting a text into speech and 96.63% in giving a correct prosody of the words. The respondents of the research agreed that the system's output was above average in terms of the naturalness of the voice produced by the system.},
booktitle = {Proceedings of the 2021 5th International Conference on Natural Language Processing and Information Retrieval},
pages = {61–66},
numpages = {6},
keywords = {Text to Speech using Prosody, Tagalog Text to Speech Synthesizer, Speech Synthesizer using Hidden Markov Model, Prosody Analysis using Part of Speech},
location = {Sanya, China},
series = {NLPIR '21}
}

@article{10.1109/TASLP.2024.3434497,
author = {Das, Anurag and Gutierrez-Osuna, Ricardo},
title = {Improving Mispronunciation Detection Using Speech Reconstruction},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3434497},
doi = {10.1109/TASLP.2024.3434497},
abstract = {Training related machine learning tasks simultaneously can lead to improved performance on both tasks. Text- to-speech (TTS) and mispronunciation detection and diagnosis (MDD) both operate using phonetic information and we wanted to examine whether a boost in MDD performance can be by two tasks. We propose a network that reconstructs speech from the phones produced by the MDD system and computes a speech reconstruction loss. We hypothesize that the phones produced by the MDD system will be closer to the ground truth if the reconstructed speech sounds closer to the original speech. To test this, we first extract wav2vec features from a pre-trained model and feed it to the MDD system along with the text input. The MDD system then predicts the target annotated phones and then synthesizes speech from the predicted phones. The system is therefore trained by computing both a speech reconstruction loss as well as an MDD loss. Comparing the proposed systems against an identical system but without speech reconstruction and another state-of-the-art baseline, we found that the proposed system achieves higher mispronunciation detection and diagnosis (MDD) scores. On a set of sentences unseen during training, the and speaker verification simultaneously can lead to improve proposed system achieves higher MDD scores, which suggests that reconstructing the speech signal from the predicted phones helps the system generalize to new test sentences. We also tested whether the system can generate accented speech when the input phones have mispronunciations. Results from our perceptual experiments show that speech generated from phones containing mispronunciations sounds more accented and less intelligible than phones without any mispronunciations, which suggests that the system can identify differences in phones and generate the desired speech signal.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {4420–4433},
numpages = {14}
}

@article{10.1109/TASLP.2024.3449148,
author = {Liu, Zhao-Ci and Chen, Liping and Hu, Ya-Jun and Ling, Zhen-Hua and Pan, Jia},
title = {PE-Wav2vec: A Prosody-Enhanced Speech Model for Self-Supervised Prosody Learning in TTS},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3449148},
doi = {10.1109/TASLP.2024.3449148},
abstract = {This paper investigates leveraging large-scale untranscribed speech data to enhance the prosody modelling capability of &lt;italic&gt;text-to-speech&lt;/italic&gt; (TTS) models. On the basis of the self-supervised speech model wav2vec 2.0, &lt;italic&gt;Prosody-Enhanced wav2vec&lt;/italic&gt; (PE-wav2vec) is proposed by introducing prosody learning. Specifically, prosody learning is achieved by applying supervision from the &lt;italic&gt;linear predictive coding&lt;/italic&gt; (LPC) residual signals on the initial Transformer blocks in the wav2vec 2.0 architecture. The embedding vectors extracted with the initial Transformer blocks of the PE-wav2vec model are utilised as prosodic representations for the corresponding frames in a speech utterance. To apply the PE-wav2vec representations in TTS, an acoustic model named &lt;italic&gt;Speech Synthesis model conditioned on Self-Supervisedly Learned Prosodic Representations&lt;/italic&gt; (S4LPR) is designed on the basis of FastSpeech 2. The experimental results demonstrate that the proposed PE-wav2vec model can provide richer prosody descriptions of speech than the vanilla wav2vec 2.0 model can. Furthermore, the S4LPR model using PE-wav2vec representations can effectively improve the subjective naturalness and reduce the objective distortions of synthetic speech compared with baseline models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {4199–4210},
numpages = {12}
}

@article{10.1109/TASLP.2024.3369537,
author = {Saeki, Takaaki and Maiti, Soumi and Li, Xinjian and Watanabe, Shinji and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
title = {Text-Inductive Graphone-Based Language Adaptation for Low-Resource Speech Synthesis},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3369537},
doi = {10.1109/TASLP.2024.3369537},
abstract = {Neural text-to-speech (TTS) systems have made significant progress in generating natural synthetic speech. However, neural TTS requires large amounts of paired training data, which limits its applicability to a small number of resource-rich languages. Previous work on low-resource TTS has addressed the data hungriness based on transfer learning from a multilingual model to low-resource languages, but it still relies heavily on the availability of paired data for the target languages. In this paper, we propose a text-inductive language adaptation framework for low-resource TTS to address the cost of collecting the paired data for low-resource languages. To inject textual knowledge during transfer learning, our framework employs a two-stage adaptation scheme that utilizes both text-only and supervised data for the target language. In the text-based adaptation stage, we update the language-aware embedding layer with a masked language model objective using text-only data for the target language. In the supervised adaptation stage, the entire TTS model is updated using paired data for the target language. We also propose a graphone-based multilingual training method that jointly uses graphemes and International Phonetic Alphabet symbols (referred to as graphones) for resource-rich languages, while using only graphemes for low-resource languages. This approach facilitates the transfer of pronunciation knowledge from resource-rich to low-resource languages. Through extensive evaluations, we demonstrate that 1) our framework with text-based adaptation outperforms the previous supervised transfer learning approach and 2) the proposed graphone-based training method further improves the performance of both multilingual TTS and low-resource language adaptation. With only 5 minutes of paired data for fine-tuning, our method achieved highly intelligible synthetic speech with the character error rates of around 6 % for a target language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1829–1844},
numpages = {16}
}

@inproceedings{10.1145/3686424.3686461,
author = {Liu, Haiqi and Su, Yuanxing},
title = {The Design and Realization of Global Competence-Oriented Vocational English Listening Training System for the Navy},
year = {2024},
isbn = {9798400710360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686424.3686461},
doi = {10.1145/3686424.3686461},
abstract = {As an important component of global competence, English competence, especially the listening ability of PLA Naval officers and men is stressed in the new era to build a first-class navy. Compared with new and higher requirements, the present naval vocational courses show some deficiencies. This paper introduced a vocational English listening training system which adopts the latest AI technologies/tools to make up for them and the preliminary attempt has achieved good results.},
booktitle = {Proceedings of the 2024 Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Education Digitalization and Computer Science},
pages = {216–222},
numpages = {7},
location = {Shenzhen, China},
series = {EDCS '24}
}

@article{10.1109/TASLP.2023.3290423,
author = {Lian, Jiachen and Zhang, Chunlei and Anumanchipalli, Gopala K. and Yu, Dong},
title = {Unsupervised TTS Acoustic Modeling for TTS With Conditional Disentangled Sequential VAE},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3290423},
doi = {10.1109/TASLP.2023.3290423},
abstract = {In this paper, we propose a novel unsupervised text-to-speech acoustic model training scheme, named UTTS, which does not require text-audio pairs. UTTS is a multi-speaker speech synthesizer that supports zero-shot voice cloning, it is developed from a perspective of disentangled speech representation learning. The framework offers a flexible choice of a speaker's duration model, timbre feature (identity) and content for TTS inference. We leverage recent advancements in self-supervised speech representation learning as well as speech synthesis front-end techniques for system development. Specifically, we employ our recently formulated Conditional Disentangled Sequential Variational Auto-encoder (C-DSVAE) as the backbone UTTS AM, which offers well-structured content representations given unsupervised alignment (UA) as condition during training. For UTTS inference, we utilize a lexicon to map input text to the phoneme sequence, which is expanded to the frame-level forced alignment (FA) with a speaker-dependent duration model. Then, we develop an alignment mapping module that converts FA to UA. Finally, the C-DSVAE, serving as the self-supervised TTS AM, takes the predicted UA and a target speaker embedding to generate the mel spectrogram, which is ultimately converted to waveform with a neural vocoder. We show how our method enables speech synthesis without using a paired TTS corpus in AM development stage. Experiments demonstrate that UTTS can synthesize speech of high naturalness and intelligibility measured by human and objective evaluations. Audio samples are available at our demo page.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2548–2557},
numpages = {10}
}

@article{10.1109/TASLP.2022.3205755,
author = {Kotani, Gaku and Saito, Daisuke and Minematsu, Nobuaki},
title = {Voice Conversion Based on Deep Neural Networks for Time-Variant Linear Transformations},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3205755},
doi = {10.1109/TASLP.2022.3205755},
abstract = {This paper describes a novel framework of voice conversion to improve the conversion performance against the amount of training data. In voice conversion, deep neural networks are used as conversion models that map source to target features. In this framework, it generally needs a larger amount of training data and bigger models to build more accurate conversion models. This condition, however, will reduce the usability of voice conversion. In this paper, in order to improve the conversion performance versus the amount of training data, a top-down knowledge is introduced into models as prior. We expect that we can take advantage of top-down knowledge we have instead of preparing a large amount of data. In the proposed method, the conversion process of features is restricted to time-variant linear transformation on cepstral space. It explicitly utilizes an attribute of voice conversion i.e. homo-domain mapping, which is not common in automatic speech recognition or text-to-speech synthesis. In other words, in VC, the input and output are on the same feature domain. In addition, it also makes it possible to explicitly consider the physical difference between speakers such as the difference of vocal tract length. The assumption of the homo-domain mapping is related to conversion methods based on spectral differentials, and then the relation is discussed in the paper. Experiments demonstrate the effectiveness of our proposal and the way that the constraint of linear transformation works is investigated.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {2981–2992},
numpages = {12}
}

@inproceedings{10.1145/3548636.3548648,
author = {Zeng, Rui and Xiong, Shengwu},
title = {Lip to Speech Synthesis Based on Speaker Characteristics Feature Fusion},
year = {2022},
isbn = {9781450396820},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548636.3548648},
doi = {10.1145/3548636.3548648},
abstract = {Lip to speech synthesis (Lip2Speech) is a technology that reconstructs speech from the silent talking face video. With the development of deep learning, achievements have been made in this field. Due to the silent talking face video does not contain the speaker characteristics information, reconstructing speech directly from the silent talking video will lose the characteristic information of the speaker, thus reducing the quality of the reconstructed speech. In this paper we proposed a new framework using the pre-trained speaker encoder network which extract the speaker characteristics information. More specially: (1) The pretrained speaker encoder network generates a fixed-dimensional embedding vector from a few seconds of given speaker's speech, which contains the speaker characteristics information, (2) The content encoder uses a stack of 3D convolutions to extracts the content information of the video, (3) a sequence-to-sequence synthesis network based on Tacotron2 that generates Mel-spectrogram from silent video, conditioned on the speaker's identity embedding. Experimental results show that, using the pretrained speaker encoder can improved the speech reconstruction quality.},
booktitle = {Proceedings of the 4th International Conference on Information Technology and Computer Communications},
pages = {78–83},
numpages = {6},
keywords = {speaker characteristic information, multi-speaker speech synthesis, lip to speech synthesis, feature fusion},
location = {Guangzhou, China},
series = {ITCC '22}
}

@article{10.1109/TASLP.2023.3273414,
author = {Sun, Siqi and Richmond, Korin and Tang, Hao},
title = {Improving Seq2Seq TTS Frontends With Transcribed Speech Audio},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3273414},
doi = {10.1109/TASLP.2023.3273414},
abstract = {Due to the data inefficiency and low speech quality of grapheme-based end-to-end text-to-speech (TTS), having a separate high-performance TTS linguistic frontend is still commonly regarded as necessary. However, a TTS frontend is itself difficult to build and maintain, since it requires abundant linguistic knowledge for its construction. In this article, we start by bootstrapping an integrated sequence-to-sequence (Seq2Seq) TTS frontend using a pre-existing pipeline-based frontend and large amounts of unlabelled normalized text, achieving promising memorization and generalisation abilities. To overcome the performance limitation imposed by the pipeline-based frontend, this work proposes a Forced Alignment (FA) method to decode the pronunciations from transcribed speech audio and then use them to update the Seq2Seq frontend. Our experiments demonstrate the effectiveness of our proposed FA method, which can significantly improve the word token accuracy from 52.6% to 91.2% for out-of-dictionary words. In addition, it can also correct the pronunciation of homographs from transcribed speech audio and potentially improve the homograph disambiguation performance of the Seq2Seq frontend.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1940–1952},
numpages = {13}
}

@inproceedings{10.1145/3459212.3459222,
author = {Cai, Yueqing and Rao, Wenbi},
title = {A Transformer-based Chinese Non-autoregressive Speech Synthesis Scheme},
year = {2021},
isbn = {9781450388917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459212.3459222},
doi = {10.1145/3459212.3459222},
abstract = {At present, the main research hotspot in the field of speech synthesis is still English speech synthesis, and there are few non-autoregressive Chinese speech synthesis models. During the Chinese migration process of FastSpeech2, we found that the naturalness of the synthesized audio was not good enough and there were some abnormal interruptions and incorrect pronunciation. Inspired by the training method of generative adversarial network, we use FastSpeech2 as the generator, and add a discriminator to force FastSpeech2 to generate audio more similar to the real audio. In order to realize a complete text to Mel spectrogram speech synthesis scheme, we design a text-to-phoneme converter based on corpus and rule constraints. And we conduct experiments on Baker dataset. The results show that our model achieves a better Mel Cepstral Distance than that of FastSpeech2. And our model can achieve a mean opinion score of 3.94, which is slightly better than the original model.},
booktitle = {Proceedings of the 2021 3rd International Conference on Image, Video and Signal Processing},
pages = {59–64},
numpages = {6},
keywords = {generative adversarial network, Speech synthesis, Non-autoregressive model, Chinese text-to-speech model},
location = {Singapore, Singapore},
series = {IVSP '21}
}

@article{10.1109/TASLP.2023.3313424,
author = {Liu, Chang and Ling, Zhen-Hua and Chen, Ling-Hui},
title = {Pronunciation Dictionary-Free Multilingual Speech Synthesis Using Learned Phonetic Representations},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3313424},
doi = {10.1109/TASLP.2023.3313424},
abstract = {This article presents a multilingual speech synthesis approach that leverages learned phonetic representations to eliminate the need for pronunciation dictionaries in target languages. The learned phonetic representations consist of unsupervised phonetic representations (UPR) and supervised phonetic representations (SPR). To extract UPRs, a pre-trained wav2vec 2.0 model is utilized, while a language-independent automatic speech recognition (LI-ASR) model with a connectionist temporal classification (CTC) loss is employed to derive segment-level SPRs from the speech data of target languages. An acoustic model using UPRs and SPRs as intermediate representations is then designed, comprising a UPR predictor, an SPR predictor, and a representation-to-mel-spectrogram (RTM) converter. The two predictors generate UPRs and SPRs from texts, respectively. The RTM converter first combines UPRs with SPRs using a Transformer-based encoder, and then feeds the merged representations into a decoder to produce mel-spectrograms. Considering the difficulty of collecting large training corpora for all languages in multilingual speech synthesis, the parameters of both the two predictors and the RTM converter can be pre-trained on non-target languages to further improve model performance. Experimental results on six target languages demonstrate that our method outperformed the approaches directly predicting mel-spectrograms from character or phoneme sequences, and pre-training the acoustic model using a multilingual corpus further improved the performance of synthetic speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3706–3716},
numpages = {11}
}

@article{10.1109/TASLP.2024.3453598,
author = {Li, Yang and Yu, Cheng and Sun, Guangzhi and Zu, Weiqin and Tian, Zheng and Wen, Ying and Pan, Wei and Zhang, Chao and Wang, Jun and Yang, Yang and Sun, Fanglei},
title = {Cross-Utterance Conditioned VAE for Speech Generation},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3453598},
doi = {10.1109/TASLP.2024.3453598},
abstract = {Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4263–4276},
numpages = {14}
}

@inproceedings{10.1145/3570945.3607331,
author = {Santos, Carlos Pereira and de Groot, Phil and Hagen, Jens and Boudry, Agathe and Mayer, Igor},
title = {CUBE: Conversational User-Interface-Based Embodiment: Developing a Digital Humans Embodiment for Conversational Agents: Design, Implementation, and Integration Challenges},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607331},
doi = {10.1145/3570945.3607331},
abstract = {Our study introduces an open general-purpose platform for the embodiment of conversational AI systems. Conversational User-interface Based Embodiment (CUBE) is designed to streamline the integration of embodied solutions into text-based dialog managers, providing flexibility for customization depending on the specific use case and application. CUBE is responsible for naturally interacting with users by listening, observing, and responding to them.A detailed account of the design and implementation of the solution is provided, as well as a thorough examination of how it can be integrated by developers and AI dialogue manager integrators. Through interviews with developers, insight was gained into the advantages of such systems. Additionally, key areas that require further research were identified in the current challenges in achieving natural interaction between the user and the embodiments. CUBE bridges some of the gaps by providing controls to further develop natural non-verbal communication.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {34},
numpages = {8},
keywords = {Artificial Intelligence, Digital Embodiment, User Interface, Virtual Human},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@article{10.1109/TASLP.2024.3451951,
author = {Gong, Cheng and Wang, Xin and Cooper, Erica and Wells, Dan and Wang, Longbiao and Dang, Jianwu and Richmond, Korin and Yamagishi, Junichi},
title = {ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3451951},
doi = {10.1109/TASLP.2024.3451951},
abstract = {Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker's voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker's voice, even without any training data for the new, unseen language.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4036–4051},
numpages = {16}
}

@article{10.1109/TASLP.2023.3278184,
author = {Zhang, Ya-Jie and Zhang, Chao and Song, Wei and Zhang, Zhengchen and Wu, Youzheng and He, Xiaodong},
title = {Prosody Modelling With Pre-Trained Cross-Utterance Representations for Improved Speech Synthesis},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3278184},
doi = {10.1109/TASLP.2023.3278184},
abstract = {When humans speak multiple utterances in a continuous manner, the prosodic features generated in each utterance are related to those in its neighbouring utterances. Such cross-utterance (CU) dependencies are often ignored by the current neural text-to-speech (TTS) systems, which reduces the naturalness and expressiveness of the synthesized speeches. In this article, we propose to improve the prosody modelling ability of neural TTS systems using pre-trained CU acoustic and text representations. Such CU acoustic representations are derived using the Wav2Vec 2.0 model (W2V2) from the synthesized audios of the past utterances, while the CU text representations are extracted using the Bidirectional Encoder Representation from Transformers (BERT) model from the scripts of the future utterances. Experimental results on a Mandarin audiobook and an English audiobook showed the naturalness and expressiveness of the synthesized audios were significantly improved by incorporating such pre-trained W2V2 and BERT CU representations into the Fastspeech2 TTS framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {2812–2823},
numpages = {12}
}

@article{10.1109/TASLP.2021.3049336,
author = {Huang, Wen-Chin and Hayashi, Tomoki and Wu, Yi-Chiao and Kameoka, Hirokazu and Toda, Tomoki},
title = {Pretraining Techniques for Sequence-to-Sequence Voice Conversion},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3049336},
doi = {10.1109/TASLP.2021.3049336},
abstract = {Sequence-to-sequence (seq2seq) voice conversion (VC) models are attractive owing to their ability to convert prosody. Nonetheless, without sufficient data, seq2seq VC models can suffer from unstable training and mispronunciation problems in the converted speech, thus far from practical. To tackle these shortcomings, we propose to transfer knowledge from other speech processing tasks where large-scale corpora are easily available, typically text-to-speech (TTS) and automatic speech recognition (ASR). We argue that VC models initialized with such pretrained ASR or TTS model parameters can generate effective hidden representations for high-fidelity, highly intelligible converted speech. In this work, we examine our proposed method in a parallel, one-to-one setting. We employed recurrent neural network (RNN)-based and Transformer based models, and through systematical experiments, we demonstrate the effectiveness of the pretraining scheme and the superiority of Transformer based models over RNN-based models in terms of intelligibility, naturalness, and similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {745–755},
numpages = {11}
}

@article{10.1109/TASLP.2022.3196879,
author = {Novitasari, Sashi and Sakti, Sakriani and Nakamura, Satoshi},
title = {A Machine Speech Chain Approach for Dynamically Adaptive Lombard TTS in Static and Dynamic Noise Environments},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3196879},
doi = {10.1109/TASLP.2022.3196879},
abstract = {Recent end-to-end text-to-speech synthesis (TTS) systems have successfully synthesized high-quality speech. However, TTS speech intelligibility degrades in noisy environments because most of these systems were not designed to handle noisy environments. Several works attempted to address this problem by using offline fine-tuning to adapt their TTS to noisy conditions. Unlike machines, humans never perform offline fine-tuning. Instead, they speak with the Lombard effect in noisy places, where they dynamically adjust their vocal effort to improve the audibility of their speech. This ability is supported by the speech chain mechanism, which involves auditory feedback passing from speech perception to speech production. This paper proposes an alternative approach to TTS in noisy environments that is closer to the human Lombard effect. Specifically, we implement Lombard TTS in a machine speech chain framework to synthesize speech with dynamic adaptation. Our TTS performs adaptation by generating speech utterances based on the auditory feedback that consists of the automatic speech recognition (ASR) loss as the speech intelligibility measure and the speech-to-noise ratio (SNR) prediction as power measurement. Two versions of TTS are investigated: non-incremental TTS with utterance-level feedback and incremental TTS (ITTS) with short-term feedback to reduce the delay without significant performance loss. Furthermore, we evaluate the TTS systems in both static and dynamic noise conditions. Our experimental results show that auditory feedback enhanced the TTS speech intelligibility in noise.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2673–2688},
numpages = {16}
}

@article{10.1145/3570161,
author = {Aloufi, Ranya and Haddadi, Hamed and Boyle, David},
title = {Paralinguistic Privacy Protection at the Edge},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {2471-2566},
url = {https://doi.org/10.1145/3570161},
doi = {10.1145/3570161},
abstract = {Voice user interfaces and digital assistants are rapidly entering our lives and becoming singular touch points spanning our devices. These always-on services capture and transmit our audio data to powerful cloud services for further processing and subsequent actions. Our voices and raw audio signals collected through these devices contain a host of sensitive paralinguistic information that is transmitted to service providers regardless of deliberate or false triggers. As our emotional patterns and sensitive attributes like our identity, gender, and well-being are easily inferred using deep acoustic models, we encounter a new generation of privacy risks by using these services. One approach to mitigate the risk of paralinguistic-based privacy breaches is to exploit a combination of cloud-based processing with privacy-preserving, on-device paralinguistic information learning and filtering before transmitting voice data.In this article we introduce EDGY, a configurable, lightweight, disentangled representation learning framework that transforms and filters high-dimensional voice data to identify and contain sensitive attributes at the edge prior to offloading to the cloud. We evaluate EDGY’s on-device performance and explore optimization techniques, including model quantization and knowledge distillation, to enable private, accurate, and efficient representation learning on resource-constrained devices. Our results show that EDGY runs in tens of milliseconds with 0.2% relative improvement in “zero-shot” ABX score or minimal performance penalties of approximately 5.95% word error rate (WER) in learning linguistic representations from raw voice signals, using a CPU and a single-core ARM processor without specialized hardware.},
journal = {ACM Trans. Priv. Secur.},
month = apr,
articleno = {19},
numpages = {27},
keywords = {model optimization, disentangled representation learning, Deep Learning, voice synthesis, speech analysis, privacy, Internet of Things (IoT), Voice user interface}
}

@inproceedings{10.1145/3460179.3460188,
author = {My-Thanh Nguyen, Thi and Hai Diep, Thanh and Bien Ngo, Bac and Bich Le, Ngoc and Quy Dao, Xuan},
title = {Design of Online Learning Platform with Vietnamese Virtual Assistant},
year = {2021},
isbn = {9781450388948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460179.3460188},
doi = {10.1145/3460179.3460188},
abstract = {This paper proposes an online learning platform with a Vietnamese Virtual Assistant (VVA) that is utilized to help instructors in presenting lessons as well as testing and assessing learners. Currently, in online learning platforms, the lesson content is recorded in videos. In the proposed approach, the lesson content is delivered through slides (PDF format) combined with the instructor's voice and face that are synthesized from the text (TTS: Text-to-Speech and SDF: Speech-driven-Face). In addition, our approach ensures the highest quality in displaying the lesson content and allows us to edit the lesson content directly from text without recording video again as the current way.},
booktitle = {Proceedings of the 2021 6th International Conference on Intelligent Information Technology},
pages = {51–57},
numpages = {7},
keywords = {Chatbot, Online learning, Speech-driven-Face, Speech-to-Text, Text-to-Speech, Virtual assistant},
location = {Ho Chi Minh, Viet Nam},
series = {ICIIT '21}
}

@inproceedings{10.1145/3643489.3661130,
author = {Alateeq, Ahmed and Roantree, Mark and Gurrin, Cathal},
title = {Voxento-Pro: An Advanced Voice Lifelog Retrieval Interaction for Multimodal Lifelogs},
year = {2024},
isbn = {9798400705502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643489.3661130},
doi = {10.1145/3643489.3661130},
abstract = {We present an advanced version called Voxento-Pro which is an interactive voice-based lifelog retrieval system. This system has been developed to participate in the seventh ACM Lifelog Search Challenge LSC'24, at ICMR'24 in Thailand. In Voxento-Pro, we introduce a conversational query methodology by utilising OpenAI's Assistant API and employ OpenAI's Whisper technology for state-of-the-art speech recognition and synthesis. This novel version features a more natural interaction mechanism, which enhances the user's experience. In addition, the user interface (UI) was redesigned and introduced a new chat interface and other components. The backend retrieval API was rebuilt with a new technology to support fast and efficient API interactions. Data processing of the lifelog data resulted in about 20% of non-important images being identified and 27% of missing data being filled with Geocoding APIs.},
booktitle = {Proceedings of the 7th Annual ACM Workshop on the Lifelog Search Challenge},
pages = {105–110},
numpages = {6},
keywords = {lifelog, interactive retrieval, voice interaction, conversational search},
location = {Phuket, Thailand},
series = {LSC '24}
}

@article{10.1109/TASLP.2024.3407509,
author = {Wang, Linqin and Huang, Xiang and Yu, Zhengtao and Peng, Hao and Gao, Shengxiang and Mao, Cunli and Huang, Yuxin and Dong, Ling and Yu, Philip S.},
title = {Zero-Shot Text Normalization via Cross-Lingual Knowledge Distillation},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3407509},
doi = {10.1109/TASLP.2024.3407509},
abstract = {Text normalization (TN) is a crucial preprocessing step in text-to-speech synthesis, which pertains to the accurate pronunciation of numbers and symbols within the text. Existing neural network-based TN methods have shown significant success in rich-resource languages. However, these methods are data-driven and highly rely on a large number of labeled datasets, which are not practical in zero-resource settings. Rule-based weighted finite-state transducers (WFST) are a common measure for zero-shot TN, but WFST-based TN approaches encounter challenges with ambiguous input, particularly in cases where the normalized form is context-dependent. On the other hand, conventional neural TN methods suffer from unrecoverable errors. In this paper, we propose ZSTN, a novel zero-shot TN framework based on cross-lingual knowledge distillation, which utilizes annotated data to train the teacher model on rich-resource language and unlabelled data to train the student model on zero-resource language. Furthermore, it incorporates expert knowledge from WFST into a knowledge distillation neural network. Concretely, a TN model with WFST pseudo-labels augmentation is trained as a teacher model in the source language. Subsequently, the student model is supervised by soft-labels from the teacher model and WFST pseudo-labels from the target language. By leveraging cross-lingual knowledge distillation, we address contextual ambiguity in the text, while WFST mitigates unrecoverable errors of the neural model. Additionally, ZSTN is adaptable to different zero-resource languages by using the joint loss function for the teacher model and WFST constraints. We also release a zero-shot text normalization dataset in five languages. We compare ZSTN with seven zero-shot TN benchmarks on public datasets in four languages for the teacher model and zero-shot datasets in five languages for the student model. The results demonstrate that the proposed ZSTN excels in performance without the need for labeled data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {4631–4646},
numpages = {16}
}

@article{10.1109/TASLP.2023.3306716,
author = {Xu, Xuexin and Shi, Liang and Chen, Xunquan and Lin, Pingyuan and Lian, Jie and Chen, Jinhui and Zhang, Zhihong and Hancock, Edwin R.},
title = {Any-to-Any Voice Conversion With Multi-Layer Speaker Adaptation and Content Supervision},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3306716},
doi = {10.1109/TASLP.2023.3306716},
abstract = {Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3431–3445},
numpages = {15}
}

@article{10.1109/TASLP.2023.3288418,
author = {Lu, Yen-Ju and Chang, Chia-Yu and Yu, Cheng and Liu, Ching-Feng and Hung, Jeih-weih and Watanabe, Shinji and Tsao, Yu},
title = {Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3288418},
doi = {10.1109/TASLP.2023.3288418},
abstract = {Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this article, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC information improves SE performance. Moreover, the SE model trained with the BPC-based E2E-ASR outperforms that with the phoneme-based E2E-ASR. The results suggest that objectives with misclassification of phonemes by the ASR system may lead to imperfect feedback, and BPC could be a potentially better choice. Finally, it is noted that combining the most-confusable phonetic targets into the same BPC when calculating the additional objective can effectively improve the SE performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2738–2750},
numpages = {13}
}

@article{10.1145/3486674,
author = {Mendez, Javier and Bierzynski, Kay and Cu\'{e}llar, M. P. and Morales, Diego P.},
title = {Edge Intelligence: Concepts, Architectures, Applications, and Future Directions},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {1539-9087},
url = {https://doi.org/10.1145/3486674},
doi = {10.1145/3486674},
abstract = {The name edge intelligence, also known as Edge AI, is a recent term used in the past few years to refer to the confluence of machine learning, or broadly speaking artificial intelligence, with edge computing. In this article, we revise the concepts regarding edge intelligence, such as cloud, edge, and fog computing, the motivation to use edge intelligence, and compare current approaches and analyze application scenarios. To provide a complete review of this technology, previous frameworks and platforms for edge computing have been discussed in this work to provide the general view of the basis for Edge AI. Similarly, the emerging techniques to deploy deep learning models at the network edge, as well as specialized platforms and frameworks to do so, are review in this article. These devices, techniques, and frameworks are analyzed based on relevant criteria at the network edge, such as latency, energy consumption, and accuracy of the models, to determine the current state of the art as well as current limitations of the proposed technologies. Because of this, it is possible to understand the current possibilities to efficiently deploy state-of-the-art deep learning models at the network edge based on technologies such as artificial intelligence accelerators, tensor processing units, and techniques that include federated learning and gossip training. Finally, the challenges of Edge AI are discussed in the work, as well as the future directions that can be extracted from the evolution of the edge computing and Internet of Things approaches.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {48},
numpages = {41},
keywords = {artificial intelligence, deep learning, machine learning, edge computing, Edge AI, Edge intelligence}
}

@article{10.1109/TASLP.2022.3161159,
author = {Cheng, Gaofeng and Miao, Haoran and Yang, Runyan and Deng, Keqi and Yan, Yonghong},
title = {ETEH: Unified Attention-Based End-to-End ASR and KWS Architecture},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3161159},
doi = {10.1109/TASLP.2022.3161159},
abstract = {Even though attention-based end-to-end (E2E) automatic speech recognition (ASR) models have been yielding state-of-the-art recognition accuracy, they still fall behind many of the ASR models deployed in the industry in some crucial functionalities such as online processing and precise timestamps generating. This weakness prevents attention-based E2E ASR models from being applied in several essential speech tasks, such as online speech recognition and keyword searching (KWS). In this paper, we describe our proposed unified attention-based E2E ASR and KWS architecture–ETEH, which supports, in one model, both online and offline ASR decoding modes, thus allowing for precise and reliable KWS. “ETE” stands for attention-based E2E modeling, whereas “H” represents the hybrid gaussian mixture model and hidden Markov model (GMM-HMM). As a combination of both, ETEH is an attention-based E2E ASR architecture which utilizes the frame-wise time alignment (FTA) generated by GMM-HMM ASR models. This FTA is used to better the model in two ways: first, it helps the monotonic attentions of ETEH models to capture more accurate word time stamps, thus resulting in lower latency for online decoding; second, it helps ETEH models to provide accurate and reliable KWS results. Furthermore, we are able to combine both offline and online modes in one ETEH model and establish a concise system by adopt the universal training strategy. ETEH is functional and unique, and to the best of our knowledge, we can hardly find a comparable single attention-based E2E ASR system as the baseline. To evaluate ASR accuracy and latency for ETEH, we use our previously proposed monotonic truncated attention (MTA) based online CTC/attention (OCA) ASR models as baselines. Experimental results show that ETEH ASR models gain significant improvement in ASR latency compared to the baseline. To evaluate KWS performance, we compare ETEH models with CTC-based KWS models. Results demonstrate that our ETEH models achieve significantly better KWS performance compared to the CTC baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1360–1373},
numpages = {14}
}

@inproceedings{10.1145/3594441.3594465,
author = {Deshmukh, Vaidehi and Khaparde, Arti},
title = {Intelligent Sanskrit translator using NLP},
year = {2023},
isbn = {9798400700613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594441.3594465},
doi = {10.1145/3594441.3594465},
abstract = {The rapid evolution of Human beings as a species can be credited to their ability to commune with one another and efficiently drive ideas, messages and intent past each other. One of the antediluvian and well-structured languages, Sanskrit, is being relegated only to use in scriptures during modern times. Our intent is to build a virtual assistant (voice/chat) which communicates through Sanskrit ensuring this language becomes the linchpin of understanding machines and relaying information and knowledge not only for an extensive heterogeneity of vernacular population but for the world. Studying various Machine Learning and Neural Network models, understanding their scope, underlying principles and application hence facilitating deep understanding of the scope of AI Assistants and aid in building a Sanskrit Voice Bot. Various algorithm explore include linear regression and logistic regression, whose reach is limited to linearly related/ separable data, which was test by deploying gradient descent algorithm. Support Vector Machine kernels resolve this problem by providing linear as well as polynomial decision boundary. Principal Component Analysis finds its major application in dimensionality reduction and Anomaly Detection would be used to detect any out of the bound data input. Furthermore, Sequence Models would play a major role in all the required Natural Language Processing},
booktitle = {Proceedings of the 2023 8th International Conference on Information and Education Innovations},
pages = {145–151},
numpages = {7},
keywords = {Natural Language Processing, artificial intelligence, data mining, machine learning},
location = {Manchester, United Kingdom},
series = {ICIEI '23}
}

@inproceedings{10.1145/3689187.3709614,
author = {Prather, James and Leinonen, Juho and Kiesler, Natalie and Gorson Benario, Jamie and Lau, Sam and MacNeil, Stephen and Norouzi, Narges and Opel, Simone and Pettit, Vee and Porter, Leo and Reeves, Brent N. and Savelka, Jaromir and Smith, David H. and Strickroth, Sven and Zingaro, Daniel},
title = {Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709614},
doi = {10.1145/3689187.3709614},
abstract = {Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {300–338},
numpages = {39},
keywords = {artificial intelligence, computing education, genai, generative ai, large language models, pedagogical practices, teaching computing},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@article{10.1145/3658134,
author = {Zhang, Zeyi and Ao, Tenglong and Zhang, Yuyao and Gao, Qingzhe and Lin, Chuan and Chen, Baoquan and Liu, Libin},
title = {Semantic Gesticulator: Semantics-Aware Co-Speech Gesture Synthesis},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3658134},
doi = {10.1145/3658134},
abstract = {In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin. We will release the code and dataset for academic research.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {136},
numpages = {17},
keywords = {co-speech gesture synthesis, multi-modality, retrieval augmentation}
}

@article{10.1109/TASLP.2024.3463395,
author = {Chen, Haolin and Garner, Philip N.},
title = {Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3463395},
doi = {10.1109/TASLP.2024.3463395},
abstract = {We are motivated primarily by the adaptation of text-to-speech synthesis models; however we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. Nevertheless, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker-factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning performance, and using the Kronecker-factored approximation produces a better preservation of the pre-training knowledge than the diagonal ones.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4253–4262},
numpages = {10}
}

@inproceedings{10.1145/3658852.3659065,
author = {Cotton, Kelsey and De Vries, Katja and Tatar, K\i{}van\c{c}},
title = {Singing for the Missing: Bringing the Body Back to AI Voice and Speech Technologies},
year = {2024},
isbn = {9798400709944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658852.3659065},
doi = {10.1145/3658852.3659065},
abstract = {Technological advancements in deep learning for speech and voice have contributed to a recent expansion in applications for voice cloning, synthesis and generation. Invisibilised stakeholders in this expansion are numerous absent bodies, whose voices and voice data have been integral to the development and refinement of these speech technologies. This position paper probes current working practices for voice and speech in machine learning and AI, in which the bodies of voices are “invisibilised". We examine the facts and concerns about the voice-Body in applications of AI-voice technology. We do this through probing the wider connections between voice data and Schaefferian listening; speculating on the consequences of missing Bodies in AI-Voice; and by examining how vocalists and artists working with synthetic Bodies and AI-voices are ‘bringing the Body back’ in their own practices. We contribute with a series of considerations for how practitioners and researchers may help to ‘bring the Body back’ into AI-voice technologies.},
booktitle = {Proceedings of the 9th International Conference on Movement and Computing},
articleno = {2},
numpages = {12},
keywords = {AI, STS, artificial intelligence, body, musical AI, voice},
location = {Utrecht, Netherlands},
series = {MOCO '24}
}

@inproceedings{10.1145/3705927.3705939,
author = {Deshpande, Kedar and Sodhi, Manjit Singh and Raniyer, Nidhi and Rao, Madhav},
title = {A Time-Distributed CNN-LSTM with Attention Model for Speech Based Emotion Recognition},
year = {2025},
isbn = {9798400709586},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705927.3705939},
doi = {10.1145/3705927.3705939},
abstract = {Emotion recognition is expected to play a critical role in improving user experiences for digital products in the near future. In this context, most of the past work has emphasized on emotion recognition through images or video streams, whereas only recently the other modalities have come into prominence. Emotion is generally conceived in multi-modal forms which primarily includes facial expressions, hand movements, spoken words, and its associated tone, intensity, and speed. This work proposes emotion recognition through audio snippets. We propose a 2D CNN-LSTM with attention model to accurately predict emotion from audio stream. The model is trained on RAVDESS dataset which contains audio files for 8 emotions including Happy, Calm, Neutral, Sad, Fear, Disgust, Surprise and Angry. Mel-Spectrogram of each audio signal was used to train the model. Each Mel-Spectrogram representing audio stream, was framed using a window size of 128, with a stride of 64, which was then supplied as inputs to train, validate and test the model. Three Time-Distributed 2D CNN blocks were used to extract implicit features from the windowed spectrogram, followed by an LSTM layer to extract temporal properties. The final parts of the model involve an attention layer, a fully connected layer and a softmax layer to classify the audio stream into one of the 8 classes. The proposed model achieved a test accuracy of 84% which is far superior when compared to other state-of-the-art (SOTA) models trained on RAVDESS dataset. The model files are made freely available for further usage and easy adoption to the researchers and developers’ community.},
booktitle = {Proceedings of the 2024 7th International Conference on Digital Medicine and Image Processing},
pages = {67–71},
numpages = {5},
keywords = {Attention Model, CNN-LSTM, Emotion Recognition, Speech data},
location = {
},
series = {DMIP '24}
}

@article{10.1109/TASLP.2022.3171967,
author = {Gao, Changfeng and Cheng, Gaofeng and Li, Ta and Zhang, Pengyuan and Yan, Yonghong},
title = {Self-Supervised Pre-Training for Attention-Based Encoder-Decoder ASR Model},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3171967},
doi = {10.1109/TASLP.2022.3171967},
abstract = {End-to-end (E2E) models, including the attention-based encoder-decoder (AED) models, have achieved promising performance on the automatic speech recognition (ASR) task. However, the supervised training process of the E2E model needs a large amount of speech-text paired data. In contrast, self-supervised pre-training can pre-train the model on the unlabeled data and then fine-tune it on the limited labeled data to realize better performance. Most of the previous self-supervised pre-training methods focus on learning hidden representations from speech but ignore how to utilize the unpaired text. As a result, previous works often pre-train an acoustic encoder and then fine-tune it as a classification based ASR model, such as Connectionist Temporal Classification (CTC) based model, rather than an AED model. In this paper, we propose a self-supervised pre-training method for the AED model (SP-AED). The SP-AED method contains acoustic pre-training for the encoder, linguistic pre-training for the decoder, and an adaptive combination fine-tuning for the whole system. We first design a linguistic pre-training method for decoder by utilizing the text-only data. The decoder will be pre-trained as a noise-condition language model to learn the prior distribution of the text. Then, we pre-train the AED encoder with the wav2vec2.0 method with some modifications. Finally, we combine the pre-trained encoder and decoder and fine-tune them on the limited labeled data. We design an adaptive combination method during fine-tuning by modifying the decoder’s input and output to prevent catastrophic forgetting. Experiments prove that compared with the random initialized models, the SP-AED pre-trained models can realize up to 17% relative improvement. And with similar model size or computational cost, we can get comparable results to other classification-based models on both English and Chinese corpus.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1763–1774},
numpages = {12}
}

@article{10.1109/TASLP.2022.3230453,
author = {Prakash, Anusha and Murthy, Hema A.},
title = {Exploring the Role of Language Families for Building Indic Speech Synthesisers},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3230453},
doi = {10.1109/TASLP.2022.3230453},
abstract = {Building end-to-end speech synthesisers for Indian languages is challenging, given the lack of adequate clean training data and multiple grapheme representations across languages. This work explores the importance of training multilingual and multi-speaker text-to-speech (TTS) systems based on &lt;italic&gt;language families&lt;/italic&gt;. The objective is to exploit the phonotactic properties of language families, where small amounts of accurately transcribed data across languages can be pooled together to train TTS systems. These systems can then be adapted to new languages belonging to the same family in extremely low-resource scenarios. TTS systems are trained separately for Indo-Aryan and Dravidian language families, and their performance is compared to that of a combined Indo-Aryan+Dravidian voice. We also investigate the amount of training data required for a language in a multilingual setting. Same-family and cross-family synthesis and adaptation to unseen languages are analysed. The analyses show that language family-wise training of Indic systems is the way forward for the Indian subcontinent, where a large number of languages are spoken.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {734–747},
numpages = {14}
}

@article{10.1109/TASLP.2021.3138707,
author = {Deng, Keqi and Cheng, Gaofeng and Yang, Runyan and Yan, Yonghong},
title = {Alleviating ASR Long-Tailed Problem by Decoupling the Learning of Representation and Classification},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138707},
doi = {10.1109/TASLP.2021.3138707},
abstract = {Recently, we have witnessed excellent improvement of end-to-end (E2E) automatic speech recognition (ASR). However, how to tackle the long-tailed data distribution problem while maintaining E2E ASR models' performance for high-frequency tokens is still challenging. To solve this challenge, we propose a novel decoupled ASR learning method for the sequence-to-sequence ASR architecture in this paper. Our method decouples the learning procedure of this model into two stages: representation learning and classification learning. In the representation learning stage, we use the encoder output of a pretrained language model as one of the ASR model’s learning targets, and propose threshold log cosine embedding loss (TLCE-loss) as the objective function. A frequency-mask cross-entropy loss (FMCE-loss) is also designed as an auxiliary loss. In the classification learning stage, we find that introducing a temperature into softmax function helps reduce the influence of negative samples on tail classes, thus mitigating the biased learning process for the classifier. Furthermore, we propose a weighted softmax (w-softmax) to adjust ASR posterior probabilities according to the token appearing frequency during inference. Additionally, we introduce tail word/character error rate (TWER / TCER) and head word/character error rate (HWER / HCER) that respectively evaluate the ASR accuracy for tail and head words/characters. Experimental results on the Switchboard and HKUST corpora show that our proposed method greatly outperforms the baseline, especially in TWER / TCER reduction. To the best of our knowledge, this is the first work to use a decoupled ASR learning method to alleviate the long-tailed problem in sequence-to-sequence ASR.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {340–354},
numpages = {15}
}

@inproceedings{10.5555/3523760.3523873,
author = {Fujii, Ayaka and Kristiina, Jokinen},
title = {Open Source System Integration Towards Natural Interaction with Robots},
year = {2022},
publisher = {IEEE Press},
abstract = {Speech is an intuitive way to interact with social robots: spoken language dialogues can help users to express their intents in a natural and flexible manner. In recent years, there has been remarkable progress in artificial intelligence related to spoken dialogue technology, including speech recognition and natural language processing. In this paper, we present the integration of the open source speech recognition, natural language processing, and dialogue management components into a robot software platform, and also report on a preliminary experiment of the integrated system using real users. Gesturing of the robot, which is also important in human-robot interaction, is combined with the spoken content of the robot utterance and included in the dialogue management component. As the dialogue domain we chose mealtime discussions on food and recipes, since spoken communication with a companion robot in such scenarios is considered natural and useful.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {768–772},
numpages = {5},
keywords = {open source, robot system, speech interaction, system integration},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@article{10.1109/TASLP.2023.3268571,
author = {Zhang, Guangyan and Qin, Ying and Zhang, Wenjie and Wu, Jialun and Li, Mei and Gai, Yutao and Jiang, Feijun and Lee, Tan},
title = {iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis Based on Disentanglement Between Prosody and Timbre},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3268571},
doi = {10.1109/TASLP.2023.3268571},
abstract = {Cross-speaker emotion transfer is a common approach to generating emotional speech when speech data with emotion labels from target speakers is not available. This paper presents a novel cross-speaker emotion transfer system named iEmoTTS. The system is composed of an emotion encoder, a prosody predictor, and a timbre encoder. The emotion encoder extracts the identity of emotion type and the respective emotion intensity from the mel-spectrogram of input speech. The emotion intensity is measured by the posterior probability that the input utterance carries that emotion. Unlike many other studies which focus on disentangling speaker and style factors of speech, the iEmoTTS is designed to achieve cross-speaker emotion transfer via disentanglement between prosody and timbre. Prosody is considered the primary carrier of emotion-related speech characteristics, and timbre accounts for the essential characteristics for speaker identification. Zero-shot emotion transfer, meaning that the speech of target speakers is not seen in model training, is also realized with iEmoTTS. Extensive experiments of subjective evaluation have been carried out. The results demonstrate the effectiveness of iEmoTTS compared with other recently proposed systems of cross-speaker emotion transfer. It is shown that iEmoTTS can produce speech with designated emotion types and controllable emotion intensity. With appropriate information bottleneck capacity, iEmoTTS is able to transfer emotional information to a new speaker effectively. Audio samples are publicly available.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1693–1705},
numpages = {13}
}

@inproceedings{10.1145/3507623.3507634,
author = {Po Shun Chen, Abbott and Wu Liu, Chai},
title = {Crafting ASR and Conversational Models for an Agriculture Chatbot},
year = {2022},
isbn = {9781450385930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507623.3507634},
doi = {10.1145/3507623.3507634},
abstract = {In recent years, artificial intelligence chatbots have attracted more and more attention. The stability and accuracy of automatic speech recognition (ASR) have been improved, making voice more critical in the transaction process and voice consultation of e-commerce purchases. ASR matches the learning model based on contextual cues. Eliminating unnecessary text plays an important role. We use the LSTM model and change it to contextualized custom text. In addition, to use our robot for testing, we propose a multi-task model that can jointly perform content re-scoring and has excellent responsiveness in the text of the input entity. Therefore, this article recommends using ASR technology to interpret and predict the answers to the LSTM model, allowing users to obtain expected results from actual measurements and understand which aspects are suitable for predicting specific emotions tested on this group. This article discusses chatbots and the design techniques used in platform translation, early and modern chatbots combined with ASR and artificial intelligence technology.},
booktitle = {Proceedings of the 2021 4th International Conference on Computational Intelligence and Intelligent Systems},
pages = {61–66},
numpages = {6},
keywords = {Neural network, Machine learning, Generative and Rulue based, ASR conversational Design techniques, AI Chatbots},
location = {Tokyo, Japan},
series = {CIIS '21}
}

@article{10.1109/TASLP.2021.3125142,
author = {Zhou, Yi and Tian, Xiaohai and Li, Haizhou},
title = {Language Agnostic Speaker Embedding for Cross-Lingual Personalized Speech Generation},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3125142},
doi = {10.1109/TASLP.2021.3125142},
abstract = {Cross-lingual personalized speech generation seeks to synthesize a target speaker’s voice from only a few training samples that are in a different language. One popular technique is to condition a speech synthesizer on a speaker embedding, that characterizes the target speaker. Unfortunately, such a speaker embedding is usually affected by the language being spoken, which compromises the speaker similarity in cross-lingual personalized speech generation. In this paper, we propose a novel speaker encoding mechanism that learns a language agnostic speaker embedding to characterize speaker individuality. Specifically, we adopt an encoder-decoder architecture to disentangle the language information from speaker embeddings via multi-task learning. We conduct experiments on both voice conversion and text-to-speech synthesis between English and Mandarin that involve cross-lingual speech generation. All objective and subjective evaluations consistently confirm that the proposed speaker embedding is language agnostic, thus improving cross-lingual personalized speech generation in terms of speaker similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {3427–3439},
numpages = {13}
}

@article{10.1109/TASLP.2021.3066047,
author = {Zhang, Mingyang and Zhou, Yi and Zhao, Li and Li, Haizhou},
title = {Transfer Learning From Speech Synthesis to Voice Conversion With Non-Parallel Training Data},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3066047},
doi = {10.1109/TASLP.2021.3066047},
abstract = {We present a novel voice conversion (VC) framework by learning from a text-to-speech (TTS) synthesis system, that is called TTS-VC transfer learning or TTL-VC for short. We first develop a multi-speaker speech synthesis system with sequence-to-sequence encoder-decoder architecture, where the encoder extracts the linguistic representations of input text, while the decoder, conditioned on target speaker embedding, takes the context vectors and the attention recurrent network cell output to generate target acoustic features. We take advantage of the fact that TTS system maps input text to speaker independent context vectors, thus re-purpose such a mapping to supervise the training of the latent representations of an encoder-decoder voice conversion system. In the voice conversion system, the encoder takes speech instead of text as the input, while the decoder is functionally similar to the TTS decoder. As we condition the decoder on a speaker embedding, the system can be trained on non-parallel data for any-to-any voice conversion. During voice conversion training, we present both text and speech to speech synthesis and voice conversion networks respectively. At run-time, the voice conversion network uses its own encoder-decoder architecture without the need of text input. Experiments show that the proposed TTL-VC system outperforms two competitive voice conversion baselines consistently, namely phonetic posteriorgram and AutoVC methods, in terms of speech quality, naturalness, and speaker similarity.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1290–1302},
numpages = {13}
}

@article{10.1109/TASLP.2023.3331813,
author = {Li, Jingbei and Li, Sipan and Chen, Ping and Zhang, Luwen and Meng, Yi and Wu, Zhiyong and Meng, Helen and Tian, Qiao and Wang, Yuping and Wang, Yuxuan},
title = {Joint Multiscale Cross-Lingual Speaking Style Transfer With Bidirectional Attention Mechanism for Automatic Dubbing},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3331813},
doi = {10.1109/TASLP.2023.3331813},
abstract = {Automatic dubbing, which generates a corresponding version of the input speech in another language, can be widely utilized in many real-world scenarios, such as video and game localization. In addition to synthesizing the translated scripts, automatic dubbing further transfers the speaking style in the original language to the dubbed speeches to give audiences the impression that the characters are speaking in their native tongue. However, state-of-the-art automatic dubbing systems only model the transfer on the duration and speaking rate, disregarding the other aspects of speaking style, such as emotion, intonation and emphasis, which are also crucial to fully understand the characters and speech. In this paper, we propose a joint multiscale cross-lingual speaking style transfer framework to simultaneously model the bidirectional speaking style transfer between two languages at both the global scale (i.e., utterance level) and local scale (i.e., word level). The global and local speaking styles in each language are extracted and utilized to predict the global and local speaking styles in the other language with an encoder-decoder framework for each direction and a shared bidirectional attention mechanism for both directions. A multiscale speaking style-enhanced FastSpeech 2 is then utilized to synthesize the desired speech with the predicted global and local speaking styles for each language. The experimental results demonstrate the effectiveness of our proposed framework, which outperforms a baseline with only duration transfer in objective and subjective evaluations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {517–528},
numpages = {12}
}

@inproceedings{10.5555/3721488.3721700,
author = {Liu, Simon L.},
title = {Personalized Caring: Integrating EEG/Visual Analysis with ChatGPT for MCI Assistance},
year = {2025},
publisher = {IEEE Press},
abstract = {This paper presents an AI-driven assistance scheme for individuals with mild cognitive impairment (MCI). The scheme combines electroencephalogram (EEG) sensing to monitor brain activity with a visual sensing module that captures environmental images. The analyzed EEG and visual data are fused to trigger personalized, context-aware conversations through cloud-based AI services such as ChatGPT. Text-to-speech and voice recognition technologies ensure smooth communication, while voice style transfer simulates familiar accents to improve comfort for MCI patients. The effectiveness of the proposal is validated by a prototype and simulations that incorporate EEG/visual data analysis, and personalized AI chatting.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1463–1467},
numpages = {5},
keywords = {ai assistance, eeg, mci, visual perception},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1109/TASLP.2021.3052688,
author = {Wu, Xixin and Cao, Yuewen and Lu, Hui and Liu, Songxiang and Kang, Shiyin and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
title = {Exemplar-Based Emotive Speech Synthesis},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3052688},
doi = {10.1109/TASLP.2021.3052688},
abstract = {Expressive text-to-speech (E-TTS) synthesis is important for enhancing user experience in communication with machines using the speech modality. However, one of the challenges in E-TTS is the lack of a precise description of emotions. Previous categorical specifications may be insufficient for describing complex emotions. The dimensional specifications face the difficulty of ambiguity in annotation. This work advocates a new approach of describing emotive speech acoustics using spoken exemplars. We investigate methods to extract emotion descriptions from the input exemplar of emotive speech. The measures are combined to form two descriptors, based on capsule network (CapNet) and residual error network (RENet). The first is designed to consider the spatial information in the input exemplary spectrogram, and the latter is to capture the contrastive information between emotive acoustic expressions. Two different approaches are applied for conversion from the variable-length feature sequence to fixed-size description vector: (1) dynamic routing groups similar capsules to the output description; and (2) recurrent neural network's hidden states store the temporal information for the description. The two descriptors are integrated to a state-of-the-art sequence-to-sequence architecture to obtain an end-to-end architecture that is optimized as a whole towards the same goal of generating correct emotive speech. Experimental results on a public audiobook dataset demonstrate that the two exemplar-based approaches achieve significant performance improvement over the baseline system in both emotion similarity and speech quality.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {874–886},
numpages = {13}
}

@article{10.1145/3618110,
author = {Nuthakki, Praveena and Katamaneni, Madhavi and J. N., Chandra Sekhar and Gubbala, Kumari and Domathoti, Bullarao and Maddumala, Venkata Rao and Jetti, Kumar Raja},
title = {Deep Learning based Multilingual Speech Synthesis using Multi Feature Fusion Methods},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3618110},
doi = {10.1145/3618110},
abstract = {The poor intelligibility and out-of-the-ordinary nature of the traditional concatenation speech synthesis technologies are two major problems. CNN's context deep learning approaches aren't robust enough for sensitive speech synthesis. Our suggested approach may satisfy such needs and modify the complexities of voice synthesis. The suggested model's minimal aperiodic distortion makes it an excellent candidate for a communication recognition model. Our suggested method is as close to human speech as possible, despite the fact that speech synthesis has a number of audible flaws. Additionally, there is excellent hard work to be done in incorporating sentiment analysis into text categorization using natural language processing. The intensity of feeling varies greatly from nation to country. To improve their voice synthesis outputs, models need to include more and more concealed layers &amp; nodes into the updated mixture density network. For our suggested algorithm to perform at its best, we need a more robust network foundation and optimization methods. We hope that after reading this article and trying out the example data provided, both experienced researchers and those just starting out would have a better grasp of the steps involved in creating a deep learning approach. Overcoming fitting issues with less data in training, the model is making progress. More space is needed to hold the input parameters in the DL-based method.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
keywords = {Speech to Text, Machine Learning, Deep Learning, Natural Language Processing}
}

@article{10.1109/TASLP.2021.3076369,
author = {Liu, Rui and Sisman, Berrak and Gao, Guanglai and Li, Haizhou},
title = {Expressive TTS Training With Frame and Style Reconstruction Loss},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3076369},
doi = {10.1109/TASLP.2021.3076369},
abstract = {We propose a novel training strategy for Tacotron-based text-to-speech (TTS) system that improves the speech styling at utterance level. One of the key challenges in prosody modeling is the lack of reference that makes explicit modeling difficult. The proposed technique doesn’t require prosody annotations from training data. It doesn’t attempt to model prosody explicitly either, but rather encodes the association between input text and its prosody styles using a Tacotron-based TTS framework. This study marks a departure from the style token paradigm where prosody is explicitly modeled by a bank of prosody embeddings. It adopts a combination of two objective functions: 1) frame level reconstruction loss, that is calculated between the synthesized and target spectral features; 2) utterance level style reconstruction loss, that is calculated between the deep style features of synthesized and target speech. The style reconstruction loss is formulated as a perceptual loss to ensure that utterance level speech style is taken into consideration during training. Experiments show that the proposed training strategy achieves remarkable performance and outperforms the state-of-the-art baseline in both naturalness and expressiveness. To our best knowledge, this is the first study to incorporate utterance level perceptual quality as a loss function into Tacotron training for improved expressiveness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1806–1818},
numpages = {13}
}

@article{10.1145/3701194,
author = {Carik, Buse and Ping, Kaike and Ding, Xiaohan and Rho, Eugenia H.},
title = {Exploring Large Language Models Through a Neurodivergent Lens: Use, Challenges, Community-Driven Workarounds, and Concerns},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {1},
url = {https://doi.org/10.1145/3701194},
doi = {10.1145/3701194},
abstract = {Despite the increasing use of large language models (LLMs) in everyday life among neurodivergent individuals, our knowledge of how they engage with and perceive LLMs remains limited. In this study, we investigate how neurodivergent individuals interact with LLMs by qualitatively analyzing topically related discussions from 61 neurodivergent communities on Reddit. Our findings reveal 20 specific LLM use cases across five core thematic areas of use among neurodivergent users: emotional well-being, mental health support, interpersonal communication, learning, and professional development and productivity. We also identified key challenges, including overly neurotypical LLM responses and the limitations of text-based interactions. In response to such challenges, some users actively seek advice by sharing input prompts and corresponding LLM responses. Others develop workarounds by experimenting and modifying prompts to be more neurodivergent-friendly. Despite these efforts, users have significant concerns around LLM use, including potential overreliance and fear of replacing human connections. Our analysis highlights the need to make LLMs more inclusive for neurodivergent users and implications around how LLM technologies can reinforce unintended consequences and behaviors.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {GROUP15},
numpages = {28},
keywords = {ADHD, artificial intelligence, autism, dyslexia, large language models, neurodiversity, reddit, social anxiety}
}

@article{10.1145/3568166,
author = {Iniesto, Francisco and Coughlan, Tim and Lister, Kate and Devine, Peter and Freear, Nick and Greenwood, Richard and Holmes, Wayne and Kenny, Ian and McLeod, Kevin and Tudor, Ruth},
title = {Creating ‘a Simple Conversation’: Designing a Conversational User Interface to Improve the Experience of Accessing Support for Study},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3568166},
doi = {10.1145/3568166},
abstract = {Administrative processes are ubiquitous in modern life and have been identified as a particular burden to those with accessibility needs. Students who have accessibility needs often have to understand guidance, fill in complex forms, and communicate with multiple parties to disclose disabilities and access appropriate support. Conversational user interfaces (CUIs) could allow us to reimagine such processes, yet there is currently limited understanding of how to design these to be accessible, or whether such an approach would be preferred. In the ADMINS (Assistants for the Disclosure and Management of Information about Needs and Support) project, we implemented a virtual assistant (VA) which is designed to enable students to disclose disabilities and to provide guidance and suggestions about appropriate support. ADMINS explores the potential of CUIs to reduce administrative burden and improve the experience of arranging support by replacing a static form with written or spoken dialogue. This article reports the results of two trials conducted during the project. A beta trial using an early version of the VA provided understanding of accessibility challenges and issues in user experience. The beta trial sample included 22 students who had already disclosed disabilities and 3 disability support advisors. After improvements to the design, a larger main trial was conducted with 134 students who disclosed their disabilities to the university using both the VA and the existing form-based process. The results show that the VA was preferred by most participants to completing the form (64.9% vs 23.9%). Qualitative and quantitative feedback from the trials also identified accessibility and user experience barriers for improving CUI design, and an understanding of benefits and preferences that can inform further development of accessible CUIs for this design space.},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {6},
numpages = {29},
keywords = {trial, user experience, accessibility, artificial intelligence, virtual assistants, chatbots, Conversational user interfaces}
}

@article{10.1109/TASLP.2021.3076867,
author = {Liu, Songxiang and Cao, Yuewen and Wang, Disong and Wu, Xixin and Liu, Xunying and Meng, Helen},
title = {Any-to-Many Voice Conversion With Location-Relative Sequence-to-Sequence Modeling},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3076867},
doi = {10.1109/TASLP.2021.3076867},
abstract = {This paper proposes an any-to-many location-relative, sequence-to-sequence (seq2seq), non-parallel voice conversion approach, which utilizes text supervision during training. In this approach, we combine a bottle-neck feature extractor (BNE) with a seq2seq synthesis module. During the training stage, an encoder-decoder-based hybrid connectionist-temporal-classification-attention (CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck layer. A BNE is obtained from the phoneme recognizer and is utilized to extract speaker-independent, dense and rich spoken content representations from spectral features. Then a multi-speaker location-relative attention based seq2seq synthesis model is trained to reconstruct spectral features from the bottle-neck features, conditioning on speaker representations for speaker identity control in the generated speech. To mitigate the difficulties of using seq2seq models to align long sequences, we down-sample the input spectral feature along the temporal dimension and equip the synthesis model with a discretized mixture of logistic (MoL) attention mechanism. Since the phoneme recognizer is trained with large speech recognition data corpus, the proposed approach can conduct any-to-many voice conversion. Objective and subjective evaluations show that the proposed any-to-many approach has superior voice conversion performance in terms of both naturalness and speaker similarity. Ablation studies are conducted to confirm the effectiveness of feature selection and model design strategies in the proposed approach. The proposed VC approach can readily be extended to support any-to-any VC (also known as one/few-shot VC), and achieve high performance according to objective and subjective evaluations.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1717–1728},
numpages = {12}
}

@article{10.1109/TASLP.2022.3202126,
author = {Xue, Liumeng and Soong, Frank K. and Zhang, Shaofei and Xie, Lei},
title = {ParaTTS: Learning Linguistic and Prosodic Cross-Sentence Information in Paragraph-Based TTS},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3202126},
doi = {10.1109/TASLP.2022.3202126},
abstract = {Recent advancements in neural end-to-end text-to-speech (TTS) models have shown high-quality, natural synthesized speech in a conventional sentence-based TTS. However, it is still challenging to reproduce similar high quality when a whole paragraph is considered in TTS, where a large amount of contextual information needs to be considered in building a paragraph-based TTS model. To alleviate the difficulty in training, we propose to model linguistic and prosodic information by considering cross-sentence, embedded structure in training. Three sub-modules, including linguistics-aware, prosody-aware and sentence-position networks, are trained together with a modified Tacotron2. Specifically, to learn the information embedded in a paragraph and the relations among the corresponding component sentences, we utilize linguistics-aware and prosody-aware networks. The information in a paragraph is captured by encoders and the inter-sentence information in a paragraph is learned with multi-head attention mechanisms. The relative sentence position in a paragraph is explicitly exploited by a sentence-position network. Trained on a storytelling audio-book corpus (4.08 hours), recorded by a female Mandarin Chinese speaker, the proposed TTS model demonstrates that it can produce rather natural and good-quality speech paragraph-wise. The cross-sentence contextual information, such as break and prosodic variations between consecutive sentences, can be better predicted and rendered than the sentence-based model. Tested on paragraph texts, of which the lengths are similar to, longer than, or much longer than the typical paragraph length of the training data, the TTS speech produced by the new model is consistently preferred over the sentence-based model in subjective tests and confirmed in objective measures.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {2854–2864},
numpages = {11}
}

@inproceedings{10.1145/3606038.3616157,
author = {Sarfati, Noah and Yerushalmy, Ido and Chertok, Michael and Keller, Yosi},
title = {Generating Factually Consistent Sport Highlights Narrations},
year = {2023},
isbn = {9798400702693},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606038.3616157},
doi = {10.1145/3606038.3616157},
abstract = {Sports highlights are an important form of media for fans worldwide, as they provide short videos that capture key moments from games, often accompanied by the original commentaries of the game's announcers. However, traditional forms of presenting sports highlights have limitations in conveying the complexity and nuance of the game. In recent years, the use of Large Language Models (LLMs) for natural language generation has emerged and is a promising approach for generating narratives that can provide a more compelling and accessible viewing experience. In this paper, we propose an end-to-end solution to enhance the experience of watching sports highlights by automatically generating factually consistent narrations using LLMs and crowd noise extraction. Our solution involves several steps, including extracting the source of information from the live broadcast using a transcription model, prompt engineering, and comparing out-of-the-box models for consistency evaluation. We also propose a new dataset annotated on generated narratives from 143 Premier League plays and fine-tune a Natural Language Inference (NLI) model on it, achieving 92% precision. Furthermore, we extract crowd noise from the original video to create a more immersive and realistic viewing experience for sports fans by adapting speech enhancement SOTA models on a brand new dataset created from 155 Ligue 1 games.},
booktitle = {Proceedings of the 6th International Workshop on Multimedia Content Analysis in Sports},
pages = {15–22},
numpages = {8},
keywords = {speech enhancing, prompt engineering, natural language inference, large language models (llms), hallucinations, factual consistency evaluation},
location = {Ottawa ON, Canada},
series = {MMSports '23}
}

@article{10.1109/TASLP.2023.3271151,
author = {Jiang, Wenbin and Yu, Kai},
title = {Speech Enhancement With Integration of Neural Homomorphic Synthesis and Spectral Masking},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3271151},
doi = {10.1109/TASLP.2023.3271151},
abstract = {Speech enhancement refers to suppressing the background noise to improve the perceptual quality and intelligibility of the observed noisy speech. Recently, speech enhancement algorithms based on deep neural networks (DNNs) have replaced traditional algorithms based on statistical signal processing and have become mainstream in the research field. However, most DNN-based speech enhancement methods commonly operate on the frequency domain and do not use the speech production model, which makes the models prone to under-suppress the noise or over-suppress the speech. To address the shortcoming, we propose a novel speech enhancement method integrating neural homomorphic synthesis and complex spectral masking. Specifically, we use a shared-encoder and multi-decoder neural network architecture. For the neural homomorphic synthesis branch, the speech signal is separated into excitation and vocal tract components through liftering the cepstrum, two DNN decoders are applied to estimate the target components independently, and the denoised speech is synthesized by the estimated minimum-phase signal and the noisy phase. For the spectral masking branch, another DNN decoder is adopted to estimate the complex mask of the target spectrum, and the denoised speech spectrum is obtained by masking the noisy spectrum. The two branches respectively estimate speech signals, and the final enhanced speech is obtained by merging the two branches of estimated speech. Experimental results on two popular datasets show that the proposed method achieves state-of-the-art level performance, with only 920 K model parameters.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1758–1770},
numpages = {13}
}

@inproceedings{10.1145/3708359.3712162,
author = {Cheng, Zhuoyi and Chen, Pei and Song, Wenzheng and Zhang, Hongbo and Li, Zhuoshu and Sun, Lingyun},
title = {An Exploratory Study on How AI Awareness Impacts Human-AI Design Collaboration},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712162},
doi = {10.1145/3708359.3712162},
abstract = {The collaborative design process is intrinsically complicated and dynamic, and researchers have long been exploring how to enhance efficiency in this process. As Artificial Intelligence technology evolves, it has been widely used as a design tool and exhibited the potential as a design collaborator. Nevertheless, problems concerning how designers should communicate with AI in collaborative design remain unsolved. To address this research gap, we referred to how designers communicate fluently in human-human design collaboration, and found awareness to be an important ability for facilitating communication by understanding their collaborators and current situation. However, previous research mainly studied and supported human awareness, the possible impact AI awareness would bring to the human-AI collaborative design process, and the way to realize AI awareness remain unknown. In this study, we explored how AI awareness will impact human-AI collaboration through a Wizard-of-Oz experiment. Both quantitative and qualitative results supported that enabling AI to have awareness can enhance the communication fluidity between human and AI, thus enhancing collaboration efficiency. We further discussed the results and concluded design implications for future human-AI collaborative design systems.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {157–172},
numpages = {16},
keywords = {Human-AI collaboration, Design collaboration, Generative AI, AI Awareness, Human-AI communication},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3681716.3681725,
author = {Luo, Sandra},
title = {Navigating the Diverse Challenges of Speech Emotion Recognition: A Deep Learning Perspective},
year = {2024},
isbn = {9798400718236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3681716.3681725},
doi = {10.1145/3681716.3681725},
abstract = {Speech Emotion Recognition (SER) aims to identify and interpret emotional states conveyed through speech signals. Originally a branch of Affective Computing research, SER has burgeoned into a commercially viable field, seamlessly integrating into various facets of daily life. Traditional SER systems are limited by manual feature extraction, noise, and voice quality interference. Deep learning has transformed this domain by enabling the development of neural network architectures that learn complex representations directly from raw speech data while simultaneously mitigating noise. Nonetheless, advancements in deep learning introduce their own set of challenges, comprising two main categories: 1) challenges stemming from the limitation of model architectures and complexities in using multiple modeling techniques; and 2) challenges related to trustworthiness, including scarcity of training data, privacy concerns, security issues, and considerations of AI fairness. This paper surveys the challenges in the current practices from a deep learning perspective. We argue that despite the diverse nature of these challenges, prevalent solutions predominantly reside within the realm of deep learning methodology. As deep learning's extensive utility in SER now spans various domains—encompassing data augmentation, overfitting mitigation, privacy preservation, debiasing, and the promotion of fairness in AI systems—and continues to evolve, navigating the challenges and embracing the advancements in deep learning remains pivotal in its ongoing integration and enhancement across diverse domains of application.},
booktitle = {Proceedings of the 27th International Academic Mindtrek Conference},
pages = {133–146},
numpages = {14},
keywords = {AI Fairness, Artificial Intelligence, Conversational AI, Deep Learning, Emotional AI, Machine Learning, Speech Emotion Recognition, Trustworthiness},
location = {Tampere, Finland},
series = {Mindtrek '24}
}

@article{10.1109/TASLP.2023.3272470,
author = {Guo, Haohan and Xie, Fenglong and Wu, Xixin and Soong, Frank K. and Meng, Helen},
title = {MSMC-TTS: Multi-Stage Multi-Codebook VQ-VAE Based Neural TTS},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3272470},
doi = {10.1109/TASLP.2023.3272470},
abstract = {This article aims to improve neural TTS with vector-quantized, compact speech representations. We propose a Vector-Quantized Variational AutoEncoder (VQ-VAE) based feature analyzer to encode acoustic features into sequences with different time resolutions, and quantize them with multiple VQ codebooks to form the Multi-Stage Multi-Codebook Representation (MSMCR). The TTS system, MSMC-TTS, is proposed to predict better speech via this representation. In prediction, the multi-stage predictor is trained to map the input text sequence to MSMCRs in stages, by minimizing Euclidean distance and “triplet loss”. In synthesis, the neural vocoder converts ground-truth or predicted MSMCRs into speech waveforms. The proposed system is trained with single-speaker TTS datasets and tested in various scenarios for comprehensive evaluation. In TTS evaluation, MSMC-TTS obtains MOS of 4.34 and 4.10 on English and Chinese datasets, which significantly outperforms VITS with scores of 3.78 and 3.90. Meanwhile, compared with Mel-Spectrograms, the domain discrepancy between prediction and ground truth is lower in MSMCRs with the higher Domain-classification Error Rate (DER). Furthermore, this system shows lower modeling complexity and data size requirements, preserving excellent performance even with fewer model parameters or training data. The noticeable improvement in analysis-synthesis and TTS from multiple codebooks and stages also validate them as vital components in seeking a more profitable speech representation and building high-performance neural TTS.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1811–1824},
numpages = {14}
}

@article{10.1109/TASLP.2022.3156797,
author = {Lee, Moa and Lee, Junmo and Chang, Joon-Hyuk},
title = {Non-Autoregressive Fully Parallel Deep Convolutional Neural Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3156797},
doi = {10.1109/TASLP.2022.3156797},
abstract = {Deep learning-based speech synthesis evolves by employing a sequence-to-sequence (seq2seq) structure with an attention mechanism. The seq2seq speech synthesis model consists of a pair of the encoder for delivering the linguistic features and the decoder for predicting the mel-spectrogram, and learns the alignment between text and speech through the attention mechanism. The decoder predicts the mel-spectrogram by an autoregressive flow that considers the current input and what they have learned from previous inputs. This is beneficial when processing the sequential data, as in speech synthesis. However, the recursive generation of speech typically requires extensive training time, which slows the speed of synthesis. To overcome these obstacles, we propose a non-autoregressive framework for fully parallel deep convolutional neural speech synthesis. Firstly, we design a new synthesis paradigm that integrates a time-varying metatemplate (TVMT), whose length is modeled with a separate conditional distribution, to prepare the decoder input. The decoding step converts the TVMT into spectral features, which eliminates the autoregressive flow. Secondly, we propose a structure that uses multiple decoders interconnected by up-down chains with an iterative attention mechanism. The decoder chains distribute the burden of decoding, progressively infusing the information obtained from the training target example into the chains to refine the predicted spectral features at each decoding step. For each decoder, the attention mechanism is repeatedly applied to produce the elaborated alignment between the linguistic features and the TVMT, which is gradually transformed into the spectral features. The proposed architecture substantially improves the synthesis speed, and the resulting speech quality is superior to that of a conventional autoregressive model.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {1150–1159},
numpages = {10}
}

@inproceedings{10.1145/3537674.3554742,
author = {Amezaga, Naroa and Hajek, Jeremy},
title = {Availability of Voice Deepfake Technology and its Impact for Good and Evil},
year = {2022},
isbn = {9781450393911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3537674.3554742},
doi = {10.1145/3537674.3554742},
abstract = {Artificial Intelligence and especially Machine Learning and Deep Learning techniques are increasingly populating today's technological and social landscape. These advancements have overwhelmingly contributed to the development of Speech Synthesis, also known as Text-To-Speech, where speech is artificially produced from text by means of computer technology [1]. But currently, there is a fundamental common drawback: unnatural, robotic and impersonal synthesized voices [2].So, what happens when the robotic computer voice no longer sounds like a computer, but sounds like you? That's where Voice Cloning technology comes into play, which allows one to generate an artificial speech that resembles a targeted human voice. This new practice offers many benefits, but with its development, the generation of fake voices and videos, known as “deepfakes”, has risen, causing a loss of trust and greater fear towards technology [3].In this way, the objective of this paper is to analyze the availability of voice deepfake technologies, its ease of construction and its impact for good and evil. We chose to focus on the educational field by implementing a “deepfake professor” via a survey of readily available voice deepfake technologies. The goal is then to demonstrate the potential capabilities for good and for evil that need to be considered with this technology, so we also conduct an analysis about the misuse, the current regulation, and the future of it.The results of the case study show that it is possible to clone someone's voice with a standard laptop, with no need of high-performance computing resources and based on just a few seconds of reference audio, which creates a superior user experience, but at the same time, reveals how easily can anyone have access to voice cloning. This expresses very well the importance of the new challenges opened by this potential technology and the need of safeguarding and regulation that future generations will have to deal with. There is no doubt that to understand the dynamics and impact of voice cloning and to reach more solid conclusions, future research is needed.},
booktitle = {Proceedings of the 23rd Annual Conference on Information Technology Education},
pages = {23–28},
numpages = {6},
location = {Chicago, IL, USA},
series = {SIGITE '22}
}

@inproceedings{10.1145/3675094.3678991,
author = {Li, Yunjia and Liu, Haiming and Wald, Mike},
title = {DeepVision: Heads-up Computing and AI in Education},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678991},
doi = {10.1145/3675094.3678991},
abstract = {Heads-up computing together with AI can enhance in-class learning experiences. In this position paper, we propose the development of a multimodal AI system called DeepVision that integrates Automatic Speech Recognition (ASR), Large Language Models (LLM), Large Vision Models (LVM), Information Retrieval (IR) and Inclusive User Experience Design (IUX) to convert real-time lectures into multiple knowledge representations. These will be visualized on heads-up communication devices such as Augmented Reality (AR) and Mixed Reality (MR) devices. The initiative is a collaboration between Habitat Learn Limited (HLL) and the University of Southampton, leveraging HLL's existing software and extensive data repository to address the challenges of traditional and digital learning environments, especially for students with disabilities or language differences.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {627–630},
numpages = {4},
keywords = {ai, ar, heads-up computing, inclusive user experience design, large language model, multimodal information access and retrieval},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3613904.3641912,
author = {Uhl, Jakob Carl and Gutierrez, Rodrigo and Regal, Georg and Schrom-Feiertag, Helmut and Schuster, Benjamin and Tscheligi, Manfred},
title = {Choosing the Right Reality: A Comparative Analysis of Tangibility in Immersive Trauma Simulations},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641912},
doi = {10.1145/3613904.3641912},
abstract = {In the field of medical first responder training, the choice of training modality is crucial for skill retention and real-world application. This study introduces the Green Manikin, an advanced Mixed Reality (MR) tool, conceptually combining the immersiveness of Virtual Reality (VR) with the tangibility of real-world training, and compares it against traditional real-world simulations and VR training. Our findings indicate that MR and real-world settings excel in Self and Social Presence, and in intention to use, offering heightened psychological presence suitable for complex training scenarios. Effort expectancy was highest in real-world environments, suggesting their ease of use for basic skill acquisition. This nuanced understanding allows for better tailoring of training modalities to specific educational objectives. Our research validates the utility of MR and offers a framework for selecting the most effective training environment for different learning outcomes in medical first responder training.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {187},
numpages = {17},
keywords = {chroma-key, comparative study, medical first responder, mixed reality, modality, training},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1109/TASLP.2022.3164181,
author = {Li, Tao and Wang, Xinsheng and Xie, Qicong and Wang, Zhichao and Xie, Lei},
title = {Cross-Speaker Emotion Disentangling and Transfer for End-to-End Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3164181},
doi = {10.1109/TASLP.2022.3164181},
abstract = {The cross-speaker emotion transfer task in text-to-speech (TTS) synthesis particularly aims to synthesize speech for a target speaker with the emotion transferred from reference speech recorded by another (source) speaker. During the emotion transfer process, the identity information of the source speaker could also affect the synthesized results, resulting in the issue of speaker leakage, i.e., synthetic speech may have the voice identity of the source speaker rather than the target speaker. This paper proposes a new method with the aim to synthesize controllable emotional expressive speech and meanwhile maintain the target speaker’s identity in the cross-speaker emotion TTS task. The proposed method is a Tacotron2-based framework with emotion embedding as the conditioning variable to provide emotion information. Two emotion disentangling modules are contained in our method to 1) get speaker-irrelevant and emotion-discriminative embedding, and 2) explicitly constrain the emotion and speaker identity of synthetic speech to be that as expected. Moreover, we present an intuitive method to control the emotion strength in the synthetic speech for the target speaker. Specifically, the learned emotion embedding is adjusted with a flexible scalar value, which allows controlling the emotion strength conveyed by the embedding. Extensive experiments have been conducted on a Mandarin disjoint corpus, and the results demonstrate that the proposed method is able to synthesize reasonable emotional speech for the target speaker. Compared to the state-of-the-art reference embedding learned methods, our method gets the best performance on the cross-speaker emotion transfer task, indicating that our method achieves the new state-of-the-art performance on learning the speaker-irrelevant emotion embedding. Furthermore, the strength ranking test and pitch trajectories plots demonstrate that the proposed method can effectively control the emotion strength, leading to prosody-diverse synthetic speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1448–1460},
numpages = {13}
}

@inproceedings{10.1145/3531028.3531052,
author = {Jim\'{e}nez, Carolina and Alvarez, Yevit and Meza-Zamata, Jessica and Grados-Espinoza, Herbert and Rubi\~{n}os, Santiago and Teran-Dianderas, Ciro and Grados, Juan},
title = {Design of an alert and monitoring system of vital signs with virtual assistance for older adults with visual disabilities},
year = {2022},
isbn = {9781450395847},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531028.3531052},
doi = {10.1145/3531028.3531052},
abstract = {Older adults with visual disabilities need constant monitoring of their vital signs, which due to studies already carried out it has been discovered that having this information can prevent disease and even death. [1] Monitoring vital signs is a complicated task, since measuring these biomedical parameters requires sophisticated, large, and expensive equipment. [2] So far, there is no such low-priced handheld device with virtual assistance on the market. That is why we want to cover this need with a vital sign monitoring system that is wearable to facilitate the movement of the patient. The monitoring system must be simple to use and low cost, so that it can be implemented in elderly patients with visual impairment who do not have the availability to always go to the hospital to monitor their vital signs. For this reason, we propose, through the intervention of technology, the use of a wearable system integrated with a virtual assistant with artificial intelligence that helps the patient in their daily activities such as when taking their medicines, making emergency calls, or automatically sending an alert message to your doctor or family member if your vital signs are outside the normal range.},
booktitle = {Proceedings of the 2022 5th International Conference on Electronics, Communications and Control Engineering},
pages = {149–154},
numpages = {6},
keywords = {older adult with visual impairment, Volunteer system, Vital signs, Virtual assistant with artificial intelligence},
location = {Higashi-ku, Japan},
series = {ICECC '22}
}

@inproceedings{10.1145/3652037.3652070,
author = {Frijns, Helena Anna and Vetter, Ralf and Hirschmanner, Matthias and Grabler, Reinhard and Vogel, Laura and Koeszegi, Sabine Theresia},
title = {Co-design of Robotic Technology with Care Home Residents and Care Workers},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3652070},
doi = {10.1145/3652037.3652070},
abstract = {This paper reports on a co-design workshop series with residents and care workers in care homes, in which they ideate robotic technologies by starting from basic functionalities. We investigate whether introducing technology components to older adults and care workers in care homes enables them to imagine usage scenarios for (robotic) technology in care. We compared outcomes and engagement across groups: one group of care workers, one group of residents, and a mixed group. Having an interactive prototype prompted most responses in the resident-only group. The inclusion of both care workers and residents highlighted the different and sometimes conflicting interests in an institutional care context, contrasting responsibility for safety and the experience of living in a care home.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {177–186},
numpages = {10},
keywords = {Co-Design, Older adults, Participatory Design, Robots for care},
location = {Crete, Greece},
series = {PETRA '24}
}

@inproceedings{10.1145/3613904.3642183,
author = {Jaber, Razan and Zhong, Sabrina and Kuoppam\"{a}ki, Sanna and Hosseini, Aida and Gessinger, Iona and Brumby, Duncan P and Cowan, Benjamin R. and Mcmillan, Donald},
title = {Cooking With Agents: Designing Context-aware Voice Interaction},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642183},
doi = {10.1145/3613904.3642183},
abstract = {Voice Agents (VAs) are touted as being able to help users in complex tasks such as cooking and interacting as a conversational partner to provide information and advice while the task is ongoing. Through conversation analysis of 7 cooking sessions with a commercial VA, we identify challenges caused by a lack of contextual awareness leading to irrelevant responses, misinterpretation of requests, and information overload. Informed by this, we evaluated 16 cooking sessions with a wizard-led context-aware VA. We observed more fluent interaction between humans and agents, including more complex requests, explicit grounding within utterances, and complex social responses. We discuss reasons for this, the potential for personalisation, and the division of labour in VA communication and proactivity. Then, we discuss the recent advances in generative models and the VAs interaction challenges. We propose limited context awareness in VAs as a step toward explainable, explorable conversational interfaces.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {551},
numpages = {13},
keywords = {conversation analysis, conversational user interfaces, cooking, voice interfaces},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1109/TASLP.2023.3313426,
author = {Xie, Chao and Toda, Tomoki},
title = {Noisy-to-Noisy Voice Conversion Under Variations of Noisy Condition},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3313426},
doi = {10.1109/TASLP.2023.3313426},
abstract = {Voiceconversion (VC) refers to the transformation of the speaker identity of a speech to the target one without altering the linguistic content. As recent VC techniques have made significant progress, implementing them in real-world scenarios is also considered, where speech data have some inevitable interferences, the most common of which are background sounds. On the other hand, background sounds are informative and need to be retained in some applications, such as VC in movies/videos. To address these issues, we have proposed a noisy-to-noisy (N2N) VC framework that does not rely on clean VC data and models the noisy speech directly by using noise as conditions. Previous experimental results have proven its effectiveness. In this article, we further improve its performance by introducing the pre-trained noise-conditioned VC model. Moreover, to further explore the impacts of introducing noise conditions, the performance in more realistic situations is evaluated in which the training set possesses speaker-dependent noisy conditions. The experimental results demonstrate the effectiveness of the pre-training strategy and the degradation of its performance under strict noisy conditions. We then proposed a noise augmentation method to overcome the limitation. Further experiments showed the effectiveness of the augmentation method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {3871–3882},
numpages = {12}
}

@inproceedings{10.1145/3590003.3590004,
author = {Song, Wenchao and He, Qiang and Chen, Guowei},
title = {Virtual Human Talking-Head Generation},
year = {2023},
isbn = {9781450399449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590003.3590004},
doi = {10.1145/3590003.3590004},
abstract = {Abstract: Virtual humans created by computers using deep learning technology are being used widely in a variety of fields, including personal assistance, intelligent customer service, and online education. Human-computer interaction systems integrate multi-modal technologies like speech recognition, dialogue systems, speech synthesis, and virtual digital human video synthesis as one of the applications of virtual humans. In this paper, we first design the framework for a human-computer interaction system based on a virtual human; next, we classify the talking head video synthesis model according to the generation of a virtual human's depth; finally, we conduct a systematic review of the technical developments in talking head video generation over the last five years, highlighting seminal work.},
booktitle = {Proceedings of the 2023 2nd Asia Conference on Algorithms, Computing and Machine Learning},
pages = {1–5},
numpages = {5},
keywords = {Virtual Human, Talking-head Generation, Multi-modal Human Computer Interaction},
location = {Shanghai, China},
series = {CACML '23}
}

@article{10.1109/TASLP.2022.3190717,
author = {Wang, Tao and Yi, Jiangyan and Fu, Ruibo and Tao, Jianhua and Wen, Zhengqi},
title = {CampNet: Context-Aware Mask Prediction for End-to-End Text-Based Speech Editing},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190717},
doi = {10.1109/TASLP.2022.3190717},
abstract = {The text-based speech editor allows the editing of speech through intuitive cutting, copying, and pasting operations to speed up the process of editing speech. However, the major drawback of current systems is that edited speech often sounds unnatural due to cut-copy-paste operation. In addition, it is not obvious how to synthesize records according to a new word not appearing in the transcript. This paper first proposes a novel end-to-end text-based speech editing method called context-aware mask prediction network (CampNet), which can solve unnatural prosody in the edited region and synthesize the speech corresponding to the unseen words in the transcript. Secondly, to cover various situations of text-based speech editing, we design three text-based operations based on CampNet: deletion, insertion, and replacement. Thirdly, to synthesize the speech corresponding to long text, a word-level autoregressive generation method is proposed. Fourthly, we propose a speaker adaptation method using only one sentence for CampNet and explore the ability of few-shot learning based on CampNet, which provides a new idea for speech forgery tasks. The subjective and objective experiments on VCTK and LibriTTS datasets&lt;xref ref-type="fn" rid="fn1"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/xref&gt;&lt;fn id="fn1"&gt;&lt;label&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/label&gt;&lt;p&gt;Examples of generated speech can be found at &lt;uri&gt;https://hairuo55.github.io/CampNet&lt;/uri&gt;&lt;/p&gt;&lt;/fn&gt; show that the speech editing results based on CampNet are better than TTS technology, manual editing, and VoCo method. We also conduct detailed ablation experiments to explore the effect of the CampNet structure on its performance. Finally, the experiment shows that speaker adaptation with only one sentence can further improve the naturalness of speech editing for one-shot learning.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2241–2254},
numpages = {14}
}

@inproceedings{10.1145/3700523.3700531,
author = {Li, Yaxuan and Jian, Yang},
title = {LOW-RESOURCE BURMESE SPEECH SYNTHESIS BASED ON VISUAL TEXT EMBEDDING AND DIFFUSION MODEL},
year = {2024},
isbn = {9798400717840},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700523.3700531},
doi = {10.1145/3700523.3700531},
abstract = {With the advancement of deep learning and speech synthesis technology, the synthetic speech of high-resource languages such as Chinese and English has approximated natural speech closely. Burmese belongs to the Tibeto-Burman family of the Sino-Tibetan language group. Due to limitations in the scale of corpus data and depth of research, there has remained a significant gap between synthesized speech and natural speech. Traditional text embedding methods have struggled to accurately capture the complex character characteristics of Burmese, resulting in mispronunciation and unnaturalness. Under the premise of low resources, this paper proposed three methods to improve the naturalness of Burmese speech synthesis: (1) The method of "visual text embedding + phoneme text embedding" was used in the front end of speech synthesis to integrate the features of the two embedding methods to better extract complex character features and context information. (2) In order to alleviate the problem of insufficient training data due to low resources, RoBERTa pre-trained model was used in the text encoder stage. (3) In order to effectively improve the inference speed of the diffusion model, OT-CFM training and inference methods were used in the decoder. Experimental results show that by using the "visual text embedding + phoneme text embedding" method and RoBERTa pre-training model, the subjective MOS and objective MCD evaluation results of Burmese speech synthesis reach 4.17 and 7.24 respectively, which are 0.65 and 1.72 higher than the baseline model respectively. Using OT-CFM method, the inference speed is improved by 54.18% under the premise that MOS is 4.21.},
booktitle = {Proceedings of the 2024 4th International Conference on Artificial Intelligence, Automation and Algorithms},
pages = {34–40},
numpages = {7},
keywords = {Burmese, RoBERTa model, Speech Synthesis, Visual text embedding},
location = {
},
series = {AI2A '24}
}

@article{10.1145/3700822,
author = {Choi, Minsoo and Cui, Dixuan and Volonte, Matias and Koilias, Alexandros and Kao, Dominic and Mousas, Christos},
title = {Toward Understanding the Effects of Intelligence of a Virtual Character during an Immersive Jigsaw Puzzle Co-Solving Task},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {1544-3558},
url = {https://doi.org/10.1145/3700822},
doi = {10.1145/3700822},
abstract = {In virtual reality, creating intelligent virtual characters has been a long-lasting endeavor. However, while researchers have investigated several aspects of a virtual character’s intelligence, little attention has been paid to the impact of the implemented intelligence levels assigned to a virtual character during human–virtual character collaboration. Thus, we conducted a within-group study ( (N=24) ) to explore how three different intelligence levels (low vs. medium vs. high) assigned to a virtual character can impact how study participants perceive that virtual character and interact with the task they are instructed to complete. Specifically, for our study, we developed a jigsaw puzzle game and instructed our participants to solve it with the help of a virtual character. During the jigsaw puzzle solving process, we collected application logs related to how the participants executed the task and observed the virtual environment. Moreover, after each condition, we asked the participants to respond using a questionnaire that examined their social presence, how they perceived the character’s intelligence and compared it with their own, and how they rated the virtual character’s realism. Our results indicated that the different intelligence levels assigned to the virtual characters impacted participants’ responses on several variables, including co-presence, perceived intelligence, intelligence comparison, and character interaction and behavior realism. Moreover, based on the collected logged data, we found that the intelligence levels assigned to our virtual character significantly impacted the performance of our participants. Our results could be valuable to the research community for creating more engaging experiences with intelligent virtual characters for collaborative tasks in immersive environments.},
journal = {ACM Trans. Appl. Percept.},
month = jan,
articleno = {9},
numpages = {28},
keywords = {virtual character, virtual reality, intelligence, jigsaw puzzle, puzzle co-solving, collaboration}
}

@article{10.1109/TASLP.2024.3426996,
author = {Wang, Syu-Siang and Chen, Jia-Yang and Bai, Bo-Ren and Fang, Shih-Hau and Tsao, Yu},
title = {Unsupervised Face-Masked Speech Enhancement Using Generative Adversarial Networks With Human-in-the-Loop Assessment Metrics},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3426996},
doi = {10.1109/TASLP.2024.3426996},
abstract = {The utilization of face masks is an essential healthcare measure, particularly during times of pandemics, yet it can present challenges in communication in our daily lives. To address this problem, we propose a novel approach known as the human-in-the-loop StarGAN (HL–StarGAN) face-masked speech enhancement method. HL–StarGAN comprises discriminator, classifier, metric assessment predictor, and generator that leverages an attention mechanism. The metric assessment predictor, referred to as MaskQSS, incorporates human participants in its development and serves as a “human-in-the-loop” module during the learning process of HL–StarGAN. The overall HL–StarGAN model was trained using an unsupervised learning strategy that simultaneously focuses on the reconstruction of the original clean speech and the optimization of human perception. To implement HL–StarGAN, we created a face-masked speech database named “FMVD,” which comprises recordings from 34 speakers in three distinct face-masked scenarios and a clean condition. We conducted subjective and objective tests on the proposed HL–StarGAN using this database. The outcomes of the test results are as follows: (1) MaskQSS successfully predicted the quality scores of face-masked voices, outperforming several existing speech assessment methods. (2) The integration of the MaskQSS predictor enhanced the ability of HL–StarGAN to transform face-masked voices into high-quality speech; this enhancement is evident in both objective and subjective tests, outperforming conventional StarGAN and CycleGAN-based systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {3826–3837},
numpages = {12}
}

@inproceedings{10.1145/3658644.3670285,
author = {Li, Xinfeng and Li, Kai and Zheng, Yifan and Yan, Chen and Ji, Xiaoyu and Xu, Wenyuan},
title = {SafeEar: Content Privacy-Preserving Audio Deepfake Detection},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670285},
doi = {10.1145/3658644.3670285},
abstract = {Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited remarkable performance in generating realistic and natural audio. However, their dark side, audio deepfake poses a significant threat to both society and individuals. Existing countermeasures largely focus on determining the genuineness of speech based on complete original audio recordings, which however often contain private content. This oversight may refrain deepfake detection from many applications, particularly in scenarios involving sensitive information like business secrets. In this paper, we propose SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within. Our key idea is to devise a neural audio codec into a novel decoupling model that well separates the semantic and acoustic information from audio samples, and only use the acoustic information (e.g., prosody and timbre) for deepfake detection. In this way, no semantic content will be exposed to the detector. To overcome the challenge of identifying diverse deepfake audio without semantic clues, we enhance our deepfake detector with real-world codec augmentation. Extensive experiments conducted on four benchmark datasets demonstrate SafeEar's effectiveness in detecting various deepfake techniques with an equal error rate (EER) down to 2.02%. Simultaneously, it shields five-language speech content from being deciphered by both machine and human auditory analysis, demonstrated by word error rates (WERs) all above 93.93% and our user study. Furthermore, our benchmark constructed for anti-deepfake and anti-content recovery evaluation helps provide a basis for future research in the realms of audio privacy preservation and deepfake detection.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {3585–3599},
numpages = {15},
keywords = {audio deepfake detection, privacy preservation},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@article{10.1109/TASLP.2024.3439996,
author = {Zhang, Mingyang and Zhou, Yi and Ren, Yi and Zhang, Chen and Yin, Xiang and Li, Haizhou},
title = {RefXVC: Cross-Lingual Voice Conversion With Enhanced Reference Leveraging},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3439996},
doi = {10.1109/TASLP.2024.3439996},
abstract = {This paper proposes RefXVC, a method for cross-lingual voice conversion (XVC) that leverages reference information to improve conversion performance. Previous XVC works generally take an average speaker embedding to condition the speaker identity, which does not account for the changing timbre of speech that occurs with different pronunciations. To address this, our method uses both global and local speaker embeddings to capture the timbre changes during speech conversion. Additionally, we observed a connection between timbre and pronunciation in different languages and utilized this by incorporating a timbre encoder and a pronunciation matching network into our model. Furthermore, we found that the variation in tones is not adequately reflected in a sentence, and therefore, we used multiple references to better capture the range of a speaker's voice. The proposed method outperformed existing systems in terms of both speech quality and speaker similarity, highlighting the effectiveness of leveraging reference information in cross-lingual voice conversion.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {4146–4156},
numpages = {11}
}

@inproceedings{10.1145/3641343.3641428,
author = {Wang, Tao and Li, Juanjuan and Wu, Weibo and Xu, Yi},
title = {Health Parameter Monitoring System Based on Human-Computer Speech Interaction},
year = {2024},
isbn = {9798400716775},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641343.3641428},
doi = {10.1145/3641343.3641428},
abstract = {Out of concern for the health of diverse populations, a health parameter monitoring system based on human-computer speech interaction is designed. The system uses STM32F407 as Microcontroller Unit and integrates various sensors, including heart rate and blood oxygen sensor MAX30102, temperature and humidity sensor DHT22, gas sensor SGP30 and NEO-6M module to continuously monitor wearer's health parameters, environmental air quality and geographic coordinates. All monitored parameters are dynamically displayed on both LCD screen and the mobile application interface. In emergency situations, the speech synthesis module SYN6288 is used to provide speech alarms, and L610 automatically dials the alarm. Additionally, combining LD3320 and SYN6288, the wearer can engage in speech interactions with the system at any time, which is more direct and convenient than key interaction, enhancing user's experience. Tests indicated that the measurement accuracy of the system was above 96%, the alarm accuracy was above 98%, and the response time from detecting of anomalies to the system alarm was within 2s. Compared with the same type of fall intelligent alarm system, the dialing alarm of this system is more effective and has a faster response time, satisfying practical usage standards.},
booktitle = {Proceedings of the 3rd International Conference on Electronic Information Technology and Smart Agriculture},
pages = {411–419},
numpages = {9},
keywords = {Emergency Alerting, Health Parameter Measurement, Human-computer Speech Interaction, L610 Communication Module, STM32F407},
location = {Sanya, China},
series = {ICEITSA '23}
}

@article{10.1109/TASLP.2021.3060810,
author = {Yang, Jichen and Wang, Hongji and Das, Rohan Kumar and Qian, Yanmin},
title = {Modified Magnitude-Phase Spectrum Information for Spoofing Detection},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3060810},
doi = {10.1109/TASLP.2021.3060810},
abstract = {Most of the existing feature representations for spoofing countermeasures consider information either from the magnitude or phase spectrum. We hypothesize that both magnitude and phase spectra can be beneficial for spoofing detection (SD) when collectively used to capture the signal artifacts. In this work, we propose a novel feature referred to as modified magnitude-phase spectrum (MMPS) to capture both magnitude and phase information from the speech signal. The constant-Q transform is used to obtain the magnitude and phase information in terms of MMPS, which can be denoted as CQT-MMPS. We then use this information for the proposal of a handcrafted feature, namely, constant-Q modified octave coefficients (CQMOC). To evaluate the proposed CQT-MMPS and CQMOC features, three classic anti-spoofing models are adopted, including the Gaussian mixture model (GMM), the light CNN (LCNN) and the ResNet. Additionally, since there is usually no prior knowledge about the spoofing kind in real-world applications, two novel methods referred to as three-class classifiers with maximum spoofing-score (TCMS) and multi-task learning (MTL) are designed for unknown-kind SD (UKSD). The experimental results on ASVspoof 2019 corpus show that CQMOC outperforms most of the commonly-used handcrafted features, and the CQT-based MMPS performs better than the magnitude-phase spectrum and the commonly-used log power spectrum. Further, the MMPS-based systems can achieve comparable or even better performance when compared with the state-of-the-art systems. We find that the newly-designed TCMS and MTL methods outperform the combination-based method for UKSD and meanwhile, generalize much better than the respective-kind-based methods in cross-spoofing-kind evaluation scenarios.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1065–1078},
numpages = {14}
}

@inproceedings{10.1145/3472749.3474795,
author = {Fan, Jiayue and Xu, Chenning and Yu, Chun and Shi, Yuanchun},
title = {Just Speak It: Minimize Cognitive Load for Eyes-Free Text Editing with a Smart Voice Assistant},
year = {2021},
isbn = {9781450386357},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472749.3474795},
doi = {10.1145/3472749.3474795},
abstract = {Entering text precisely by voice, users might encounter colloquial inserts, inappropriate wording, and recognition errors, which brings difficulties to voice editing. Users need to locate the errors and then correct them. In eyes-free scenarios, this select-modify mode brings a cognitive burden and a risk of error. This paper introduces neural networks and pre-trained models to understand users’ revision intention based on semantics, reducing the need for the information from users’ statements. We present two strategies. One is to remove the colloquial inserts automatically. The other is to allow users to edit by just speaking out the target words without having to say the context and the incorrect text. Accordingly, our approach can predict whether to insert or replace, the incorrect text to replace, and the position to insert. We implement these strategies in SmartEdit, an eyes-free voice input agent controlled with earphone buttons. The evaluation shows that our techniques reduce the cognitive load and decrease the average failure rate by 54.1% compared to descriptive command or re-speaking.},
booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
pages = {910–921},
numpages = {12},
keywords = {voice-based text editing, voice user interfaces, natural language processing., eyes-free, Text editing},
location = {Virtual Event, USA},
series = {UIST '21}
}

@inproceedings{10.1145/3595916.3626403,
author = {Ma, Qiaowei and Zhong, Jinghui and Yang, Yitao and Liu, Weiheng and Gao, Ying and Ng, Wing},
title = {A Lightweight and Efficient Model for Audio Anti-Spoofing},
year = {2024},
isbn = {9798400702051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3595916.3626403},
doi = {10.1145/3595916.3626403},
abstract = {With the rapid development of speech conversion and speech synthesis algorithms, automatic speaker verification (ASV) systems are vulnerable to spoofing attacks. In recent years, researchers had proposed anti-spoofing systems based on hand-crafted features. However, using hand-crafted features rather than raw waveform will lose implicit information for audio anti-spoofing. Inspired by the promising performance of ConvNeXt in classification tasks, we reference the network architecture design of ConvNeXt and propose a Lightweight and Efficient Model for Audio Anti-Spoofing (LEMAAS). With no preceding feature extraction process, we employ raw waveforms as direct inputs to our proposed model. By integrating with the channel attention module and using the focal loss function, the proposed model can focus on the most informative features representation of speech and the difficult samples that are hard to classify. Experimental results show that our proposed system could achieve an equal error rate of 0.64% and min-tDCF of 0.0187 for the ASVspoof 2019 LA evaluation dataset, which outperforms the state-of-the-art systems. Moreover, even when trained only on the ASVspoof 2019 LA dataset, the model still achieved equal error rates of 0.86% and 1.18% on the ASVspoof 2015 development dataset and evaluation dataset, respectively. This demonstrates that our model has achieved promising generalization performance during cross-dataset testing.},
booktitle = {Proceedings of the 5th ACM International Conference on Multimedia in Asia},
articleno = {31},
numpages = {7},
keywords = {audio anti-spoofing, automatic speaker verification, cross-dataset testing, hand-crafted features},
location = {Tainan, Taiwan},
series = {MMAsia '23}
}

@inproceedings{10.1145/3556384.3556398,
author = {Hossain, Prommy Sultana and Chakrabarty, Amitabha and Jalil Piran, Md. and Suh, Doug Young},
title = {Stacked Convolutional Autoencoder with Multi-label Extreme Learning Machine (SCAE-MLELM) for Bangla Regional Language Classification},
year = {2022},
isbn = {9781450396912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556384.3556398},
doi = {10.1145/3556384.3556398},
abstract = {Several studies emphasized the importance of detecting the dialect spoken by the user, to improve customer-agent relationships in call centers operated by telecomunication companies. and to create advanced speech recognizing Artificial Intelligence devices using machine learning (ML) techniques. However, the extensive Bangla speech recognition research performed in the past did not address this issue. Therefore, in this paper, we propose a model that uses deep learning (DL) techniques, as a subset of ML, to classify the regional language in the audio signal of the Bangla speech. We build a model based on Stacked Convolutional Autoencoders (SCAE) and Multi-label Extreme Learning Machines (MLELM). SCAE extracts the spatial and temporal features from the input data and produces a detailed feature vector. In order to provide a soft classification score and hard labels, the information is passed onto a series of MLELM networks. For Bangla speech dataset created during this research, the proposed method produced an accuracy score of 95%. Furthermore, the proposed system performs remarkably well in classifying the age and gender of the speaker in Bangla.},
booktitle = {Proceedings of the 2022 5th International Conference on Signal Processing and Machine Learning},
pages = {88–93},
numpages = {6},
location = {Dalian, China},
series = {SPML '22}
}

@article{10.1145/3473499,
author = {Xu, Fan and Dan, Yangjie and Yan, Keyu and Ma, Yong and Wang, Mingwen},
title = {Low-Resource Language Discrimination toward Chinese Dialects with Transfer Learning and Data Augmentation},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3473499},
doi = {10.1145/3473499},
abstract = {Chinese dialects discrimination is a challenging natural language processing task due to scarce annotation resource. In this article, we develop a novel Chinese dialects discrimination framework with transfer learning and data augmentation (CDDTLDA) in order to overcome the shortage of resources. To be more specific, we first use a relatively larger Chinese dialects corpus to train a source-side automatic speech recognition (ASR) model. Then, we adopt a simple but effective data augmentation method (i.e., speed, pitch, and noise disturbance) to augment the target-side low-resource Chinese dialects, and fine-tune another target ASR model based on the previous source-side ASR model. Meanwhile, the potential common semantic features between source-side and target-side ASR models can be captured by using self-attention mechanism. Finally, we extract the hidden semantic representation in the target ASR model to conduct Chinese dialects discrimination. Our extensive experimental results demonstrate that our model significantly outperforms state-of-the-art methods on two benchmark Chinese dialects corpora.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {27},
numpages = {21},
keywords = {data augmentation, transfer learning, dialects discrimination, chinese dialects, Low-resource}
}

@inproceedings{10.1145/3411764.3445171,
author = {Zaheer, Nimra and Ahmad, Obaid Ullah and Ahmed, Ammar and Khan, Muhammad Shehryar and Shabbir, Mudassir},
title = {SEMOUR: A Scripted Emotional Speech Repository for Urdu},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445171},
doi = {10.1145/3411764.3445171},
abstract = {Designing reliable Speech Emotion Recognition systems is a complex task that inevitably requires sufficient data for training purposes. Such extensive datasets are currently available in only a few languages, including English, German, and Italian. In this paper, we present SEMOUR, the first scripted database of emotion-tagged speech in the Urdu language, to design an Urdu Speech Recognition System. Our gender-balanced dataset contains 15,040 unique instances recorded by eight professional actors eliciting a syntactically complex script. The dataset is phonetically balanced, and reliably exhibits a varied set of emotions as marked by the high agreement scores among human raters in experiments. We also provide various baseline speech emotion prediction scores on the database, which could be used for various applications like personalized robot assistants, diagnosis of psychological disorders, and getting feedback from a low-tech-enabled population, etc. On a random test sample, our model correctly predicts an emotion with a state-of-the-art 92% accuracy.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {12},
keywords = {speech emotion recognition, speech dataset, machine learning, human annotation, digital recording, deep learning, Urdu language, Emotional speech},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3708359.3712151,
author = {Maniar, Natasha and Chan, Samantha W.T. and Zulfikar, Wazeer and Ren, Scott and Xu, Christine and Maes, Pattie},
title = {MemPal: Leveraging Multimodal AI and LLMs for Voice-Activated Object Retrieval in Homes of Older Adults},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712151},
doi = {10.1145/3708359.3712151},
abstract = {Older adults have increasing difficulty with retrospective memory, hindering their abilities to perform daily activities and posing stress on caregivers to ensure their wellbeing. Recent developments in Artificial Intelligence (AI) and large context-aware multimodal models offer an opportunity to create memory support systems that assist older adults with common issues like object finding. This paper discusses the development of an AI-based, wearable memory assistant, MemPal, that helps older adults with a common problem, finding lost objects at home, and presents results from tests of the system in older adults’ own homes. Using visual context from a wearable camera, the multimodal LLM system creates a real-time automated text diary of the person’s activities for memory support purposes, offering object retrieval assistance using a voice-based interface. The system is designed to support additional use cases like context-based proactive safety reminders and recall of past actions. We report on a quantitative and qualitative study with N=15 older adults within their own homes that showed improved performance of object finding with audio-based assistance compared to no aid and positive overall user perceptions on the designed system. We discuss further applications of MemPal’s design as a multi-purpose memory aid and future design guidelines to adapt memory assistants to older adults’ unique needs.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {993–1015},
numpages = {23},
keywords = {memory assistant, large language models, large visual language models, voice interfaces, context-aware agent, multimodal systems, wearables, older adults},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3611450.3611476,
author = {Qiao, Zhi and Yang, Jian and Wang, Zhan},
title = {Multi-Feature Cross-Lingual Transfer Learning Approach for Low-Resource Vietnamese Speech Synthesis},
year = {2023},
isbn = {9798400707605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611450.3611476},
doi = {10.1145/3611450.3611476},
abstract = {Abstract—Based on neural network end-to-end speech synthesis systems, high-quality speech can be synthesized when there is sufficient training data. However, it is difficult for languages with small datasets to synthesize speech with high quality and naturalness. Vietnamese is a tonal language, belonging to the Vietic branch of the Austroasiatic language family, which uses a spelling system. To improve the quality and naturalness of speech synthesis with limited dataset resources, we first use transfer learning to improve the acoustic model of Vietnamese by leveraging the similarities in pronunciation and grammar between Mandarin Chinese and Vietnamese. Secondly, based on the prosodic characteristics of Vietnamese, we use a "speech-text" alignment tool to extract prosodic boundary information and supplement it to the training text sequence. Using FastSpeech2 as the experimental baseline system, we designed and added a prosody embedding layer. The experimental results show that the model trained with prosodic markers has better prosody expression compared to the original text. Furthermore, compared to the baseline system, adding the prosody embedding layer improved the prosody expression of the synthesized speech and eliminated the need for marked text during speech synthesis.},
booktitle = {Proceedings of the 2023 3rd International Conference on Artificial Intelligence, Automation and Algorithms},
pages = {175–180},
numpages = {6},
keywords = {Transfer Learning, Speech Synthesis, Prosody Embedding, Prosodic, Keywords—Vietnamese},
location = {Beijing, China},
series = {AI2A '23}
}

@article{10.1109/TASLP.2021.3060813,
author = {Zhao, Guanlong and Ding, Shaojin and Gutierrez-Osuna, Ricardo},
title = {Converting Foreign Accent Speech Without a Reference},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3060813},
doi = {10.1109/TASLP.2021.3060813},
abstract = {Foreign accent conversion (FAC) is the problem of generating a synthetic voice that has the voice identity of a second-language (L2) learner and the pronunciation patterns of a native (L1) speaker. This synthetic voice has been referred to as a “golden-speaker” in the pronunciation-training literature. FAC is generally achieved by building a voice-conversion model that maps utterances from a source (L1) speaker onto the target (L2) speaker. As such, FAC requires that a reference utterance from the L1 speaker be available at synthesis time. This greatly restricts the application scope of the FAC system. In this work, we propose a “reference-free” FAC system that eliminates the need for reference L1 utterances at synthesis time, and transforms L2 utterances directly. The system is trained in two steps. First, a conventional FAC procedure is used to create a golden-speaker using utterances from a reference L1 speaker (which are then discarded) and the L2 speaker. Second, a pronunciation-correction model is trained to convert L2 utterances to match the golden-speaker utterances obtained in the first step. At synthesis time, the pronunciation-correction model directly transforms a novel L2 utterance into its golden-speaker counterpart. Our results show that the system reduces foreign accents in novel L2 utterances, achieving a 20.5% relative reduction in word-error-rate of an American English automatic speech recognizer and a 19% reduction in perceptual ratings of foreign accentedness obtained through listening tests. Over 73% of the listeners also rated golden-speaker utterances as having the same voice identity as the original L2 utterances.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2367–2381},
numpages = {15}
}

@article{10.1109/TASLP.2022.3171971,
author = {Chen, Bo and Du, Chenpeng and Yu, Kai},
title = {Neural Fusion for Voice Cloning},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3171971},
doi = {10.1109/TASLP.2022.3171971},
abstract = {Voice cloning is a technique to build text-to-speech applications for individuals. When only very limited training data is available, it is challenging to preserve both high speech quality and high speaker similarity. We propose a neural fusion architecture to incorporate a unit concatenation method into a parametric text-to-speech model to address this issue. Unlike the hybrid unit concatenation system, the proposed fusion architecture is still an end-to-end neural network model. It consists of a text encoder, an acoustic decoder, and a phoneme-level reference encoder. The reference encoder extracts phoneme-level embeddings corresponding to the cloning audio segments, and the text encoder infers phoneme-level embeddings from the input text. One of the two embeddings is then selected and sent to the decoder. We use auto-regressive distribution modeling and decoder refinement after the selection stage to overcome the concatenation discontinuity problem. Experimental results show that the neural fusion system significantly improves the speaker similarity using the selected units with the highest probability. The speech naturalness remains similar to the directly decoded systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1993–2001},
numpages = {9}
}

@inproceedings{10.1145/3544548.3580706,
author = {Rekimoto, Jun},
title = {WESPER: Zero-shot and Realtime Whisper to Normal Voice Conversion for Whisper-based Speech Interactions},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580706},
doi = {10.1145/3544548.3580706},
abstract = {Recognizing whispered speech and converting it to normal speech creates many possibilities for speech interaction. Because the sound pressure of whispered speech is significantly lower than that of normal speech, it can be used as a semi-silent speech interaction in public places without being audible to others. Converting whispers to normal speech also improves the speech quality for people with speech or hearing impairments. However, conventional speech conversion techniques do not provide sufficient conversion quality or require speaker-dependent datasets consisting of pairs of whispered and normal speech utterances. To address these problems, we propose WESPER, a zero-shot, real-time whisper-to-normal speech conversion mechanism based on self-supervised learning. WESPER consists of a speech-to-unit (STU) encoder, which generates hidden speech units common to both whispered and normal speech, and a unit-to-speech (UTS) decoder, which reconstructs speech from the encoded speech units. Unlike the existing methods, this conversion is user-independent and does not require a paired dataset for whispered and normal speech. The UTS decoder can reconstruct speech in any target speaker’s voice from speech units, and it requires only an unlabeled target speaker’s speech data. We confirmed that the quality of the speech converted from a whisper was improved while preserving its natural prosody. Additionally, we confirmed the effectiveness of the proposed approach to perform speech reconstruction for people with speech or hearing disabilities.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {700},
numpages = {12},
keywords = {artificial intelligence, neural networks, self-supervised learning, silent speech, speech interaction, whispered voice, whispered voice conversion},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{10.1145/3708282.3708290,
author = {Ma, Qiancheng and Zhang, Liyang and Zhu, Jinyan},
title = {Exploration and Functional Optimization of a Multi-functional Guidance Cap},
year = {2025},
isbn = {9798400709869},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708282.3708290},
doi = {10.1145/3708282.3708290},
abstract = {With continuous innovations in real-time positioning and navigation systems, their applications are expanding. Through field surveys and a review of international literature, it has been found that there exists a significant research gap in the field of multi-functional guidance caps. The guidance cap is a modern technology-based equipment designed to assist the navigation of visually impaired individuals, aiming to address navigation difficulties and safety hazards encountered in daily life. This study explores the operational principles of a guidance cap utilizing GPS positioning and OpenCV image recognition. It provides a detailed overview of the main features and functionalities of the guidance cap, including GPS positioning, map navigation, directional indicators, obstacle detection, and adaptability across multiple scenarios. The research introduces the system design of the guidance cap and outlines the programming of relevant code, while also identifying challenges and potential directions for improvement.},
booktitle = {Proceedings of the 2024 International Conference on Artificial Intelligence of Things and Computing},
pages = {37–41},
numpages = {5},
keywords = {GPS positioning, OpenCV, guidance cap, map navigation, obstacle detection, ultrasonic module},
location = {
},
series = {AITC '24}
}

@inproceedings{10.1145/3660043.3660127,
author = {Zeng, Chunhong},
title = {Research on the application of TTS technology in intelligent English automatic translation system},
year = {2024},
isbn = {9798400716157},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660043.3660127},
doi = {10.1145/3660043.3660127},
abstract = {This paper introduces a design method of intelligent English translation system based on TTS technology. On this basis, according to the actual needs of English translation, TTS technology (text analysis, prosody control and speech synthesis) is used to design the system software, which mainly includes continuous speech automatic segmentation and annotation, prosody control, speech synthesis and speech library clipping. Through the above hardware design and software design, an intelligent automatic English translation system has been put into use. The experimental results show that compared with the control system, the deviation of phonological control parameters obtained by the design system is smaller and the phonological naturalness factor is larger, which fully indicates that the design system is more accurate in English translation.},
booktitle = {Proceedings of the 2023 International Conference on Information Education and Artificial Intelligence},
pages = {472–475},
numpages = {4},
location = {Xiamen, China},
series = {ICIEAI '23}
}

@article{10.1109/TASLP.2022.3190715,
author = {Luo, Zhaojie and Lin, Shoufeng and Liu, Rui and Baba, Jun and Yoshikawa, Yuichiro and Ishiguro, Hiroshi},
title = {Decoupling Speaker-Independent Emotions for Voice Conversion via Source-Filter Networks},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190715},
doi = {10.1109/TASLP.2022.3190715},
abstract = {Emotional voice conversion (VC) aims to convert a neutral voice to an emotional one while retaining the linguistic information and speaker identity. We note that the decoupling of emotional features from other speech information (such as content, speaker identity, etc.) is the key to achieving promising performance. Some recent attempts of speech representation decoupling on the neutral speech cannot work well on the emotional speech, due to the more complex entanglement of acoustic properties in the latter. To address this problem, here we propose a novel Source-Filter-based Emotional VC model (SFEVC) to achieve proper filtering of speaker-independent emotion cues from both the timbre and pitch features. Our SFEVC model consists of multi-channel encoders, emotion separate encoders, pre-trained speaker-dependent encoders, and the corresponding decoder. Note that all encoder modules adopt a designed information bottleneck auto-encoder. Additionally, to further improve the conversion quality for various emotions, a novel training strategy based on the 2D Valence-Arousal (VA) space is proposed. Experimental results show that the proposed SFEVC along with a VA training strategy outperforms all baselines and achieves the state-of-the-art performance in speaker-independent emotional VC with nonparallel data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {11–24},
numpages = {14}
}

@inproceedings{10.1145/3581783.3612173,
author = {Liu, Dong and Mao, Qirong and Gao, Lijian and Ren, Qinghua and Chen, Zhenghan and Dong, Ming},
title = {TE-KWS: Text-Informed Speech Enhancement for Noise-Robust Keyword Spotting},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3612173},
doi = {10.1145/3581783.3612173},
abstract = {Keyword spotting (KWS) presents a formidable challenge, particularly in high-noise environments. Traditional denoising algorithms that rely solely on speech have difficulty recovering speech that has been severely corrupted by noise. In this investigation, we develop an adaptive text-informed denoising model to bolster reliable keyword identification in the presence of considerable noise degradation. The whole proposed TE-KWS incorporates a tripartite branch structure, where the speech branch (SB) takes noisy speech as input which provides the raw speech information, the alignment branch (AB) accommodates aligned text input which facilitates accurate restoration of the corresponding speech when text with alignment is preserved, and the text branch (TB) handles unaligned text which prompts the model to autonomously learn the alignment between speech and text. To make the proposed denoising model more beneficial for KWS, following the training of the whole model,the alignment branch (AB) is frozen, and the model is fine-tuned by leveraging its speech restoration and forced alignment capabilities. Subsequently, the input for the text branch (TB) is supplanted with designated keywords, and a heavier denoising penalty is applied on the keywords period, thereby explicitly intensifying the speech restoration ability of the model for keywords. Finally, the Combined Adversarial Domain Adaptation (CADA) is implemented to enhance the robustness of KWS with regard to data pre-and post-speech enhancement (SE). Experimental results indicate that our approach not only markedly ameliorates highly corrupted speech, achieving SOTA performance for marginally corrupted speech, but also bolsters the efficacy and generalizability of prevailing mainstream KWS models.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {601–610},
numpages = {10},
keywords = {keyword spotting, speech enhancement, text-informed speech enhancement},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@article{10.1109/TASLP.2021.3120586,
author = {Wu, Xixin and Cao, Yuewen and Lu, Hui and Liu, Songxiang and Wang, Disong and Wu, Zhiyong and Liu, Xunying and Meng, Helen},
title = {Speech Emotion Recognition Using Sequential Capsule Networks},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3120586},
doi = {10.1109/TASLP.2021.3120586},
abstract = {Speech emotion recognition (SER) is an indispensable part of fluid human-machine interaction and attracts lots of research attentions. Recent work on SER has successfully applied convolutional neural networks (CNNs) to learn feature representations from speech spectrograms. However, the fundamental problem of CNNs is that the spatial information in spectrograms is lost, which includes positional and relationship information of low-level features, such as pitch and formant frequencies. We propose a novel architecture of sequential capsule networks (CapNets) by leveraging the advantange of CapNets that spatial information can be preserved in capsules and passed to upper capsule layers via dynamic routing. Also, the dynamic routing algorithm provides an effective alternative to pooling or storing recurrent hidden states for obtaining utterance-level features from the sequential capsule outputs. To further improve the model's ability to capture contextual information, we introduce a recurrent connection to the sequential structure. The experimental comparison of the proposed systems and previously published systems using CNNs and recurrent neural networks (RNNs) based on the IEMOCAP corpus demonstrates the effectiveness of the proposed sequential CapNets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3280–3291},
numpages = {12}
}

@inproceedings{10.5555/3523760.3523842,
author = {Bermuth, Daniel and Poeppel, Alexander and Reif, Wolfgang},
title = {Jaco: An Offline Running Privacy-aware Voice Assistant},
year = {2022},
publisher = {IEEE Press},
abstract = {With the recent advance in speech technology, smart voice assistants have been improved and are now used by many people. But often these assistants are running online as a cloud service and are not always known for a good protection of users' privacy. This paper presents the architecture of a novel voice assistant, called Jaco, with the following features: (a) It can run completely offline, even on low resource devices like a RaspberryPi. (b) Through a skill concept it can be easily extended. (c) The architectural focus is on protecting users' privacy, but without restricting capabilities for developers. (d) It supports multiple languages. (e) It is competitive with other voice assistant solutions. In this respect the assistant combines and extends the advantages of other approaches.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {618–622},
numpages = {5},
keywords = {human-computer interaction, multilingual smart voice assistant, offline voice assistant},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@article{10.1109/TASLP.2021.3059114,
author = {Saito, Yuki and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
title = {Perceptual-Similarity-Aware Deep Speaker Representation Learning for Multi-Speaker Generative Modeling},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3059114},
doi = {10.1109/TASLP.2021.3059114},
abstract = {We propose novel deep speaker representation learning that considers perceptual similarity among speakers for multi-speaker generative modeling. Following its success in accurate discriminative modeling of speaker individuality, knowledge of deep speaker representation learning (i.e., speaker representation learning using deep neural networks) has been introduced to multi-speaker generative modeling. However, the conventional discriminative algorithm does not necessarily learn speaker embeddings suitable for such generative modeling, which may result in lower quality and less controllability of synthetic speech. We propose three representation learning algorithms that utilize a perceptual speaker similarity matrix obtained by large-scale perceptual scoring of speaker-pair similarity. The algorithms train a speaker encoder to learn speaker embeddings with three different representations of the matrix: a set of vectors, the Gram matrix, and a graph. Furthermore, we propose an active learning algorithm that iterates the perceptual scoring and speaker encoder training. To obtain accurate embeddings while reducing costs of scoring and training, the algorithm selects unscored speaker-pairs to be scored next on the basis of the sequentially-trained speaker encoder's similarity prediction results. Experimental evaluation results show that 1) the proposed representation learning algorithms learn speaker embeddings strongly correlated with perceptual speaker-pair similarity, 2) the embeddings improve synthetic speech quality in speech autoencoding tasks better than conventional d-vectors learned by discriminative modeling, 3) the proposed active learning algorithm achieves higher synthetic speech quality while reducing costs of scoring and training, and 4) among the proposed similarity {vector, matrix, graph} embedding algorithms, the first achieves the best speaker similarity for synthetic speech and the third gives the most improvement in the synthetic speech naturalness.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1033–1048},
numpages = {16}
}

@inproceedings{10.1145/3675812.3675842,
author = {Wang, XianChuang and Shu, LiLi and Hong, Xin and Fang, HaiGuang},
title = {A Research on Multimodal Interaction Data Architecture and Application Scenarios of Intelligent Educational Robots Based on the Theory of Multiple Intelligences},
year = {2024},
isbn = {9798400716805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675812.3675842},
doi = {10.1145/3675812.3675842},
abstract = {With in-depth research on artificial intelligence in education, the design of multimodal interaction data architectures for educational robots and their application scenarios has become a new areas of research growth.The current interaction modes of intelligent educational robots are relatively limited, and the design of application scenarios is somewhat lacking.In response to the above issues, the problems encountered by intelligent educational robots in actual application scenarios have been summarized through literature research and field investigation methods.Moreover, the multimodal interaction data architecture and its application scenarios for intelligent educational robots were designed in conjunction with the usage needs of teachers in schools within a certain smart education experimental zone.At the same time, by integrating increasingly mature technologies include knowledge graphs and deep learning, an analysis was conducted on how to improve the functions of intelligent educational robots to better serve classroom teaching in the future.},
booktitle = {Proceedings of the 2024 9th International Conference on Distance Education and Learning},
pages = {16–22},
numpages = {7},
location = {Guangzhou, China},
series = {ICDEL '24}
}

@inproceedings{10.1145/3552463.3557019,
author = {Gautam, Sushant and Midoglu, Cise and Shafiee Sabet, Saeed and Kshatri, Dinesh Baniya and Halvorsen, P\r{a}l},
title = {Soccer Game Summarization using Audio Commentary, Metadata, and Captions},
year = {2022},
isbn = {9781450394932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552463.3557019},
doi = {10.1145/3552463.3557019},
abstract = {Soccer is one of the most popular sports globally, and the amount of soccer-related content worldwide, including video footage, audio commentary, team/player statistics, scores, and rankings, is enormous and rapidly growing. Consequently, the generation of multimodal summaries is of tremendous interest for broadcasters and fans alike, as a large percentage of audiences prefer to follow only the main highlights of a game. However, annotating important events and producing summaries often requires expensive equipment and a lot of tedious, cumbersome, manual labour. In this context, recent developments in Artificial Intelligence (AI) have shown great potential. The goal of this work is to create an automated soccer game summarization pipeline using AI. In particular, our focus is on the generation of complete game summaries in continuous text format with length constraints, based on raw game multimedia, as well as readily available game metadata and captions where applicable, using Natural Language Processing (NLP) tools along with heuristics. We curate and extend a number of soccer datasets, implement an end-to-end pipeline for the automatic generation of text summaries, present our preliminary results from the comparative analysis of various summarization methods within this pipeline using different input modalities, and provide a discussion of open challenges in the field of automated game summarization.},
booktitle = {Proceedings of the 1st Workshop on User-Centric Narrative Summarization of Long Videos},
pages = {13–22},
numpages = {10},
keywords = {soccer game summary, nlp, football, automated pipeline, ai},
location = {Lisboa, Portugal},
series = {NarSUM '22}
}

@article{10.1109/TASLP.2021.3124420,
author = {Ding, Yi-Yang and Lin, Hao-Jian and Liu, Li-Juan and Ling, Zhen-Hua and Hu, Yu},
title = {Robustness of Speech Spoofing Detectors Against Adversarial Post-Processing of Voice Conversion},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3124420},
doi = {10.1109/TASLP.2021.3124420},
abstract = {With the development of speech synthesis and voice conversion techniques, the quality of artificially generated speech has been significantly improved and detecting such spoofing speech becomes crucial to practical applications, such as automatic speaker verification (ASV). State-of-the-art neural-network-based spoofing detection models can distinguish most artificial utterances from natural ones effectively in the latest ASVspoof 2019 evaluation. Motivated by recent progresses of adversarial example generation, this paper studies the robustness of neural-network-based speech spoofing detectors against adversarial attacks. To this end, an adversarial post-processing network (APN) is proposed which generates adversarial examples against a white-box anti-spoofing model by post-processing the speech waveforms produced by a baseline voice conversion system. Experimental results demonstrate the adversarial ability of our proposed APNs against the white-box anti-spoofing models which were used as the adversarial targets of APNs at the training stage. For example, the equal error rate (EER) of a fused detection model based on light convolution neural networks (LCNNs) increased from 0.278% to 12.743% under the white-box condition without degrading the subjective quality of converted speech. Furthermore, the trained APNs can also perform against the detectors with either unseen structures or unseen features by raising their EERs in our experiments. All these results indicate the threat of adversarial speech generation to the performance of state-of-the-art spoofing detection models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {3415–3426},
numpages = {12}
}

@inproceedings{10.1145/3485832.3485892,
author = {Zhang, Zhaohe (John) and Yang, Edwin and Fang, Song},
title = {CommanderGabble: A Universal Attack Against ASR Systems Leveraging Fast Speech},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3485892},
doi = {10.1145/3485832.3485892},
abstract = {Automatic Speech Recognition (ASR) systems are widely used in various online transcription services and personal digital assistants. Emerging lines of research have demonstrated that ASR systems are vulnerable to hidden voice commands, i.e., audio that can be recognized by ASRs but not by humans. Such attacks, however, often either highly depend on white-box knowledge of a specific machine learning model or require special hardware to construct the adversarial audio. This paper proposes a new model-agnostic and easily-constructed attack, called CommanderGabble, which uses fast speech to camouflage voice commands. Both humans and ASR systems often misinterpret fast speech, and such misinterpretation can be exploited to launch hidden voice command attacks. Specifically, by carefully manipulating the phonetic structure of a target voice command, ASRs can be caused to derive a hidden meaning from the manipulated, high-speed version. We implement the discovered attacks both over-the-wire and over-the-air, and conduct a suite of experiments to demonstrate their efficacy against 7 practical ASR systems. Our experimental results show that the over-the-wire attacks can disguise as many as 96 out of 100 tested voice commands into adversarial ones, and that the over-the-air attacks are consistently successful for all 18 chosen commands in multiple real-world scenarios.},
booktitle = {Proceedings of the 37th Annual Computer Security Applications Conference},
pages = {720–731},
numpages = {12},
keywords = {ASR misinterpretation, adversarial audio, syllabification},
location = {Virtual Event, USA},
series = {ACSAC '21}
}

@article{10.1109/TASLP.2022.3184889,
author = {Li, Andong and Zheng, Chengshi and Yu, Guochen and Cai, Juanjuan and Li, Xiaodong},
title = {Filtering and Refining: A Collaborative-Style Framework for Single-Channel Speech Enhancement},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3184889},
doi = {10.1109/TASLP.2022.3184889},
abstract = {In low signal-to-noise ratio (SNR) acoustic scenarios, it remains fairly challenging to extract the target speech from its noisy mixture. In this paper, we propose a collaborative-style framework, namely, filtering and refining network (FRNet) for single-channel speech enhancement, recovering the complex spectrum of the target speech from coarse and fine-grained perspectives. Specifically, we devise a two-branch structure dubbed filtering-refining module (FRM). In the filtering block, the phase impact is ignored, and we only focus on coarse filtering in the magnitude domain. In the refining block, instead of predicting the irregular phase distribution directly, we estimate the complex residual for phase modification and spectrum rehabilitation, which takes the harmonic structure but with rather sparse energy distribution. By cascading FRMs repeatedly, we can reconstruct the target spectrum progressively. Furthermore, we propose a two-stream feature encoder to extract the feature representation of magnitude and phase individually, and the utilization of feature recalibration layers can preserve the prominent information from multiple scales. Extensive experiments are conducted on the WSJ0-SI84, Voicebank+Demand, and DNS-Challenge corpora. Evaluation results show that the proposed system performs favorably against previous advanced systems and achieves overall state-of-the-art performance in PESQ, ESTOI, SDR, and DNSMOS metrics.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2156–2172},
numpages = {17}
}

@inproceedings{10.5555/3721488.3721686,
author = {Khan, Muhammad Haris and Asfaw, Selamawit and Iarchuk, Dmitrii and Altamirano Cabrera, Miguel and Moreno, Luis and Tokmurziyev, Issatay and Tsetserukou, Dzmitry},
title = {Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing},
year = {2025},
publisher = {IEEE Press},
abstract = {This paper introduces Shake-VLA, a Vision-Language-Action (VLA) model-based system designed to enable bimanual robotic manipulation for automated cocktail preparation. The system integrates a vision module for detecting ingredient bottles and reading labels, a speech-to-text module for interpreting user commands, and a language model to generate task-specific robotic instructions. Force Torque (FT) sensors are employed to precisely measure the quantity of liquid poured, ensuring accuracy in ingredient proportions during the mixing process. The system architecture includes a Retrieval-Augmented Generation (RAG) module for accessing and adapting recipes, an anomaly detection mechanism to address ingredient availability issues, and bimanual robotic arms for dexterous manipulation. Experimental evaluations demonstrated a high success rate across system components, with the speech-to-text module achieving a 93% success rate in noisy environments, the vision module attaining a 91% success rate in object and label detection in cluttered environment, the anomaly module successfully identified 95% of discrepancies between detected ingredients and recipe requirements, and the system achieved an overall success rate of 100% in preparing cocktails, from recipe formulation to action generation.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1393–1397},
numpages = {5},
keywords = {bimanual manipulation, generative ai, human-robot interaction, vision-language-action model.},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1109/TASLP.2022.3182268,
author = {Ai, Yang and Ling, Zhen-Hua and Wu, Wei-Lu and Li, Ang},
title = {Denoising-and-Dereverberation Hierarchical Neural Vocoder for Statistical Parametric Speech Synthesis},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3182268},
doi = {10.1109/TASLP.2022.3182268},
abstract = {This paper presents a denoising and dereverberation hierarchical neural vocoder (DNR-HiNet) to convert noisy and reverberant acoustic features into clean speech waveforms. The DNR-HiNet vocoder is built by modifying the amplitude spectrum predictor (ASP) in the original HiNet vocoder. This modified denoising and dereverberation ASP (DNR-ASP) can predict clean log amplitude spectra from input degraded acoustic features. To achieve this, the DNR-ASP first predicts the log amplitude spectra of noisy and reverberant speech, the log amplitude spectra of additive noise and the room impulse response (RIR) and then performs initial denoising and dereverberation by signal processing algorithms. The initially processed log amplitude spectra are then enhanced by another neural network to obtain the final clean log amplitude spectra. We also introduce a bandwidth extension model and a frequency resolution extension model into the DNR-ASP to further improve its performance. Finally, a statistical parametric speech synthesis (SPSS) method with DNR-HiNet is proposed to deal with the situation that the quality of target speaker’s recordings is degraded by noise and reverberation. Experimental results indicate that the DNR-HiNet vocoder was able to generate denoised and dereverberated waveforms given noisy and reverberant acoustic features and outperformed the original HiNet vocoder and a few other neural vocoders. On speech enhancement tasks, its performance was competitive with several advanced speech enhancement methods. Furthermore, the SPSS method with DNR-HiNet achieved better quality of synthetic speech than the conventional approach which directly applied speech enhancement to the degraded adaptation data.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2036–2048},
numpages = {13}
}

@article{10.1109/TASLP.2021.3082671,
author = {Torres, Humberto M. and G\"{u}emes, Mercedes and Gurlekian, Jorge A. and Evin, Diego A.},
title = {F0 Perturbation Due to Articulatory Movements: Filtering, Characterization and Applications},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3082671},
doi = {10.1109/TASLP.2021.3082671},
abstract = {This paper addresses the issue of local disturbances in the fundamental frequency contour of speech, caused by the articulation of voiced/unvoiced consonant phonemes. Depending on the intended use of the F0 contour, these disturbances are usually eliminated by a filtering, smoothing or stylization procedure. These procedures that seek to preserve only the F0 points perceptually relevant, are generally applied roughly at a global level, which may not completely eliminate micro intonation in some cases or distort macro intonation in others. In this work we propose a local filtering algorithm based on a fine level analysis of the microprosodic morphologies. The performance of the algorithm is validated by a perceptual experiment. Assuming the algorithm allows partial/total disturbance elimination, we perform a statistical description of the perturbation morphologies. Statistics were collected from a corpus of 741 sentences designed to study Argentine Spanish prosody. The corpus was recorded by four professional announcers native speakers from Buenos Aires city. The results show that perturbation morphologies are affected by: consonant phoneme identity; global F0 contour shape; and speaker identity. As an application case, we use the proposed filtering algorithm as a pre-processing stage in our automatic prominent syllable detection system, with a statistically significant improvement in its performance.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1977–1986},
numpages = {10}
}

@article{10.1109/TASLP.2023.3297961,
author = {Carofilis, Andr\'{e}s and Alegre, Enrique and Fidalgo, Eduardo and Fern\'{a}ndez-Robles, Laura},
title = {Improvement of Accent Classification Models Through Grad-Transfer From Spectrograms and Gradient-Weighted Class Activation Mapping},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3297961},
doi = {10.1109/TASLP.2023.3297961},
abstract = {Automatic accent classification is an active research field concerning speech processing. It can be useful to identify a speaker's region of origin, which can be applied in police investigations carried out by Law Enforcement Agencies, as well as for the improvement of current speech recognition systems. This article presents a novel descriptor called Grad-Transfer, extracted using the Gradient-weighted Class Activation Mapping (Grad-CAM) method based on convolutional neural network (CNN) interpretability. Additionally, we propose a methodology for accent classification that implements Grad-Transfer, which is based on transferring the knowledge acquired by a CNN to a classical machine learning algorithm. The article works on two hypotheses: the coarse localization maps produced by Grad-CAM on spectrograms are able to highlight the regions of the spectrograms that are important for predicting accents, and Grad-Transfer descriptors computed from audios represent distinctive descriptions of the target accents. These hypotheses were demonstrated experimentally, clustering the generated Grad-Transfer descriptors according to the original accent of the audios using Birch and &lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$k$&lt;/tex-math&gt;&lt;/inline-formula&gt;-means algorithms. We carried out experiments on the Voice Cloning Toolkit dataset, seeing an increase of macro average accuracy, and unweighted average recall in the results obtained by a Gaussian Naive Bayes classifier up to 23.00%, and 23.58%, respectively, compared to a model trained with spectrograms. This demonstrates that Grad-Transfer is able to improve the performance of accent classification models and opens the door to new implementations in similar tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2859–2871},
numpages = {13}
}

@article{10.1145/3711828,
author = {Wang, Shiming and Chen, Li-Ping and Ai, Yang and Hu, Yajun and Ling, Zhen-Hua},
title = {PhonemeVec: A Phoneme-Level Contextual Prosody Representation For Speech Synthesis},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3711828},
doi = {10.1145/3711828},
abstract = {Recently, fine-grained prosody representations have emerged and attracted growing attention to address the one-to-many problem in text-to-speech (TTS). In this article, we propose the PhonemeVec, a pre-trained prosody representations with considering the contextual information. To obtain the contextual prosody representations, we improve the data2vec framework according to the characteristics of prosody to extract the PhonemeVec from the low-band mel-spectrogram, and pre-train on a 960 hours Chinese corpus with high quality and diverse pronunciation. PhonemeVec is subsequently integrated into FastSpeech2, supervising the prosody modeling of the text encoder. Experiments conducted on the Blizzard Challenge 2019 dataset show that the integration of PhonemeVec results in the synthesis of more natural speech. Additionally, objective evaluations confirm that the application of PhonemeVec reduces the distortions between the generated speech and original recordings in terms of duration and F0. Audio samples can be found at .},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {29},
numpages = {15},
keywords = {Speech synthesis, pre-training, phoneme-level prosody attributes, contextual prosody representation}
}

@article{10.1145/3569934,
author = {Gong, Lina and Zhang, Jingxuan and Wei, Mingqiang and Zhang, Haoxiang and Huang, Zhiqiu},
title = {What Is the Intended Usage Context of This Model? An Exploratory Study of Pre-Trained Models on Various Model Repositories},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3569934},
doi = {10.1145/3569934},
abstract = {There is a trend of researchers and practitioners to directly apply pre-trained models to solve their specific tasks. For example, researchers in software engineering (SE) have successfully exploited the pre-trained language models to automatically generate the source code and comments. However, there are domain gaps in different benchmark datasets. These data-driven (or machine learning based) models trained on one benchmark dataset may not operate smoothly on other benchmarks. Thus, the reuse of pre-trained models introduces large costs and additional problems of checking whether arbitrary pre-trained models are suitable for the task-specific reuse or not. To our knowledge, software engineers can leverage code contracts to maximize the reuse of existing software components or software services. Similar to the software reuse in the SE field, reuse SE could be extended to the area of pre-trained model reuse. Therefore, according to the model card’s and FactSheet’s guidance for suppliers of pre-trained models on what information they should be published, we propose model contracts including the pre- and post-conditions of pre-trained models to enable better model reuse. Furthermore, many non-trivial yet challenging issues have not been fully investigated, although many pre-trained models are readily available on the model repositories. Based on our model contract, we conduct an exploratory study of 1908 pre-trained models on six mainstream model repositories (i.e., the TensorFlow Hub, PyTorch Hub, Model Zoo, Wolfram Neural Net Repository, Nvidia, and Hugging Face) to investigate the gap between necessary pre- and post-condition information and actual specifications. Our results clearly show that (1) the model repositories tend to provide confusing information of the pre-trained models, especially the information about the task’s type, model, training set, and (2) the model repositories cannot provide all of our proposed pre/post-condition information, especially the intended use, limitation, performance, and quantitative analysis. On the basis of our new findings, we suggest that (1) the developers of model repositories shall provide some necessary options (e.g., the training dataset, model algorithm, and performance measures) for each of pre/post-conditions of pre-trained models in each task type, (2) future researchers and practitioners provide more efficient metrics to recommend suitable pre-trained model, and (3) the suppliers of pre-trained models should report their pre-trained models in strict accordance with our proposed pre/post-condition and report their models according to the characteristics of each condition that has been reported in the model repositories.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {69},
numpages = {57},
keywords = {model contract, model reuse, pre-trained models, Software engineering for artificial intelligence}
}

@article{10.1109/TASLP.2023.3301212,
author = {Hsu, Po-chun and Liu, Da-rong and Liu, Andy T. and Lee, Hung-yi},
title = {Parallel Synthesis for Autoregressive Speech Generation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3301212},
doi = {10.1109/TASLP.2023.3301212},
abstract = {Autoregressive neural vocoders have achieved outstanding performance in speech synthesis tasks such as text-to-speech and voice conversion. An autoregressive vocoder predicts a sample at some time step conditioned on those at previous time steps. Though it synthesizes natural human speech, the iterative generation inevitably makes the synthesis time proportional to the utterance length, leading to low efficiency. Many works were dedicated to generating the whole speech sequence in parallel and proposed GAN-based, flow-based, and score-based vocoders. This paper proposed a new thought for the autoregressive generation. Instead of iteratively predicting samples in a time sequence, the proposed model performs frequency-wise autoregressive generation (FAR) and bit-wise autoregressive generation (BAR) to synthesize speech. In FAR, a speech utterance is split into frequency subbands, and a subband is generated conditioned on the previously generated one. Similarly, in BAR, an 8-bit quantized signal is generated iteratively from the first bit. By redesigning the autoregressive method to compute in domains other than the time domain, the number of iterations in the proposed model is no longer proportional to the utterance length but to the number of subbands/bits, significantly increasing inference efficiency. Besides, a post-filter is employed to sample signals from output posteriors; its training objective is designed based on the characteristics of the proposed methods. Experimental results show that the proposed model can synthesize speech faster than real-time without GPU acceleration. Compared with baseline vocoders, the proposed model achieves better MUSHRA results and shows good generalization ability for unseen speakers and 44 kHz speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3095–3111},
numpages = {17}
}

@inproceedings{10.5555/3721488.3721714,
author = {Park, Cheonshu and Cho, Miyoung and Shin, Minjung and Ryu, Jeh-Kwang and Jang, Minsu},
title = {Adaptive Robot-mediated Assessment using LLM for Enhanced Survey Quality in Older Adults Care Programs},
year = {2025},
publisher = {IEEE Press},
abstract = {This study presents an adaptive human-robot interaction (HRI) system that evaluates older adult participants' satisfaction with personalized healthcare programs. By integrating the CLOi robot with a large language model (LLM), the system conducts satisfaction surveys that adapt in real-time to participant responses. The system was applied to evaluate healthcare programs that include physical health measurements, exercise assessments, and virtual reality (VR) experiences. The system utilizes the CLOi robot and Claude API to analyze response clarity in real-time, automatically generating contextually appropriate follow-up questions when responses are deemed ambiguous. This adaptive questioning strategy ensures comprehensive response quality before proceeding to subsequent survey items. We conducted a preliminary feasibility study with five older adult participants to evaluate our approach. The system leverages LLM prompts to analyze gaps between question intent and participant responses, generating targeted follow-up questions as needed. Results demonstrate that our LLM-enhanced robotic interview system effectively reduced response ambiguity through dynamic follow-up questioning, achieving an 85% response resolution rate. This adaptive approach improved the clarity and specificity of healthcare satisfaction assessments for older adults.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1534–1538},
numpages = {5},
keywords = {adaptive robot-mediated assessment, human-robot interaction (hri), llm-based question generation},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1109/TASLP.2021.3125143,
author = {Lin, Ju and van Wijngaarden, Adriaan J. de Lind and Wang, Kuang-Ching and Smith, Melissa C.},
title = {Speech Enhancement Using Multi-Stage Self-Attentive Temporal Convolutional Networks},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3125143},
doi = {10.1109/TASLP.2021.3125143},
abstract = {Multi-stage learning is an effective technique for invoking multiple deep-learning modules sequentially. This paper applies multi-stage learning to speech enhancement by using a multi-stage structure, where each stage comprises a self-attention (SA) block followed by stacks of temporal convolutional network (TCN) blocks with doubling dilation factors. Each stage generates a prediction that is refined in a subsequent stage. A feature fusion block is inserted at the input of later stages to re-inject original information. The resulting multi-stage speech enhancement system, multi-stage SA-TCN, is compared with state-of-the-art deep-learning speech enhancement methods using the LibriSpeech and VCTK datasets. The multi-stage SA-TCN system’s hyperparameters are fine-tuned, and the impact of the SA block, the feature fusion block, and the number of stages are determined. The use of a multi-stage SA-TCN system as a front-end for automatic speech recognition systems is also investigated. It is shown that the multi-stage SA-TCN systems perform well relative to other state-of-the-art systems in terms of speech enhancement and speech recognition scores.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {3440–3450},
numpages = {11}
}

@article{10.1145/3716379,
author = {Zhang, Yixuan and He, Ningyu and Gao, Jianting and Cao, Shangtong and Liu, Kaibo and Wang, Haoyu and Ma, Yun and Huang, Gang and Liu, Xuanzhe},
title = {DrWASI: LLM-assisted Differential Testing for WebAssembly System Interface Implementations},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3716379},
doi = {10.1145/3716379},
abstract = {WebAssembly (Wasm) is an emerging binary format that serves as a compilation target for over 40 programming languages. Wasm runtimes provide execution environments that enhance portability by abstracting away operating systems and hardware details. A key component in these runtimes is the WebAssembly System Interface (WASI), which manages interactions with operating systems, like file operations. Considering the critical role of Wasm runtimes, the community has aimed to detect their implementation bugs. However, no work has focused on WASI-specific bugs that can affect the original functionalities of running Wasm binaries and cause unexpected results. To fill the void, we present DrWASI, the first general-purpose differential testing framework for WASI implementations. Our approach uses a large language model to generate seeds and applies variant and environment mutation strategies to expand and enrich the test case corpus. We then perform differential testing across major Wasm runtimes. By leveraging dynamic and static information collected during and after the execution, DrWASI can identify bugs. Our evaluation shows that DrWASI uncovered 33 unique bugs, with all confirmed and 7 fixed by developers. This research represents a pioneering step in exploring a promising yet under-explored area of the Wasm ecosystem, providing valuable insights for stakeholders.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
keywords = {WebAssembly, WebAssembly System Interface}
}

@article{10.1109/TASLP.2021.3074757,
author = {Zhang, Ya-Jie and Ling, Zhen-Hua},
title = {Extracting and Predicting Word-Level Style Variations for Speech Synthesis},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3074757},
doi = {10.1109/TASLP.2021.3074757},
abstract = {This paper proposes a speech synthesis method based on unsupervisedly-learned fine-grained style representations, named word-level style variations (WSVs), in order to improve the naturalness of synthetic speech. The whole model contains a WSV extractor and a WSV predictor. The WSV extractor is jointly trained with a sequence-to-sequence (Seq2seq) synthesizer and learns a WSV vector from the mel-spectrogram of each prosodic word in the training set by extending the global style token (GST) framework. In contrast to GST weights which describe the global styles of utterances, WSVs operate at word-level and are expected to describe local style properties, such as stresses. Besides, Gumbel softmax is adopted and the extracted WSVs are close to one-hot vectors which facilitate the subsequent prediction task. The WSV predictor is a deterministic model which generates the sequence of WSV vectors from input text using an autoregressive LSTM network. In addition to phonetic information, e.g., phoneme sequences, Bidirectional Encoder Representation from Transformers (BERT) model is employed by the predictor to obtain the semantic descriptions of input text for better predicting the latent speech representation, i.e., WSVs. The WSV predictor is trained by considering both the accuracy of WSV prediction and the distortion of mel-spectrograms recovered from the predicted WSVs. Experimental results show that our proposed method can achieve better naturalness of synthetic speech than baseline Tacotron2, text-predicted global style token (TP-GST) and BERT-Tacotron2 models.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1582–1593},
numpages = {12}
}

@article{10.1145/3459744,
author = {Zhang, Xinlei and Miyaki, Takashi and Rekimoto, Jun},
title = {JustSpeak: Automated, User-Configurable, Interactive Agents for Speech Tutoring},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3459744},
doi = {10.1145/3459744},
abstract = {Conversational agents are widely used in many situations, especially for speech tutoring. However, their contents and functions are often pre-defined and not customizable for people without technical backgrounds, thus significantly limiting their flexibility and usability. Besides, conventional agents often cannot provide feedback in the middle of training sessions because they lack technical approaches to evaluate users' speech dynamically. We propose JustSpeak: automated and interactive speech tutoring agents with various configurable feedback mechanisms, using any speech recordings with its transcription text as the template for speech training. In JustSpeak, we developed an automated procedure to generate customized tutoring agents from user-inputted templates. Moreover, we created a set of methods to dynamically synchronize speech recognizers' behavior with the agent's tutoring progress, making it possible to detect various speech mistakes dynamically such as being stuck, mispronunciation, and rhythm deviations. Furthermore, we identified the design primitives in JustSpeak to create different novel feedback mechanisms, such as adaptive playback, follow-on training, and passive adaptation. They can be combined to create customized tutoring agents, which we demonstrate with an example for language learning. We believe JustSpeak can create more personalized speech learning opportunities by enabling tutoring agents that are customizable, always available, and easy-to-use.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {202},
numpages = {24},
keywords = {tutoring agent, speech recognition, speech, language learning, intelligent tutoring system (ITS), conversational agent}
}

@inproceedings{10.1145/3490099.3511147,
author = {Laban, Philippe and Ye, Elicia and Korlakunta, Srujay and Canny, John and Hearst, Marti},
title = {NewsPod: Automatic and Interactive News Podcasts},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511147},
doi = {10.1145/3490099.3511147},
abstract = {News podcasts are a popular medium to stay informed and dive deep into news topics. Today, most podcasts are handcrafted by professionals. In this work, we advance the state-of-the-art in automatically generated podcasts, making use of recent advances in natural language processing and text-to-speech technology. We present NewsPod, an automatically generated, interactive news podcast. The podcast is divided into segments, each centered on a news event, with each segment structured as a Question and Answer conversation, whose goal is to engage the listener. A key aspect of the design is the use of distinct voices for each role (questioner, responder), to better simulate a conversation. Another novel aspect of NewsPod allows listeners to interact with the podcast by asking their own questions and receiving automatically generated answers. We validate the soundness of this system design through two usability studies, focused on evaluating the narrative style and interactions with the podcast, respectively. We find that NewsPod is preferred over a baseline by participants, with 80% claiming they would use the system in the future.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {691–706},
numpages = {16},
keywords = {automatic podcast, conversational, interactive podcast, news podcast, podcast, question answering, question generation, summarization},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3594409.3594415,
author = {Wang, Zhipeng and Xu, Hongjing and Chen, Shuoying and Guo, Yuhang},
title = {Forward Translation to Mix Data for Speech Translation},
year = {2023},
isbn = {9781450398398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594409.3594415},
doi = {10.1145/3594409.3594415},
abstract = {End-to-End speech translation means that using a model to translate speech in one language into text in another language. Currently, the main challenge in the field of speech translation is data scarcity. Existing works solve this problem by using text information or applying data augmentation. However, these works only focus on the exploitation of a single corpus, ignoring the full use of existing human-labeled different-sources data. In this paper, we introduce a simple method to solve the data scarcity problem: training a model with simply mixed data and applying the forward translation method to expand the training set. We perform experiments on covost v2 French-English and mTEDx French-English. Our experiments demonstrate that combining the mixture of speech translation corpora with forward translation can yield a better result than the method without mixing.},
booktitle = {Proceedings of the 2023 7th International Conference on Innovation in Artificial Intelligence},
pages = {178–182},
numpages = {5},
keywords = {Speech translation, Forward-translation, Domain adaption, Data scarcity},
location = {Harbin, China},
series = {ICIAI '23}
}

@inproceedings{10.1145/3664647.3680770,
author = {Zheng, Rui-Chen and Ai, Yang and Ling, Zhen-Hua},
title = {Speech Reconstruction from Silent Lip and Tongue Articulation by Diffusion Models and Text-Guided Pseudo Target Generation},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680770},
doi = {10.1145/3664647.3680770},
abstract = {This paper studies the task of speech reconstruction from ultrasound tongue images and optical lip videos recorded in a silent speaking mode, where people only activate their intra-oral and extra-oral articulators without producing real speech. This task falls under the umbrella of articulatory-to-acoustic (A2A) conversion and may also be referred to as a silent speech interface. To overcome the domain discrepancy between silent and standard vocalized articulation, we introduce a novel pseudo target generation strategy. It integrates the text modality to align with articulatory movements, thereby guiding the generation of pseudo acoustic features for supervised training on speech reconstruction from silent articulation. Furthermore, we propose to employ a denoising diffusion probabilistic model as the fundamental architecture for the A2A conversion task and train the model using a combined training approach with the generated pseudo acoustic features. Experiments show that our proposed method significantly improves the intelligibility and naturalness of the reconstructed speech in the silent speaking mode compared to all baseline methods. Specifically, the word error rate of the reconstructed speech decreases by approximately 5% when measured using an automatic speech recognition engine for intelligibility assessment, and the subjective mean opinion score for naturalness improves by 0.14. Moreover, analytical experiments reveal that the proposed pseudo target generation strategy can generate pseudo acoustic features that synchronize better with articulatory movements than previous strategies. Samples are available at our project page.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6559–6568},
numpages = {10},
keywords = {articulatory-to-acoustic conversion, diffusion probablistic model, pseudo target, silent speech interface},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3652988.3673918,
author = {Zhang, Taiyu and Zhang, Xuesong and Cools, Robbe and Simeone, Adalberto},
title = {Focus Agent: LLM-Powered Virtual Focus Group},
year = {2024},
isbn = {9798400706257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652988.3673918},
doi = {10.1145/3652988.3673918},
abstract = {In the domain of Human-Computer Interaction, focus groups represent a widely utilised yet resource-intensive methodology, often demanding the expertise of skilled moderators and meticulous preparatory efforts. This study introduces the “Focus Agent,” a Large Language Model (LLM) powered framework that simulates both the focus group (for data collection) and acts as a moderator in a focus group setting with human participants. To assess the data quality derived from the Focus Agent, we ran five focus group sessions with a total of 23 human participants as well as deploying the Focus Agent to simulate these discussions with AI participants. Quantitative analysis indicates that Focus Agent can generate opinions similar to those of human participants. Furthermore, the research exposes some improvements associated with LLMs acting as moderators in focus group discussions that include human participants.},
booktitle = {Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents},
articleno = {10},
numpages = {10},
keywords = {Human-computer Interaction, Intelligent Virtual Agent, Multi Agent Simulation, Virtual Focus Group},
location = {GLASGOW, United Kingdom},
series = {IVA '24}
}

@article{10.1109/TASLP.2024.3399607,
author = {Liu, Haohe and Yuan, Yi and Liu, Xubo and Mei, Xinhao and Kong, Qiuqiang and Tian, Qiao and Wang, Yuping and Wang, Wenwu and Wang, Yuxuan and Plumbley, Mark D.},
title = {AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3399607},
doi = {10.1109/TASLP.2024.3399607},
abstract = {Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three &lt;italic&gt;AudioLDM 2&lt;/italic&gt; variants demonstrate competitive performance of the &lt;italic&gt;AudioLDM 2&lt;/italic&gt; framework against previous approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {2871–2883},
numpages = {13}
}

@inproceedings{10.1145/3522664.3528592,
author = {Borg, Markus and Bengtsson, Johan and \"{O}sterling, Harald and Hagelborn, Alexander and Gagner, Isabella and Tomaszewski, Piotr},
title = {Quality assurance of generative dialog models in an evolving conversational agent used for Swedish language practice},
year = {2022},
isbn = {9781450392754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3522664.3528592},
doi = {10.1145/3522664.3528592},
abstract = {Due to the migration megatrend, efficient and effective second-language acquisition is vital. One proposed solution involves AI-enabled conversational agents for person-centered interactive language practice. We present results from ongoing action research targeting quality assurance of proprietary generative dialog models trained for virtual job interviews. The action team elicited a set of 38 requirements for which we designed corresponding automated test cases for 15 of particular interest to the evolving solution. Our results show that six of the test case designs can detect meaningful differences between candidate models. While quality assurance of natural language processing applications is complex, we provide initial steps toward an automated framework for machine learning model selection in the context of an evolving conversational agent. Future work will focus on model selection in an MLOps setting.},
booktitle = {Proceedings of the 1st International Conference on AI Engineering: Software Engineering for AI},
pages = {22–32},
numpages = {11},
keywords = {AI quality, action research, conversational agent, generative dialog model, requirements engineering, software testing},
location = {Pittsburgh, Pennsylvania},
series = {CAIN '22}
}

@article{10.1145/3517195,
author = {Mehra, Pramod and Verma, Shashi Kant},
title = {BERIS: An mBERT-based Emotion Recognition Algorithm from Indian Speech},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517195},
doi = {10.1145/3517195},
abstract = {Emotions, the building blocks of the human intellect, play a vital role in Artificial Intelligence (AI). For a robust AI-based machine, it is important that the machine understands human emotions. COVID-19 has introduced the world to no-touch intelligent systems. With an influx of users, it is critical to create devices that can communicate in a local dialect. A multilingual system is required in countries like India, which has a large population and a diverse range of languages. Given the importance of multilingual emotion recognition, this research introduces BERIS, an Indian language emotion detection system. From the Indian sound recording, BERIS estimates both acoustic and textual characteristics. To extract the textual features,&nbsp;we used Multilingual&nbsp;Bidirectional Encoder Representations from Transformers. For acoustics, BERIS computes the Mel Frequency Cepstral Coefficients and Linear Prediction coefficients, and Pitch. The features extracted are merged in a linear array. Since the dialogues are of varied lengths, the data are normalized to have arrays of equal length. Finally, we split the data into training and validated set to construct a predictive model. The model can predict emotions from the new input. On all the datasets presented, quantitative and qualitative evaluations show that the proposed algorithm outperforms state-of-the-art approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {106},
numpages = {19},
keywords = {Pitch, LPC, MFCC, emotion recognition, mBERT}
}

@inproceedings{10.1145/3629296.3629363,
author = {Carvalho, Vitor Manuel and Freitas, Diamantino Silva},
title = {Enabling visually disabled persons to create technical drawings: a review with a preliminary contribution},
year = {2024},
isbn = {9798400709111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629296.3629363},
doi = {10.1145/3629296.3629363},
abstract = {This white paper is dedicated to creating geometrical content by visually impaired persons, aiming to increase the effectiveness of several existing tools and solutions. It starts with an updated analysis of the state of the art regarding their use as an aid in accessing and designing of those contents. We observed that, besides being only a few, the existing solutions fail in one or more of the following requirements: continuous tactile feedback, error correction tools, compatibility with the most common assistive technologies and the possibility of sharing the created image, thus not allowing a sufficiently effective use. Inspired by our previous work in accessibility to mathematics and vectorised graphics by blind persons and in the multimodal human-computer interface paradigm, we created and are now presenting a preliminary contribution to improving the situation for the specific case of the creation of technical drawings. The proposed solution is based on an iterative and flexible interface with a navigation method that considers the management of the user's cognitive load during the creation process, using some Gestalt principles, the primary interface modalities, and continuous feedback on what is being created. The implementation is currently underway using web technologies to interconnect the interface devices, allowing the user to create standard geometric shapes by combining inputs through a command line, a voice interface, and a drawing pad. Continuous tactile feedback is provided through braille, a general-purpose tactile graphic display, and acoustic feedback through text-to-speech. An embossing printer and audio sonification will also be used. After the development of the application, tests with users will follow to improve and validate the proposed solution.},
booktitle = {Proceedings of the 15th International Conference on Education Technology and Computers},
pages = {416–422},
numpages = {7},
keywords = {Computer Application, Visual Content Creation, Visual Impairment},
location = {Barcelona, Spain},
series = {ICETC '23}
}

@article{10.1109/TASLP.2023.3306711,
author = {Zhang, Yuxiang and Li, Zhuo and Lu, Jingze and Hua, Hua and Wang, Wenchao and Zhang, Pengyuan},
title = {The Impact of Silence on Speech Anti-Spoofing},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3306711},
doi = {10.1109/TASLP.2023.3306711},
abstract = {The current speech anti-spoofing countermeasures (CMs) show excellent performance on specific datasets. However, removing the silence of test speech through Voice Activity Detection (VAD) can severely degrade performance. In this article, the impact of silence on speech anti-spoofing is analyzed. First, the reasons for the impact are explored, including the proportion of silence duration and the content of silence. The proportion of silence duration in spoof speech generated by text-to-speech (TTS) algorithms is lower than that in bonafide speech. And the content of silence generated by different waveform generators varies compared to bonafide speech. Then the impact of silence on model prediction is explored. Even after retraining, the spoof speech generated by neural network based end-to-end TTS algorithms suffers a significant rise in error rates when the silence is removed. To demonstrate the reasons for the impact of silence on CMs, the attention distribution of a CM is visualized through class activation mapping (CAM). Furthermore, the implementation and analysis of the experiments masking silence or non-silence demonstrates the significance of the proportion of silence duration for detecting TTS and the importance of silence content for detecting voice conversion (VC). Based on the experimental results, improving the robustness of CMs against unknown spoofing attacks by masking silence is also proposed. Finally, the attacks on anti-spoofing CMs through concatenating silence, and the mitigation of VAD and silence attack through low-pass filtering are introduced.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3374–3389},
numpages = {16}
}

@article{10.1145/3579591,
author = {Hu, Siying and Yen, Hen Chen and Yu, Ziwei and Zhao, Mingjian and Seaborn, Katie and Liu, Can},
title = {Wizundry: A Cooperative Wizard of Oz Platform for Simulating Future Speech-based Interfaces with Multiple Wizards},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579591},
doi = {10.1145/3579591},
abstract = {Wizard of Oz (WoZ) as a prototyping method has been used to simulate intelligent user interfaces, particularly for speech-based systems. However, as our societies' expectations on artificial intelligence (AI) grows, the question remains whether a single Wizard is sufficient for it to simulate smarter systems and more complex interactions. Optimistic visions of 'what artificial intelligence (AI) can do' places demands on WoZ platforms to simulate smarter systems and more complex interactions. This raises the question of whether the typical approach of employing a single Wizard is sufficient. Moreover, while existing work has employed multiple Wizards in WoZ studies, a multi-Wizard approach has not been systematically studied in terms of feasibility, effectiveness, and challenges. We offer Wizundry, a real-time, web-based WoZ platform that allows multiple Wizards to collaboratively operate a speech-to-text based system remotely. We outline the design and technical specifications of our open-source platform, which we iterated over two design phases. We report on two studies in which participant-Wizards were tasked with negotiating how to cooperatively simulate an interface that can handle natural speech for dictation and text editing as well as other intelligent text processing tasks. We offer qualitative findings on the Multi-Wizard experience for Dyads and Triads of Wizards. Our findings reveal the promises and challenges of the multi-Wizard approach and open up new research questions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {115},
numpages = {34},
keywords = {Wizard of Oz, collaboration, dictation, multi-wizard, speech-based interfaces, voice user interface}
}

@inproceedings{10.1145/3484824.3484875,
author = {Suresh, Nalina and Mukabe, Nkandu and Hashiyana, Valerianus and Limbo, Anton and Hauwanga, Aina},
title = {Career Counseling Chatbot on Facebook Messenger using AI},
year = {2022},
isbn = {9781450387637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3484824.3484875},
doi = {10.1145/3484824.3484875},
abstract = {Increasingly we observe that newly graduated university students struggle to find employment, often ending up in fields completely different from those that they studied in, and employees reporting low levels of satisfaction in the careers that they choose. One of the reasons for this could be a lack of adequate, useful career guidance when an individual is in university or recently graduated. Chatbots are useful resources and a topic of interest in the realm of Computer Science and AI for their ability to mimic experts in different applications, as well as being able to replicate human interaction to varying degrees.Research suggests that the use of a chatbot in offering career counseling can serve as an efficient means to provide this useful service in environments where a dedicated career counsellor may not be available. A lack of appropriate, adequate career counseling means youth may wind up pursuing careers that are chosen for them by their parents or only chosen for the promise of high salaries. These decisions may be made without taking into consideration whether or not they align with the individual's interests and values.This can lead to feelings of dissatisfaction in one's career, which is detrimental not only to the wellbeing of the individual but also to the overall productivity of the business in which they are hired. Hence, the development of the chatbot will better inform users, assisting them in the ways of choosing a career. This will allow them to consider careers they may not have thought available to them, that would be more fulfilling and satisfactory than one that does not align with their interests.This research adopted both Research methodology and Software Development methodology. Under Research Methodology, a survey was conducted to gather information about individuals' opinions regarding their possible career choices, and what types of career advice they would like to receive from the system. In addition, the latest research on career guidance was considered in forming the basis of the questions asked. This information was used in chatbot system development on the Facebook Messenger platform, using tools such as the Facebook SDK, the Messenger Platform APIs, and JavaScript, and the Wit.ai API which enables the use of Natural Language Processing, which is a use case of AI techniques.The chatbot understands user input and give relevant and appropriate responses, reliably and in real-time. In conclusion, the results obtained are hoped to result in positive proclivity towards adopting the system, thus serving as a useful asset to any universities or institutions that might wish to utilize it.},
booktitle = {Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence},
pages = {65–73},
numpages = {9},
keywords = {Natural language Processing, Chatbot, Career Counseling},
location = {Windhoek, Namibia},
series = {DSMLAI '21'}
}

@article{10.1145/3701728,
author = {Luo, Xiangzhong and Liu, Di and Kong, Hao and Huai, Shuo and Chen, Hui and Xiong, Guochu and Liu, Weichen},
title = {Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3701728},
doi = {10.1145/3701728},
abstract = {Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs in real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate this computational gap and enable ubiquitous embedded intelligence, we focus in this survey on discussing recent efficient deep learning infrastructures for embedded computing systems, spanning from training to inference, from manual to automated, from convolutional neural networks to transformers, from transformers to vision transformers, from vision models to large language models, from software to hardware, and from algorithms to applications. Specifically, we discuss recent efficient deep learning infrastructures for embedded computing systems from the lens of (1) efficient manual network design for embedded computing systems, (2) efficient automated network design for embedded computing systems, (3) efficient network compression for embedded computing systems, (4) efficient on-device learning for embedded computing systems, (5) efficient large language models for embedded computing systems, (6) efficient deep learning software and hardware for embedded computing systems, and (7) efficient intelligent applications for embedded computing systems. We also envision promising future directions and trends, which have the potential to deliver more ubiquitous embedded intelligence. We believe this survey has its merits and can shed light on future research, which can largely help researchers to quickly and smoothly get started in this emerging field.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = dec,
articleno = {21},
numpages = {100},
keywords = {Embedded computing systems, embedded intelligence, artificial intelligence, efficient deep learning algorithms, efficient network design, efficient neural architecture search, efficient model compression, efficient on-device learning, efficient large language models, efficient deep learning software and hardware, intelligent embedded applications}
}

@article{10.1109/TASLP.2022.3233236,
author = {Zhang, Lin and Wang, Xin and Cooper, Erica and Evans, Nicholas and Yamagishi, Junichi},
title = {The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3233236},
doi = {10.1109/TASLP.2022.3233236},
abstract = {Automatic speaker verification is susceptible to various manipulations and spoofing, such as text-to-speech synthesis, voice conversion, replay, tampering, adversarial attacks, and so on. We consider a new spoofing scenario called &lt;italic&gt;“Partial Spoof”&lt;/italic&gt; (PS) in which synthesized or transformed speech segments are embedded into a bona fide utterance. While existing countermeasures (CMs) can detect fully spoofed utterances, there is a need for their adaptation or extension to the PS scenario. We propose various improvements to construct a significantly more accurate CM that can detect and locate short-generated spoofed speech segments at finer temporal resolutions. First, we introduce newly developed self-supervised pre-trained models as enhanced feature extractors. Second, we extend our PartialSpoof database by adding segment labels for various temporal resolutions. Since the short spoofed speech segments to be embedded by attackers are of variable length, six different temporal resolutions are considered, ranging from as short as 20 ms to as large as 640 ms. Third, we propose a new CM that enables the simultaneous use of the segment-level labels at different temporal resolutions as well as utterance-level labels to execute utterance- and segment-level detection at the same time. We also show that the proposed CM is capable of detecting spoofing at the utterance level with low error rates in the PS scenario as well as in a related logical access (LA) scenario. The equal error rates of utterance-level detection on the PartialSpoof database and ASVspoof 2019 LA database were 0.77 and 0.90%, respectively.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {813–825},
numpages = {13}
}

@article{10.1109/TASLP.2021.3093823,
author = {Zhou, Xiao and Ling, Zhen-Hua and Dai, Li-Rong},
title = {UnitNet: A Sequence-to-Sequence Acoustic Model for Concatenative Speech Synthesis},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3093823},
doi = {10.1109/TASLP.2021.3093823},
abstract = {This paper presents UnitNet, a sequence-to-sequence (Seq2Seq) acoustic model for concatenative speech synthesis. Comparing with the Tacotron2 model for Seq2Seq speech synthesis, UnitNet utilizes the phone boundaries of training data and its decoder contains autoregressive structures at both phone and frame levels. This hierarchical architecture can not only extract embedding vectors for representing phone-sized units in the corpus but also measure the dependency among consecutive units, which makes the UnitNet model capable of guiding the selection of phone-sized units for concatenative speech synthesis. A byproduct of this model is that it can also be applied to statistical parametric speech synthesis (SPSS) and improve the robustness of Seq2Seq acoustic feature prediction since it adopts interpretable transition probability prediction rather than attention mechanism for frame-level alignment. Experimental results show that our UnitNet-based concatenative speech synthesis method not only outperforms the unit selection methods using hidden Markov models and Tacotron-based unit embeddings, but also achieves better naturalness and faster inference speed than the SPSS method using FastSpeech and Parallel WaveGAN. Besides, the UnitNet-based SPSS method makes fewer synthesis errors than Tacotron2 and FastSpeech without naturalness degradation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2643–2655},
numpages = {13}
}

@article{10.1145/3698773,
author = {Sun, Jiaqi and Deng, Xianjun and liu, Shenghao and Fan, Xiaoxuan and Huang, Yongling and He, Yuanyuan and Wu, Celimuge and Park, Jong Hyuk},
title = {Contrastive Learning based Speech Spoofing Detection for Multimedia Security in Edge Intelligence},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1551-6857},
url = {https://doi.org/10.1145/3698773},
doi = {10.1145/3698773},
abstract = {Artificial intelligence (AI) empowered edge computing has given rise to a new paradigm and effectively facilitated the promotion and development of multimedia applications. The speech assistant is one of the significant services provided by multimedia applications, which aims to offer intelligent interactive experiences between humans and machines. However, malicious attackers may exploit spoofed speeches to deceive speech assistants, posing great challenges to the security of multimedia applications. The limited resources of multimedia terminal devices hinder their ability to effectively load speech spoofing detection models. Furthermore, processing and analyzing speech in the cloud can result in poor real-time performance and potential privacy risks. Existing speech spoofing detection methods rely heavily on annotated data and exhibit poor generalization capabilities for unseen spoofed speeches. To address these challenges, this paper first proposes the Coordinate Attention Network (CA2Net) that consists of coordinate attention blocks and Res2Net blocks. CA2Net can simultaneously extract temporal and spectral speech feature information and represent multi-scale speech features at a granularity level. Besides, a contrastive learning-based speech spoofing detection framework named GEMINI is proposed. GEMINI can be effectively deployed on edge nodes and autonomously learn speech features with strong generalization capabilities. GEMINI first performs data augmentation on speech signals and extracts conventional acoustic features to enhance the feature robustness. Subsequently, GEMINI utilizes the proposed CA2Net to further explore the discriminative speech features. Then, a tensor-based multi-attention comparison model is employed to maximize the consistency between speech contexts. GEMINI continuously updates CA2Net with contrastive learning, which enables CA2Net to effectively represent speech signals and accurately detect spoofed speeches. Extensive experiments on the ASVspoof2019 dataset show that GEMINI reduces the Equal Error Rate and tandem Detection Cost Function by up to 96.75% and 96.35% in the physical access scenario, and by up to 86.62% and 87.71% in the logical access scenario compared to peer methods.},
note = {Just Accepted},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = oct,
keywords = {Edge intelligence, Multimedia applications, Speech spoofing detection, Contrastive learning, Coordinate attention}
}

@inproceedings{10.5555/3721488.3721576,
author = {Reimann, Merle M. and Hindriks, Koen V. and Kunneman, Florian A. and Oertel, Catharine and Skantze, Gabriel and Leite, Iolanda},
title = {What Can You Say to a Robot? Capability Communication Leads to More Natural Conversations},
year = {2025},
publisher = {IEEE Press},
abstract = {When encountering a robot in the wild, it is not inherently clear to human users what the robot's capabilities are. When encountering misunderstandings or problems in spoken interaction, robots often just apologize and move on, without additional effort to make sure the user understands what happened. We set out to compare the effect of two speech based capability communication strategies (proactive, reactive) to a robot without such a strategy, in regard to the user's rating of and their behavior during the interaction. For this, we conducted an in-person user study with 120 participants who had three speech-based interactions with a social robot in a restaurant setting. Our results suggest that users preferred the robot communicating its capabilities proactively and adjusted their behavior in those interactions, using a more conversational interaction style while also enjoying the interaction more.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {708–716},
numpages = {9},
keywords = {dialogue management, human-robot-interaction, spoken interaction, user study},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3664647.3680622,
author = {Xu, Junhao and Chen, Jingjing and Song, Xue and Han, Feng and Shan, Haijun and Jiang, Yu-Gang},
title = {Identity-Driven Multimedia Forgery Detection via Reference Assistance},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680622},
doi = {10.1145/3664647.3680622},
abstract = {Recent advancements in "deepfake" techniques have paved the way for generating various media forgeries. In response to the potential hazards of these media forgeries, many researchers engage in exploring detection methods, increasing the demand for high-quality media forgery datasets. Despite this, existing datasets have certain limitations. Firstly, most datasets focus on manipulating visual modality and usually lack diversity, as only a few forgery approaches are considered. Secondly, the quality of media is often inadequate in clarity and naturalness. Meanwhile, the size of the dataset is also limited. Thirdly, it is commonly observed that real-world forgeries are motivated by identity, yet the identity information of the individuals portrayed in these forgeries within existing datasets remains under-explored. For detection, identity information could be an essential clue to boost performance. Moreover, official media concerning relevant identities on the Internet can serve as prior knowledge, aiding both the audience and forgery detectors in determining the true identity. Therefore, we propose an identity-driven multimedia forgery dataset, IDForge, which contains 249,138 video shots sourced from 324 wild videos of 54 celebrities collected from the Internet. The fake video shots involve 9 types of manipulation across visual, audio, and textual modalities. Additionally, IDForge provides extra 214,438 real video shots as a reference set for the 54 celebrities. Correspondingly, we propose the Reference-assisted Multimodal Forgery Detection Network (R-MFDN), aiming at the detection of deepfake videos. Through extensive experiments on the proposed dataset, we demonstrate the effectiveness of R-MFDN on the multimedia detection task. The dataset is available at: https://github.com/xyyandxyy/IDForge.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {3887–3896},
numpages = {10},
keywords = {deepfake detection, forgery detection dataset, identity information},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3652037.3663956,
author = {Roy, Ayon and Karim, Enamul and Bin Farukee, Minhaz and Makedon, Fillia},
title = {ChatGPT as an Assistive Technology: Enhancing Human-Computer Interaction for People with Speech Impairments},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3663956},
doi = {10.1145/3652037.3663956},
abstract = {Communication challenges faced by individuals with speech impairments present a unique set of difficulties, often hindering effective interaction. This research paper centers on addressing these challenges by employing ChatGPT, a sophisticated large language model (LLM) developed by OpenAI, within the framework of Human-Computer Interaction (HCI). The study investigates the intricate landscape of speech impairments, emphasizing the inherent complexities in vocal expression. Our work highlights the pivotal role of ChatGPT in offering a tailored and adaptable communication solution.The paper highlights the contributions to HCI principles and assistive technologies, showcasing the innovative integration of ChatGPT. Emphasizing interdisciplinary collaboration, the study positions itself at the forefront of leveraging large language models to provide a comprehensive theoretical framework tailored to the unique needs of individuals with speech impairments.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {63–66},
numpages = {4},
keywords = {AI, Assistive Technologies, ChatGPT, HCI, Large Language Models, Speech Impairments},
location = {Crete, Greece},
series = {PETRA '24}
}

@inproceedings{10.1145/3626705.3627775,
author = {Gallo, Simone and Paterno, Fabio and Malizia, Alessio},
title = {Conversational Interfaces in IoT Ecosystems: Where We Are, What Is Still Missing},
year = {2023},
isbn = {9798400709210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626705.3627775},
doi = {10.1145/3626705.3627775},
abstract = {In the last few years, text and voice-based conversational agents have become more and more popular all over the world as virtual assistants for a variety of tasks. In addition, the deployment on the market of many smart objects connected with these agents has introduced the possibility of controlling and personalising the behaviour of several connected objects using natural language. This has the potential to allow people, also those without a technical background, to effectively control and use the wide variety of connected objects and services. In this paper, we present an analysis of how conversational agents have been used to interact with smart environments (such as smart homes). For this purpose, we have carried out a systematic literature review considering publications selected from the ACM and IEEE digital libraries to investigate the technologies used to design and develop conversational agents for IoT settings, including Artificial Intelligence techniques, the purpose that they have been used for, and the level of user involvement in such studies. The resulting analysis is useful to better understand how this field is evolving and indicate the challenges still open in this area that should be addressed in future research work to allow people to completely benefit from this type of solution.},
booktitle = {Proceedings of the 22nd International Conference on Mobile and Ubiquitous Multimedia},
pages = {279–293},
numpages = {15},
keywords = {Conversational Agents, Internet of Things, User Experience},
location = {Vienna, Austria},
series = {MUM '23}
}

@inproceedings{10.1145/3568739.3568801,
author = {Xiao, Zihui and Fan, Junjun and Gao, Wei},
title = {On the Construction of Knowledge-Communication-Oriented Computer Spoken Corpus for Endangered Languages},
year = {2023},
isbn = {9781450398091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568739.3568801},
doi = {10.1145/3568739.3568801},
abstract = {This Corpus/corpora is a computer database that stores language materials. Corpus in the world is dominated by lingua franca corpus. However, these corpora have limited samples and are difficult to meet the needs of machine artificial intelligence. The paper proposes to build a Knowledge-Communication-Oriented spoken corpus for Endangered Languages to enrich the content of corpus research. A Knowledge-Communication-Oriented Spoken Corpus was divided into three sub-corporas, the words sub-corpu, the sentences sub-corpus, and the narrative discourses sub-corpus. This paper mainly introduces the methods of constructing the spoken corpus of endangered languages from the aspects of corpus collection, corpus arrangement and corpus annotation.},
booktitle = {Proceedings of the 6th International Conference on Digital Technology in Education},
pages = {369–374},
numpages = {6},
keywords = {Theory of computation, Regular languages, Formal languages and automata theory, Endangered language, Corpus annotation, Corpus, Computational linguistics},
location = {Hangzhou, China},
series = {ICDTE '22}
}

@article{10.1145/3616012,
author = {Zhang, Weizhao and Yang, Hongwu},
title = {Improving Sequence-to-sequence Tibetan Speech Synthesis with Prosodic Information},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3616012},
doi = {10.1145/3616012},
abstract = {There are about 6,000 languages worldwide, most of which are low-resource languages. Although the current speech synthesis (or text-to-speech, TTS) for major languages (e.g., Mandarin, English, French) has achieved good results, the voice quality of TTS for low-resource languages (e.g., Tibetan) still needs to be further improved. Because prosody plays a significant role in natural speech, the article proposes two sequence-to-sequence (seq2seq) Tibetan TTS models with prosodic information fusion to improve the voice quality of synthesized Tibetan speech. We first constructed a large-scale Tibetan corpus for seq2seq TTS. Then we designed a prosody generator to extract prosodic information from the Tibetan sentences. Finally, we trained two seq2seq Tibetan TTS models by fusing prosodic information, including feature-level and model-level prosodic information fusion. The experimental results showed that the proposed two seq2seq Tibetan TTS models, which fuse prosodic information, could effectively improve the voice quality of synthesized speech. Furthermore, the model-level prosodic information fusion only needs 60% ~ 70% of the training data to synthesize a voice similar to the baseline seq2seq Tibetan TTS. Therefore, the proposed prosodic information fusion methods can improve the voice quality of synthesized speech for low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {225},
numpages = {13},
keywords = {low-resource language, prosodic information fusion, Tibetan speech synthesis, Sequence-to-sequence speech synthesis}
}

@inproceedings{10.1145/3561877.3561889,
author = {Zhang, Liping and Yu, Kefeng and Tong, Jiangang and Li, Man and Hu, Yingmao and Wu, Ying},
title = {Research on Intelligent Customer Service Application Architecture Based on Digital Transformation},
year = {2022},
isbn = {9781450396837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561877.3561889},
doi = {10.1145/3561877.3561889},
abstract = {With the application of emerging digital information technologies such as big data, artificial intelligence and cloud computing becoming more and more mature, all walks of life are accelerating the transformation to digitization. Taking the application of China Telecom intelligent customer service as an example, this paper introduces the typical application of China Telecom customer service system in digital transformation in detail. Based on the functions of the existing customer service system, this paper puts forward the functional architecture and implementation planning scheme of intelligent application by optimizing the key modules of intelligent voice navigation. Then, combined with experimental cases, the effectiveness and feasibility of this system are verified. Finally, the future development direction is summarized and discussed. The research results of this paper strengthen the correlation between intelligent customer service application and artificial customer service, and improve the work efficiency of artificial customer service. At the same time, this paper not only provides an empirical basis for the research of enterprise application digital transformation, but also provides new insights for the optimization of other application systems.},
booktitle = {Proceedings of the 5th International Conference on Information Science and Systems},
pages = {77–82},
numpages = {6},
keywords = {Intelligent voice, Intelligent customer service, Digital transformation, China Telecom},
location = {Beijing, China},
series = {ICISS '22}
}

@inproceedings{10.1145/3607720.3607727,
author = {Oukrich, Nadia and Tamega, Bougary and Laaz, Naziha},
title = {Matia Application: An AI Multi-Lingual Assistant For Visually Impaired And Blind People},
year = {2023},
isbn = {9798400700194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607720.3607727},
doi = {10.1145/3607720.3607727},
abstract = {The World Health Organization (WHO) estimates that at least 2.2 billion people suffer from some form of vision impairment. This widespread problem has resulted in a staggering $411 billion loss of productivity per year. To tackle this issue, researchers have employed IoT devices to aid visually impaired individuals in navigating their environments, but these solutions can be both inconvenient and costly. Mobile technology, on the other hand, has the potential to provide a more cost-effective and user-friendly solution. However, current mobile applications for the visually impaired only support English or French and require an internet connection to use. To address these limitations, a new mobile application named "Matia" has been developed to offer a solution in three languages: Arabic, French, and English, without the need for IoT devices or an internet connection. This paper describes the design and implementation of Matia application. The application's accuracy in recognizing five datasets (clothes, fruits, house equipment, pets, and banknotes) ranges from 74% to 90%, which is higher than existing literature. Further functionalities, such as guidance assistance and danger recognition, are under development to make the application even more useful in various situations.},
booktitle = {Proceedings of the 6th International Conference on Networking, Intelligent Systems &amp; Security},
articleno = {6},
numpages = {7},
location = {Larache, Morocco},
series = {NISS '23}
}

@inproceedings{10.1145/3469261.3469405,
author = {Woszczyk, Dominika and Lee, Alvin and Demetriou, Soteris},
title = {Open, Sesame! Introducing Access Control to Voice Services},
year = {2021},
isbn = {9781450386012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469261.3469405},
doi = {10.1145/3469261.3469405},
abstract = {Personal voice assistants (VAs) are shown to be vulnerable against record-and-replay, and other acoustic attacks which allow an adversary to gain unauthorized control of connected devices within a smart home. Existing defenses either lack detection and management capabilities or are too coarse-grained to enable flexible policies on par with other computing interfaces. In this work, we present Sesame, a lightweight framework for edge devices which is the first to enable fine-grained access control of smart-home voice commands. Sesame, combines three components: Automatic Speech Recognition, Natural Language Understanding (NLU) and a Policy module. We implemented Sesame on Android devices and demonstrate that our system can enforce security policies for both Alexa and Google Home in real-time (362ms end-to-end inference time), with a lightweight (&lt;25MB) NLU model which exhibits minimal accuracy loss compared to its non-compact equivalent.},
booktitle = {Proceedings of the 1st Workshop on Security and Privacy for Mobile AI},
pages = {7–12},
numpages = {6},
keywords = {voice services, smart home, google home, alexa, access control},
location = {Virtual, WI, USA},
series = {MAISP'21}
}

@article{10.1145/3419368,
author = {Preum, Sarah Masud and Munir, Sirajum and Ma, Meiyi and Yasar, Mohammad Samin and Stone, David J. and Williams, Ronald and Alemzadeh, Homa and Stankovic, John A.},
title = {A Review of Cognitive Assistants for Healthcare: Trends, Prospects, and Future Directions},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3419368},
doi = {10.1145/3419368},
abstract = {Healthcare cognitive assistants (HCAs) are intelligent systems or agents that interact with users in a context-aware and adaptive manner to improve their health outcomes by augmenting their cognitive abilities or complementing a cognitive impairment. They assist a wide variety of users ranging from patients to their healthcare providers (e.g., general practitioner, specialist, surgeon) in several situations (e.g., remote patient monitoring, emergency response, robotic surgery). While HCAs are critical to ensure personalized, scalable, and efficient healthcare, there exists a knowledge gap in finding the emerging trends, key challenges, design guidelines, and state-of-the-art technologies suitable for developing HCAs. This survey aims to bridge this gap for researchers from multiple domains, including but not limited to cyber-physical systems, artificial intelligence, human-computer interaction, robotics, and smart health. It provides a comprehensive definition of HCAs and outlines a novel, practical categorization of existing HCAs according to their target user role and the underlying application goals. This survey summarizes and assorts existing HCAs based on their characteristic features (i.e., interactive, context-aware, and adaptive) and enabling technological aspects (i.e., sensing, actuation, control, and computation). Finally, it identifies critical research questions and design recommendations to accelerate the development of the next generation of cognitive assistants for healthcare.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {130},
numpages = {37},
keywords = {virtual assistant, virtual agent, smart health, personal assistant, intelligent assistant, intelligent agent, healthcare application, agent based systems for healthcare, Cognitive assistant}
}

@inproceedings{10.1145/3689217.3690615,
author = {Zhang, Zhisheng and Yang, Qianyi and Wang, Derui and Huang, Pengyang and Cao, Yuxin and Ye, Kai and Hao, Jie},
title = {Mitigating Unauthorized Speech Synthesis for Voice Protection},
year = {2024},
isbn = {9798400712098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689217.3690615},
doi = {10.1145/3689217.3690615},
abstract = {With just a few speech samples, it is possible to perfectly replicate a speaker's voice in recent years, while malicious voice exploitation ( e.g., telecom fraud for illegal financial gain) has brought huge hazards in our daily lives. Therefore, it is crucial to protect publicly accessible speech data that contains sensitive information, such as personal voiceprints. Most previous defense methods have focused on spoofing speaker verification systems in timbre similarity but the synthesized deepfake speech is still of high quality. In response to the rising hazards, we devise an effective, transferable, and robust proactive protection technology named Pivotal Objective Perturbation (POP) that applies imperceptible error-minimizing noises on original speech samples to prevent them from being effectively learned for text-to-speech (TTS) synthesis models so that high-quality deepfake speeches cannot be generated. We conduct extensive experiments on state-of-the-art (SOTA) TTS models utilizing objective and subjective metrics to comprehensively evaluate our proposed method. The experimental results demonstrate outstanding effectiveness and transferability across various models. Compared to the speech unclarity score of 21.94% from voice synthesizers trained on samples without protection, POP-protected samples significantly increase it to 127.31%. Moreover, our method shows robustness against noise reduction and data augmentation techniques, thereby greatly reducing potential hazards.},
booktitle = {Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis},
pages = {13–24},
numpages = {12},
keywords = {speech synthesis, unlearnable examples, voice protection},
location = {Salt Lake City, UT, USA},
series = {LAMPS '24}
}

@article{10.1145/3433607,
author = {Hair, Adam and Ballard, Kirrie J. and Markoulli, Constantina and Monroe, Penelope and Mckechnie, Jacqueline and Ahmed, Beena and Gutierrez-Osuna, Ricardo},
title = {A Longitudinal Evaluation of Tablet-Based Child Speech Therapy with Apraxia World},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3433607},
doi = {10.1145/3433607},
abstract = {Digital games can make speech therapy exercises more enjoyable for children and increase their motivation during therapy. However, many such games developed to date have not been designed for long-term use. To address this issue, we developed Apraxia World, a speech therapy game specifically intended to be played over extended periods. In this study, we examined pronunciation improvements, child engagement over time, and caregiver and automated pronunciation evaluation accuracy while using our game over a multi-month period. Ten children played Apraxia World at home during two counterbalanced 4-week treatment blocks separated by a 2-week break. In one treatment phase, children received pronunciation feedback from caregivers and in the other treatment phase, utterances were evaluated with an automated framework built into the game. We found that children made therapeutically significant speech improvements while using Apraxia World, and that the game successfully increased engagement during speech therapy practice. Additionally, in offline mispronunciation detection tests, our automated pronunciation evaluation framework outperformed a traditional method based on goodness of pronunciation scoring. Our results suggest that this type of speech therapy game is a valid complement to traditional home practice.},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {3},
numpages = {26},
keywords = {speech sound disorders (SSDs), serious games, computer-aided pronunciation training (CAPT), childhood apraxia of speech (CAS), Games for health}
}

@article{10.1145/3659600,
author = {Xu, Zhenyu and Xu, Hailin and Lu, Zhouyang and Zhao, Yingying and Zhu, Rui and Wang, Yujiang and Dong, Mingzhi and Chang, Yuhu and Lv, Qin and Dick, Robert P. and Yang, Fan and Lu, Tun and Gu, Ning and Shang, Li},
title = {Can Large Language Models Be Good Companions? An LLM-Based Eyewear System with Conversational Common Ground},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659600},
doi = {10.1145/3659600},
abstract = {Developing chatbots as personal companions has long been a goal of artificial intelligence researchers. Recent advances in Large Language Models (LLMs) have delivered a practical solution for endowing chatbots with anthropomorphic language capabilities. However, it takes more than LLMs to enable chatbots that can act as companions. Humans use their understanding of individual personalities to drive conversations. Chatbots also require this capability to enable human-like companionship. They should act based on personalized, real-time, and time-evolving knowledge of their users. We define such essential knowledge as the common ground between chatbots and their users, and we propose to build a common-ground-aware dialogue system from an LLM-based module, named OS-1, to enable chatbot companionship. Hosted by eyewear, OS-1 can sense the visual and audio signals the user receives and extract real-time contextual semantics. Those semantics are categorized and recorded to formulate historical contexts from which the user's profile is distilled and evolves over time, i.e., OS-1 gradually learns about its user. OS-1 combines knowledge from real-time semantics, historical contexts, and user-specific profiles to produce a common-ground-aware prompt input into the LLM module. The LLM's output is converted to audio, spoken to the wearer when appropriate. We conduct laboratory and in-field studies to assess OS-1's ability to build common ground between the chatbot and its user. The technical feasibility and capabilities of the system are also evaluated. Our results show that by utilizing personal context, OS-1 progressively develops a better understanding of its users. This enhances user satisfaction and potentially leads to various personal service scenarios, such as emotional support and assistance.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {87},
numpages = {41},
keywords = {Smart eyewear, common ground, context-aware, large language model}
}

@inproceedings{10.1145/3708359.3712145,
author = {Ho, Hoang Phuoc and Ramesh, Vani and Zaloudek, Ivo and Rikhtehgar, Delaram Javdani and Wang, Shenghui},
title = {Enhancing Visitor Engagement in Interactive Art Exhibitions with Visual-Enhanced Conversational Agents},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712145},
doi = {10.1145/3708359.3712145},
abstract = {Conversational agents in art exhibitions can enhance user engagement and understanding of artworks by providing contextual information, especially through voice interactions. However, creating a deeper personal connection with art — which often requires direct aesthetic and visual experiences — remains a challenge. This paper examines how integrating visual perception into conversational agents can enhance alignment with visitors’ artistic interpretations, thereby fostering deeper engagement with interactive art exhibitions. We introduce a voice-based conversational agent enhanced with visual capabilities via a multimodal large language model (MLLM), allowing the agent to perceive, interpret and discuss artworks in real-time with visitors. The system utilizes a simplified Retrieval-Augmented Generation (RAG) architecture, which collects voice inputs, retrieves relevant information from a domain knowledge graph, and uses the LLM to generate conversational responses, which are then converted into voice outputs. A user study with 36 participants, divided into two groups, was conducted to compare the enhanced system with a baseline system that lacked visual input. Our results show that the visually enhanced system significantly improved visitor engagement and perception. Content analysis of the conversational transcripts further revealed a wider range of conversational topics, deeper visitor perceptions, and the agent’s ability to provide more nuanced, visually-related discussions.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {660–671},
numpages = {12},
keywords = {Conversational Agents, Voice User Interfaces, Multimodal Interaction, Large Language Models, Human-Computer Interaction, User Engagement, Cultural Heritage},
location = {
},
series = {IUI '25}
}

@article{10.1109/TASLP.2024.3426978,
author = {Mishra, Jagabandhu and Prasanna, S. R. Mahadeva},
title = {Implicit Self-Supervised Language Representation for Spoken Language Diarization},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3426978},
doi = {10.1109/TASLP.2024.3426978},
abstract = {The use of spoken language diarization (LD) as a preprocessing system might be essential in a code-switched (CS) scenario. Furthermore, implicit frameworks are preferable to explicit ones, as implicit frameworks can be easily adapted to deal with low/zero resource languages. Inspired by speaker diarization literature, three frameworks based on (a) fixed segmentation, (b) change-point-based segmentation, and (c) end-to-end (E2E) are used in this study to perform LD. The initial exploration in the constructed text-to-speech female language diarization (TTSF-LD) dataset shows, that using the x-vector as implicit language representation with appropriate analysis window length achieves, comparable performance to explicit LD. The best implicit LD performance of 6.4% in terms of Jaccard error rate (JER) is achieved by using the E2E framework. However, using the natural Microsoft CS dataset, the performance of the E2E implicit LD degrades to 60.4% JER. The performance degradation is due to the inability of the x-vector representation to capture language-specific traits. To address this shortcoming, a self-supervised implicit language representation framework is used in this study. Compared to the x-vector representation, the self-supervised representation yields a relative improvement of 63.9%, achieving a JER of 21.8% when used in conjunction with the E2E framework.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {3393–3407},
numpages = {15}
}

@article{10.1109/TASLP.2022.3172632,
author = {Oglic, Dino and Cvetkovic, Zoran and Sollich, Peter and Renals, Steve and Yu, Bin},
title = {Towards Robust Waveform-Based Acoustic Models},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3172632},
doi = {10.1109/TASLP.2022.3172632},
abstract = {We study the problem of learning robust acoustic models in adverse environments, characterized by a significant mismatch between training and test conditions. This problem is of paramount importance for the deployment of speech recognition systems that need to perform well in unseen environments. First, we characterize data augmentation theoretically as an instance of vicinal risk minimization, which aims at improving risk estimates during training by replacing the delta functions that define the empirical density over the input space with an approximation of the marginal population density in the vicinity of the training samples. More specifically, we assume that local neighborhoods centered at training samples can be approximated using a mixture of Gaussians, and demonstrate theoretically that this can incorporate robust inductive bias into the learning process. We then specify the individual mixture components implicitly via data augmentation schemes, designed to address common sources of spurious correlations in acoustic models. To avoid potential confounding effects on robustness due to information loss, which has been associated with standard feature extraction techniques (e.g., &lt;sc&gt;fbank&lt;/sc&gt; and &lt;sc&gt;mfcc&lt;/sc&gt; features), we focus on the waveform-based setting. Our empirical results show that the approach can generalize to unseen noise conditions, with 150% relative improvement in out-of-distribution generalization compared to training using the standard risk minimization principle. Moreover, the results demonstrate competitive performance relative to models learned using a training sample designed to match the acoustic conditions characteristic of test utterances.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1977–1992},
numpages = {16}
}

@inproceedings{10.1145/3658644.3690311,
author = {Doan, Thien-Phuc and Dinh-Xuan, Hung and Ryu, Taewon and Kim, Inho and Lee, Woongjae and Hong, Kihun and Jung, Souhwan},
title = {Trident of Poseidon: A Generalized Approach for Detecting Deepfake Voices},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690311},
doi = {10.1145/3658644.3690311},
abstract = {Deepfakes, an increasingly prevalent form of information attack, pose serious threats to security and privacy. Deepfake voice attacks, in particular, have the potential to cause widespread disruption, creating an urgent need for an effective detection system. In this research, we propose the Trident of Poseidon - a novel set of triad training strategies aimed at enhancing the generalizability of deepfake voice detection models. Our solution comprises three key components: (1) Supervised Contrastive Learning, (2) Hard Negative Mining by Audio Re-synthesizing, and (3) Effective Proactive Batch Sampling. Together, these enable the model to learn more robust features. Our extensive experiments demonstrate that our approach outperforms existing methods in both in-domain and out-of-domain testing scenarios, making significant strides toward securing digital media against deepfake voice attacks.Furthermore, we conducted a deeper analysis to explore whether deepfake voices can be categorized into families. By identifying the factors that contribute to the formation of a deepfake voice family, we can better organize a deepfake voice corpus, thereby reducing the effort needed to combat the arms race challenge. Finally, to promote practical utility and community-wide adoption, we have made our solution publicly available as a web application available on deepfake.aisrc.technology, where users can utilize this tool to test for potential deepfake voices.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2222–2235},
numpages = {14},
keywords = {deepfake voice detection, domain generalization, speech synthesis},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3507473.3507484,
author = {Li, Chenran and Wang, Pengju and Li, Wenjing and Ma, Zixuan and liu, Hanran and Liu, Dayan},
title = {Research on Key Information Recognition System Based on Bayesian Classification},
year = {2022},
isbn = {9781450385213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507473.3507484},
doi = {10.1145/3507473.3507484},
abstract = {In order to improve the efficiency of image content retrieval, combined with computer vision technology, an intelligent key information detection and recognition system applied to images is proposed. The system is divided into four parts: positioning the text area, detecting key information, recognizing key information and broadcasting voice. Firstly, the image is preprocessed to obtain a clear binary image. Secondly, the dynamic line segmentation algorithm is used to obtain the candidate text areas of the image, and the non-text areas are filtered out with the revelatory rule. Then, the Bayesian classification algorithm is used to discriminate the candidate text region to obtain the key information. Finally, the image of key information is converted into editable text through Optical Character Recognition technology, and the text is converted into speech using text-to-speech conversion technology. The experimental results show that the recognition system of key information is simple and efficient, and the classification accuracy is good, which is suitable for image key information recognition.},
booktitle = {Proceedings of the 2021 3rd International Conference on Software Engineering and Development},
pages = {71–75},
numpages = {5},
keywords = {key information recognition, key information detection, Dynamic line segmentation, Bayesian classification},
location = {Xiamen, China},
series = {ICSED '21}
}

@inproceedings{10.1145/3595916.3626406,
author = {Zhang, Peng and Chen, Yida and Li, Meijuan and Zhao, Hui and Zhang, Jianqiang and Wang, Fuqiang and Wu, Xiaoming},
title = {Speech Spoofing Detection Based on Graph Attention Networks with Spectral and Temporal Information},
year = {2024},
isbn = {9798400702051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3595916.3626406},
doi = {10.1145/3595916.3626406},
abstract = {Automatic speaker verification (ASV) systems are vulnerable to synthetic speech attacks. Synthetic algorithms usually introduce artifacts in specific sub-bands or time segments. However, under unknown spoofing attacks, it is challenging to choose the right domain for effective detection. In this paper, we propose a speech spoofing detection method based on graph attention networks with spectral and temporal information. First, high-level features of raw audio are extracted using SENet channel attention to enhance the spatial correlation between speech frames. Then, spectral graph and temporal graph are constructed for the high-level features using graph attention networks. Finally, we design a new heterogeneous multi-domain co-graph attention module to process the information from different domains for effective speech spoofing detection. The proposed model was evaluated on the ASVspoof 2019 dataset and obtains a min t-DCF of 0.0264 and an EER of 0.94%, exhibiting competitive performance. Experiments also show its effectiveness when detecting unknown types of attacks.},
booktitle = {Proceedings of the 5th ACM International Conference on Multimedia in Asia},
articleno = {34},
numpages = {7},
keywords = {anti-spoofing, attention mechanism., graph attention networks, speech spoofing detection},
location = {Tainan, Taiwan},
series = {MMAsia '23}
}

@inproceedings{10.1145/3675094.3678382,
author = {Janaka, Nuwan and Zhao, Shengdong and Hsu, David and Tan Jing Wen, Sherisse and Koh, Chun Keat},
title = {TOM: A Development Platform For Wearable Intelligent Assistants},
year = {2024},
isbn = {9798400710582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3675094.3678382},
doi = {10.1145/3675094.3678382},
abstract = {Advanced wearable digital assistants can significantly enhance task performance, reduce user burden, and provide personalized guidance to improve users' abilities. However, developing these assistants presents several challenges. To address this, we introduce TOM (The Other Me), a conceptual architecture and open-source software platform (https://github.com/TOM-Platform) that supports the development of wearable intelligent assistants that are contextually aware of both the user and the environment. Collaboratively developed with researchers and developers, TOM meets their diverse requirements. TOM facilitates the creation of intelligent assistive AR applications for daily activities, supports the recording and analysis of user interactions, and provides assistance for various activities, as demonstrated in our preliminary evaluations.},
booktitle = {Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {837–843},
numpages = {7},
keywords = {ai assistance, ai/ml, ar, augmented reality, context-aware system, hmd, interactions, mr, smart glasses, wearable, xr},
location = {Melbourne VIC, Australia},
series = {UbiComp '24}
}

@inproceedings{10.1145/3440084.3441194,
author = {Matul\'{\i}k, Martin and Vavre\v{c}ka, Michal and Vidovi\'{c}ov\'{a}, Lucie},
title = {Edutainment Software for the Pepper Robot},
year = {2021},
isbn = {9781450388894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440084.3441194},
doi = {10.1145/3440084.3441194},
abstract = {We present our software for the Pepper robot that is suitable for human-robot interaction in the area of edutainment. The combination of education and entertainment is achieved by modifying a state-of-the-art conversational artificial intelligence system, developing several interactive quiz applications for the robot and implementing these into Pepper, a humanoid robot. In the paper, we describe the technical details of the chatbot implementation and also the description of the software for the edutainment.},
booktitle = {Proceedings of the 2020 4th International Symposium on Computer Science and Intelligent Control},
articleno = {18},
numpages = {5},
keywords = {Social Robots, Humanoid Robots, HRI, Edutainment, Chatbot},
location = {Newcastle upon Tyne, United Kingdom},
series = {ISCSIC 2020}
}

@inproceedings{10.1145/3539618.3591876,
author = {Qu, Xinghua and Liu, Hongyang and Sun, Zhu and Yin, Xiang and Ong, Yew Soon and Lu, Lu and Ma, Zejun},
title = {Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions and Prospects},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591876},
doi = {10.1145/3539618.3591876},
abstract = {Conversational recommender systems (CRSs) have become crucial emerging research topics in the field of RSs, thanks to their natural advantages of explicitly acquiring user preferences via interactive conversations and revealing the reasons behind recommendations. However, the majority of current CRSs are text-based, which is less user-friendly and may pose challenges for certain users, such as those with visual impairments or limited writing and reading abilities. Therefore,for the first time, this paper investigates the potential of voice-based CRS (VCRSs) to revolutionize the way users interact with RSs in a natural, intuitive, convenient, and accessible fashion. To support such studies, we create two VCRSs benchmark datasets in the e-commerce and movie domains, after realizing the lack of such datasets through an exhaustive literature review. Specifically, we first empirically verify the benefits and necessity of creating such datasets. Thereafter, we convert the user-item interactions to text-based conversations through the ChatGPT-driven prompts for generating diverse and natural templates, and then synthesize the corresponding audios via the text-to-speech model. Meanwhile, a number of strategies are delicately designed to ensure the naturalness and high quality of voice conversations. On this basis, we further explore the potential solutions and point out possible directions to build end-to-end VCRSs by seamlessly extracting and integrating voice-based inputs, thus delivering performance-enhanced, self-explainable, and user-friendly VCRSs. Our study aims to establish the foundation and motivate further pioneering research in the emerging field of VCRSs. This aligns with the principles of explainable AI and AI for social good, viz., utilizing technology's potential to create a fair, sustainable, and just world. Our codes and datasets are available on GitHub (https://github.com/hyllll/VCRS ).},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2701–2711},
numpages = {11},
keywords = {ai for social good, conversational recommender systems, explainable ai, voice-based recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.5555/3721488.3721593,
author = {Skantze, Gabriel and Irfan, Bahar},
title = {Applying General Turn-taking Models to Conversational Human-Robot Interaction},
year = {2025},
publisher = {IEEE Press},
abstract = {Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {859–868},
numpages = {10},
keywords = {conversational ai, human-robot interaction, large language model, turn-taking},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1145/3478101,
author = {Wang, Dawei and Chen, Kai and Wang, Wei},
title = {Demystifying the Vetting Process of Voice-controlled Skills on Markets},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3478101},
doi = {10.1145/3478101},
abstract = {Smart speakers, such as Google Home and Amazon Echo, have become popular. They execute user voice commands via their built-in functionalities together with various third-party voice-controlled applications, called skills. Malicious skills have brought significant threats to users in terms of security and privacy. As a countermeasure, only skills passing the strict vetting process can be released onto markets. However, malicious skills have been reported to exist on markets, indicating that the vetting process can be bypassed. This paper aims to demystify the vetting process of skills on main markets to discover weaknesses and protect markets better. To probe the vetting process, we carefully design numerous skills, perform the Turing test, a test for machine intelligence, to determine whether humans or machines perform vetting, and leverage natural language processing techniques to analyze their behaviors. Based on our comprehensive experiments, we gain a good understanding of the vetting process (e.g., machine or human testers and skill exploration strategies) and discover some weaknesses. In this paper, we design three types of attacks to verify our results and prove an attacker can embed sensitive behaviors in skills and bypass the strict vetting process. Accordingly, we also propose countermeasures to these attacks and weaknesses.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {130},
numpages = {28},
keywords = {vetting process, Skill, Google-Home, Alexa}
}

@inproceedings{10.1145/3491102.3517625,
author = {Schmidt, Susanne and Zimmermann, Sven and Mason, Celeste and Steinicke, Frank},
title = {Simulating Human Imprecision in Temporal Statements of Intelligent Virtual Agents},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517625},
doi = {10.1145/3491102.3517625},
abstract = {Research on intelligent virtual agents (IVAs) often concerns the implementation of human-like behavior by integrating artificial intelligence algorithms. Thus far, few studies focused on mimicry of cognitive imperfections inherent to humans in IVAs. Neglecting to implement such imperfect behavior in IVAs might result in less believable or engaging human-agent interactions. In this paper, we simulate human imprecision in conversational IVAs’ temporal statements. We conducted a survey to identify temporal statement patterns, transferred them to a conversational IVA, and conducted a user study evaluating the effects of time precision on perceived anthropomorphism and usefulness. Statistical analyses reveal significant interaction between time precision and agents’ use of memory aids, indicating that (i) imprecise agents are perceived as more human-like than precise agents when responding immediately, and (ii) unnaturally high levels of temporal precision can be compensated for by memory aid use. Further findings underscore the value of continued inquiry into cultural variations.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {422},
numpages = {15},
keywords = {conversational agents, intelligent virtual agents, time perception},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3664647.3680777,
author = {Zhang, Zhedong and Li, Liang and Cong, Gaoxiang and Yin, Haibing and Gao, Yuhan and Yan, Chenggang and Hengel, Anton van den and Qi, Yuankai},
title = {From Speaker to Dubber: Movie Dubbing with Prosody and Duration Consistency Learning},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680777},
doi = {10.1145/3664647.3680777},
abstract = {Movie Dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of one brief reference audio. The wide variations in emotion, pace, and environment that dubbed speech must exhibit to achieve real alignment make dubbing a complex task. Considering the limited scale of the movie dubbing datasets (due to copyright) and the interference from background noise, directly learning from movie dubbing datasets limits the pronunciation quality of learned models. To address this problem, we propose a two-stage dubbing method that allows the model to first learn pronunciation knowledge before practicing it in movie dubbing. In the first stage, we introduce a multi-task approach to pre-train a phoneme encoder on a large-scale text-speech corpus for learning clear and natural phoneme pronunciations. For the second stage, we devise a prosody consistency learning module to bridge the emotional expression with the phoneme-level dubbing prosody attributes (pitch and energy). Finally, we design a duration consistency reasoning module to align the dubbing duration with the lip movement. Extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. The demos are available at https://speaker2dubber.github.io/.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {7523–7532},
numpages = {10},
keywords = {movie dubbing, two-stage framework, visual voice cloning},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@inproceedings{10.1145/3652988.3673965,
author = {Elfleet, Morad and Chollet, Mathieu},
title = {Investigating the Impact of Multimodal Feedback on User-Perceived Latency and Immersion with LLM-Powered Embodied Conversational Agents in Virtual Reality},
year = {2024},
isbn = {9798400706257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652988.3673965},
doi = {10.1145/3652988.3673965},
abstract = {Our research investigates the impact of latency on presence and immersion in virtual reality (VR) environments, focusing on interactions with LLM-powered Embodied Conversational Agents (ECAs). We explore the effectiveness of multimodal feedback strategies—including filled pauses, nonverbal turn-taking behaviours, and visual feedback—in mitigating perceived latency. Eighteen participants were subjected to both a baseline condition, without feedback interventions, and a feedback-enhanced condition. Our findings indicate that the feedback condition significantly improved the sense of presence and immersion. We also found that perceived response time and users’ impressions of the agents improved, thereby increasing willingness for future interactions. Additionally, chatbot experience positively correlated with agent likeability, whereas VR experience showed no significant correlation. These results highlight the effectiveness of feedback modalities in enhancing spatial presence and overall immersion, despite latency issues in VR interactions with LLM-powered agents.},
booktitle = {Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents},
articleno = {12},
numpages = {9},
keywords = {Embodied Conversational Agents, Latency, Presence and Immersion, Virtual Reality},
location = {GLASGOW, United Kingdom},
series = {IVA '24}
}

@inproceedings{10.1145/3652988.3673916,
author = {Cumbal, Ronald and Kantharaju, Reshmashree and Paetzel-Pr\"{u}smann, Maike and Kennedy, James},
title = {Let Me Finish First - The Effect of Interruption-Handling Strategy on the Perceived Personality of a Social Agent},
year = {2024},
isbn = {9798400706257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652988.3673916},
doi = {10.1145/3652988.3673916},
abstract = {This paper presents an experiment with three artificial agents adopting different strategies when being interrupted by human conversational partners. The agent either ignored the interruption (the most common behavior in conversational engines to date), yielded the turn to the human conversational partner right away, or acknowledged the interruption, finished its thought and then responded to the content of the interruption. Our results show that this change in the agent’s conversational behavior had a significant impact on which personality traits people assigned to the agent, as well as how much they enjoyed interacting with it. Moreover, the data also indicates that human interlocutors adapted their own conversational behavior. Our findings suggest that the interactive behavior of an artificial agent should be carefully designed to match its desired personality and the intended conversational dynamics.},
booktitle = {Proceedings of the 24th ACM International Conference on Intelligent Virtual Agents},
articleno = {14},
numpages = {10},
keywords = {Barge-in, Group interaction, Overlapping speech, Speech interruption, Spoken dialogue system},
location = {GLASGOW, United Kingdom},
series = {IVA '24}
}

@inproceedings{10.1145/3560905.3568500,
author = {Yan, Zhenyu and Tan, Rui and Song, Qun and Lu, Chris Xiaoxuan},
title = {Telesonar: Robocall Alarm System by Detecting Echo Channel and Breath Timing},
year = {2023},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3560905.3568500},
doi = {10.1145/3560905.3568500},
abstract = {Massive fraudulent and phishing robocalls present threats to societies. The integration of artificial intelligence technologies, including dialogue and voice generation systems, renders the robocalls more deceptive. Existing countermeasures such as caller ID, call provenance, voiceprint, and fake voice detection have respective limitations and are heavyweight for end users' smartphones. This paper studies detecting the acoustic echo channel on the remote end of a call based on the received voice. The positive detection result evidencing the physical setup of an audio system is indicative of a human caller. However, the acoustic echo cancellation mechanisms of most audio systems and the use of earphone/headset diminish echoes significantly. To address these issues, the proposed Telesonar transmits short chirps during the vulnerable time of echo cancellation, detects the tiny echo remnants from the received voice, and passively analyzes the timing of caller's breath sounds to confirm a human caller. Extensive real experiments under a wide range of settings show that Telesonar correctly recognizes human callers with a rate of over 95%, while wrongly recognizing voice robots as human with a rate of 3.8%.},
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
pages = {61–74},
numpages = {14},
keywords = {internet-of-things systems, mobile systems, robocall detection},
location = {Boston, Massachusetts},
series = {SenSys '22}
}

@article{10.1145/3568308,
author = {Kaur, Navdeep and Singh, Parminder},
title = {Modelling of Speech Parameters of Punjabi by Pre-trained Deep Neural Network Using Stacked Denoising Autoencoders},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3568308},
doi = {10.1145/3568308},
abstract = {Statistical parametric speech synthesis techniques such as deep neural network (DNN) and hidden Markov model (HMM) have grown in popularity since last decade over the concatenative speech synthesis approaches by modelling excitation and spectral parameters of speech to synthesize the waveforms from the written text. Due to inappropriate acoustic modelling, speech synthesized using HMM-based speech synthesis sounds muffled. DNN tried to improve the acoustic model by replacing decision trees in HMM with powerful regression model. Further, the performance of a deep neural network is greatly enhanced using pre-learning either restricted Boltzmann machines (RBM) or autoencoders. RBMs are capable to map multi-modal property of speech but result in spectral distortion of synthesized speech waveforms as non-consideration of reconstruction error. This article proposed the model of deep neural network, which is pre-trained using stacked denoising autoencoders to map speech parameters of the Punjabi language. Denoising autoencoders work by adding noise in the training data and then reconstructing the original measurements to reduce the reconstruction error. The synthesized voice using the proposed model showed the VARN of 0.82, F0 RMSE (Hz) 9.03, and V/UV error rate of 4.04% have been observed.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {86},
numpages = {17},
keywords = {reconstruction error, restricted Boltzmann machines, stacked denoising auto-encoders, Pre-training deep neural network}
}

@inproceedings{10.5555/3721488.3721514,
author = {Koike, Amy and Okafuji, Yuki and Hoshimure, Kenya and Baba, Jun},
title = {What Drives You to Interact?: The Role of User Motivation for a Robot in the Wild},
year = {2025},
publisher = {IEEE Press},
abstract = {In this paper, we aim to understand how user motivation shapes human-robot interaction (HRI) in the wild. To explore this, we conducted a field study by deploying a fully autonomous conversational robot in a shopping mall over two days. Through sequential video analysis, we identified five patterns of interaction fluency (Smooth, Awkward, Active, Messy, and Quiet), four types of user motivation for interacting with the robot (Function, Experiment, Curiosity, and Education), and user positioning towards the robot. We further analyzed how these motivations and positioning influence interaction fluency. Our findings suggest that incorporating users' motivation types into the design of robot behavior can enhance interaction fluency, engagement, and user satisfaction in real-world HRI scenarios.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {183–192},
numpages = {10},
keywords = {field experiment, human-robot interaction, qualitative analysis, service robots, social robots},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3617733.3617750,
author = {Jadaone, Jerico G. and Loristo, Karl Marco G. and Oledan, Justin V. and Saavedra, Therese Angel G. and Domingo, Iluminada Vivien R.},
title = {Aralin Baybayin: Intelligent Tutoring System for Baybayin Scripts with Character Recognition},
year = {2023},
isbn = {9798400707735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617733.3617750},
doi = {10.1145/3617733.3617750},
abstract = {Aralin Baybayin is an Intelligent Tutoring System (ITS) designed to help those interested in learning the Baybayin Script, such as historians, foreigners, and fellow Filipinos. It provides an efficient and convenient way to learn how to read and write the Baybayin Script. The proponents assessed the quality of the software and system product using ISO 25010 and the motivational experience of users with 130 respondents consisting of IT and Baybayin Experts and third year students studying Bachelor of Science in Information Technology at Polytechnic University of the Philippines. The proponents are able to deliver a learning motivational experience for the users with a verbal interpretation of agree. The system was also able to utilize organized, progressive, and interactive lessons for the users. Aralin Baybayin also met the user's acceptance through the use of ISO 25010 with a verbal interpretation of agree. For optical character recognition, through training the datasets Aralin Baybayin has proved that CNN models are suitable for Baybayin character recognition with a result of final training accuracy of 99.16% and testing accuracy 98.60%. An accuracy rate of 91.55% was achieved by accurately transliterating 412 out of 450 words. The researchers conclude that Aralin Baybayin is an effective tool for promoting more efficient and interactive learning of the Baybayin Script using the Intelligent Tutoring System (ITS).},
booktitle = {Proceedings of the 2023 11th International Conference on Computer and Communications Management},
pages = {102–106},
numpages = {5},
keywords = {Optical Character Recognition, Neural Networks Baybayin, Intelligent Tutoring System, Common European Framework Reference.&nbsp;},
location = {Nagoya, Japan},
series = {ICCCM '23}
}

@inproceedings{10.1145/3570945.3607317,
author = {Shoa, Alon and Oliva, Ramon and Slater, Mel and Friedman, Doron},
title = {Sushi with Einstein: Enhancing Hybrid Live Events with LLM-Based Virtual Humans},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607317},
doi = {10.1145/3570945.3607317},
abstract = {It is becoming increasingly easier to set up multi-user virtual reality sessions, and these can become viable alternatives to video conference in events such as international conferences. Moreover, it is possible to enhance such events with automated virtual humans, who may participate in the discussion. This paper presents the behind-the-scenes work of a panel session titled "Is virtual reality genuine reality?", which was held during a physical symposium, "XR for the people," in June 2022. The panel featured a virtual Albert Einstein, based on a large language model (LLM), as a panelist, alongside three international experts having a live conference panel discussion. The VR discussion was broadcast live on stage, and a moderator was able to communicate with both the live audience, the virtual world participants, and the virtual agent (Einstein). We provide lessons learned from the implementation and from the live production, and discuss the potential and pitfalls of using LLM-based virtual humans for multi-user VR in live hybrid events.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {22},
numpages = {6},
keywords = {AI, GPT3, Persona Reconstruction, VR},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@inproceedings{10.1145/3630106.3658911,
author = {Hutiri, Wiebke and Papakyriakopoulos, Orestis and Xiang, Alice},
title = {Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators},
year = {2024},
isbn = {9798400704505},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658911},
doi = {10.1145/3630106.3658911},
abstract = {The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens’ homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {359–376},
numpages = {18},
keywords = {Deepfakes, Generative AI, Harms, Multimodal, Speech Generation, Speech Synthesis, Taxonomy, Voice Cloning},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}

@inproceedings{10.1145/3605731.3605872,
author = {Kitchat, Kotcharat and Chen, Yen-Ju and Sun, Min-Te and Surasak, Thattapon},
title = {Taillight Signal Recognition via Sequential Learning},
year = {2023},
isbn = {9798400708428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605731.3605872},
doi = {10.1145/3605731.3605872},
abstract = {In autonomous driving, it is crucial to capture the driving intentions of other vehicles on the road, which can then be used for the autonomous driving vehicle to plan a safe route. This study proposes a system to identify the driving intention of other vehicles from their taillight signals. To achieve this goal, both the positions of taillights (i.e., spatial features) and the change of the status of taillights over time (i.e., temporal features) need to be properly extracted and recognized. In our system, a longer sequence of 32 frames is used as input to capture the complete change of taillights. In addition, a transfer-learned classical convolutional neural network and a light-weight WaveNet are adopted to extract spatial and temporal features of the input sequence, respectively. Moreover, the dataset is augmented to ensure the convergence of model training. The experiment results indicate that our system outperforms the state of the art approaches in taillight recognition.},
booktitle = {Proceedings of the 52nd International Conference on Parallel Processing Workshops},
pages = {1–7},
numpages = {7},
keywords = {WaveNet, autonomous driving, computer vision, taillight recognition},
location = {Salt Lake City, UT, USA},
series = {ICPP Workshops '23}
}

@inproceedings{10.5555/3721488.3721663,
author = {Chojnowski, Oliver and Eberhard, Alexander and Schiffmann, Michael and M\"{u}ller, Ana and Richert, Anja},
title = {Human-like Nonverbal Behavior with MetaHumans in Real-World Interaction Studies: An Architecture Using Generative Methods and Motion Capture},
year = {2025},
publisher = {IEEE Press},
abstract = {Socially interactive agents are gaining prominence in domains like healthcare, education, and service contexts, particularly virtual agents due to their inherent scalability. To facilitate authentic interactions, these systems require verbal and nonverbal communication through e.g., facial expressions and gestures. While natural language processing technologies have rapidly advanced, incorporating human-like nonverbal behavior into real-world interaction contexts is crucial for enhancing the success of communication, yet this area remains underexplored. One barrier is creating autonomous systems with sophisticated conversational abilities that integrate human-like nonverbal behavior. This paper presents a distributed architecture using Epic Games' MetaHuman, combined with advanced conversational AI and camera-based user management, that supports methods like motion capture, handcrafted animation, and generative approaches for nonverbal behavior. We share insights into a system architecture designed to investigate nonverbal behavior in socially interactive agents, deployed in a three-week field study in the Deutsches Museum Bonn, showcasing its potential in realistic nonverbal behavior research.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1279–1283},
numpages = {5},
keywords = {generative ai, motion capture, nonverbal behavior metahuman, socially interactive agents},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3664647.3680795,
author = {Cai, Zhixi and Ghosh, Shreya and Adatia, Aman Pankaj and Hayat, Munawar and Dhall, Abhinav and Gedeon, Tom and Stefanov, Kalin},
title = {AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3680795},
doi = {10.1145/3664647.3680795},
abstract = {The detection and localization of highly realistic deepfake audio-visual content are challenging even for the most advanced state-of-the-art methods. While most of the research efforts in this domain are focused on detecting high-quality deepfake images and videos, only a few works address the problem of the localization of small segments of audio-visual manipulations embedded in real videos. In this research, we emulate the process of such content generation and propose the AV-Deepfake1M dataset. The dataset contains content-driven (i) video manipulations, (ii) audio manipulations, and (iii) audio-visual manipulations for more than 2K subjects resulting in a total of more than 1M videos. The paper provides a thorough description of the proposed data generation pipeline accompanied by a rigorous analysis of the quality of the generated data. The comprehensive benchmark of the proposed dataset utilizing state-of-the-art deepfake detection and localization methods indicates a significant drop in performance compared to previous datasets. The proposed dataset will play a vital role in building the next-generation deepfake localization methods. The dataset and associated code are available at https://github.com/ControlNet/AV-Deepfake1M.},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {7414–7423},
numpages = {10},
keywords = {datasets, deepfake, detection, localization},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3695766,
author = {Maddu, Sandeep and Sanapala, Viziananda Row},
title = {A survey on NLP tasks, resources and techniques for low-resource Telugu-English code-mixed text},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3695766},
doi = {10.1145/3695766},
abstract = {With the proliferation of informal content on various social media platforms in the form of posts, comments, and feedback, the importance of analyzing text in code-mixed form is gaining importance. Telugu, a low-resource Indian language, has a lot of online content being generated in code-mixed form. However, the lack of large corpora, annotated data and Natural Language Processing (NLP) resources are impeding research on Telugu-English code-mixed data. This paper provides a survey of existing literature on Telugu-English code-mixed text in the areas of resources, POS tagging, Named Entity Recognition, language identification, sentiment analysis, application tasks, dialog systems, and Question-Answering. Various datasets being used by the researchers in the field, along with methods applied to them are detailed. Research gaps are identified to provide future direction for researchers working in this field.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
keywords = {code-mixing, Telugu, Natural Language Processing, survey, Part-of-Speech (POS) tagging, Named Entity Recognition (NER), Language Identification, Sentiment Analysis, Question-Answering}
}

@inproceedings{10.1145/3568162.3576971,
author = {Jeffcock, Joe and Hansen, Mark and Ruiz Garate, Virginia},
title = {Transformers and Human-robot Interaction for Delirium Detection},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3576971},
doi = {10.1145/3568162.3576971},
abstract = {An estimated 20% of patients admitted to hospital wards are affected by delirium. Early detection is recommended to treat underlying causes of delirium, however workforce strain in general wards often causes it to remain undetected. This work proposes a robotic implementation of the Confusion Assessment Method for the Intensive Care Unit (CAM-ICU) to aid early detection of delirium. Interactive features of the assessment are performed by Human-robot Interaction while a Transformer-based deep learning model predicts the Richmond Agitation Sedation Scale (RASS) level of the patient from image sequences; thermal imaging is used to maintain patient anonymity. A user study involving 18 participants role-playing each of alert, agitated, and sedated levels of the RASS is performed to test the HRI components and collect a dataset for deep learning. The HRI system achieved accuracies of 1.0 and 0.833 for the inattention and disorganised thinking features of the CAM-ICU, respectively, while the trained action recognition model achieved a mean accuracy of 0.852 on the classification of RASS levels during cross-validation. The three features represent a complete set of capabilities for automated delirium detection using the CAM-ICU, and the results demonstrate the feasibility of real-world deployment in hospital general wards.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {466–474},
numpages = {9},
keywords = {delirium, health-care, machine learning, machine vision, transformers},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@article{10.1145/3564769,
author = {Kumar Attar, Rakesh and Goyal, Vishal and Goyal, Lalit},
title = {State of the Art of Automation in Sign Language: A Systematic Review},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564769},
doi = {10.1145/3564769},
abstract = {Sign language is the fundamental communication language of deaf people. Efforts to develop sign language generation systems can make the life of these people smooth and effortless. Despite the importance of sign language generation systems, there is a paucity of a systematic literature review. This is the foremost recognizable scholastic literature review of sign language generation systems. It presents a scholastic database of the literature between 1998 and 2020 and suggests classification criteria to systematize research studies. Four hundred fourteen research studies were recognized and reviewed for their direct pertinence to sign language generation systems. One hundred sixty-two research studies were subsequently chosen, examined, and classified. Each of the 162 chosen research papers was categorized based on 30 sign languages and was further comparatively analyzed based on seven comparison parameters (input form, translation technologies, application domain, use of parsers/grammars, manual/non-manual features, accuracy, and output form). It is evident from our research findings that the majority of research on sign language generation was carried out using data-driven approaches in the absence of proper grammar rules and generated only manual signs. This research study may provide researchers a roadmap toward future research directions and facilitate the compilation of information in the field of sign language generation.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {94},
numpages = {80},
keywords = {HamNoSys, SiGML, virtual avatar, Interlingua, Machine translation}
}

@article{10.1109/TASLP.2023.3282097,
author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Grondin, Fran\c{c}ois and Bronzi, Mirko},
title = {Exploring Self-Attention Mechanisms for Speech Separation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3282097},
doi = {10.1109/TASLP.2023.3282097},
abstract = {Transformers have enabled impressive improvements in deep learning. They often outperform recurrent and convolutional models in many tasks while taking advantage of parallel processing. Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets. This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!. Moreover, we extend our model to perform speech enhancement and provide experimental evidence on denoising and dereverberation tasks. Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers. We found that they reduce memory requirements significantly. For example, we show that the Reformer-based attention outperforms the popular Conv-TasNet model on the WSJ0-2Mix dataset while being faster at inference and comparable in terms of memory consumption.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2169–2180},
numpages = {12}
}

@inproceedings{10.1145/3610977.3634966,
author = {Kim, Callie Y. and Lee, Christine P. and Mutlu, Bilge},
title = {Understanding Large-Language Model (LLM)-powered Human-Robot Interaction},
year = {2024},
isbn = {9798400703225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610977.3634966},
doi = {10.1145/3610977.3634966},
abstract = {Large-language models (LLMs) hold significant promise in improving human-robot interaction, offering advanced conversational skills and versatility in managing diverse, open-ended user requests in various tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from text and voice interaction and vary by task and context. To better understand these requirements, we conducted a user study (n = 32) comparing an LLM-powered social robot against text- and voice-based agents, analyzing task-based requirements in conversational tasks, including choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues and excel in connection-building and deliberation, but fall short in logical communication and may induce anxiety. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.},
booktitle = {Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {371–380},
numpages = {10},
keywords = {human-robot interaction, large language models, social robots},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3458380.3458405,
author = {Zhang, Ya-Jie and Ling, Zhen-Hua},
title = {Learning Deep and Wide Contextual Representations Using BERT for Statistical Parametric Speech Synthesis},
year = {2021},
isbn = {9781450389365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458380.3458405},
doi = {10.1145/3458380.3458405},
abstract = {In this paper, we propose a method of learning deep and wide contextual representations for statistical parametric speech synthesis (SPSS) using BERT, a pre-trained language representation model. Traditional acoustic models in SPSS utilize phoneme sequences and prosody labels as input, and can not make full use of the deep linguistic representations of current and surrounding sentences. Therefore, this paper designs two context encoders, i.e., a sentence-window context encoder and a paragraph-level context encoder, to integrate the contextual representations extracted from multiple sentences by BERT into Tacotron2 via an extra attention module. The parameters of BERT are pre-trained and then fine-tuned together with other components in the model. Experimental results on the Blizzard Challenge 2019 dataset show that both context encoders can reduce the errors of acoustic feature prediction and improve the subjective performance of synthetic speech comparing with the baseline Tacotron2 model.},
booktitle = {Proceedings of the 2021 5th International Conference on Digital Signal Processing},
pages = {146–150},
numpages = {5},
keywords = {speech synthesis, Tacotron2, BERT},
location = {Chengdu, China},
series = {ICDSP '21}
}

@inproceedings{10.5555/3721488.3721679,
author = {Irfan, Bahar and Skantze, Gabriel},
title = {Between You and Me: Ethics of Self-Disclosure in Human-Robot Interaction},
year = {2025},
publisher = {IEEE Press},
abstract = {As we move toward a future where robots are increasingly part of daily life, the privacy risks associated with interactions, particularly those relying on cloud-based large language models (LLMs), are becoming more pressing. Users may unknowingly share sensitive information in environments, such as homes or hospitals. To explore these risks, we conducted a study with 39 native English speakers using a Furhat robot with an integrated LLM. Participants discussed two moral dilemmas: (i) dishonesty, sharing personal stories of justified lying, and (ii) robot disobedience, discussing whether robots should disobey commands. On average, participants disclosed personal stories 45% of the time when asked in both scenarios. The main reason for non-disclosure was difficulty recalling examples quickly (33.3-56%), rather than reluctance to share (7.2-16%). However, most participants reported a lack of discomfort and concern about sharing personal information with the robot, indicating limited awareness of the privacy risks involved in such disclosures.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1357–1362},
numpages = {6},
keywords = {ethics, human-robot interaction, large language model, moral dilemmas, privacy, self-disclosure},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1145/3592604,
author = {Ahmed, Usman and Lin, Jerry Chun-Wei and Srivastava, Gautam and Yun, Unil},
title = {Semi-Supervised Lexicon-Aware Embedding for News Article Time Estimation},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3592604},
doi = {10.1145/3592604},
abstract = {In the information retrieval community, Temporal Information Retrieval (TIR) has become increasingly popular. Documents focused on the time surrounding their publication are more likely to be accurate and contain information relevant to the reader. In this study, we explore the inverted pyramid paradigm by extracting temporal expressions from news documents, standardizing their values, and evaluating them based on their position within the text. We present a lexicon expansion method that employs WordNet as input. This approach enhances the lexicon by grouping words with similar meanings, potentially improving the accuracy of event detection algorithms. Additionally, this process can introduce new words and phrases to the lexicon, expanding the vocabulary. Using each tagged dataset, a classifier is trained with a pre-trained network. A pool of unlabeled data are processed, and high-confidence pseudo-labels are assigned. Pseudo-labels are generated by leveraging the partially trained model and the original labelled data. As the classifier predicts the correct label for a data sample, the pseudo-labels of other data samples are updated, and vice versa. At the end of this process, the predictions from different matching classifiers are combined. It takes several rounds to label the unlabeled inputs using this method. To evaluate the proposed solutions, we conducted experiments on 4,500 online news articles relevant to temporal retrieval. LSTM, BiLSTM, and BERT models with and without lexicon expansion were assessed based on log loss and relative divergence of entropy. A jointly trained semi-supervised learning model achieved a mean KL divergence of 0.89, an F1 score of 0.74 for temporal events, and 0.63 for non-temporal events. Besides alleviating data sparsity issues and enabling the training of more complex networks, this technique can also serve as an alternative to data augmentation methods.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
keywords = {Attention network, Data augmentation, Data sparsity, News classification}
}

@inproceedings{10.5555/3523760.3523946,
author = {Yuguchi, Akishige and Kawano, Seiya and Yoshino, Koichiro and Ishi, Carlos Toshinori and Kawanishi, Yasutomo and Nakamura, Yutaka and Minato, Takashi and Saito, Yasuki and Minoh, Michihiko},
title = {Butsukusa: A Conversational Mobile Robot Describing Its Own Observations and Internal States},
year = {2022},
publisher = {IEEE Press},
abstract = {This paper presents an autonomous conversational mobile robot Butsukusa that can describe its own observations and internal states during patrolling tasks. The proposed robot can observe the surrounding environment using the recognition module for objects, humans, environment, localization, and speech and then move autonomously around an indoor living space. Interaction skills via language are required for the robot to perform in such human-centered spaces. To investigate a better communication protocol with users, we evaluate various language generation patterns based on different observations and interaction patterns. The evaluation results indicate that the importance of describing the robot's observation results and internal states, as well as the necessity of an appropriate description, depends on the situation.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1114–1118},
numpages = {5},
keywords = {conversational robots, human-robot interaction, intelligent robots, service robots, social robots},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3687311.3687331,
author = {Wang, Yang},
title = {A study on the integrated blended teaching model of English interpreting under the background of “Internet +”},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687331},
doi = {10.1145/3687311.3687331},
abstract = {Under the background of "Internet Plus", this study constructs an integrated hybrid English interpretation teaching model, which integrates advanced computer technology and educational theories to achieve significant improvement in teaching efficiency and quality. Through online independent learning, offline classroom interaction, practical interpretation practice and intelligent evaluation, the learning path is optimized, and the interactive and adaptive teaching content is improved. Using big data and machine learning technology, this model realizes real-time analysis of students' learning behavior, provides personalized learning suggestions, and greatly enhances students' language application ability. In addition, the experimental results show that students' basic language skills have been comprehensively improved, especially interpreting skills, demonstrating the high efficiency of blended teaching mode in improving complex language processing ability.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {104–108},
numpages = {5},
location = {Guilin, China},
series = {IECT '24}
}

@inproceedings{10.1145/3490035.3490284,
author = {Gupta, Anchit and Khan, Faizan Farooq and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C. V.},
title = {Intelligent video editing: incorporating modern talking face generation algorithms in a video editor},
year = {2021},
isbn = {9781450375962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490035.3490284},
doi = {10.1145/3490035.3490284},
abstract = {This paper proposes a video editor based on OpenShot with several state-of-the-art facial video editing algorithms as added functionalities. Our editor provides an easy-to-use interface to apply modern lip-syncing algorithms interactively. Apart from lip-syncing, the editor also uses audio and facial re-enactment to generate expressive talking faces. The manual control improves the overall experience of video editing without missing out on the benefits of modern synthetic video generation algorithms. This control enables us to lip-sync complex dubbed movie scenes, interviews, television shows, and other visual content. Furthermore, our editor provides features that automatically translate lectures from spoken content, lip-sync of the professor, and background content like slides. While doing so, we also tackle the critical aspect of synchronizing background content with the translated speech. We qualitatively evaluate the usefulness of the proposed editor by conducting human evaluations. Our evaluations show a clear improvement in the efficiency of using human editors and an improved video generation quality.We attach demo videos with the supplementary material clearly explaining the tool and also showcasing multiple results.},
booktitle = {Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing},
articleno = {25},
numpages = {9},
keywords = {video editing, talking head generation, speech-to-speech translation, lip-sync, human in the loop},
location = {Jodhpur, India},
series = {ICVGIP '21}
}

@inproceedings{10.1145/3552466.3556532,
author = {Hua, Hua and Chen, Ziyi and Zhang, Yuxiang and Li, Ming and Zhang, Pengyuan},
title = {Improving Spoofing Capability for End-to-end Any-to-many Voice Conversion},
year = {2022},
isbn = {9781450394963},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552466.3556532},
doi = {10.1145/3552466.3556532},
abstract = {Audio deep synthesis techniques have been able to generate high-quality speech whose authenticity is difficult for humans to recognize. Meanwhile, many anti-spoofing systems have been developed to capture artifacts in the synthesized speech that are imperceptible to human hearing, thus a continuous escalating race of 'attacking and defending' in voice deepfake has started. Hence, to further improve the probability of successfully cheating anti-spoofing systems, we propose a fully end-to-end, any-to-many voice conversion method based on a non-autoregressive structure with the addition of two light but strong post-processing strategies namely silence replacement and global noise perturbation. Experimental results show that the proposed method performs better than current baselines in fooling several state-of-the-art anti-spoofing systems. Better naturalness and speaker similarity are also achieved, resulting in our proposed method showing high deception performance against humans.},
booktitle = {Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia},
pages = {93–100},
numpages = {8},
keywords = {voice conversion, spoofing, speech deepfake, post-processing},
location = {Lisboa, Portugal},
series = {DDAM '22}
}

@article{10.1145/3548457,
author = {Lahoti, Pawan and Mittal, Namita and Singh, Girdhari},
title = {A Survey on NLP Resources, Tools, and Techniques for Marathi Language Processing},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3548457},
doi = {10.1145/3548457},
abstract = {Natural Language Processing (NLP) has been in practice for the past couple of decades, and extensive work has been done for the Western languages, particularly the English language. The Eastern counterpart, especially the languages of the Indian subcontinent, needs attention as not much language processing work has been done on these languages. Western languages are rich in dictionaries, WordNet, and associated tools, while Indian languages are lagging behind in this segment. Marathi is the third most spoken language in India and the 15th most spoken language worldwide. Lack of resources, complex linguistic facts, and the inclusion of prevalent dialects of neighbors have resulted in limited work for Marathi. The aim of this study is to provide an insight into the various linguistic resources, tools, and state-of-the-art techniques applied to the processing of the Marathi language. Initially, morphological descriptions of the Marathi language are provided, followed by a discussion on the characteristics of the Marathi language. Thereafter, for Marathi language, the availability of corpus, tools, and techniques to be used to develop NLP tasks is reviewed. Finally, gap analysis is discussed in current research and future directions for this new and dynamic area of research are listed that will benefit the Marathi Language Processing research community.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {47},
numpages = {34},
keywords = {Word Sense Disambiguation (WSD), Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Marathi resources, Marathi morphology, Marathi language}
}

@article{10.1109/TASLP.2023.3301217,
author = {Lei, Shun and Zhou, Yixuan and Chen, Liyang and Wu, Zhiyong and Wu, Xixin and Kang, Shiyin and Meng, Helen},
title = {MSStyleTTS: Multi-Scale Style Modeling With Hierarchical Context Information for Expressive Speech Synthesis},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3301217},
doi = {10.1109/TASLP.2023.3301217},
abstract = {Expressive speech synthesis is crucial for many human-computer interaction scenarios, such as audiobooks, podcasts, and voice assistants. Previous works focus on predicting the style embeddings at one single scale from the information within the current sentence. Whereas, context information in neighboring sentences and multi-scale nature of style in human speech are neglected, making it challenging to convert multi-sentence text into natural and expressive speech. In this article, we propose MSStyleTTS, a style modeling method for expressive speech synthesis, to capture and predict styles at different levels from a wider range of context rather than a sentence. Two sub-modules, including multi-scale style extractor and multi-scale style predictor, are trained together with a FastSpeech 2 based acoustic model. The predictor is designed to explore the hierarchical context information by considering structural relationships in context and predict style embeddings at global-level, sentence-level and subword-level. The extractor extracts multi-scale style embedding from the ground-truth speech and explicitly guides the style prediction. Evaluations on both in-domain and out-of-domain audiobook datasets demonstrate that the proposed method significantly outperforms the three baselines. In addition, we conduct the analysis of the context information and multi-scale style representations that have never been discussed before.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {3290–3303},
numpages = {14}
}

@inproceedings{10.1145/3472307.3484167,
author = {Liu, Yu and Mohammadi, Gelareh and Song, Yang and Johal, Wafa},
title = {Speech-based Gesture Generation for Robots and Embodied Agents: A Scoping Review},
year = {2021},
isbn = {9781450386203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472307.3484167},
doi = {10.1145/3472307.3484167},
abstract = {Humans use gestures as a means of non-verbal communication. Often accompanying speech, these gestures have several purposes but in general, aim to convey an intended message to the receiver. Researchers have tried to develop systems to allow embodied agents to be better communicators when interacting with humans via using gestures. In this article, we present a scoping literature review of the methods and the metrics used to generate and evaluate co-speech gestures. After collecting a set of papers using a term search on the Scopus database, we analysed the content of these papers based on methodology (i.e., model, the dataset used), evaluation measures (i.e., objective and subjective) and limitations. The results indicate that data-driven approaches are used more frequently. In terms of evaluation measures, we found a trend of combining objective and subjective metrics, while no standards exist for either. This literature review provides an overview of the research in the area and, more specifically insights the trends and the challenges to be met in building a system to automatically generate gestures for embodied agents.},
booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
pages = {31–38},
numpages = {8},
keywords = {survey, robot, literature review, gesture generation, co-speech gestures},
location = {Virtual Event, Japan},
series = {HAI '21}
}

@article{10.1145/3678517,
author = {Aldeen, Mohammed and Young, Jeffrey and Liao, Song and Chang, Tsu-Yao and Cheng, Long and Cai, Haipeng and Luo, Xiapu and Hu, Hongxin},
title = {End-Users Know Best: Identifying Undesired Behavior of Alexa Skills Through User Review Analysis},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3678517},
doi = {10.1145/3678517},
abstract = {The Amazon Alexa marketplace has grown rapidly in recent years due to third-party developers creating large amounts of content and publishing directly to a skills store. Despite the growth of the Amazon Alexa skills store, there have been several reported security and usability concerns, which may not be identified during the vetting phase. However, user reviews can offer valuable insights into the security &amp; privacy, quality, and usability of the skills. To better understand the effects of these problematic skills on end-users, we introduce ReviewTracker, a tool capable of discerning and classifying semantically negative user reviews to identify likely malicious, policy violating, or malfunctioning behavior on Alexa skills. ReviewTracker employs a pre-trained FastText classifier to identify different undesired skill behaviors. We collected over 700,000 user reviews spanning 6 years with more than 200,000 negative sentiment reviews. ReviewTracker was able to identify 17,820 reviews reporting violations related to Alexa policy requirements across 2,813 skills, and 131,855 reviews highlighting different types of user frustrations associated with 9,294 skills. In addition, we developed a dynamic skill testing framework using ChatGPT to conduct two distinct types of tests on Alexa skills: one using a software-based simulation for interaction to explore the actual behaviors of skills and another through actual voice commands to understand the potential factors causing discrepancies between intended skill functionalities and user experiences. Based on the number of the undesired skill behavior reviews, we tested the top identified problematic skills and detected more than 228 skills violating at least one policy requirement. Our results demonstrate that user reviews could serve as a valuable means to identify undesired skill behaviors.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {89},
numpages = {28}
}

@inproceedings{10.1145/3593013.3594005,
author = {Brewer, Robin N. and Harrington, Christina and Heldreth, Courtney},
title = {Envisioning Equitable Speech Technologies for Black Older Adults},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594005},
doi = {10.1145/3593013.3594005},
abstract = {There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {379–388},
numpages = {10},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3623809.3623863,
author = {Mochizuki, Shota and Yamashita, Sanae and Kawasaki, Kazuyoshi and Yuasa, Reiko and Kubota, Tomonori and Ogawa, Kohei and Baba, Jun and Higashinaka, Ryuichiro},
title = {Investigating the Intervention in Parallel Conversations},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623809.3623863},
doi = {10.1145/3623809.3623863},
abstract = {In recent years, a framework of parallel conversations has been proposed to facilitate efficient conversations through cooperation between humans and dialogue systems. This approach aims to enable simultaneous conversations with multiple users by enabling the system to handle basic conversation and human operators to intervene when problems arise in the system’s conversation. Previous studies on parallel conversations have primarily focused on delegating simple exchanges such as greetings and acknowledgments to the system, with humans taking over for more complex interactions like providing guidance. Recent advancements in large language models may change this situation, enabling dialogue systems to engage in more advanced interactions. In this study, to examine which interventions will be made when large language models are utilized, we placed six dialogue robots based on large language models in an actual facility and conducted a field experiment involving parallel conversations for about a month. Our analysis of the collected data on dialogues and interventions showed that the most frequent interventions were made for supporting interactions when the system failed to react to the user utterances, indicating the limitations of using large language models alone and clarifying our next steps for facilitating smoother parallel conversations.},
booktitle = {Proceedings of the 11th International Conference on Human-Agent Interaction},
pages = {30–38},
numpages = {9},
keywords = {conversation, dialogue system, intervention, large language model},
location = {Gothenburg, Sweden},
series = {HAI '23}
}

@article{10.1109/TASLP.2023.3268568,
author = {Tao, Ruijie and Lee, Kong Aik and Das, Rohan Kumar and Hautam\"{a}ki, Ville and Li, Haizhou},
title = {Self-Supervised Training of Speaker Encoder With Multi-Modal Diverse Positive Pairs},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3268568},
doi = {10.1109/TASLP.2023.3268568},
abstract = {We study a novel neural speaker encoder and its training strategies for speaker recognition without using any identity labels. The speaker encoder is trained to extract a fixed dimensional speaker embedding from a spoken utterance of variable length. Contrastive learning is a typical self-supervised learning technique. However, the contrastive learning of the speaker encoder depends very much on the sampling strategy of positive and negative pairs. It is common that we sample a positive pair of segments from the same utterance. Unfortunately, such a strategy, denoted as poor-man's positive pairs (PPP), lacks the necessary diversity. In this work, we propose a multi-modal contrastive learning technique with novel sampling strategies. By cross-referencing between speech and face data, we find diverse positive pairs (DPP) for contrastive learning, thus improving the robustness of speaker encoder. We train the speaker encoder on the VoxCeleb2 dataset without any speaker labels, and achieve an equal error rate (EER) of 2.89%, 3.17% and 6.27% under the proposed progressive clustering strategy, and an EER of 1.44%, 1.77% and 3.27% under the two-stage learning strategy with pseudo labels, on the three test sets of VoxCeleb1. This novel solution outperforms the state-of-the-art self-supervised learning methods by a large margin, at the same time, achieves comparable results with the supervised learning counterpart. We also evaluate our self-supervised learning technique on the LRS2 and LRW datasets, where speaker information is unavailable. All experiments suggest that the proposed neural architecture and sampling strategies are robust across datasets.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1706–1719},
numpages = {14}
}

@article{10.1145/3625679,
author = {Khullar, Aman and Nkemelu, Daniel and Nguyen, V. Cuong and Best, Michael L.},
title = {Hate Speech Detection in Limited Data Contexts Using Synthetic Data Generation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3625679},
doi = {10.1145/3625679},
abstract = {A growing body of work has focused on text classification methods for detecting the increasing amount of hate speech posted online. This progress has been limited to only a select number of highly resourced languages causing detection systems to either under-perform or not exist in limited data contexts. This is mostly caused by a lack of training data, which are expensive to collect and curate in these settings. In this work, we propose a data augmentation approach that addresses the problem of lack of data for online hate speech detection in limited data contexts using synthetic data generation techniques. Given a handful of hate speech examples in a high-resource language such as English, we present three methods to synthesize new examples of hate speech data in a target language that retains the hate sentiment in the original examples but transfers the hate targets. We apply our approach to generate training data for hate speech classification tasks in Hindi and Vietnamese. Our findings show that a model trained on synthetic data performs comparably to, and in some cases outperforms, a model trained only on the samples available in the target domain. This method can be adopted to bootstrap hate speech detection models from scratch in limited data contexts. As the growth of social media within these contexts continues to outstrip response efforts, this work furthers our capacities for detection, understanding, and response to hate speech. Disclaimer: This work contains terms that are offensive and hateful. These, however, cannot be avoided due to the nature of the work.},
journal = {ACM J. Comput. Sustain. Soc.},
month = jan,
articleno = {4},
numpages = {18},
keywords = {Hate speech, synthetic data, machine learning, low-resource text classification, digital threats, democracy}
}

@article{10.1109/TASLP.2021.3138720,
author = {Liu, Da-rong and Hsu, Po-chun and Chen, Yi-chen and Huang, Sung-feng and Chuang, Shun-po and Wu, Da-yi and Lee, Hung-yi},
title = {Learning Phone Recognition From Unpaired Audio and Phone Sequences Based on Generative Adversarial Network},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138720},
doi = {10.1109/TASLP.2021.3138720},
abstract = {ASRhas been shown to achieve great performance recently. However, most of them rely on massive paired data, which is not feasible for low-resource languages worldwide. This paper investigates how to learn directly from unpaired phone sequences and speech utterances. We design a two-stage iterative framework. GAN training is adopted in the first stage to find the mapping relationship between unpaired speech and phone sequence. In the second stage, another HMM model is introduced to train from the generator’s output, which boosts the performance and provides a better segmentation for the next iteration. In the experiment, we first investigate different choices of model designs. Thenwe compare the framework to different types of baselines: (i) supervised methods (ii) acoustic unit discovery based methods (iii) methods learning from unpaired data. Our framework performs consistently better than all acoustic unit discovery methods and previous methods learning from unpaired data based on the TIMIT dataset.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {230–243},
numpages = {14}
}

@inproceedings{10.1145/3474085.3475247,
author = {Liang, Paul Pu and Wu, Peter and Ziyin, Liu and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
title = {Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475247},
doi = {10.1145/3474085.3475247},
abstract = {How can we generalize to a new prediction task at test time when it also uses a new modality as input? More importantly, how can we do this with as little annotated data as possible? This problem of cross-modal generalization is a new research milestone with concrete impact on real-world applications. For example, can an AI system start understanding spoken language from mostly written text? Or can it learn the visual steps of a new recipe from only text descriptions? In this work, we formalize cross-modal generalization as a learning paradigm to train a model that can (1) quickly perform new tasks (from new domains) while (2) being originally trained on a different input modality. Such a learning paradigm is crucial for generalization to low-resource modalities such as spoken speech in rare languages while utilizing a different high-resource modality such as text. One key technical challenge that makes it different from other learning paradigms such as meta-learning and domain adaptation is the presence of different source and target modalities which will require different encoders. We propose an effective solution based on meta-alignment, a novel method to align representation spaces using strongly and weakly paired cross-modal data while ensuring quick generalization to new tasks across different modalities. This approach uses key ideas from cross-modal learning and meta-learning, and presents strong results on the cross-modal generalization problem. We benchmark several approaches on 3 real-world classification tasks: few-shot recipe classification from text to images of recipes, object classification from images to audio of objects, and language classification from text to spoken speech across 100 languages spanning many rare languages. Our results demonstrate strong performance even when the new target modality has only a few (1-10) labeled samples and in the presence of noisy labels, a scenario particularly prevalent in low-resource modalities.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {2680–2689},
numpages = {10},
keywords = {multimodal learning, meta-learning, cross-modal retrieval, cross-modal alignment},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3511808.3557224,
author = {Shen, Wei and He, Xiaonan and Zhang, Chuheng and Zhang, Xuyun and Xie, Jian},
title = {A Transformer-Based User Satisfaction Prediction for Proactive Interaction Mechanism in DuerOS},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557224},
doi = {10.1145/3511808.3557224},
abstract = {Recently, spoken dialogue systems have been widely deployed in a variety of applications, serving a huge number of end-users. A common issue is that the errors resulting from noisy utterances, semantic misunderstandings, or lack of knowledge make it hard for a real system to respond properly, possibly leading to an unsatisfactory user experience. To avoid such a case, we consider a proactive interaction mechanism where the system predicts the user satisfaction with the candidate response before giving it to the user. If the user is not likely to be satisfied according to the prediction, the system will ask the user a suitable question to determine the real intent of the user instead of providing the response directly. With such an interaction with the user, the system can give a better response to the user. Previous models that predict the user satisfaction are not applicable to DuerOS which is a large-scale commercial dialogue system. They are based on hand-crafted features and thus can hardly learn the complex patterns lying behind millions of conversations and temporal dependency in multiple turns of the conversation. Moreover, they are trained and evaluated on the benchmark datasets with adequate labels, which are expensive to obtain in a commercial dialogue system. To face these challenges, we propose a pipeline to predict the user satisfaction to help DuerOS decide whether to ask for clarification in each turn. Specifically, we propose to first generate a large number of weak labels and then train a transformer-based model to predict the user satisfaction with these weak labels. Moreover, we propose a metric, contextual user satisfaction, to evaluate the experience under the proactive interaction mechanism. At last, we deploy and evaluate our model on DuerOS, and observe a 19% relative improvement on the accuracy of user satisfaction prediction and 2.3% relative improvement on user experience.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {1777–1786},
numpages = {10},
keywords = {user satisfaction prediction, transformer-based model, dialogue system},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3592097,
author = {Ao, Tenglong and Zhang, Zeyi and Liu, Libin},
title = {GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3592097},
doi = {10.1145/3592097},
abstract = {The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {42},
numpages = {18},
keywords = {co-speech gesture synthesis, multi-modality, style editing, diffusion models, CLIP}
}

@inproceedings{10.1145/3491102.3501914,
author = {Zhang, Chao and Yao, Cheng and Wu, Jiayi and Lin, Weijia and Liu, Lijuan and Yan, Ge and Ying, Fangtian},
title = {StoryDrawer: A Child–AI Collaborative Drawing System to Support Children's Creative Visual Storytelling},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501914},
doi = {10.1145/3491102.3501914},
abstract = {Visual storytelling is a new approach to creative expression based on verbal and figural creativity. The keys to visual storytelling are narrating and drawing over a period of time, which can be beneficial but also demanding on creativity for children. Informed by need-finding investigations, we developed StoryDrawer, a co-creative system that supports visual storytelling for children aged 6–10 years through collaborative drawing between children and artificial intelligence (AI). The system includes a context-based voice agent and two AI-driven collaborative strategies: the real-time transformation of children's telling into drawings, and the generation of abstract sketches with semantic similarity to existing story content. We conducted a 2 \texttimes{} 2 study with 64 children to evaluate the efficacy of StoryDrawer by varying the two strategies in four conditions. The results suggest that StoryDrawer provoked participants’ creative and elaborate ideas and contributed to their creative outcomes during an engaging visual storytelling experience.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {311},
numpages = {15},
keywords = {Child–AI collaboration, Children, Co-creative system, Creativity support tool, Drawing, Visual storytelling},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.5555/3721488.3721590,
author = {Kamelabad, Alireza M. and Inoue, Elin and Skantze, Gabriel},
title = {Comparing Monolingual and Bilingual Social Robots as Conversational Practice Companions in Language Learning},
year = {2025},
publisher = {IEEE Press},
abstract = {This study explores the impact of monolingual and bilingual robots in Robot-Assisted Language Learning (RALL) for non-native Swedish learners. In a within-group design, 47 participants interacted with a social robot under two conditions: a monolingual robot that communicated exclusively in Swedish and a bilingual robot capable of switching between Swedish and English. Each participant engaged in multiple role-play scenarios designed to match their language proficiency levels, and their experiences were assessed through surveys and behavioral data. The results show that the bilingual robot was generally favored by participants, leading to a more relaxed, enjoyable experience. The perceived learning was improved at the end of the experiment regardless of the condition. These findings suggest that incorporating bilingual support in language-learning robots may enhance user engagement and effectiveness, particularly for lower-proficiency learners.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {829–838},
numpages = {10},
keywords = {bilingual, conversation practice, dialogue systems, monolingual, rall, robot assisted language learning},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1109/TASLP.2024.3359352,
author = {Baas, Matthew and Kamper, Herman},
title = {Disentanglement in a GAN for Unconditional Speech Synthesis},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3359352},
doi = {10.1109/TASLP.2024.3359352},
abstract = {Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) – a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional speech synthesis. It is also substantially faster than existing top-performing diffusion models. We confirm that ASGAN's latent space is disentangled: we demonstrate how simple linear operations in the space can be used to perform several tasks unseen during training. Specifically, we perform evaluations in voice conversion, speech enhancement, speaker verification, and keyword classification. Our work indicates that GANs are still highly competitive in the unconditional speech synthesis landscape, and that disentangled latent spaces can be used to aid generalization to unseen tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {1324–1335},
numpages = {12}
}

@inproceedings{10.1145/3623809.3623889,
author = {Yamashita, Sanae and Mochizuki, Shota and Kawasaki, Kazuyoshi and Kubota, Tomonori and Ogawa, Kohei and Baba, Jun and Higashinaka, Ryuichiro},
title = {Investigating the Effects of Dialogue Summarization on Intervention in Human-System Collaborative Dialogue},
year = {2023},
isbn = {9798400708244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623809.3623889},
doi = {10.1145/3623809.3623889},
abstract = {Dialogue systems are widely utilized in chatbots and call centers. However, it is often difficult for such systems to deliver fully autonomous dialogue. For users to have a better dialogue experience, a framework for human-system collaborative dialogue is proposed in which a human operator takes over the dialogue when needed, engaging in conversation with the user instead of the system (we call this process intervention). Operators join the dialogue in the middle; therefore, it is believed that dialogue summarization can be helpful for interventions. However, it is currently unclear whether dialogue summarization is actually useful. Therefore, in this study, we aim to investigate the usefulness of dialogue summaries for interventions through a field experiment conducted at an actual facility combining an aquarium and a zoo. The results of the field experiment revealed that dialogue summaries were more useful for intervention than dialogue history. Furthermore, we found no differences in the word categories included in the operator utterances during interventions irrespective of whether the dialogue history or dialogue format summary was presented to the operators, suggesting that dialogue format summary has content similar to that of dialogue history but improves the usefulness in intervention.},
booktitle = {Proceedings of the 11th International Conference on Human-Agent Interaction},
pages = {316–324},
numpages = {9},
keywords = {conversation, dialogue system, intervention, summarization},
location = {Gothenburg, Sweden},
series = {HAI '23}
}

@inproceedings{10.1145/3581641.3584093,
author = {Kristensson, Per Ola and Mjelde, Morten and Vertanen, Keith},
title = {Understanding Adoption Barriers to Dwell-Free Eye-Typing: Design Implications from a Qualitative Deployment Study and Computational Simulations},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584093},
doi = {10.1145/3581641.3584093},
abstract = {Eye-typing is a slow and cumbersome text entry method typically used by individuals with no other practical means of communication. As an alternative, prior HCI research has proposed dwell-free eye-typing as a potential improvement that eliminates time-consuming and distracting dwell-timeouts. However, it is rare that such research ideas are translated into working products. This paper reports on a qualitative deployment study of a product that was developed to allow users access to a dwell-free eye-typing research solution. This allowed us to understand how such a research solution would work in practice, as part of users’ current communication solutions in their own homes. Based on interviews and observations, we discuss a number of design issues that currently act as barriers preventing widespread adoption of dwell-free eye-typing. The study findings are complemented with computational simulations in a range of conditions that were inspired by the findings in the deployment study. These simulations serve to both contextualize the qualitative findings and to explore quantitative implications of possible interface redesigns. The combined analysis gives rise to a set of design implications for enabling wider adoption of dwell-free eye-typing in practice.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {607–620},
numpages = {14},
keywords = {dwell-free eye-typing, eye-typing, gaze communication},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3472307.3484181,
author = {Boukaram, Halim-Antoine and Ziadee, Micheline and Sakr, Majd F},
title = {Mitigating the Effects of Delayed Virtual Agent Response Time Using Conversational Fillers},
year = {2021},
isbn = {9781450386203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472307.3484181},
doi = {10.1145/3472307.3484181},
abstract = {Virtual agents increasingly rely on cloud-based services, which makes them vulnerable to unpredictable network latency and service response times. In this work, we evaluate the use of conversational fillers to mitigate the impact of delay in system response time on users’ perception of a virtual agent. These fillers are uttered by the agent to keep the user engaged until the response is ready. We present the findings of a study run on the Mechanical Turk platform with 360 participants who interacted with a virtual agent. We tested two types of conversational fillers. The first type were generic utterances that simply asked the user to hold. The second adopted contextualized fillers that assume some semantic knowledge of the input and contain some of its elements. To test the generalizability of the different fillers, we ran two task-based experiments. The first task was to get a recipe and the second was to find a restaurant. Contextualized fillers positively affected participants’ rating of the agent’s response time but did not impact the agent’s likeability.},
booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
pages = {130–138},
numpages = {9},
keywords = {user studies, system response time, response time delays, human-agent interaction, conversational fillers, conversational agents, context},
location = {Virtual Event, Japan},
series = {HAI '21}
}

@inproceedings{10.1145/3664647.3681044,
author = {Ye, Zhen and Ju, Zeqian and Liu, Haohe and Tan, Xu and Chen, Jianyi and Lu, Yiwen and Sun, Peiwen and Pan, Jiahao and Bian, Weizhen and He, Shulin and Xue, Wei and Liu, Qifeng and Guo, Yike},
title = {FlashSpeech: Efficient Zero-Shot Speech Synthesis},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664647.3681044},
doi = {10.1145/3664647.3681044},
abstract = {Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models. However, the generation process of both methods is slow and computationally intensive. Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge. In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5% of the inference time compared with previous work. FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher. Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural. The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation. Our experimental results demonstrate the superior performance of FlashSpeech. Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity. Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like voice conversion, speech editing, and diverse speech sampling. Audio samples can be found in https://flashspeech.github.io/},
booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
pages = {6998–7007},
numpages = {10},
keywords = {adversarial training, latent consistency model, zero-shot speech synthesis},
location = {Melbourne VIC, Australia},
series = {MM '24}
}

@article{10.1145/3486675,
author = {Azmi, Aqil M. and Alnefaie, Rehab M. and Aboalsamh, Hatim A.},
title = {Light Diacritic Restoration to Disambiguate Homographs in Modern Arabic Texts},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3486675},
doi = {10.1145/3486675},
abstract = {Diacritic restoration (also known as diacritization or vowelization) is the process of inserting the correct diacritical markings into a text. Modern Arabic is typically written without diacritics, e.g., newspapers. This lack of diacritical markings often causes ambiguity, and though natives are adept at resolving, there are times they may fail. Diacritic restoration is a classical problem in computer science. Still, as most of the works tackle the full (heavy) diacritization of text, we, however, are interested in diacritizing the text using a fewer number of diacritics. Studies have shown that a fully diacritized text is visually displeasing and slows down the reading. This article proposes a system to diacritize homographs using the least number of diacritics, thus the name “light.” There is a large class of words that fall under the homograph category, and we will be dealing with the class of words that share the spelling but not the meaning. With fewer diacritics, we do not expect any effect on reading speed, while eye strain is reduced. The system contains morphological analyzer and context similarities. The morphological analyzer is used to generate all word candidates for diacritics. Then, through a statistical approach and context similarities, we resolve the homographs. Experimentally, the system shows very promising results, and our best accuracy is 85.6%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {60},
numpages = {14},
keywords = {morphological analysis, homographs, disambiguation, automatic diacritization, Arabic language}
}

@inproceedings{10.1145/3652037.3663915,
author = {Muazu, Fatima A and Adedoyin, Festus A and Dogan, Huseyin and Mavengere, Nicholas and Whittington, Paul},
title = {The use of mobile learning in special education needs and disabilities (SEND) settings: state-of-the-art classification of studies},
year = {2024},
isbn = {9798400717604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652037.3663915},
doi = {10.1145/3652037.3663915},
abstract = {In developed countries, the use of mobile learning particularly has changed the delivery of teaching and learning in mainstream and special schools and evidently improved academic performance, there is still limited research on its use in underserved regions of the world. The purpose of this study is to conduct a review of existing studies on the application of mobile learning as an assistive technology in special education to enable the understanding of the depth of research in the field especially in the African context. The study adopts a systematic literature review approach to guide literature search, identification, and selection on EBSCOHOST and Scopus databases. 34 articles that were published from 2019-2024 in any language were included in this review. The review further classified these studies in terms of their years of publication, countries, aims of research, research methods and target disability the interventions were employed for. The findings revealed that there are a substantial number of studies that specifically considered the application of mobile learning in the education of special needs learners with autism spectrum disorder and intellectual disabilities and fewer studies targeted auditory, visual and communication impairments, and specific learning disabilities that included dyslexia, dyscalculia, and dysgraphia. In terms of countries/regions of research, there were more studies conducted in Asia and Europe, sub-Saharan African countries had the least representations. Quantitative research methods were the most adopted methods of research.},
booktitle = {Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {486–495},
numpages = {10},
keywords = {mobile learning apps pedagogy, mobile learning apps usability, mobile learning impact on teaching and learning, mobile learning in K-12 education},
location = {Crete, Greece},
series = {PETRA '24}
}

@article{10.1109/TASLP.2021.3126925,
author = {Hong, Joanna and Kim, Minsu and Park, Se Jin and Ro, Yong Man},
title = {Speech Reconstruction With Reminiscent Sound Via Visual Voice Memory},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3126925},
doi = {10.1109/TASLP.2021.3126925},
abstract = {The goal of this work is to reconstruct speech from silent video, in both speaker dependent and independent ways. Unlike previous works that have been mostly restricted to a speaker dependent setting, we propose Visual Voice memory to restore essential auditory information to generate proper speech from different speakers and even unseen speakers. The proposed memory takes additional auditory information that corresponds to the input face movements and stores the auditory contexts that can be recalled by the given input visual features. Specifically, the Visual Voice memory contains value and key memory slots, where value memory slots are for saving the audio features, and key memory slots are for storing the visual features in the same location of the saved audio features. Guiding each memory to properly save each feature, the model can adequately produce the speech. Hence, our method employs both video and audio information during training time but does not require any additional auditory input during inference. Our key contributions are: (1) proposing the Visual Voice memory that brings rich information of audio that complements the visual features, thus producing high-quality speech from silent video, and (2) enabling multi-speaker and unseen speaker training by memorizing auditory features and the corresponding visual features. We validate the proposed framework on GRID and Lip2Wav datasets and show that our method surpasses the performance of previous works on both multi-speaker and speaker independent settings. We also demonstrate that the Visual Voice memory contains meaningful information to reconstruct speech.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {3654–3667},
numpages = {14}
}

@inproceedings{10.1145/3622896.3622923,
author = {Shi, Lijuan},
title = {A Unified Mixed-Bandwidth ASR Framework with Generative Adversarial Network},
year = {2023},
isbn = {9798400708190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622896.3622923},
doi = {10.1145/3622896.3622923},
abstract = {The recognition of mixed-bandwidth audio presents a challenge for both academic and industrial fields, with potentially greater implications for the latter. In this paper, we present a unified ASR architecture for mixed-bandwidth audio's recognition, here we innovatively propose to use the generative adversarial network and two discriminators to help achieving the ability of recognizing mixed sampling audio and guaranteeing the performance of ASR system. Through the adaptive training process of trained generator and ASR system, the performance can be further improved. We conduct experiments on the libri-speech dataset and demonstrate that our method can successfully recognize mixed-bandwidth audio and improve the accuracy of the ASR system by 3.65% in the narrowband data. Overall, the proposed unified ASR architecture provides a promising solution for the recognition of mixed-bandwidth audio in various settings.},
booktitle = {Proceedings of the 2023 4th International Conference on Control, Robotics and Intelligent System},
pages = {160–165},
numpages = {6},
location = {Guangzhou, China},
series = {CCRIS '23}
}

@inproceedings{10.1145/3674912.3674920,
author = {Kostova, Snezhana and Lekova, Anna},
title = {Social Humanoid Robots as Assistive Technology for individuals with ASD - assessment of good practices},
year = {2024},
isbn = {9798400716843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674912.3674920},
doi = {10.1145/3674912.3674920},
abstract = {In this paper we have identified several good/promising practices from the literature of using Social Humanoid Robots (SHR) as which can contribute to the education, social integration and improvement of lifestyle of individuals with Autism Spectrum Disorder (ASD). Our interest is on the commercial SHR currently available on the market. We provide analysis and evaluation of the effectiveness, advantages and drawbacks, accessibility, user feedback and expert opinions, ethical considerations and future directions. The analysis proof that SHR are promising computer based Assistive Technology (AT) for individuals with ASD.},
booktitle = {Proceedings of the International Conference on Computer Systems and Technologies 2024},
pages = {69–75},
numpages = {7},
location = {Ruse, Bulgaria},
series = {CompSysTech '24}
}

@inproceedings{10.1109/ASE51524.2021.9678514,
author = {Uddin, MD Kafil and He, Qiang and Han, Jun and Chua, Caslon},
title = {Mining cross-domain apps for software evolution: a feature-based approach},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678514},
doi = {10.1109/ASE51524.2021.9678514},
abstract = {The skyrocketing growth of mobile apps and mobile devices has significantly fueled the competition among app developers. They have leveraged the app store capabilities to analyse app data and identify app improvement opportunities. Existing research has shown that app developers mostly rely on in-domain (i.e., same domain or same app) data to improve their apps. However, relying on in-domain data results in low diversity and lacks novelty in recommended features. In this work, we present an approach that automatically identifies, classifies and ranks relevant popular features from cross-domain apps for recommendation to any given target app. It includes the following three steps: 1) identify cross-domain apps that are relevant to the target app in terms of their features; 2) filter and group semantically the features of the relevant cross-domain apps that are complementary to the target app; 3) rank and prioritize the complementary cross-domain features (in terms of their domain, app, feature and popularity characteristics) for adoption by the target app's developers. We have run extensive experiments on 100 target apps from 10 categories over 15,200 cross-domain apps from 31 categories. The experimental results have shown that our approach to identifying, grouping and ranking complementary cross-domain features for recommendation has achieved an accuracy level of over 89%. Our semantic feature grouping technique has also significantly outperformed two existing baseline techniques. The empirical evaluation validates the efficacy of our approach in providing personalised feature recommendation and enhancing app's user serendipity.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {743–755},
numpages = {13},
keywords = {software evolution, mobile apps, feature extraction, app store mining, app improvement, app competition},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3527188.3561926,
author = {Lala, Divesh and Inoue, Koji and Kawahara, Tatsuya and Sawada, Kei},
title = {Backchannel Generation Model for a Third Party Listener Agent},
year = {2022},
isbn = {9781450393232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3527188.3561926},
doi = {10.1145/3527188.3561926},
abstract = {In this work we propose a listening agent which can be used in a conversation between two humans. We firstly conduct a corpus analysis to identify three different categories of backchannel which the agent can use - responsive interjections, expressive interjections and shared laughs. From this data we train and evaluate a continuous backchannel generation model consisting of separate timing and form prediction models. We then conduct a subjective experiment to compare our model to random, dyadic, and ground truth models. We find that our model outperforms a random baseline and is comparable to the dyadic model despite the low evaluation of expressive interjections. We suggest that the perception of expressive interjections contribute significantly to the perception of the agent’s empathy and understanding of the conversation. The results also show the need for a more robust model to generate expressive interjections, perhaps aided by the use of linguistic features.},
booktitle = {Proceedings of the 10th International Conference on Human-Agent Interaction},
pages = {114–122},
numpages = {9},
keywords = {multiparty dialogue, listening agent, dialogue systems},
location = {Christchurch, New Zealand},
series = {HAI '22}
}

@inproceedings{10.1145/3637528.3671633,
author = {Shenoy, Ashish and Lu, Yichao and Jayakumar, Srihari and Chatterjee, Debojeet and Moslehpour, Mohsen and Chuang, Pierce and Harpale, Abhay and Bhardwaj, Vikas and Xu, Di and Zhao, Shicong and Zhao, Longfang and Ramchandani, Ankit and Dong, Xin Luna and Kumar, Anuj},
title = {Lumos: Empowering Multimodal LLMs with Scene Text Recognition},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671633},
doi = {10.1145/3637528.3671633},
abstract = {We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5690–5700},
numpages = {11},
keywords = {hand-object interaction, multimodal llms, nlp, ocr, on-device, salient region of interest detection, scene text recognition, text understanding},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3511808.3557131,
author = {Huang, Jizhou and Wang, Haifeng and Wang, Shaolei},
title = {DuIVRS: A Telephonic Interactive Voice Response System for Large-Scale POI Attribute Acquisition at Baidu Maps},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557131},
doi = {10.1145/3511808.3557131},
abstract = {The task of POI attribute acquisition, which aims at completing missing attributes (e.g., POI name, address, status, phone, and open/close time) for a point of interest (POI) or updating existing attribute values of a POI, plays an essential role in enabling users to entertain location-based services using commercial map applications, such as Baidu Maps. Existing solutions have adopted street views or web documents to acquire POI attributes, which have a major limitation in applying for large-scale production due to the labor-intensive and time-consuming nature of collecting data, error accumulation in processing textual/visual data in unstructured or free format, and necessitating post-processing steps with manual efforts. In this paper, we present our efforts and findings from a 3-year longitudinal study on designing and implementing DuIVRS, which is an alternative, fully automatic, and production-proven solution for large-scale POI attribute acquisition via completely machine-directed dialogues. Specifically, DuIVRS is designed to proactively acquire POI attributes via a telephonic interactive voice response system, whose tasks are to generate machine-initiative directed dialogues, make scripted telephone calls to businesses, and interact with people who answered the phone to achieve predefined goals through multi-turn dialogues. DuIVRS has already been deployed in production at Baidu Maps since December 2018, which greatly improves productivity and reduces production cost of POI attribute acquisition. As of December 31, 2021, DuIVRS has made 140 million calls and 42 million POI attribute updates within a 3-year period, which represents an approximately 3-year workload for a high-performance team of 1,000 call center workers. This demonstrates that DuIVRS is an industrial-grade and robust solution for cost-effective, large-scale acquisition of POI attributes.},
booktitle = {Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management},
pages = {3182–3191},
numpages = {10},
keywords = {task oriented dialogue system, knowledge acquisition, interactive voice response system, POI attribute acquisition, Baidu maps},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3568162.3578633,
author = {Amioka, Saya and Janssens, Ruben and Wolfert, Pieter and Ren, Qiaoqiao and Pinto Bernal, Maria Jose and Belpaeme, Tony},
title = {Limitations of Audiovisual Speech on Robots for Second Language Pronunciation Learning},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3578633},
doi = {10.1145/3568162.3578633},
abstract = {The perception of audiovisual speech plays an important role in infants' first language acquisition and continues to be important for language understanding beyond infancy. Beyond that, the perception of speech and congruent lip motion supports language understanding for adults, and it has been suggested that second language learning benefits from audiovisual speech, as it helps learners distinguish speech sounds in the target language. In this paper, we study whether congruent audiovisual speech on a robot facilitates the learning of Japanese pronunciation. 27 native-Dutch speaking participants were trained in Japanese pronunciation by a social robot. The robot demonstrated 30 Japanese words of varying complexity using either congruent audiovisual speech, incongruent visual speech, or computer-generated audiovisual speech. Participants were asked to imitate the robot's pronunciation, recordings of which were rated by native Japanese speakers. Against expectation, the results showed that congruent audiovisual speech resulted in lower pronunciation performance than low-fidelity or incongruent speech. We show that our learners, being native Dutch speakers, are only very weakly sensitive to audiovisual Japanese speech which possibly explains why learning performance does not seem to benefit from audiovisual speech.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {359–367},
numpages = {9},
keywords = {audiovisual speech, human-robot interaction, multi-modal interaction, orofacial animations, robot-assisted language learning},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.5555/3523760.3523804,
author = {Green, Haley N. and Islam, Md Mofijul and Ali, Shahira and Iqbal, Tariq},
title = {Who's Laughing NAO? Examining Perceptions of Failure in a Humorous Robot Partner},
year = {2022},
publisher = {IEEE Press},
abstract = {Social robots are being deployed to interact with people in various scenarios, where they are expected to incorporate human-like conversational strategies to achieve fluency in interactions. For example, current robots are designed to perform advanced communication strategies (i.e., personal anecdotes, explanations, and apologies) to recover from task failure. However, these tactics are not always sufficient for failure recovery as they can be lengthy and insufficient for encouraging future interactions. In human-human interactions, people often use humor as a low-risk and engaging method for managing failures. Thus, the successful execution of advanced, human-like humor could enable robots to recover from task failures more efficiently. In this paper, we present a human-robot interaction study exploring how a robot's utilization of various human-like humor types (i.e., affiliative, aggressive, self-enhancing, and self-defeating) are perceived by human teammate (n=32) and an external observer of the interaction (n=256). Additionally, we have explored the effects of performance, humor type, perspective, and previous experience with robots on the participants' perceptions of warmth, competence, and the robot as a teammate. Our results indicate that dyadic participants rated the successful robot to be more competent and a better teammate than the bystander participants. Additionally, the results indicate that participants with less experience with robots found the successful robot to be more competent than participants with high levels of experience. These findings will enable the human-robot interaction community to develop more engaging robots for fluent interactive experiences in the future.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {313–322},
numpages = {10},
keywords = {failure, human-robot interaction, humor, recovery},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@article{10.1109/TASLP.2021.3134566,
author = {Mary, Narla John Metilda Sagaya and Umesh, Srinivasan and Katta, Sandesh Varadaraju},
title = {S-Vectors and TESA: Speaker Embeddings and a Speaker Authenticator Based on Transformer Encoder},
year = {2021},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3134566},
doi = {10.1109/TASLP.2021.3134566},
abstract = {One of the most popular speaker embeddings is x-vectors, which are obtained from an architecture that gradually builds a larger temporal context with layers. In this paper, we propose to derive speaker embeddings from Transformer’s encoder trained for speaker classification. Self-attention, on which Transformer’s encoder is built, attends to all the features over the entire utterance and might be more suitable in capturing the speaker characteristics in an utterance. We refer to the speaker embeddings obtained from the proposed speaker classification model as s-vectors to emphasize that they are obtained from an architecture that heavily relies on self-attention. Through experiments, we demonstrate that s-vectors perform better than x-vectors. In addition to the s-vectors, we also propose a new architecture based on Transformer’s encoder for speaker verification as a replacement for speaker verification based on conventional probabilistic linear discriminant analysis (PLDA). This architecture is inspired by the next sentence prediction task of bidirectional encoder representations from Transformers (BERT), and we feed the s-vectors of two utterances to verify whether they belong to the same speaker. We name this architecture the Transformer encoder speaker authenticator (TESA). Our experiments show that the performance of s-vectors with TESA is better than s-vectors with conventional PLDA-based speaker verification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {404–413},
numpages = {10}
}

@inproceedings{10.5555/3523760.3523790,
author = {Tang, Brian and Sullivan, Dakota and Cagiltay, Bengisu and Chandrasekaran, Varun and Fawaz, Kassem and Mutlu, Bilge},
title = {CONFIDANT: A Privacy Controller for Social Robots},
year = {2022},
publisher = {IEEE Press},
abstract = {As social robots become increasingly prevalent in day-to-day environments, they will participate in conversations and appropriately manage the information shared with them. However, little is known about how robots might appropriately discern the sensitivity of information, which has major implications for human-robot trust. As a first step to address a part of this issue, we designed a privacy controller, CONFIDANT, for conversational social robots, capable of using contextual metadata (e.g., sentiment, relationships, topic) from conversations to model privacy boundaries. Afterwards, we conducted two crowdsourced user studies. The first study (n = 174) focused on whether a variety of human-human interaction scenarios were perceived as either private/sensitive or non-private/non-sensitive. The findings from our first study were used to generate association rules. Our second study (n = 95) evaluated the effectiveness and accuracy of the privacy controller in human-robot interaction scenarios by comparing a robot that used our privacy controller against a baseline robot with no privacy controls. Our results demonstrate that the robot with the privacy controller outperforms the robot without the privacy controller in privacy-awareness, trustworthiness, and social-awareness. We conclude that the integration of privacy controllers in authentic human-robot conversations can allow for more trustworthy robots. This initial privacy controller will serve as a foundation for more complex solutions.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {205–214},
numpages = {10},
keywords = {human-robot conversations, privacy, trust},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3503161.3548081,
author = {Hegde, Sindhu B. and Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.},
title = {Lip-to-Speech Synthesis for Arbitrary Speakers in the Wild},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548081},
doi = {10.1145/3503161.3548081},
abstract = {In this work, we address the problem of generating speech from silent lip videos for any speaker in the wild. In stark contrast to previous works, our method (i) is not restricted to a fixed number of speakers, (ii) does not explicitly impose constraints on the domain or the vocabulary and (iii) deals with videos that are recorded in the wild as opposed to within laboratory settings. The task presents a host of challenges, with the key one being that many features of the desired target speech, like voice, pitch and linguistic content, cannot be entirely inferred from the silent face video. In order to handle these stochastic variations, we propose a new VAE-GAN architecture that learns to associate the lip and speech sequences amidst the variations. With the help of multiple powerful discriminators that guide the training process, our generator learns to synthesize speech sequences in any voice for the lip movements of any person. Extensive experiments on multiple datasets show that we outperform all baselines by a large margin. Further, our network can be fine-tuned on videos of specific identities to achieve a performance comparable to single-speaker models that are trained on $4times$ more data. We conduct numerous ablation studies to analyze the effect of different modules of our architecture. We also provide a demo video that demonstrates several qualitative results along with the code and trained models on our website http://cvit.iiit.ac.in/research/projects/cvit-projects/lip-to-speech-synthesis.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {6250–6258},
numpages = {9},
keywords = {talking-face videos, speech synthesis, lip-to-speech, hybrid vae-gan},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1109/TASLP.2021.3053391,
author = {Wang, Xinsheng and Qiao, Tingting and Zhu, Jihua and Hanjalic, Alan and Scharenborg, Odette},
title = {Generating Images From Spoken Descriptions},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3053391},
doi = {10.1109/TASLP.2021.3053391},
abstract = {Text-based technologies, such as text translation from one language to another, and image captioning, are gaining popularity. However, approximately half of the world's languages are estimated to be lacking a commonly used written form. Consequently, these languages cannot benefit from text-based technologies. This paper presents 1) a new speech technology task, i.e., a speech-to-image generation (S2IG) framework which translates speech descriptions to photo-realistic images 2) without using any text information, thus allowing unwritten languages to potentially benefit from this technology. The proposed speech-to-image framework, referred to as S2IGAN, consists of a speech embedding network and a relation-supervised densely-stacked generative model. The speech embedding network learns speech embeddings with the supervision of corresponding visual information from images. The relation-supervised densely-stacked generative model synthesizes images, conditioned on the speech embeddings produced by the speech embedding network, that are semantically consistent with the corresponding spoken descriptions. Extensive experiments are conducted on four public benchmark databases: two databases that are commonly used in text-to-image generation tasks, i.e., CUB-200 and Oxford-102 for which we created synthesized speech descriptions, and two databases with natural speech descriptions which are often used in the field of cross-modal learning of speech and images, i.e., Flickr8k and Places. Results on these databases demonstrate the effectiveness of the proposed S2IGAN on synthesizing high-quality and semantically-consistent images from the speech signal, yielding a good performance and a solid baseline for the S2IG task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {850–865},
numpages = {16}
}

@inproceedings{10.1145/3534678.3539030,
author = {Huang, Jizhou and Wang, Haifeng and Ding, Shiqiang and Wang, Shaolei},
title = {DuIVA: An Intelligent Voice Assistant for Hands-free and Eyes-free Voice Interaction with the Baidu Maps App},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539030},
doi = {10.1145/3534678.3539030},
abstract = {Mobile map apps such as the Baidu Maps app have become a ubiquitous and essential tool for users to find optimal routes and get turn-by-turn navigation services while driving. However, interacting with such apps while driving through visual-manual interaction modality inevitably causes driver distraction, due to the highly conspicuous nature of the time-sharing, multi-tasking behavior of the driver. In this paper, we present our efforts and findings of a 4-year longitudinal study on designing and implementing DuIVA, which is an intelligent voice assistant (IVA) embedded in the Baidu Maps app for hands-free, eyes-free human-to-app interaction in a fully voice-controlled manner. Specifically, DuIVA is designed to enable users to control the functionalities of Baidu Maps (e.g., navigation and location search) through voice interaction, rather than visual-manual interaction, which minimizes driver distraction and promotes safe driving by allowing the driver to keep "eyes on the road and hands on the wheel'' while interacting with the Baidu Maps app. DuIVA has already been deployed in production at Baidu Maps since November 2017, which facilitates a better interaction modality with the Baidu Maps app and improves the accessibility and usability of the app by providing users with in-app voice activation, natural language queries, and multi-round dialogue. As of December 31, 2021, over 530 million users have used DuIVA, which demonstrates that DuIVA is an industrial-grade and production-proven solution for in-app intelligent voice assistants.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3040–3050},
numpages = {11},
keywords = {baidu maps, eyes-free, hands-free, intelligent voice assistant, task-oriented dialogue, user-to-app interaction, voice interaction},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1109/TASLP.2023.3267610,
author = {Xu, Longting and Yang, Jichen and You, Chang Huai and Qian, Xinyuan and Huang, Daiyu},
title = {Device Features Based on Linear Transformation With Parallel Training Data for Replay Speech Detection},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3267610},
doi = {10.1109/TASLP.2023.3267610},
abstract = {Replay speech poses a growing threat to speaker verification systems, thus the detection of replay speech becomes increasingly important. A critical factor differentiating replay speech and genuine speech is the representation of device information. Replay speech carries physical device information that originates from recording device, playback device, and environmental noise. In this work, a device-related linear transformation strategy is proposed to disentangle non-device information from replay speech. First, we conduct factor analysis by introducing a common vector for both replay utterance and the corresponding genuine speech utterance on parallel training data; then, we derive an expectation maximization formula to obtain the parameters of the device-related linear transformation; subsequently, three device feature extraction methods are developed based on the device-related linear transformation. The developed device features are evaluated on ASVspoof 2017 version 2.0 and ASVspoof 2021 physical access corpora. The experimental results demonstrate that our proposed linear transformation strategy is effective for replay spoofing detection, and the resultant device features outperform many typical features. Moreover, our spoofing detection systems display superior performance over several competitive state-of-the-art systems.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1574–1586},
numpages = {13}
}

@inproceedings{10.1145/3490099.3511129,
author = {Heck, Melanie and Shon, Seong Hyun and Becker, Christian},
title = {Does Using Voice Authentication in Multimodal Systems Correlate With Increased Speech Interaction During Non-critical Routine Tasks?},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511129},
doi = {10.1145/3490099.3511129},
abstract = {Multimodal systems offer their functionalities through multiple communication channels. A messenger application may take either keyboard or voice input, and present incoming messages as text or audio output. This allows the users to communicate with their devices using the modality that best suits their context and personal preference. Authentication is often the first interaction with an application. The users’ login behavior can thus be used to immediately adapt the communication channel to their preferences. Yet given the sensitive nature of authentication, this interaction may not be representative for the user’s inclination to use speech input in non-critical routine tasks. In this paper, we test whether the interactions during authentication differ from non-critical routine tasks in a smart home application. Our findings indicate that, even in such a private space, the authentication behavior does not correlate with the use, nor with the perceived usability of speech input during non-critical task. We further find that short interactions with the system are not indicative of the user’s attitude towards audio output, independent of whether authentication or non-critical tasks are performed. Since security concerns are minmized in the secure environment of private spaces, our findings can be generalized to other contexts where security threats are even more apparent.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {868–877},
numpages = {10},
keywords = {modality arbitration, multimodal interfaces, smart home application},
location = {Helsinki, Finland},
series = {IUI '22}
}

@inproceedings{10.1145/3629606.3629619,
author = {Wang, Liuping and Li, Hongxin and Wu, Tong and Yang, Fan and Yang, Yang and Huang, Jin and Tian, Feng},
title = {Extrovert Increases Consensus? Exploring the Effects of Conversational Agent Personality for Group Decision Support},
year = {2024},
isbn = {9798400716454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629606.3629619},
doi = {10.1145/3629606.3629619},
abstract = {Conversational agent research has significantly shifted from solely focusing on technical capabilities to emphasizing the agent’s social and conversational abilities. Previous research indicates that the agent’s personality, as an essential characteristic in an agent’s social and conversational behaviors, has a significant impact on user experiences. However, the role of the agent personality in a discussion process when an agent acts as a group decision facilitator has not been discussed. To fill this research gap, we conducted a Wizard-of-Oz experiment with 40 participants to investigate the differences in participants’ decisions after discussing with agents with two distinct personalities (i.e., introverts and extroverts). Our data showed that the extroverted agent was more effective at facilitating group decision consensus, but participants perceived the introverted agent as more useful.},
booktitle = {Proceedings of the Eleventh International Symposium of Chinese CHI},
pages = {127–138},
numpages = {12},
keywords = {Conversational Agents, Group Decision Making, Meeting Facilitation, Personality},
location = {Denpasar, Bali, Indonesia},
series = {CHCHI '23}
}

@inproceedings{10.1145/3691573.3691594,
author = {Mauricio, Claudio and Domingues, Gustavo and Padua, Iv\~{a} and Peres, Fabiana and Teixeira, Jo\~{a}o},
title = {What if HMDs could generate audio descriptions of real scenes for visually impaired people?},
year = {2024},
isbn = {9798400709791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691573.3691594},
doi = {10.1145/3691573.3691594},
abstract = {This paper explores the feasibility and effectiveness of using the Meta Quest 3 head-mounted display to generate real-time audio descriptions of real scenes for visually impaired individuals. The objective is to enhance the spatial awareness and interaction capabilities of visually impaired users by providing continuous, detailed audio descriptions of their surroundings. The methodology involves leveraging the Meta Quest 3 to capture real-time visual data, which is then processed and converted into audio descriptions using AI-driven software. Experiments were conducted in various real-world environments to test the accuracy and usability of the audio description system. Key findings indicate that the Meta Quest 3 can effectively deliver real-time audio descriptions with high accuracy, significantly improving the navigation and situational awareness of visually impaired users. This study demonstrates the potential of HMD technology to create more inclusive and accessible experiences, highlighting the importance of continued innovation in assistive technologies for the visually impaired.},
booktitle = {Proceedings of the 26th Symposium on Virtual and Augmented Reality},
pages = {203–212},
numpages = {10},
keywords = {Accessibility, HMDs, Spatial Awareness, Visually Impaired},
location = {Manaus, Brazil},
series = {SVR '24}
}

@inproceedings{10.1145/3510858.3510968,
author = {Zhou, Wei},
title = {AI Broadcast Host Expression Technology Based on Neural Network Algorithm},
year = {2022},
isbn = {9781450390422},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510858.3510968},
doi = {10.1145/3510858.3510968},
abstract = {With the continuous improvement of people's living standards, the demand for high quality, high efficiency and high-quality information transmission is becoming stronger and stronger. Especially with the rapid development of Internet technology, traditional media forms can no longer meet the current social communication needs. Under the background of the new era, new media methods such as networked broadcasting and multimedia also came into being and have been rapidly promoted and used. Therefore, based on neural network algorithm, this paper studies AI broadcasting host expression technology. Firstly, this paper expounds the concept of AI broadcasting, and then expounds the impact of AI on future broadcasting hosts and the problems of existing AI broadcasting. Finally, according to the optimization of neural network algorithm, this paper tests all aspects of AI broadcast host expression technology. The test results show that after the optimization of neural network algorithm, the average value of emotional expression of AI broadcast host is relatively stable in expression technology, and the audio output is also stable. At the same time, there is a sense of law in speech speed and speech rhythm, but it will still be unstable.},
booktitle = {2021 International Conference on Aviation Safety and Information Technology},
pages = {371–375},
numpages = {5},
location = {Changsha, China},
series = {ICASIT 2021}
}

@inproceedings{10.1145/3629526.3645054,
author = {Sarkar, Souvika and Babar, Mohammad Fakhruddin and Hassan, Md Mahadi and Hasan, Monowar and Karmaker Santu, Shubhra Kanti},
title = {Processing Natural Language on Embedded Devices: How Well Do Transformer Models Perform?},
year = {2024},
isbn = {9798400704444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629526.3645054},
doi = {10.1145/3629526.3645054},
abstract = {Voice-controlled systems are becoming ubiquitous in many IoT-specific applications such as home/industrial automation, automotive infotainment, and healthcare. While cloud-based voice services (eg Alexa, Siri) can leverage high-performance computing servers, some use cases (eg robotics, automotive infotainment) may require to execute the natural language processing (NLP) tasks offline, often on resource-constrained embedded devices. Transformer-based language models such as BERT and its variants are primarily developed with compute-heavy servers in mind. Despite the great performance of BERT models across various NLP tasks, their large size and numerous parameters pose substantial obstacles to offline computation on embedded systems. Lighter replacement of such language models (eg DistilBERT and TinyBERT) often sacrifice accuracy, particularly for complex NLP tasks. Until now, it is still unclear ca whether the state-of-the-art language models, viz BERT and its variants are deployable on embedded systems with a limited processor, memory, and battery power and cb if they do, what are the "right'' set of configurations and parameters to choose for a given NLP task. This paper presents aperformance study of transformer language models under different hardware configurations and accuracy requirements and derives empirical observations about these resource/accuracy trade-offs. In particular, we study how the most commonly used BERT-based language models (viz BERT, RoBERTa, DistilBERT, and TinyBERT) perform on embedded systems. We tested them on textitfour off-the-shelf embedded platforms (hardware) with 2 GB and 4 GB memory (ie a total of textiteight hardware configurations) and textitfour datasets (ie HuRIC, GoEmotion, CoNLL, WNUT17) running various NLP tasks. Our study finds that executing complex NLP tasks (such as "sentiment'' classification) on embedded systems isfeasible even without any GPUs (eg rpi with 2 GB of RAM). We release our implementations for community use. Our findings can help designers understand the deployability and performance of transformer language models, especially those based on BERT architectures.},
booktitle = {Proceedings of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {211–222},
numpages = {12},
location = {London, United Kingdom},
series = {ICPE '24}
}

@inproceedings{10.1145/3490099.3511130,
author = {Smith, Ronnie and Dragone, Mauro},
title = {A Dialogue-Based Interface for Active Learning of Activities of Daily Living},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511130},
doi = {10.1145/3490099.3511130},
abstract = {While Human Activity Recognition (HAR) systems may benefit from Active Learning (AL) by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in HAR systems, which utilises a dataset of natural language descriptions of common activities (which we make publicly available) and semantic similarity measures. Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply our work to an existing CASAS dataset in an active learning scenario, to demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) technically, as an effective way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to use our approach and an established method, and to subsequently compare the two. Results show the potential of our approach as a user-friendly mechanism for annotation of sensor data as part of an active learning system.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {820–831},
numpages = {12},
keywords = {Active Learning (AL), Human Activity Recognition (HAR) labelling, Human-in-the-Loop (HITL) annotation, natural language, semantic similarity},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1109/TASLP.2023.3288409,
author = {Borsos, Zal\'{a}n and Marinier, Rapha\"{e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
title = {AudioLM: A Language Modeling Approach to Audio Generation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3288409},
doi = {10.1109/TASLP.2023.3288409},
abstract = {We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2523–2533},
numpages = {11}
}

@article{10.1109/TASLP.2022.3190732,
author = {Gupta, Chitralekha and Li, Haizhou and Goto, Masataka},
title = {Deep Learning Approaches in Topics of Singing Information Processing},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3190732},
doi = {10.1109/TASLP.2022.3190732},
abstract = {Singing, the vocal productionof musical tones, is one of the most important elements of music. Addressing the needs of real-world applications, the study of technologies related to singing voices has become an increasingly active area of research. In this paper, we provide a comprehensive overview of the recent developments in the field of singing information processing, specifically in the topics of singing skill evaluation, singing voice synthesis, singing voice separation, and lyrics synchronization and transcription. We will especially focus on deep learning approaches including modern representation learning techniques for singing voices. We will also provide an overview of contributions in public datasets for singing voice research.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2422–2451},
numpages = {30}
}

@article{10.1109/TASLP.2021.3078364,
author = {Narendra, N.P. and Schuller, Bj\"{o}rn and Alku, Paavo},
title = {The Detection of Parkinson's Disease From Speech Using Voice Source Information},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3078364},
doi = {10.1109/TASLP.2021.3078364},
abstract = {Developing automatic methods to detect Parkinson's disease (PD) from speech has attracted increasing interest as these techniques can potentially be used in telemonitoring health applications. This article studies the utilization of voice source information in the detection of PD using two classifier architectures: traditional pipeline approach and end-to-end approach. The former consists of feature extraction and classifier stages. In feature extraction, the baseline acoustic features—consisting of articulation, phonation, and prosody features—were computed and voice source information was extracted using glottal features that were estimated by iterative adaptive inverse filtering (IAIF) and quasi-closed phase (QCP) glottal inverse filtering methods. Support vector machine classifiers were developed utilizing the baseline and glottal features extracted from every speech utterance and the corresponding &lt;italic&gt;healthy/PD&lt;/italic&gt; labels. The end-to-end approach uses deep learning models which were trained using both raw speech waveforms and raw voice source waveforms. In the latter, two glottal inverse filtering methods (IAIF and QCP) and zero frequency filtering method were utilized. The deep learning architecture consists of a combination of convolutional layers followed by a multilayer perceptron. Experiments were performed using PC-GITA speech database. From the traditional pipeline systems, the highest classification accuracy (67.93%) was given by combination of baseline and QCP-based glottal features. From the end-to-end-systems, the highest accuracy (68.56%) was given by the system trained using QCP-based glottal flow signals. Even though classification accuracies were modest for all systems, the study is encouraging as the extraction of voice source information was found to be most effective in both approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1925–1936},
numpages = {12}
}

@inproceedings{10.1145/3544548.3581131,
author = {Zheng, Chengbo and Wu, Yuheng and Shi, Chuhan and Ma, Shuai and Luo, Jiehui and Ma, Xiaojuan},
title = {Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581131},
doi = {10.1145/3544548.3581131},
abstract = {Existing research on human-AI collaborative decision-making focuses mainly on the interaction between AI and individual decision-makers. There is a limited understanding of how AI may perform in group decision-making. This paper presents a wizard-of-oz study in which two participants and an AI form a committee to rank three English essays. One novelty of our study is that we adopt a speculative design by endowing AI equal power to humans in group decision-making. We enable the AI to discuss and vote equally with other human members. We find that although the voice of AI is considered valuable, AI still plays a secondary role in the group because it cannot fully follow the dynamics of the discussion and make progressive contributions. Moreover, the divergent opinions of our participants regarding an “equal AI” shed light on the possible future of human-AI relations.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {351},
numpages = {19},
keywords = {automated essay grading, group decision-making, human-AI collaboration, qualitative study},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3674969,
author = {Fang, Wei},
title = {INTERPRETING AND TRANSLATING THE KOREAN LANGUAGE BASED ON THE MACHINE TRANSLATION MODEL FOR COLLEGE STUDENTS},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3674969},
doi = {10.1145/3674969},
abstract = {Korean languages (KL) have the first, middle, and final consonant sounds. Because the basic consonants were produced in imitation of the human pronunciation organs, mimicking the forms of the organ of articulation when they are uttered makes it stand out. Research into text identification and translation for display boards has dominated contemporary machine vision work. When it comes to automated text translation, two examples come to mind: tour guide software and hotel room service bots. The fundamental issue is that the two languages have distinct pronunciations and grammatical structures. Many English sounds have no equivalent in the Korean language, making learning the language more difficult for Korean speakers. KL-MTM is gaining popularity in the classroom, although assessment is difficult for human raters, such as language instructors, because of the time and effort required to evaluate informational equivalency between the source-language message and its translations in the target language. Overall, students utilize them for various purposes to supplement their language education, whether at home or in the classroom. Results show a wide range of student dependencies and values for these tools, with some students relying heavily on them while others are less reliant on them. The students evaluated prominent KL-MTM tools, indicating difficulty critically analyzing their outcomes. Considerations for teaching and learning are addressed.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
keywords = {Korean languages, Machine, Translation model, Students, Education, Learning}
}

@article{10.1145/3539223,
author = {Malviya, Shrikant and Kumar, Piyush and Namasudra, Suyel and Tiwary, Uma Shanker},
title = {Experience Replay-based Deep Reinforcement Learning for Dialogue Management Optimisation},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3539223},
doi = {10.1145/3539223},
abstract = {Dialogue policy is a crucial component in task-oriented Spoken Dialogue Systems (SDSs). As a decision function, it takes the current dialogue state as input and generates appropriate system’s response. In this paper, we explore the reinforcement learning approaches to solve this problem in an Indic language scenario. Recently, Deep Reinforcement Learning (DRL) has been used to optimise the dialogue policy. However, many DRL approaches are not sample-efficient. Hence, particular attention is given to actor-critic methods based on off-policy reinforcement learning that utilise the Experience Replay (ER) technique for reducing the bias and variance to achieve high sample efficiency. ER based actor-critic methods, such as Advantage Actor-Critic Experience Replay (A2CER) are proven to deliver competitive results in gaming environments that are fully observable and have a very small action-set. While, in SDSs, the states are not fully observable and often have to deal with the large action space. Describing the limitations of traditional methods, i.e., value-based and policy-based methods, such as high variance, low sample-efficiency, and often converging to local optima, we firstly explore the use of A2CER in dialogue policy learning. It is shown to beat the current state-of-the-art deep learning methods for SDS. Secondly, to handle the issues of early-stage performance, we utilise a demonstration corpus to pre-train the models prior to on-line policy learning. We thus experiment with the A2CER on a larger action space and find it significantly faster than the current state-of-the-art. Combining both approaches, we present a novel DRL based dialogue policy optimisation method, A2CER and its effectiveness for a task-oriented SDS in the Indic language.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
keywords = {deep reinforcement learning, dialogue management, Spoken dialogue systems}
}

@article{10.1145/3436755,
author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
title = {When Machine Learning Meets Privacy: A Survey and Outlook},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436755},
doi = {10.1145/3436755},
abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {31},
numpages = {36},
keywords = {privacy, differential privacy, deep learning, Machine learning}
}

@article{10.1109/TASLP.2024.3426301,
author = {Porjazovski, Dejan and Gr\'{o}sz, Tam\'{a}s and Kurimo, Mikko},
title = {From Raw Speech to Fixed Representations: A Comprehensive Evaluation of Speech Embedding Techniques},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3426301},
doi = {10.1109/TASLP.2024.3426301},
abstract = {Speech embeddings, fixed-size representations derived from raw audio data, play a crucial role in diverse machine learning applications. Despite the abundance of speech embedding techniques, selecting the most suitable one remains challenging. Existing studies often focus on intrinsic or extrinsic aspects, seldom exploring both simultaneously. Furthermore, comparing the state-of-the-art pre-trained models with prior speech embedding solutions is notably scarce in the literature. To address these gaps, we undertake a comprehensive evaluation of both small and large-scale speech embedding models, which, in our opinion, needs to incorporate both intrinsic and extrinsic assessments. The intrinsic experiments delve into the models' ability to pick speaker-related characteristics and assess their discriminative capacities, providing insights into their inherent capabilities and internal workings. Concurrently, the extrinsic experiments evaluate whether the models learned semantic cues during pre-training. The findings underscore the superior performance of the large-scale pre-trained models, albeit at an elevated computational cost. The base self-supervised models show comparable results to their large counterparts, making them a better choice for many applications. Furthermore, we show that by selecting the most crucial dimensions, the models' performance often does not suffer drastically and even improves in some cases. This research contributes valuable insights into the nuanced landscape of speech embeddings, aiding researchers and practitioners in making informed choices for various applications.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {3546–3560},
numpages = {15}
}

@article{10.1109/TASLP.2022.3145297,
author = {An, Xiaochun and Soong, Frank K. and Xie, Lei},
title = {Disentangling Style and Speaker Attributes for TTS Style Transfer},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3145297},
doi = {10.1109/TASLP.2022.3145297},
abstract = {End-to-end neural TTS has shown improved performance in speech style transfer. However, the improvement is still limited by the available training data in both target styles and speakers. Additionally, degenerated performance is observed when the trained TTS tries to transfer the speech to a target style from a new speaker with an unknown, arbitrary style. In this paper, we propose a new approach to seen and unseen style transfer training on disjoint, multi-style datasets, i. e., datasets of different styles are recorded, one individual style by one speaker in multiple utterances. An inverse autoregressive flow (IAF) technique is first introduced to improve the variational inference for learning an expressive style representation. A speaker encoder network is then developed for learning a discriminative speaker embedding, which is jointly trained with the rest neural TTS modules. The proposed approach of seen and unseen style transfer is effectively trained with six specifically-designed objectives: reconstruction loss, adversarial loss, style distortion loss, cycle consistency loss, style classification loss, and speaker classification loss. Experiments demonstrate, both objectively and subjectively, the effectiveness of the proposed approach for seen and unseen style transfer tasks. The performance of our approach is superior to and more robust than those of four other reference systems of prior art.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {646–658},
numpages = {13}
}

@inproceedings{10.1109/SC41406.2024.00044,
author = {Lu, Chengzhi and Xu, Huanle and Li, Yudan and Chen, Wenyan and Ye, Kejiang and Xu, Chengzhong},
title = {SMIless: Serving DAG-based Inference with Dynamic Invocations under Serverless Computing},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00044},
doi = {10.1109/SC41406.2024.00044},
abstract = {The deployment of ML serving applications, featuring multiple inference functions on serverless platforms, has gained substantial popularity, leading to numerous developments of new systems. However, these systems often focus on optimizing resource provisioning and cold start management separately, ultimately resulting in higher monetary costs.This paper introduces SMIless, a highly efficient serverless system tailored for serving DAG-based ML inference in heterogeneous environments. SMIless effectively co-optimizes resource configuration and cold-start management in the context of dynamic invocations. This is achieved by seamlessly integrating adaptive pre-warming windows, striking an effective balance between performance and cost. We have implemented SMIless on top of OpenFaaS and conducted extensive evaluations using real-world ML serving applications. The experimental results demonstrate that SMIless can achieve up to a 5.73 \texttimes{} reduction in the overall costs while meeting the SLA requirements for all user requests, surpassing the performance of state-of-the-art solutions.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {38},
numpages = {17},
keywords = {DAG-based Inference, Dynamic Invocation, Serverless},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.1145/3459637.3482395,
author = {Singla, Yaman Kumar and Gupta, Avyakt and Bagga, Shaurya and Chen, Changyou and Krishnamurthy, Balaji and Shah, Rajiv Ratn},
title = {Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482395},
doi = {10.1145/3459637.3482395},
abstract = {Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a candidate's speaking proficiency in a language. ASS systems face many challenges like open grammar, variable pronunciations, and unstructured or semi-structured content. Recent deep learning approaches have shown some promise in this domain. However, most of these approaches focus on extracting features from single audio, making them suffer from the lack of speaker-specific context required to model such a complex task. We propose a novel deep learning technique for non-native ASS, called speaker-conditioned hierarchical modelling. In our technique, we take advantage of the fact that oral proficiency tests rate multiple responses for a candidate. We extract context vectors from these responses and feed them as additional speaker-specific context to our network to score a particular response. We compare our technique with strong baselines and find that such modelling improves the model's average performance by 6.92% (maximum = 12.86%, minimum = 4.51%). We further show both quantitative and qualitative insights into the importance of this additional context in solving the problem of ASS.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {1681–1691},
numpages = {11},
keywords = {spontaneous speech, multi-modal deep learning, interpretability in ai, hierarchical modeling, end-to-end neural networks, automated speech scoring, ai in education},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3503161.3547787,
author = {Tang, Jingqun and Qiao, Su and Cui, Benlei and Ma, Yuhang and Zhang, Sheng and Kanoulas, Dimitrios},
title = {You Can even Annotate Text with Voice: Transcription-only-Supervised Text Spotting},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547787},
doi = {10.1145/3503161.3547787},
abstract = {End-to-end scene text spotting has recently gained great attention in the research community. The majority of existing methods rely heavily on the location annotations of text instances (e.g., word-level boxes, word-level masks, and char-level boxes). We demonstrate that scene text spotting can be accomplished solely via text transcription, significantly reducing the need for costly location annotations. We propose a query-based paradigm to learn implicit location features via the interaction of text queries and image embeddings. These features are then made explicit during the text recognition stage via an attention activation map. Due to the difficulty of training the weakly-supervised model from scratch, we address the issue of model convergence via a circular curriculum learning strategy. Additionally, we propose a coarse-to-fine cross-attention localization mechanism for more precisely locating text instances. Notably, we provide a solution for text spotting via audio annotation, which further reduces the time required for annotation. Moreover, it establishes a link between audio, text, and image modalities in scene text spotting. Using only transcription annotations as supervision on both real and synthetic data, we achieve competitive results on several popular scene text benchmarks. The proposed method offers a reasonable trade-off between model accuracy and annotation time, allowing simplification of large-scale text spotting applications.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4154–4163},
numpages = {10},
keywords = {weakly-supervised text spotting, coarse-to-fine cross attention, audio annotation},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1145/3582900.3582907,
author = {Sarikaya, Ruhi},
title = {Intelligent Conversational Agents for Ambient Computing: A Keynote at SIGIR 2022},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3582900.3582907},
doi = {10.1145/3582900.3582907},
abstract = {We are in the midst of an AI revolution. Three primary disruptive changes set off this revolution: 1) increase in compute power, mobile internet, and advances in deep learning. The next decade is expected to be about the proliferation of Internet-of-Things (IoT) devices and sensors, which will generate exponentially larger amounts of data to reason over and pave the way for ambient computing. This will also give rise to new forms of interaction patterns with these systems. Users will have to interact with these systems under increasingly richer context and in real-time. Conversational AI has a critical role to play in this revolution, but only if it delivers on its promise of enabling natural, frictionless, and personalized interactions in any context the user is in, while hiding the complexity of these systems through ambient intelligence. However, current commercial conversational AI systems are trained primarily with a supervised learning paradigm, which is difficult, if not impossible, to scale by manually annotating data for increasingly complex sets of contextual conditions. Inherent ambiguity in natural language further complicates the problem. We need to devise new forms of learning paradigms and frameworks that will scale to this complexity. In this talk, we present some early steps we are taking with Alexa, Amazon's Conversational AI system, to move from supervised learning to self-learning methods, where the AI relies on customer interactions for supervision in our journey to ambient intelligence.Date: 14 July 2022.},
journal = {SIGIR Forum},
month = jan,
articleno = {4},
numpages = {10}
}

@inproceedings{10.5555/3523760.3523813,
author = {Berzuk, James M. and Young, James E.},
title = {More Than Words: A Framework for Describing Human-Robot Dialog Designs},
year = {2022},
publisher = {IEEE Press},
abstract = {This paper presents a novel framework for describing human-robot interaction dialog, developed from a survey and analysis of existing systems and research. We collected data from 75 published systems and conducted an iterative thematic analysis to distill the broad range of work into key underlying factors defining them. Our framework provides a language to describe human-robot dialog systems and a new way of classifying and understanding human-robot dialog, in terms of both high-level design aspects and relevant implementation details. Our quantitative survey summary further provides a detailed, contemporary snapshot of predominant approaches in the field, highlighting opportunities for further exploration.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {393–401},
numpages = {9},
keywords = {dialog, framework, human-robot interaction, social robotics, survey},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3581783.3611765,
author = {Yang, Songlin and Wang, Wei and Ling, Jun and Peng, Bo and Tan, Xu and Dong, Jing},
title = {Context-Aware Talking-Head Video Editing},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3611765},
doi = {10.1145/3581783.3611765},
abstract = {Talking-head video editing aims to efficiently insert, delete, and substitute the word of a pre-recorded video through a text transcript editor. The key challenge for this task is obtaining an editing model that generates new talking-head video clips which simultaneously have accurate lip synchronization and motion smoothness. Previous approaches, including 3DMM-based (3D Morphable Model) methods and NeRF-based (Neural Radiance Field) methods, are sub-optimal in that they either require minutes of source videos and days of training time or lack the disentangled control of verbal (e.g., lip motion) and non-verbal (e.g., head pose and expression) representations for video clip insertion. In this work, we fully utilize the video context to design a novel framework for talking-head video editing, which achieves efficiency, disentangled motion control, and sequential smoothness. Specifically, we decompose this framework to motion prediction and motion-conditioned rendering: (1) We first design an animation prediction module that efficiently obtains smooth and lip-sync motion sequences conditioned on the driven speech. This module adopts a non-autoregressive network to obtain context prior and improve the prediction efficiency, and it learns a speech-animation mapping prior with better generalization to novel speech from a multi-identity video dataset. (2) We then introduce a neural rendering module to synthesize the photo-realistic and full-head video frames given the predicted motion sequence. This module adopts a pre-trained head topology and uses only few frames for efficient fine-tuning to obtain a person-specific rendering model. Extensive experiments demonstrate that our method efficiently achieves smoother editing results with higher image quality and lip accuracy using less data than previous methods.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {7718–7727},
numpages = {10},
keywords = {face animation, neural radiance field (nerf), talking face/head synthesis, video editing},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3594806.3596572,
author = {Alessa, Abeer and Al-Khalifa, Hend},
title = {Towards Designing a ChatGPT Conversational Companion for Elderly People},
year = {2023},
isbn = {9798400700699},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594806.3596572},
doi = {10.1145/3594806.3596572},
abstract = {Loneliness and social isolation are serious and widespread problems among older people, affecting their physical and mental health, quality of life, and longevity. In this paper, we propose a ChatGPT-based conversational companion system for elderly people. The system is designed to provide companionship and help reduce feelings of loneliness and social isolation. The system was evaluated with a preliminary study. The results showed that the system was able to generate responses that were relevant to the created elderly personas. However, it is essential to acknowledge the limitations of ChatGPT, such as potential biases and misinformation, and to consider the ethical implications of using AI-based companionship for the elderly, including privacy concerns.},
booktitle = {Proceedings of the 16th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {667–674},
numpages = {8},
keywords = {ChatGPT, conversational agent, elderly, isolation, loneliness, mental health, physical health, social interaction},
location = {Corfu, Greece},
series = {PETRA '23}
}

@inproceedings{10.1145/3648536.3648538,
author = {Maheux, Marc-Antoine and L\'{e}tourneau, Dominic and Warren, Philippe and Panchea, Adina M. and Robillard, Julie M. and Michaud, Fran\c{c}ois},
title = {Designing a Tabletop SAR as an Advanced HRI Experimentation Platform},
year = {2024},
isbn = {9798400716614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648536.3648538},
doi = {10.1145/3648536.3648538},
abstract = {Tabletop robot platforms are being designed and used to experiment and develop human-robot interaction modalities. Progress in the field of socially assistive robotics is influenced by the perceptual, reasoning and operational capabilities available on these robots. This paper presents an open-source platform designed to facilitate prototyping advanced interaction capabilities onboard on a tabletop robot. It describes the hardware and software components implemented, along with resource usage and power consumption. It also presents an interaction scenario implemented on the platform to demonstrate its capabilities.},
booktitle = {Proceedings of the 2024 International Symposium on Technological Advances in Human-Robot Interaction},
pages = {10–19},
numpages = {10},
keywords = {Deep neural networks, Human-robot interaction, Robot design, Socially assistive robots},
location = {Boulder, CO, USA},
series = {TAHRI '24}
}

@inproceedings{10.1145/3459637.3481906,
author = {Wang, Zihao and Wang, Fudong and Zhang, Haipeng and Yang, Minghui and Cao, Shaosheng and Wen, Zujie and Zhang, Zhe},
title = { 'Could You Describe the Reason for the Transfer?': A Reinforcement Learning Based Voice-Enabled Bot Protecting Customers from Financial Frauds},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3481906},
doi = {10.1145/3459637.3481906},
abstract = {With the booming of the Internet finance and e-payment business, telecom and online fraud has become a serious problem which grows rapidly. In China, 351 billion RMB (approximately 0.3% of China's GDP) was lost in 2018 due to telecommunication and online fraud, influencing tens of millions of individual customers. Anti-fraud algorithms have been widely adopted by major Internet finance companies to detect and block transactions induced by scam. However, due to limited contextual information, most systems would probably mistakenly block the normal transactions, leading to poor user experience. On the other hand, if the transactions induced by scam are detected yet not fully explained to the users, the users will continue to pay, suffering from direct financial losses.To address these problems, we design a voice-enabled bot that interacts with the customers who are involved with potential telecommunication and online frauds decided by the back-end system. The bot seeks additional information from the customers through natural conversations to confirm whether the customers are scammed and identify the actual fraud types. The details about the frauds are then provided to convince the customers that they are on the edge of being scammed. Our bot adopts offline reinforcement learning (RL) to learn dialogue policies from real-world human-human chat logs. During the conversations, our bot also identifies fraud types every turn based on the dialogue state.The bot proposed outperforms baseline dialogue strategies by 2.8% in terms of task success rate, and 5% in terms of dialogue accuracy in offline evaluations. Furthermore, in the 8 months of real-world deployment, our bot lowers the dissatisfaction rate by 25% and increases the fraud prevention rate by 135% relatively, indicating a significant improvement in user experience as well as anti-fraud effectiveness. More importantly, we help prevent millions of users from being deceived, and avoid trillions of financial losses.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {4214–4223},
numpages = {10},
keywords = {reinforcement learning, outbound bot, dialogue system, dialogue risk detection, dialogue policy},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1109/TASLP.2022.3180684,
author = {Bhati, Saurabhchand and Villalba, Jes\'{u}s and \.{Z}elasko, Piotr and Moro-Velazquez, Laureano and Dehak, Najim},
title = {Unsupervised Speech Segmentation and Variable Rate Representation Learning Using Segmental Contrastive Predictive Coding},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3180684},
doi = {10.1109/TASLP.2022.3180684},
abstract = {Typically, unsupervised segmentation of speech into the phone- and wordlike units are treated as separate tasks and are often done via different methods which do not fully leverage the inter-dependence of the two tasks. Here, we unify them and propose a technique that can jointly perform both, showing that these two tasks indeed benefit from each other. Recent attempts employ self-supervised learning, such as contrastive predictive coding (CPC), where the next frame is predicted given past context. However, CPC only looks at the audio signal's frame-level structure. We overcome this limitation with a segmental contrastive predictive coding (SCPC) framework to model the signal structure at a higher level, e.g., phone level. A convolutional neural network learns frame-level representation from the raw waveform via noise-contrastive estimation (NCE). A differentiable boundary detector finds variable-length segments, which are then used to optimize a segment encoder via NCE to learn segment representations. The differentiable boundary detector allows us to train frame-level and segment-level encoders jointly. Experiments show that our single model outperforms existing phone and word segmentation methods on TIMIT and Buckeye datasets. Finally, we use SCPC to extract speech features at the segment level rather than at the uniformly spaced frame level (e.g., 10 ms) and produce variable rate representations that change according to the contents of the utterance. We lower the feature extraction rate from the typical 100 Hz to 14.5 Hz on average while still outperforming the hand-crafted features such as MFCC on the linear phone classification task.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2002–2014},
numpages = {13}
}

@article{10.1109/TASLP.2022.3153270,
author = {Lin, Weiwei and Mak, Man-Wai},
title = {Mixture Representation Learning for Deep Speaker Embedding},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153270},
doi = {10.1109/TASLP.2022.3153270},
abstract = {How to effectively convert a sequence of variable-length acoustic features to a fixed-dimension representation has always been a research focus in speaker recognition. In state-of-the-art speaker recognition systems, the conversion is implemented by concatenating the mean and the standard deviation of a sequence of frame-level features. However, a single mean and a single standard deviation are limited descriptive statistics for an acoustic sequence even with powerful feature extractors such as convolutional neural networks. In this paper, we propose a novel statistics pooling method that can produce more descriptive statistics through a mixture representation. Our approach is inspired by the expectation–maximization (EM) algorithm in Gaussian mixture models (GMMs). Instead of using traditional GMM style alignment, we novelly leverage modern deep learning tools to produce a more powerful mixture representation. The novelty includes: (1) unlike GMMs, the mixture assignments are determined by an attention network instead of the Euclidean distances between the frame-level features and explicit centers; (2) instead of using a single frame as input to the attention network, contextual frames are included to smooth out attention transition; and (3) soft-attention assignments are replaced by hard-attention assignments via the Gumbel-Softmax with straight-through estimators. With the proposed attention mechanism, we obtained a 13.7% relative improvement over vanilla mean and standard deviation pooling in the VOiCES19-eval set.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {968–978},
numpages = {11}
}

@article{10.1109/TASLP.2023.3285283,
author = {Liu, Xuechen and Wang, Xin and Sahidullah, Md and Patino, Jose and Delgado, H\'{e}ctor and Kinnunen, Tomi and Todisco, Massimiliano and Yamagishi, Junichi and Evans, Nicholas and Nautsch, Andreas and Lee, Kong Aik},
title = {ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3285283},
doi = {10.1109/TASLP.2023.3285283},
abstract = {Benchmarking initiatives support the meaningful comparison of competing solutions to prominent problems in speech and language processing. Successive benchmarking evaluations typically reflect a progressive evolution from ideal lab conditions towards to those encountered in the wild. ASVspoof, the spoofing and deepfake detection initiative and challenge series, has followed the same trend. This article provides a summary of the ASVspoof 2021 challenge and the results of 54 participating teams that submitted to the evaluation phase. For the logical access (LA) task, results indicate that countermeasures are robust to newly introduced encoding and transmission effects. Results for the physical access (PA) task indicate the potential to detect replay attacks in real, as opposed to simulated physical spaces, but a lack of robustness to variations between simulated and real acoustic environments. The Deepfake (DF) task, new to the 2021 edition, targets solutions to the detection of manipulated, compressed speech data posted online. While detection solutions offer some resilience to compression effects, they lack generalization across different source datasets. In addition to a summary of the top-performing systems for each task, new analyses of influential data factors and results for hidden data subsets, the article includes a review of post-challenge results, an outline of the principal challenge limitations and a road-map for the future of ASVspoof.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jun,
pages = {2507–2522},
numpages = {16}
}

@inproceedings{10.5555/3523760.3523770,
author = {Sandygulova, Anara and Amirova, Aida and Telisheva, Zhansaule and Zhanatkyzy, Aida and Rakhymbayeva, Nazerke},
title = {Individual Differences of Children with Autism in Robot-assisted Autism Therapy},
year = {2022},
publisher = {IEEE Press},
abstract = {Research has recognized the importance of individual differences of children with Autism Spectrum Disorder (ASD) that require interventions to meet their heterogeneous needs. This relatively large-scale study investigates a robot-assisted autism therapy (RAAT) with 34 children with diverse forms of ASD and Attention Deficit Hyperactivity Disorder (ADHD). We conducted a multi-session study with multi-purposeful activities targeting the socio-emotional abilities of children in a rehabilitation setting. We found a number of quantitative results suggesting various autism-related and demographic differences such as diverse forms of ASD, co-occurrence of ADHD, verbal skills, and age groups. The main findings are: 1) severity of ASD forms may not predict intervention outcomes but instead the co-occurrence of ADHD with LFA diagnosis may negatively impact social smiling; 2) verbal children were more generally engaged and less aggressive with the robot than non-verbal children whose curiosity rose over sessions; and 3) younger children (3.4 y.o.) showed more affection, while older children (7-12 y.o.) were better engaged through speaking more words and having longer engagement and eye contact with the robot.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {43–52},
numpages = {10},
keywords = {age, asd, children, engagement, hri, individual differences, robot, robot-assisted autism therapy, verbality},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@inproceedings{10.1145/3411764.3445659,
author = {Kuttal, Sandeep Kaur and Ong, Bali and Kwasny, Kate and Robe, Peter},
title = {Trade-offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445659},
doi = {10.1145/3411764.3445659},
abstract = {Pair programming has a documented history of benefits, such as increased code quality, productivity, self-efficacy, knowledge transfer, and reduced gender gap. Research uncovered problems with pair programming related to scheduling, collocating, role imbalance, and power dynamics. We investigated the trade-offs of substituting a human with an agent to simultaneously provide benefits and alleviate obstacles in pair programming. We conducted gender-balanced studies with human-human pairs in a remote lab with 18 programmers and Wizard-of-Oz studies with 14 programmers, then analyzed results quantitatively and qualitatively. Our comparative analysis of the two studies showed no significant differences in productivity, code quality, and self-efficacy. Further, agents facilitated knowledge transfer; however, unlike humans, agents were unable to provide logical explanations or discussions. Human partners trusted and showed humility towards agents. Our results demonstrate that agents can act as effective pair programming partners and open the way towards new research on conversational agents for programming.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {243},
numpages = {20},
keywords = {empirical evaluation, conversational agents, avatars, Pair programming},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3568162.3576963,
author = {Zhanatkyzy, Aida and Telisheva, Zhansaule and Amirova, Aida and Rakhymbayeva, Nazerke and Sandygulova, Anara},
title = {Multi-Purposeful Activities for Robot-Assisted Autism Therapy: What Works Best for Children's Social Outcomes?},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3576963},
doi = {10.1145/3568162.3576963},
abstract = {This research designed and applied 24 multi-purposeful robot activities of varying social mediation levels in a multiple-session experiment with 34 children of diverse autistic characteristics in a rehabilitation setting. This paper explores what type of robot activities can meet individual needs to bring more socio-behavioral progress and juxtaposes child characteristics to identify behavioral outcomes in each activity. This knowledge would help us to respond to the question of what activity types suit specific subgroups of Autism Spectrum Disorder (ASD). Our data analysis included coding 48.5 hours of video data for a total of 14 measures to fully capture children's activity-based socio-emotional outcomes. Overall, the activities on varying social mediation levels brought more or less positive social outcomes to all children. However, children showed some different behavioral outcomes as mediated by core autism-related and age-specific characteristics. This study provides in-depth accounts of what might be helpful in designing and applying multi-purposeful activities responsive to the diverse needs of children.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {34–43},
numpages = {10},
keywords = {autism spectrum disorder (asd), children, robot-assisted therapy},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.1145/3578358.3591330,
author = {Bauwens, Jim and De Porre, Kevin and Gonzalez Boix, Elisa},
title = {[Short paper] Towards improved collaborative text editing CRDTs by using Natural Language Processing},
year = {2023},
isbn = {9798400700866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3578358.3591330},
doi = {10.1145/3578358.3591330},
abstract = {Collaborative text editing systems are used in a variety of cloud-based products. To ensure that documents remain consistent between users, these systems often rely on CRDTs, operational transformation, or other techniques for achieving (strong) eventual consistency. CRDT-based approaches are appealing as they incorporate strategies to ensure that concurrent updates cannot conflict. However, these strategies do not necessarily take into account program semantics and may result in unexpected behaviour from the end-user's perspective.For example, conflict resolution strategies in collaborative text editors may lead to duplicate words and incorrectly merged sentences. This position paper investigates the use of deterministic natural language processing (NLP) algorithms to improve the concurrency semantics of collaborative text editing systems that rely on CRDTs, aiming to provide a better end-user experience. We explore what is needed to ensure convergence, and highlight potential difficulties with the approach.},
booktitle = {Proceedings of the 10th Workshop on Principles and Practice of Consistency for Distributed Data},
pages = {51–55},
numpages = {5},
location = {Rome, Italy},
series = {PaPoC '23}
}

@inproceedings{10.1145/3472306.3478362,
author = {Kantharaju, Reshmashree B. and Pelachaud, Catherine},
title = {Social Signals of Cohesion in Multi-party Interactions},
year = {2021},
isbn = {9781450386197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472306.3478362},
doi = {10.1145/3472306.3478362},
abstract = {Group conversation is a frequently used form of communication for exchanging ideas and making decisions. Cohesion is an emergent phenomenon that describes the members' attraction towards the group and towards working together. In this paper, we present the cohesion labels assigned to segments from [redacted], a multimodal dataset of simulated medical consultations. Then, we present the analysis performed to identify social cues that characterize cohesion and report the accuracy for classifying cohesion. Results show that non-verbal social cues like gaze, facial AUs, laughter etc., indeed convey information regarding the level of cohesion. Finally we present a preliminary evaluation conducted using the prominent cues to simulate a cohesive group of agents.},
booktitle = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},
pages = {9–16},
numpages = {8},
keywords = {Social Signals, Multimodal Database, Multi-party, Group Cohesion},
location = {Virtual Event, Japan},
series = {IVA '21}
}

@article{10.1109/TASLP.2021.3054302,
author = {Wang, Heming and Wang, DeLiang},
title = {Towards Robust Speech Super-Resolution},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3054302},
doi = {10.1109/TASLP.2021.3054302},
abstract = {Speech super-resolution (SR) aims to increase the sampling rate of a given speech signal by generating high-frequency components. This paper proposes a convolutional neural network (CNN) based SR model that takes advantage of information from both time and frequency domains. Specifically, the proposed CNN is a time-domain model that takes the raw waveform of low-resolution speech as the input, and outputs an estimate of the corresponding high-resolution waveform. During the training stage, we employ a cross-domain loss to optimize the network. We compare our model with several deep neural network (DNN) based SR models, and experiments show that our model outperforms existing models. Furthermore, the robustness of DNN-based models is investigated, in particular regarding microphone channels and downsampling schemes, which have a major impact on the performance of DNN-based SR models. By training with proper datasets and preprocessing, we improve the generalization capability for untrained microphone channels and unknown downsampling schemes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {2058–2066},
numpages = {9}
}

@article{10.1109/TASLP.2021.3120644,
author = {Wang, Xinsheng and van der Hout, Justin and Zhu, Jihua and Hasegawa-Johnson, Mark and Scharenborg, Odette},
title = {Synthesizing Spoken Descriptions of Images},
year = {2021},
issue_date = {2021},
publisher = {IEEE Press},
volume = {29},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3120644},
doi = {10.1109/TASLP.2021.3120644},
abstract = {Image captioning technology has great potential in many scenarios. However, current text-based image captioning methods cannot be applied to approximately half of the world's languages due to these languages’ lack of a written form. To solve this problem, recently the image-to-speech task was proposed, which generates spoken descriptions of images bypassing any text via an intermediate representation consisting of phonemes (image-to-phoneme). Here, we present a comprehensive study on the image-to-speech task in which, 1) several representative image-to-text generation methods are implemented for the image-to-phoneme task, 2) objective metrics are sought to evaluate the image-to-phoneme task, and 3) an end-to-end image-to-speech model that is able to synthesize spoken descriptions of images bypassing both text and phonemes is proposed. Extensive experiments are conducted on the public benchmark database Flickr8k. Results of our experiments demonstrate that 1) State-of-the-art image-to-text models can perform well on the image-to-phoneme task, and 2) several evaluation metrics, including BLEU3, BLEU4, BLEU5, and ROUGE-L can be used to evaluate image-to-phoneme performance. Finally, 3) end-to-end image-to-speech bypassing text and phonemes is feasible.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {3242–3254},
numpages = {13}
}

@article{10.1145/3596266,
author = {Chen, Meng and Lu, Li and Wang, Junhao and Yu, Jiadi and Chen, Yingying and Wang, Zhibo and Ba, Zhongjie and Lin, Feng and Ren, Kui},
title = {VoiceCloak: Adversarial Example Enabled Voice De-Identification with Balanced Privacy and Utility},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
url = {https://doi.org/10.1145/3596266},
doi = {10.1145/3596266},
abstract = {Faced with the threat of identity leakage during voice data publishing, users are engaged in a privacy-utility dilemma when enjoying the utility of voice services. Existing machine-centric studies employ direct modification or text-based re-synthesis to de-identify users' voices but cause inconsistent audibility for human participants in emerging online communication scenarios, such as virtual meetings. In this paper, we propose a human-centric voice de-identification system, VoiceCloak, which uses adversarial examples to balance the privacy and utility of voice services. Instead of typical additive examples inducing perceivable distortions, we design a novel convolutional adversarial example that modulates perturbations into real-world room impulse responses. Benefiting from this, VoiceCloak could preserve user identity from exposure by Automatic Speaker Identification (ASI), while remaining the voice perceptual quality for non-intrusive de-identification. Moreover, VoiceCloak learns a compact speaker distribution through a conditional variational auto-encoder to synthesize diverse targets on demand. Guided by these pseudo targets, VoiceCloak constructs adversarial examples in an input-specific manner, enabling any-to-any identity transformation for robust de-identification. Experimental results show that VoiceCloak could achieve over 92% and 84% successful de-identification on mainstream ASIs and commercial systems with excellent voiceprint consistency, speech integrity, and audio quality.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = jun,
articleno = {48},
numpages = {21},
keywords = {adversarial examples, voice de-identification, voice privacy preservation}
}

@inproceedings{10.1145/3558482.3590189,
author = {Wang, Yuanda and Guo, Hanqing and Wang, Guangjing and Chen, Bocheng and Yan, Qiben},
title = {VSMask: Defending Against Voice Synthesis Attack via Real-Time Predictive Perturbation},
year = {2023},
isbn = {9781450398596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558482.3590189},
doi = {10.1145/3558482.3590189},
abstract = {Deep learning based voice synthesis technology generates artificial human-like speeches, which has been used in deepfakes or identity theft attacks. Existing defense mechanisms inject subtle adversarial perturbations into the raw speech audios to mislead the voice synthesis models. However, optimizing the adversarial perturbation not only consumes substantial computation time, but it also requires the availability of entire speech. Therefore, they are not suitable for protecting live speech streams, such as voice messages or online meetings. In this paper, we propose VSMask, a real-time protection mechanism against voice synthesis attacks. Different from offline protection schemes, VSMask leverages a predictive neural network to forecast the most effective perturbation for the upcoming streaming speech. VSMask introduces a universal perturbation tailored for arbitrary speech input to shield a real-time speech in its entirety. To minimize the audio distortion within the protected speech, we implement a weight-based perturbation constraint to reduce the perceptibility of the added perturbation. We comprehensively evaluate VSMask protection performance under different scenarios. The experimental results indicate that VSMask can effectively defend against 3 popular voice synthesis models. None of the synthetic voice could deceive the speaker verification models or human ears with VSMask protection. In a physical world experiment, we demonstrate that VSMask successfully safeguards the real-time speech by injecting the perturbation over the air.},
booktitle = {Proceedings of the 16th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
pages = {239–250},
numpages = {12},
keywords = {voice synthesis, real-time adversarial machine learning, privacy enhancement},
location = {Guildford, United Kingdom},
series = {WiSec '23}
}

@article{10.1109/TASLP.2024.3389643,
author = {Fan, Cunhang and Ding, Mingming and Tao, Jianhua and Fu, Ruibo and Yi, Jiangyan and Wen, Zhengqi and Lv, Zhao},
title = {Dual-Branch Knowledge Distillation for Noise-Robust Synthetic Speech Detection},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3389643},
doi = {10.1109/TASLP.2024.3389643},
abstract = {Most research in synthetic speech detection (SSD) focuses on improving performance on standard noise-free datasets. However, in actual situations, noise interference is usually present, causing significant performance degradation in SSD systems. To improve noise robustness, this paper proposes a dual-branch knowledge distillation synthetic speech detection (DKDSSD) method. Specifically, a parallel data flow of the clean teacher branch and the noisy student branch is designed, and interactive fusion module and response-based teacher-student paradigms are proposed to guide the training of noisy data from both the data distribution and decision-making perspectives. In the noisy student branch, speech enhancement is introduced initially for denoising, aiming to reduce the interference of strong noise. The proposed interactive fusion combines denoised features and noisy features to mitigate the impact of speech distortion and ensure consistency with the data distribution of the clean branch. The teacher-student paradigm maps the student's decision space to the teacher's decision space, enabling noisy speech to behave similarly to clean speech. Additionally, a joint training method is employed to optimize both branches for achieving global optimality. Experimental results based on multiple datasets demonstrate that the proposed method performs effectively in noisy environments and maintains its performance in cross-dataset experiments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {2453–2466},
numpages = {14}
}

@inproceedings{10.1145/3477314.3507013,
author = {Firc, Anton and Malinka, Kamil},
title = {The dawn of a text-dependent society: deepfakes as a threat to speech verification systems},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507013},
doi = {10.1145/3477314.3507013},
abstract = {Recent developments in the field of deepfakes bring new threats that take advantage of the fact that it is increasingly difficult to distinguish between real and artificial media. Nowadays, mostly as fake news or disinformation; however, there are still unexplored areas such as using deepfakes to spoof voice verification. We present a real-world use case for spoofing voice authentication in a customer care call center. Based on this scenario, we evaluate the feasibility of attacking such a system and create an attacker profile. For this purpose, we examine three available speech synthesis tools and discuss their usability. We use these tools and acquired knowledge to generate a dataset including deepfake speech and assess the resilience of voice biometrics systems against deepfakes. We prove that voice biometrics systems are indeed vulnerable to deepfake powered attacks. The most significant outcome is the proposal of text-dependent verification as a novel countermeasure for presented attacks. Text-dependent verification provides higher security than text-independent verification and can be used today as the simplest protection method against deepfakes.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1646–1655},
numpages = {10},
keywords = {cybersecurity, deepfakes, machine learning, speech verification, voice biometrics},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3581641.3584099,
author = {Alam, Md Zubair Ibne and Islam, Shehnaz and Hoque, Enamul},
title = {SeeChart: Enabling Accessible Visualizations Through Interactive Natural Language Interface For People with Visual Impairments},
year = {2023},
isbn = {9798400701061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581641.3584099},
doi = {10.1145/3581641.3584099},
abstract = {Web-based data visualizations have become very popular for exploring data and communicating insights. Newspapers, journals, and reports regularly publish visualizations to tell compelling stories with data. Unfortunately, most visualizations are inaccessible to readers with visual impairments. For many charts on the web, there are no accompanying alternative (alt) texts, and even if such texts exist they do not adequately describe important insights from charts. To address the problem, we first interviewed 15 blind users to understand their challenges and requirements for reading data visualizations. Based on the insights from these interviews, we developed SeeChart, an interactive tool that automatically deconstructs charts from web pages and then converts them to accessible visualizations for blind people by enabling them to hear the chart summary as well as to interact through data points using the keyboard. Our evaluation with 14 blind participants suggests the efficacy of SeeChart in understanding key insights from charts and fulfilling their information needs while reducing their required time and cognitive burden.},
booktitle = {Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {46–64},
numpages = {19},
keywords = {Accessible visualization, alt text, blind, chart summarization, disability, interactions, natural language generation},
location = {Sydney, NSW, Australia},
series = {IUI '23}
}

@inproceedings{10.1145/3397481.3450692,
author = {Kucherenko, Taras and Jonell, Patrik and Yoon, Youngwoo and Wolfert, Pieter and Henter, Gustav Eje},
title = {A Large, Crowdsourced Evaluation of Gesture Generation Systems on Common Data: The GENEA Challenge 2020},
year = {2021},
isbn = {9781450380171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397481.3450692},
doi = {10.1145/3397481.3450692},
abstract = {Co-speech gestures, gestures that accompany speech, play an important role in human communication. Automatic co-speech gesture generation is thus a key enabling technology for embodied conversational agents (ECAs), since humans expect ECAs to be capable of multi-modal communication. Research into gesture generation is rapidly gravitating towards data-driven methods. Unfortunately, individual research efforts in the field are difficult to compare: there are no established benchmarks, and each study tends to use its own dataset, motion visualisation, and evaluation methodology. To address this situation, we launched the GENEA Challenge, a gesture-generation challenge wherein participating teams built automatic gesture-generation systems on a common dataset, and the resulting systems were evaluated in parallel in a large, crowdsourced user study using the same motion-rendering pipeline. Since differences in evaluation outcomes between systems now are solely attributable to differences between the motion-generation methods, this enables benchmarking recent approaches against one another in order to get a better impression of the state of the art in the field. This paper reports on the purpose, design, results, and implications of our challenge.},
booktitle = {Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {11–21},
numpages = {11},
keywords = {conversational agents, evaluation paradigms, gesture generation},
location = {College Station, TX, USA},
series = {IUI '21}
}

@inproceedings{10.1145/3458709.3458946,
author = {Taheri, Atieh and Weissman, Ziv and Sra, Misha},
title = {Exploratory Design of a Hands-free Video Game Controller for a Quadriplegic Individual},
year = {2021},
isbn = {9781450384285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458709.3458946},
doi = {10.1145/3458709.3458946},
abstract = {From colored pixels to hyper-realistic 3D landscapes of virtual reality, video games have evolved immensely over the last few decades. However, video game input still requires two-handed dexterous finger manipulations for simultaneous joystick and trigger or mouse and keyboard presses. In this work, we explore the design of a hands-free game control method using realtime facial expression recognition for individuals with neurological and neuromuscular diseases who are unable to use traditional game controllers. Similar to other Assistive Technologies (AT), our facial input technique is also designed and tested in collaboration with a graduate student who has Spinal Muscular Atrophy. Our preliminary evaluation shows the potential of facial expression recognition for augmenting the lives of quadriplegic individuals by enabling them to accomplish things like walking, running, flying or other adventures that may not be so attainable otherwise.},
booktitle = {Proceedings of the Augmented Humans International Conference 2021},
pages = {131–140},
numpages = {10},
keywords = {video gaming, quadriplegia, input methods, hands-free, facial expressions, facial expression recognition, Accessibility},
location = {Rovaniemi, Finland},
series = {AHs '21}
}

@inproceedings{10.5555/3523760.3523822,
author = {Do\u{g}an, Fethiye Irmak and Torre, Ilaria and Leite, Iolanda},
title = {Asking Follow-Up Clarifications to Resolve Ambiguities in Human-Robot Conversation},
year = {2022},
publisher = {IEEE Press},
abstract = {When a robot aims to comprehend its human partner's request by identifying the referenced objects in Human-Robot Conversation, ambiguities can occur because the environment might contain many similar objects or the objects described in the request might be unknown to the robot. In the case of ambiguities, most of the systems ask users to repeat their request, which assumes that the robot is familiar with all of the objects in the environment. This assumption might lead to task failure, especially in complex real-world environments. In this paper, we address this challenge by presenting an interactive system that asks for follow-up clarifications to disambiguate the described objects using the pieces of information that the robot could understand from the request and the objects in the environment that are known to the robot. To evaluate our system while disambiguating the referenced objects, we conducted a user study with 63 participants. We analyzed the interactions when the robot asked for clarifications and when it asked users to re-describe the same object. Our results show that generating follow-up clarification questions helped the robot correctly identify the described objects with fewer attempts (i.e., conversational turns). Also, when people were asked clarification questions, they perceived the task as easier, and they evaluated the task understanding and competence of the robot as higher. Our code and anonymized dataset are publicly availablefootnotehttps://github.com/IrmakDogan/Resolving-Ambiguities.},
booktitle = {Proceedings of the 2022 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {461–469},
numpages = {9},
keywords = {follow-up clarifications, referring expressions, resolving ambiguities},
location = {Sapporo, Hokkaido, Japan},
series = {HRI '22}
}

@article{10.1109/TASLP.2024.3364063,
author = {Su, Shang-Yu and Chung, Yung-Sung and Chen, Yun-Nung},
title = {Joint Dual Learning With Mutual Information Maximization for Natural Language Understanding and Generation in Dialogues},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3364063},
doi = {10.1109/TASLP.2024.3364063},
abstract = {Modular conversational systems heavily rely on the performance of their natural language understanding (NLU) and natural language generation (NLG) components. NLU focuses on extracting core semantic concepts from input texts, while NLG constructs coherent sentences based on these extracted semantics. Inspired by information theory in digital communication, we introduce a one-way communication model that mirrors human conversations, comprising two distinct phases: (1) the conversion of thoughts into messages, similar to NLG, and (2) the comprehension of received messages, similar to NLU. This paper presents a novel algorithm that trains NLU and NLG collaboratively by concatenating their models and maximizing mutual information between inputs and outputs. This approach efficiently facilitates the transmission of semantics, leading to enhanced learning performance for both components. Our experimental results, based on three benchmark datasets, consistently demonstrate significant improvements for both NLU and NLG tasks, highlighting the practical promise of our proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {2445–2452},
numpages = {8}
}

@inproceedings{10.1145/3597031.3597057,
author = {Elgammal, Mohamed A. and Awad, Omar Mohamed and Vivancos, Isak Edo and Moshovos, Andreas and Betz, Vaughn},
title = {cuSCNN : an Efficient CUDA Implementation of Sparse CNNs},
year = {2023},
isbn = {9798400700439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597031.3597057},
doi = {10.1145/3597031.3597057},
abstract = {Deep Neural Network models are becoming much larger which greatly increases their computation and memory requirements. Sparsity offers great opportunities to reduce unnecessary data transfers and computations. However, exploiting sparsity in CNN inference presents challenges such as irregularities in memory access patterns. To overcome this challenge, we propose cuSCNN, an efficient sparse CNN inference engine that leverages the sparsity of both models and activations using optimized sparse-sparse matrix convolution kernels with compressed operands. cuSCNN is motivated by the concepts introduced by the SCNN hardware accelerator[21] but modified appropriately to achieve an efficient software implementation for GPUs. We develop GPU optimizations that boost execution performance and reduce the required memory size and bandwidth. cuSCNN achieves a speedup of up to 171 \texttimes{} compared to an efficient CPU implementation and 30 \texttimes{} speedup compared to a multi-threaded CPU implementation without batching, enabling the use of inexpensive low-end memory-constrained GPUs to implement large networks with near real-time latency. Although GPU throughput can benefit from larger batch sizes, batch size 1 achieves the lowest latency and hence we focus on it.},
booktitle = {Proceedings of the 13th International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies},
pages = {107–113},
numpages = {7},
keywords = {Sparse Convolution Neural Network (SCNN), Graphics processing unit (GPU), CUDA., Accelerator},
location = {Kusatsu, Japan},
series = {HEART '23}
}

@article{10.1145/3648003,
author = {Cannav\`{o}, Alberto and Pacchiotti, Simona and Retta, Nicola and Terzoli, Martina and Spallone, Roberta and Lamberti, Fabrizio},
title = {Passive Haptics and Conversational Avatars for Interacting with Ancient Egypt Remains in High-Fidelity Virtual Reality Experiences},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3648003},
doi = {10.1145/3648003},
abstract = {As extended reality continues to grow, new possibilities arise to provide users with novel ways to experience cultural heritage (CH). In particular, applications based on virtual reality (VR), such as virtual museums, have gained increasing popularity, since they can offer new ways for preserving and presenting CH content that are not feasible in physical museums. Despite the numerous benefits, the level of immersion and presence provided by VR experiences still present challenges that could hinder the effectiveness of this technology in the CH context. In this perspective, it is crucial to provide the users with high-fidelity experiences, in which also the interaction with the objects and the characters populating virtual environments are realistic and natural. This article focuses on this challenge and specifically investigates how the combined use of tangible and speech interfaces can help improve the overall experience. To this aim, a immersive VR experience is proposed, which allows the users to manipulate virtual objects belonging to a museum collection (in the specific case, Ancient Egypt remains) by physically operating on 3D printed replicas and to talk with a curator’s avatar to get explanations by using their voice. A user study was conducted to evaluate the impact of the considered interfaces on immersion, presence, user experience, usability, and intention to visit, comparing the richest configuration against simpler setups obtained by either removing the tangible interface, the speech interface, or both (and using only handheld controllers). The results show that the combined use of the two interfaces can effectively contribute at making the CH experience in VR more engaging.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {29},
numpages = {28},
keywords = {Virtual museums, Virtual Humans, Tangible User Interfaces, Speech User Interfaces}
}

@inproceedings{10.1145/3472306.3478335,
author = {Habibie, Ikhsanul and Xu, Weipeng and Mehta, Dushyant and Liu, Lingjie and Seidel, Hans-Peter and Pons-Moll, Gerard and Elgharib, Mohamed and Theobalt, Christian},
title = {Learning Speech-driven 3D Conversational Gestures from Video},
year = {2021},
isbn = {9781450386197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472306.3478335},
doi = {10.1145/3472306.3478335},
abstract = {We propose the first approach to synthesize the synchronous 3D conversational body and hand gestures, as well as 3D face and head animations, of a virtual character from speech input. Our algorithm uses a CNN architecture that leverages the inherent correlation between facial expression and hand gestures. Synthesis of conversational body gestures is a multi-modal problem since many similar gestures can plausibly accompany the same input speech. To synthesize plausible body gestures in this setting, we train a Generative Adversarial Network (GAN) based model that measures the plausibility of the generated sequences of 3D body motion when paired with the input audio features. We also contribute a new corpus that contains more than 33 hours of annotated data from in-the-wild videos of talking people. To this end, we apply state-of-the-art monocular approaches for 3D body and hand pose estimation as well as 3D face performance capture to the video corpus. In this way, we can train on orders of magnitude more data than previous algorithms that resort to complex in-studio motion capture solutions, and thereby train more expressive synthesis algorithms. Our experiments and user study show the state-of-the-art quality of our speech-synthesized full 3D character animations.},
booktitle = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},
pages = {101–108},
numpages = {8},
keywords = {gesture synthesis, character control, audio-driven pose estimation},
location = {Virtual Event, Japan},
series = {IVA '21}
}

@inproceedings{10.1109/PACT58117.2023.00025,
author = {Prieto, Pablo and Abad, Pablo and Gregorio, Jose Angel and Puente, Valentin},
title = {Performance Characterization of Popular DNN Models on Out-of-Order CPUs},
year = {2024},
isbn = {9798350342543},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/PACT58117.2023.00025},
doi = {10.1109/PACT58117.2023.00025},
abstract = {DNN popularity, which is driving advances in a growing number of fields, has increased the amount of computing resources running this kind of applications at an unprecedent rate. Specialized hardware, such as GPUs or ASIC-based accelerators, has been the preferred platform to run these applications. However, the ubiquity of DNN models is rapidly extending the presence of this software to general-purpose CPUs. For this reason, there is a pressing need to gain understanding of the main features of state-of-the-art DNN models to adapt CPU microarchitecture accordingly. In this paper we investigated a representative set of DNN models and, based on data collected from real hardware, we evaluated how efficiently they utilize the underlying system. We analyzed overall system performance, as well as the amount of vectorization provided by CPU-optimized frameworks. We quantified the performance loss caused by processor backend, and the contribution of memory hierarchy and functional units to it. We compared the backend utilization of DNN applications to popular benchmarks such as SPEC CPU2017 and found a lower balance in the use of the elements that make up the processor microarchitecture. Although many workloads seem to be constrained by functional unit availability, in a significant group of applications we found a non-negligible impact of memory hierarchy on performance.},
booktitle = {Proceedings of the 32nd International Conference on Parallel Architectures and Compilation Techniques},
pages = {199–210},
numpages = {12},
keywords = {Deep Neural Networks, General Purpose CPU, Metrics},
location = {Vienna, AE, Austria},
series = {PACT '23}
}

@inproceedings{10.1145/3503161.3547996,
author = {Xia, Yan and Zhao, Zhou and Ye, Shangwei and Zhao, Yang and Li, Haoyuan and Ren, Yi},
title = {Video-Guided Curriculum Learning for Spoken Video Grounding},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547996},
doi = {10.1145/3503161.3547996},
abstract = {In this paper, we introduce a new task, spoken video grounding (SVG), which aims to localize the desired video fragments from spoken language descriptions. Compared with using text, employing audio requires the model to directly exploit the useful phonemes and syllables related to the video from raw speech. Moreover, we randomly add environmental noises to this speech audio, further increasing the difficulty of this task and better simulating real applications. To rectify the discriminative phonemes and extract video-related information from noisy audio, we develop a novel video-guided curriculum learning (VGCL) during the audio pre-training process, which can make use of the vital visual perceptions to help understand the spoken language and suppress the external noise. Considering during inference the model can not obtain ground truth video segments, we design a curriculum strategy that gradually shifts the input video from the ground truth to the entire video content during pre-training. Finally, the model can learn how to extract critical visual information from the entire video clip to help understand the spoken language. In addition, we collect the first large-scale spoken video grounding dataset based on ActivityNet, which is named as ActivityNet Speech dataset. Extensive experiments demonstrate our proposed video-guided curriculum learning can facilitate the pre-training process to obtain a mutual audio encoder, significantly promoting the performance of spoken video grounding tasks. Moreover, we prove that in the case of noisy sound, our model outperforms the method that grounding video with ASR transcripts, further demonstrating the effectiveness of our curriculum strategy.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5191–5200},
numpages = {10},
keywords = {video grounding, datasets, curriculum learning, contrastive learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@article{10.1145/3505560,
author = {Adiani, Deeksha and Itzkovitz, Aaron and Bian, Dayi and Katz, Harrison and Breen, Michael and Hunt, Spencer and Swanson, Amy and Vogus, Timothy J. and Wade, Joshua and Sarkar, Nilanjan},
title = {Career Interview Readiness in Virtual Reality (CIRVR): A Platform for Simulated Interview Training for Autistic Individuals and Their Employers},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1936-7228},
url = {https://doi.org/10.1145/3505560},
doi = {10.1145/3505560},
abstract = {Employment outcomes for autistic1 individuals are often poorer relative to their neurotypical (NT) peers, resulting in a greater need for other forms of financial and social support. While a great deal of work has focused on developing interventions for autistic children, relatively less attention has been paid to directly addressing the employment challenges faced by autistic adults. One key impediment to autistic individuals securing employment is the job interview. Autistic individuals often experience anxiety in interview situations, particularly with open-ended questions and unexpected interruptions. They also exhibit atypical gaze patterns that may be perceived as, but not necessarily indicative of, disinterest or inattention. In response, we developed a closed-loop adaptive virtual reality (VR)–based job interview training platform, which we have named Career Interview Readiness in VR (CIRVR). CIRVR is designed to provide an engaging, adaptive, and individualized experience to practice and refine interviewing skills in a less anxiety-inducing virtual context. CIRVR contains a real-time physiology-based stress detection module, as well as a real-time gaze detection module, to permit individualized adaptation. We also present the first prototype of the CIRVR Dashboard, which provides visualizations of data to help autistic individuals as well as potential employers and job coaches make sense of the data gathered from interview sessions. We conducted a feasibility study with 9 autistic and 8 NT individuals to assess the preliminary usability and feasibility of CIRVR. Results showed differences in perceived usability of the system between autistic and NT participants, and higher levels of stress in autistic individuals during interviews. Participants across both groups reported satisfaction with CIRVR and the structure of the interview. These findings and feedback will support future work in improving CIRVR’s features in hopes for it to be a valuable tool to support autistic job candidates as well as their potential employers.},
journal = {ACM Trans. Access. Comput.},
month = mar,
articleno = {2},
numpages = {28},
keywords = {virtual job interview, Autism Spectrum Disorder}
}

@article{10.1145/3569893,
author = {Fraile Navarro, David and Kocaballi, A. Baki and Dras, Mark and Berkovsky, Shlomo},
title = {Collaboration, not Confrontation: Understanding General Practitioners’ Attitudes Towards Natural Language and Text Automation in Clinical Practice},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3569893},
doi = {10.1145/3569893},
abstract = {General Practitioners are among the primary users and curators of textual electronic health records, highlighting the need for technologies supporting record access and administration. Recent advancements in natural language processing facilitate the development of clinical systems, automating some time-consuming record-keeping tasks. However, it remains unclear what automation tasks would benefit clinicians most, what features such automation should exhibit, and how clinicians will interact with the automation. We conducted semi-structured interviews with General Practitioners uncovering their views and attitudes toward text automation. The main emerging theme was doctor-AI collaboration, addressing a reciprocal clinician-technology relationship that does not threaten to substitute clinicians, but rather establishes a constructive synergistic relationship. Other themes included: (i) desired features for clinical text automation; (ii) concerns around clinical text automation; and (iii) the consultation of the future. Our findings will inform the design of future natural language processing systems, to be implemented in general practice.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {29},
numpages = {34},
keywords = {general practice, electronic health records, text automation, Natural language processing}
}

@inproceedings{10.1145/3490632.3490662,
author = {Zargham, Nima and Bonfert, Michael and Porzel, Robert and Doring, Tanja and Malaka, Rainer},
title = {Multi-Agent Voice Assistants: An&nbsp;Investigation&nbsp;of&nbsp;User&nbsp;Experience},
year = {2022},
isbn = {9781450386432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490632.3490662},
doi = {10.1145/3490632.3490662},
abstract = {The use of voice assistants (VAs) is spreading widely. Most common VAs consist of a single, usually female voice that responds to the user’s inquiry. We designed a VA system appearing as a group of agents, each with a different voice and a specialized task domain. We conducted a quantitative user study comparing our multi-agent approach with a conventional single-agent assistant in a smart home scenario as virtual reality (VR) simulation. The results show significantly higher user experience ratings for the multi-agent concept. Based on our findings, we discuss the potentials and challenges of designing multi-party VA systems.},
booktitle = {Proceedings of the 20th International Conference on Mobile and Ubiquitous Multimedia},
pages = {98–107},
numpages = {10},
keywords = {Voice Assistants, Virtual Reality, User Experience, Smart Home, Multiparty},
location = {Leuven, Belgium},
series = {MUM '21}
}

@article{10.1145/3424660,
author = {Gu, Renjie and Niu, Chaoyue and Wu, Fan and Chen, Guihai and Hu, Chun and Lyu, Chengfei and Wu, Zhihua},
title = {From Server-Based to Client-Based Machine Learning: A Comprehensive Survey},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3424660},
doi = {10.1145/3424660},
abstract = {In recent years, mobile devices have gained increasing development with stronger computation capability and larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept of client-based machine learning has been proposed. It leverages the users’ local hardware and local data to solve machine learning sub-problems on mobile devices and only uploads computation results rather than the original data for the optimization of the global model. Such an architecture can not only relieve computation and storage burdens on servers but also protect the users’ sensitive information. Another benefit is the bandwidth reduction because various kinds of local data can be involved in the training process without being uploaded. In this article, we provide a literature review on the progressive development of machine learning from server based to client based. We revisit a number of widely used server-based and client-based machine learning methods and applications. We also extensively discuss the challenges and future directions in this area. We believe that this survey will give a clear overview of client-based machine learning and provide guidelines on applying client-based machine learning to practice.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {6},
numpages = {36},
keywords = {machine learning, federated learning, distributed system, decentralized training, Mobile intelligence}
}

@inproceedings{10.5555/3463952.3464084,
author = {Saund, Carolyn and B\^{\i}rl\u{a}deanu, Andrei and Marsella, Stacy},
title = {CMCF: An Architecture for Realtime Gesture Generation by Clustering Gestures by Motion and Communicative Function},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Gestures augment speech by performing a variety of communicative functions in humans and virtual agents, and are often related to speech by complex semantic, rhetorical, prosodic, and affective elements. In this paper we briefly present an architecture for human-like gesturing in virtual agents that is designed to realize complex speech-to-gesture mappings by exploiting existing machine-learning based parsing tools and techniques to extract these functional elements from speech. We then deeply explore the rhetorical branch of this architecture, objectively assessing specifically whether existing rhetorical parsing techniques can classify gestures into classes with distinct movement properties. To do this, we take a corpus of spontaneously generated gestures and correlate their movement to co-speech utterances. We cluster gestures based on their rhetorical properties, and then by their movement. Our objective analysis suggests that some rhetorical structures are identifiable by our movement features while others require further exploration. We explore possibilities behind these findings and propose future experiments that may further reveal nuances of the richness of the mapping between speech and motion. This work builds towards a real-time gesture generator which performs gestures that effectively convey rich communicative functions.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1136–1144},
numpages = {9},
keywords = {clustering, communication, gesture generation, machine learning, rhetorical parsing},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@article{10.1145/3447735,
author = {Darwish, Kareem and Habash, Nizar and Abbas, Mourad and Al-Khalifa, Hend and Al-Natsheh, Huseein T. and Bouamor, Houda and Bouzoubaa, Karim and Cavalli-Sforza, Violetta and El-Beltagy, Samhaa R. and El-Hajj, Wassim and Jarrar, Mustafa and Mubarak, Hamdy},
title = {A panoramic survey of natural language processing in the Arab world},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3447735},
doi = {10.1145/3447735},
journal = {Commun. ACM},
month = mar,
pages = {72–81},
numpages = {10}
}

@inproceedings{10.1145/3643491.3660289,
author = {Sch\"{a}fer, Karla and Choi, Jeong-Eun and Zmudzinski, Sascha},
title = {Explore the World of Audio Deepfakes: A Guide to Detection Techniques for Non-Experts},
year = {2024},
isbn = {9798400705526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643491.3660289},
doi = {10.1145/3643491.3660289},
abstract = {Audio deepfakes are becoming increasingly prevalent, posing a threat to companies who may fall victim to CEO fraud, as well as individuals who may be targeted by voice scams. As a result, the need for audio deepfake detectors is growing. While academic research on detectors exists, the papers can be difficult for non-experts to understand. Therefore, this article provides an introduction to various state-of-the-art detectors for non-experts. Besides background information about audio and deep learning, evaluation metrics, and commonly used datasets for training detectors such as ASVspoof are explored. Detectors that use handcrafted features and end-to-end models such as RawNet2 and AASIST are presented understandable for beginners. Additionally, models based on self-supervised learning, which currently provide the best results on in-the-wild data are introduced.},
booktitle = {Proceedings of the 3rd ACM International Workshop on Multimedia AI against Disinformation},
pages = {13–22},
numpages = {10},
keywords = {Audio Deepfakes, Deep Learning, Detection, Disinformation},
location = {Phuket, Thailand},
series = {MAD '24}
}

@inproceedings{10.1145/3503961.3503963,
author = {Zhou, Huaming and Wang, Aili and Li, Meixin and Zhao, Yinghui and Iwahori, Yuji},
title = {Epidemic Prevention System Based on Voice Recognition Combined with Intelligent Recognition of Mask and Helmet},
year = {2022},
isbn = {9781450385886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503961.3503963},
doi = {10.1145/3503961.3503963},
abstract = {At present, COVID-19 cross-infection is easy to occur in dense places such as elevators. There are no epidemic prevention measures for construction site elevators on the market, and most of them require manual temperature measurement and reminders to wear masks and helmets to avoid the spread of the epidemic. This paper designs an intelligent epidemic prevention system for the elevator ride process in a modern construction site environment, which can achieve non-contact human temperature measurement, mask and helmet recognition and voice call elevator function. The system uses Arduino UNO as the control core, Kendryte K210 as machine vision processing module, non-contact infrared temperature sensor MLX90614, and voice recognition sensor LD3320. The system has the functions of non-contact temperature detection, mask/helmet recognition(YOLOv3) and voice call elevator. Experimental results showed that the recognition accuracy rate of helmet, mask, voice call elevator is 91.5%, 92.0% and 93.0% respectively. The temperature measurement accuracy rate is 0.2℃, which can effectively prevent the spread of the epidemic caused by contact and breathing, and has the advantages of stable, intelligent, and safe work.},
booktitle = {Proceedings of the 2021 3rd International Conference on Video, Signal and Image Processing},
pages = {8–15},
numpages = {8},
keywords = {voice call elevator, mask recognition, infrared temperature measurement, helmet recognition, YOLOv3},
location = {Wuhan, China},
series = {VSIP '21}
}

@inproceedings{10.1145/3613904.3642229,
author = {Chen, Liuqing and Xiao, Shuhong and Chen, Yunnong and Song, Yaxuan and Wu, Ruoyu and Sun, Lingyun},
title = {ChatScratch: An AI-Augmented System Toward Autonomous Visual Programming Learning for Children Aged 6-12},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642229},
doi = {10.1145/3613904.3642229},
abstract = {As Computational Thinking (CT) continues to permeate younger age groups in K-12 education, established CT platforms such as Scratch face challenges in catering to these younger learners, particularly those in the elementary school (ages 6-12). Through formative investigation with Scratch experts, we uncover three key obstacles to children’s autonomous Scratch learning: artist’s block in project planning, bounded creativity in asset creation, and inadequate coding guidance during implementation. To address these barriers, we introduce ChatScratch, an AI-augmented system to facilitate autonomous programming learning for young children. ChatScratch employs structured interactive storyboards and visual cues to overcome artist’s block, integrates digital drawing and advanced image generation technologies to elevate creativity, and leverages Scratch-specialized Large Language Models (LLMs) for professional coding guidance. Our study shows that, compared to Scratch, ChatScratch efficiently fosters autonomous programming learning, and contributes to the creation of high-quality, personally meaningful Scratch projects for children.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {649},
numpages = {19},
keywords = {Children Aged 6-12, Computational Thinking, Large Language Model, Scratch},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1109/TASLP.2024.3364100,
author = {Yao, Jiadi and Luo, Hong and Qi, Jun and Zhang, Xiao-Lei},
title = {Interpretable Spectrum Transformation Attacks to Speaker Recognition Systems},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3364100},
doi = {10.1109/TASLP.2024.3364100},
abstract = {The success of adversarial attacks on speaker recognition is mainly in white-box scenarios. When applying the adversarial voices that are generated by attacking white-box surrogate models to black-box victim models, i.e. &lt;italic&gt;transfer-based&lt;/italic&gt; black-box attacks, the transferability of the adversarial voices is not only far from satisfactory, but also lacks interpretable basis. To address these issues, in this article, we propose a general framework, named spectral transformation attack based on modified discrete cosine transform (STA-MDCT), to improve the transferability of the adversarial voices to a black-box victim model. Specifically, we first apply MDCT to the input voice. Then, we slightly modify the energy of different frequency bands for capturing the salient regions of the adversarial noise in the time-frequency domain that are critical to a successful attack. Unlike existing approaches that operate voices in the time domain, the proposed framework operates voices in the time-frequency domain, which improves the interpretability, transferability, and imperceptibility of the attack. Moreover, it can be implemented with any gradient-based attackers. To utilize the advantage of model ensembling, we not only implement STA-MDCT with a single white-box surrogate model but also with an ensemble of surrogate models. Finally, we visualize the saliency maps of adversarial voices by the class activation maps (CAM), which offer an interpretable basis for transfer-based attacks in speaker recognition for the first time. Extensive comparison results with six representative attackers show that the CAM visualization clearly explains the effectiveness of STA-MDCT and the weaknesses of the comparison methods; the proposed method outperforms the comparison methods by a large margin. Our audio samples are available on the demo website.&lt;sup&gt;1&lt;/sup&gt;},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {1531–1545},
numpages = {15}
}

@article{10.1145/3555573,
author = {Mizrahi, Elinor and Danzig, Noa and Gordon, Goren},
title = {vRobotator: A Virtual Robot Facilitator of Small Group Discussions for K-12},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555573},
doi = {10.1145/3555573},
abstract = {The COVID-19 pandemic has stressed the importance of efficient and accommodating online educational experiences. In this contribution, we present a novel system for the facilitation of small group online discussions using an avatar during video conferencing. The avatar was programmed with group facilitation best practices, whereas the content for the activities was prepared by the classes' teachers. Groups of tenth grade students interacted with the system, where we compared activities facilitated by the avatar with activities without facilitation. Our results show that students reported the activity with the avatar to be significantly more efficient, more understandable and inducing more participation compared to activities without avatar facilitation. Students also spoke significantly more with avatar facilitation. This system shows promise in future online educational activities as a facilitator of discussions with K-12 students.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {472},
numpages = {22},
keywords = {virtual agent, video conference, group activity, facilitation}
}

@inproceedings{10.1145/3613904.3642230,
author = {Lee, Jaewook and Wang, Jun and Brown, Elizabeth and Chu, Liam and Rodriguez, Sebastian S. and Froehlich, Jon E.},
title = {GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642230},
doi = {10.1145/3613904.3642230},
abstract = {Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users’ spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask “what’s over there?” or “how do I solve this math problem?” simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems, (2) examining GazePointAR’s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {408},
numpages = {20},
keywords = {LLM, augmented reality, gaze tracking, multimodal input, pointing gesture recognition, voice assistants},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3411764.3445490,
author = {Bu, Yaohua and Ma, Tianyi and Li, Weijun and Zhou, Hang and Jia, Jia and Chen, Shengqi and Xu, Kaiyuan and Shi, Dachuan and Wu, Haozhe and Yang, Zhihan and Li, Kun and Wu, Zhiyong and Shi, Yuanchun and Lu, Xiaobo and Liu, Ziwei},
title = {PTeacher: a Computer-Aided Personalized Pronunciation Training System with Exaggerated Audio-Visual Corrective Feedback},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445490},
doi = {10.1145/3411764.3445490},
abstract = {Second language (L2) English learners often find it difficult to improve their pronunciations due to the lack of expressive and personalized corrective feedback. In this paper, we present Pronunciation Teacher&nbsp;(PTeacher), a Computer-Aided Pronunciation Training (CAPT) system that provides personalized exaggerated audio-visual corrective feedback for mispronunciations. Though the effectiveness of exaggerated feedback has been demonstrated, it is still unclear how to define the appropriate degrees of exaggeration when interacting with individual learners. To fill in this gap, we interview 100 L2 English learners and 22 professional native teachers to understand their needs and experiences. Three critical metrics are proposed for both learners and teachers to identify the best exaggeration levels in both audio and visual modalities. Additionally, we incorporate the personalized dynamic feedback mechanism given the English proficiency of learners. Based on the obtained insights, a comprehensive interactive pronunciation training course is designed to help L2 learners rectify mispronunciations in a more perceptible, understandable, and discriminative manner. Extensive user studies demonstrate that our system significantly promotes the learners’ learning efficiency.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {676},
numpages = {14},
keywords = {Language Learning, Exaggerated feedback, Computer-Aided Pronunciation Training System, Audio-Visual Corrective Feedback},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3551349.3556957,
author = {Li, Suwan and Bu, Lei and Bai, Guangdong and Guo, Zhixiu and Chen, Kai and Wei, Hanlin},
title = {VITAS : Guided Model-based VUI Testing of VPA Apps},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556957},
doi = {10.1145/3551349.3556957},
abstract = {Virtual personal assistant&nbsp;(VPA) services, e.g. Amazon Alexa and Google Assistant, are becoming increasingly popular recently. Users interact with them through voice-based apps, e.g. Amazon Alexa skills and Google Assistant actions. Unlike the desktop and mobile apps which have visible and intuitive graphical user interface (GUI) to facilitate interaction, VPA apps convey information purely verbally through the voice user interface (VUI), which is known to be limited in its invisibility, single mode and high demand of user attention. This may lead to various problems on the usability and correctness of VPA apps. In this work, we propose a model-based framework named Vitas to handle VUI testing of VPA apps. Vitas interacts with the app VUI, and during the testing process, it retrieves semantic information from voice feedbacks by natural language processing. It incrementally constructs the finite state machine&nbsp;(FSM) model of the app with a weighted exploration strategy guided by key factors such as the coverage of app functionality. We conduct a large-scale testing on 41,581 VPA apps&nbsp;(i.e., skills) of Amazon Alexa, the most popular VPA service, and find that 51.29% of them have weaknesses. They largely suffer from problems such as unexpected exit/start, privacy violation and so on. Our work reveals the immaturity of the VUI designs and implementations in VPA apps, and sheds light on the improvement of several crucial aspects of VPA apps.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {115},
numpages = {12},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3659624,
author = {Cuadra, Andrea and Breuch, Justine and Estrada, Samantha and Ihim, David and Hung, Isabelle and Askaryar, Derek and Hassanien, Marwan and Fessele, Kristen L. and Landay, James A.},
title = {Digital Forms for All: A Holistic Multimodal Large Language Model Agent for Health Data Entry},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
url = {https://doi.org/10.1145/3659624},
doi = {10.1145/3659624},
abstract = {Digital forms help us access services and opportunities, but they are not equally accessible to everyone, such as older adults or those with sensory impairments. Large language models (LLMs) and multimodal interfaces offer a unique opportunity to increase form accessibility. Informed by prior literature and needfinding, we built a holistic multimodal LLM agent for health data entry. We describe the process of designing and building our system, and the results of a study with older adults (N =10). All participants, regardless of age or disability status, were able to complete a standard 47-question form independently using our system---one blind participant said it was "a prayer answered." Our video analysis revealed how different modalities provided alternative interaction paths in complementary ways (e.g., the buttons helped resolve transcription errors and speech helped provide more options when the pre-canned answer choices were insufficient). We highlight key design guidelines, such as designing systems that dynamically adapt to individual needs.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = may,
articleno = {72},
numpages = {39},
keywords = {Accessibility, Artifact or System, Field Study, Health - Clinical, Input Techniques, Interaction Design, Mobile Devices: Phones/Tablets, Older Adults, Prototyping/Implementation, Qualitative Methods, Text/Speech/Language, User Experience Design}
}

@inproceedings{10.1145/3461778.3462078,
author = {Cho, Won Ik and Cheon, Sung Jun and Kang, Woo Hyun and Kim, Ji Won and Kim, Nam Soo},
title = {Giving Space to Your Message: Assistive Word Segmentation for the Electronic Typing of Digital Minorities},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462078},
doi = {10.1145/3461778.3462078},
abstract = {For readability and disambiguation of the written text, appropriate word segmentation is recommended for documentation, and it also holds for the digitized texts. If the language is agglutinative while far from scriptio continua, for instance in the Korean language, the problem becomes more significant. However, some device users these days find it challenging to communicate via key stroking, not only for handicap but also for being unskilled. In this study, we propose a real-time assistive technology that utilizes an automatic word segmentation, designed for digital minorities who are not familiar with electronic typing. We propose a data-driven system trained upon a spoken Korean language corpus with various non-canonical expressions and dialects, guaranteeing the comprehension of contextual information. Through quantitative and qualitative comparison with other text processing toolkits, we show the reliability of the proposed system and its fit with colloquial and non-normalized texts, which fulfills the aim of supportive technology.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {1739–1747},
numpages = {9},
keywords = {Natural language processing, Digital minorities, Assistive word segmentation},
location = {Virtual Event, USA},
series = {DIS '21}
}

@article{10.1145/3610886,
author = {Kim, Gwangbin and Yeo, Dohyeon and Jo, Taewoo and Rus, Daniela and Kim, SeungJun},
title = {What and When to Explain? On-road Evaluation of Explanations in Highly Automated Vehicles},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610886},
doi = {10.1145/3610886},
abstract = {Explanations in automated vehicles help passengers understand the vehicle's state and capabilities, leading to increased trust in the technology. Specifically, for passengers of SAE Level 4 and 5 vehicles who are not engaged in the driving process, the enhanced sense of control provided by explanations reduces potential anxieties, enabling them to fully leverage the benefits of automation. To construct explanations that enhance trust and situational awareness without disturbing passengers, we suggest testing with people who ultimately employ such explanations, ideally under real-world driving conditions. In this study, we examined the impact of various visual explanation types (perception, attention, perception+attention) and timing mechanisms (constantly provided or only under risky scenarios) on passenger experience under naturalistic driving scenarios using actual vehicles with mixed-reality support. Our findings indicate that visualizing the vehicle's perception state improves the perceived usability, trust, safety, and situational awareness without adding cognitive burden, even without explaining the underlying causes. We also demonstrate that the traffic risk probability could be used to control the timing of an explanation delivery, particularly when passengers are overwhelmed with information. Our study's on-road evaluation method offers a safe and reliable testing environment and can be easily customized for other AI models and explanation modalities.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {104},
numpages = {26},
keywords = {automated vehicles, in-car extended reality, intelligibility}
}

@inproceedings{10.1145/3469595.3469599,
author = {Kocielnik, Rafal and Langevin, Raina and George, James S. and Akenaga, Shota and Wang, Amelia and Jones, Darwin P. and Argyle, Alexander and Fockele, Callan and Anderson, Layla and Hsieh, Dennis T. and Yadav, Kabir and Duber, Herbert and Hsieh, Gary and Hartzler, Andrea L.},
title = {Can I Talk to You about Your Social Needs? Understanding Preference for Conversational User Interface in Health},
year = {2021},
isbn = {9781450389983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469595.3469599},
doi = {10.1145/3469595.3469599},
abstract = {Conversational User Interfaces (CUI) are becoming increasingly utilized in Health applications due to their ability to engage patients and support clinical workflows. Yet recent reviews show that our understanding of CUI performance and user preferences towards them is still lacking. This work examines factors that explain people’s preference for engaging with a social needs screening CUI in a clinical context with 41 emergency department visitors. We demonstrate that people with low health literacy and high attitude towards emotional interaction (AEI) prefer responding to questions via CUI rather than a form-based survey. Specifically, participants with low health literacy appreciate the improved understandability offered by the CUI, whereas participants with high AEI appreciate the added level of engagement offered through conversational interactions. Our work advances the understanding of the benefits of CUI for different user groups in health contexts and beyond.},
booktitle = {Proceedings of the 3rd Conference on Conversational User Interfaces},
articleno = {4},
numpages = {10},
keywords = {vulnerable populations, understanding users, social needs, healthcare, health literacy, health, conversational user interfaces, conversational agents, chatbots},
location = {Bilbao (online), Spain},
series = {CUI '21}
}

@inproceedings{10.1145/3474085.3475463,
author = {Huang, Wencan and Pan, Wenwen and Zhao, Zhou and Tian, Qi},
title = {Towards Fast and High-Quality Sign Language Production},
year = {2021},
isbn = {9781450386517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474085.3475463},
doi = {10.1145/3474085.3475463},
abstract = {Sign Language Production (SLP) aims to automatically translate a spoken language description to its corresponding sign language video. The core procedure of SLP is to transform sign gloss intermediaries into sign pose sequences (G2P). Most existing methods for G2P are based on sequential autoregression or sequence-to-sequence encoder-decoder learning. However, by generating target pose frames conditioned on the previously generated ones, these models are prone to bringing issues such as error accumulation and high inference latency. In this paper, we argue that such issues are mainly caused by adopting autoregressive manner. Hence, we propose a novel Non-AuToregressive (NAT) model with a parallel decoding scheme, as well as an External Aligner for sequence alignment learning. Specifically, we extract alignments from the external aligner by monotonic alignment search for gloss duration prediction, which is used by a length regulator to expand the source gloss sequence to match the length of the target sign pose sequence for parallel sign pose generation. Furthermore, we devise a spatial-temporal graph convolutional pose generator in the NAT model to generate smoother and more natural sign pose sequences. Extensive experiments conducted on PHOENIX14T dataset show that our proposed model outperforms state-of-the-art autoregressive models in terms of speed and quality.},
booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
pages = {3172–3181},
numpages = {10},
keywords = {sign language production, non-autoregressive generation, deep learning},
location = {Virtual Event, China},
series = {MM '21}
}

@inproceedings{10.1145/3613424.3614287,
author = {Lee, Hyunwuk and Jang, Hyungjun and Kim, Sungbin and Kim, Sungwoo and Cho, Wonho and Ro, Won Woo},
title = {Exploiting Inherent Properties of Complex Numbers for Accelerating Complex Valued Neural Networks},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614287},
doi = {10.1145/3613424.3614287},
abstract = {Since conventional Deep Neural Networks (DNNs) use real numbers as their data, they are unable to capture the imaginary values and the correlations between real and imaginary values in applications that use complex numbers. To address this limitation, Complex Valued Neural Networks (CVNNs) have been introduced, enabling to capture the context of complex numbers for various applications such as Magnetic Resonance Imaging (MRI), radar, and sensing. CVNNs handle their data with complex numbers and adopt complex number arithmetic to their layer operations, so they exhibit distinct design challenges with real-valued DNNs. The first challenge is the data representation of the complex number, which requires two values for a single data, doubling the total data size of the networks. Moreover, due to the unique operations of the complex-valued layers, CVNNs require a specialized scheduling policy to fully utilize the hardware resources and achieve optimal performance. To mitigate the design challenges, we propose software and hardware co-design techniques that effectively resolves the memory and compute overhead of CVNNs. First, we propose Polar Form Aware Quantization (PAQ) that utilizes the characteristics of the complex number and their unique value distribution on CVNNs. Then, we propose our hardware accelerator that supports PAQ and CVNN operations. Lastly, we design a CVNN-aware scheduling scheme that optimizes the performance and resource utilization of an accelerator by aiming at the special layer operations of CVNN. PAQ achieves 62.5% data compression over CVNNs using FP16 while retaining a similar error with INT8 quantization, and our hardware support PAQ with only 2% area overhead over conventional systolic array architecture. In our evaluation, PAQ hardware with the scheduling scheme achieves a 32% lower latency and 30% lower energy consumption than other accelerators.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1121–1134},
numpages = {14},
keywords = {Quantization, Complex Valued Neural Networks, Accelerators},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{10.1145/3568162.3577004,
author = {Kamelabad, Alireza M. and Skantze, Gabriel},
title = {I Learn Better Alone! Collaborative and Individual Word Learning With a Child and Adult Robot},
year = {2023},
isbn = {9781450399647},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568162.3577004},
doi = {10.1145/3568162.3577004},
abstract = {The use of social robots as a tool for language learning has been studied quite extensively recently. Although their effectiveness and comparison with other technologies are well studied, the effects of the robot's appearance and the interaction setting have received less attention. As educational robots are envisioned to appear in household or school environments, it is important to investigate how their designed persona or interaction dynamics affect learning outcomes. In such environments, children may do the activities together or alone or perform them in the presence of an adult or another child. In this regard, we have identified two novel factors to investigate: the robot's perceived age (adult or child) and the number of learners interacting with the robot simultaneously (one or two). We designed an incidental word learning card game with the Furhat robot and ran a between-subject experiment with 75 middle school participants. We investigated the interactions and effects of children's word learning outcomes, speech activity, and perception of the robot's role. The results show that children who played alone with the robot had better word retention and anthropomorphized the robot more, compared to those who played in pairs. Furthermore, unlike previous findings from human-human interactions, children did not show different behaviors in the presence of a robot designed as an adult or a child. We discuss these factors in detail and make a novel contribution to the direct comparison of collaborative versus individual learning and the new concept of the robot's age.},
booktitle = {Proceedings of the 2023 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {368–377},
numpages = {10},
keywords = {child language learning, collaborative learning, depth of processing, l2 learning, multi-party interaction, rall, robot social role, robot-assisted language learning, second language, social robotics, task-induced involvement load, word learning},
location = {Stockholm, Sweden},
series = {HRI '23}
}

@inproceedings{10.1145/3460120.3485365,
author = {Chen, Yanjiao and Bai, Yijie and Mitev, Richard and Wang, Kaibo and Sadeghi, Ahmad-Reza and Xu, Wenyuan},
title = {FakeWake: Understanding and Mitigating Fake Wake-up Words of Voice Assistants},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485365},
doi = {10.1145/3460120.3485365},
abstract = {In the area of Internet of Things (IoT), voice assistants have become an important interface to operate smart speakers, smartphones, and even automobiles. To save power and protect user privacy, voice assistants send commands to the cloud only if a small set of preregistered wake-up words are detected. However, voice assistants are shown to be vulnerable to the FakeWake phenomena, whereby they are inadvertently triggered by innocent-sounding fuzzy words. In this paper, we present a systematic investigation of the FakeWake phenomena from three aspects. To start with, we design the first fuzzy word generator to automatically and efficiently produce fuzzy words instead of searching through a swarm of audio materials.We manage to generate 965 fuzzy words covering 8 most popular English and Chinese smart speakers. To explain the causes underlying the FakeWake phenomena, we construct an interpretable tree-based decision model, which reveals phonetic features that contribute to false acceptance of fuzzy words by wake-up word detectors. Finally, we propose remedies to mitigate the effect of FakeWake. The results show that the strengthened models are not only resilient to fuzzy words but also achieve better overall performance on original training datasets.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {1861–1883},
numpages = {23},
keywords = {voice assistants, security, interpretable machine learning, fuzzy words},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3437880.3460408,
author = {Zhang, Zhenyu and Yi, Xiaowei and Zhao, Xianfeng},
title = {Fake Speech Detection Using Residual Network with Transformer Encoder},
year = {2021},
isbn = {9781450382953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437880.3460408},
doi = {10.1145/3437880.3460408},
abstract = {Fake speech detection aims to distinguish fake speech from natural speech. This paper presents an effective fake speech detection scheme based on residual network with transformer encoder (TE-ResNet) for improving the performance of fake speech detection. Firstly, considering inter-frame correlation of the speech signal, we utilize transformer encoder to extract contextual representations of the acoustic features. Then, a residual network is used to process deep features and calculate score that the speech is fake. Besides, to increase the quantity of training data, we apply five speech data augmentation techniques on the training dataset. Finally, we fuse the different fake speech detection models on score-level by logistic regression for compensating the shortcomings of each single model. The proposed scheme is evaluated on two public speech datasets. Our experiments demonstrate that the proposed TE-ResNet outperforms the existing state-of-the-art methods both on development and evaluation datasets. In addition, the proposed fused model achieves improved performance for detection of unseen fake speech technology, which can obtain equal error rates (EERs) of 3.99% and 5.89% on evaluation set of FoR-normal dataset and ASVspoof 2019 LA dataset respectively.},
booktitle = {Proceedings of the 2021 ACM Workshop on Information Hiding and Multimedia Security},
pages = {13–22},
numpages = {10},
keywords = {transformer encoder, score-level fusion, residual network, logistic regression, fake speech detection, data augmentation},
location = {Virtual Event, Belgium},
series = {IH&amp;MMSec '21}
}

@inproceedings{10.1145/3491102.3517431,
author = {Sharif, Ather and Wang, Olivia H. and Muongchan, Alida T. and Reinecke, Katharina and Wobbrock, Jacob O.},
title = {VoxLens: Making Online Data Visualizations Accessible with an Interactive JavaScript Plug-In},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517431},
doi = {10.1145/3491102.3517431},
abstract = {JavaScript visualization libraries are widely used to create online data visualizations but provide limited access to their information for screen-reader users. Building on prior findings about the experiences of screen-reader users with online data visualizations, we present VoxLens, an open-source JavaScript plug-in that—with a single line of code—improves the accessibility of online data visualizations for screen-reader users using a multi-modal approach. Specifically, VoxLens enables screen-reader users to obtain a holistic summary of presented information, play sonified versions of the data, and interact with visualizations in a “drill-down” manner using voice-activated commands. Through task-based experiments with 21 screen-reader users, we show that VoxLens improves the accuracy of information extraction and interaction time by 122% and 36%, respectively, over existing conventional interaction with online data visualizations. Our interviews with screen-reader users suggest that VoxLens is a “game-changer” in making online data visualizations accessible to screen-reader users, saving them time and effort.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {478},
numpages = {19},
keywords = {Visualizations, accessibility, blind, low-vision., screen readers, voice-based interaction},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3491102.3517459,
author = {Pan, Lihang and Yu, Chun and Li, JiaHui and Huang, Tian and Bi, Xiaojun and Shi, Yuanchun},
title = {Automatically Generating and Improving Voice Command Interface from Operation Sequences on Smartphones},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517459},
doi = {10.1145/3491102.3517459},
abstract = {Using voice commands to automate smartphone tasks (e.g., making a video call) can effectively augment the interactivity of numerous mobile apps. However, creating voice command interfaces requires a tremendous amount of effort in labeling and compiling the graphical user interface (GUI) and the utterance data. In this paper, we propose AutoVCI, a novel approach to automatically generate voice command interface (VCI) from smartphone operation sequences. The generated voice command interface has two distinct features. First, it automatically maps a voice command to GUI operations and fills in parameters accordingly, leveraging the GUI data instead of corpus or hand-written rules. Second, it launches a complementary Q&amp;A dialogue to confirm the intention in case of ambiguity. In addition, the generated voice command interface can learn and evolve from user interactions. It accumulates the history command understanding results to annotate the user’s input and improve its semantic understanding ability. We implemented this approach on Android devices and conducted a two-phase user study with 16 and 67 participants in each phase. Experimental results of the study demonstrated the practical feasibility of AutoVCI.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {208},
numpages = {21},
keywords = {generation system, interaction-centered nature language understanding, operation sequence, voice command interface},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3447526.3472022,
author = {Ko, Yu-Jung and Putkonen, Aini and Aydin, Ali Selman and Feiz, Shirin and Wang, Yuheng and Ashok, Vikas and Ramakrishnan, IV and Oulasvirta, Antti and Bi, Xiaojun},
title = {Modeling Gliding-based Target Selection for Blind Touchscreen Users},
year = {2021},
isbn = {9781450383288},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447526.3472022},
doi = {10.1145/3447526.3472022},
abstract = {Gliding a finger on touchscreen to reach a target, that is, touch exploration, is a common selection method of blind screen-reader users. This paper investigates their gliding behavior and presents a model for their motor performance. We discovered that the gliding trajectories of blind people are a mixture of two strategies: 1) ballistic movements with iterative corrections relying on non-visual feedback, and 2) multiple sub-movements separated by stops, and concatenated until the target is reached. Based on this finding, we propose the mixture pointing model, a model that relates movement time to distance and width of the target. The model outperforms extant models, improving R2 from 0.65 for Fitts’ law to 0.76, and is superior in cross-validation and information criteria. The model advances understanding of gliding-based target selection and serves as a tool for designing interface layouts for screen-reader based touch exploration.},
booktitle = {Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction},
articleno = {29},
numpages = {14},
keywords = {pointing, gliding-based target selection, accessibility},
location = {Toulouse &amp; Virtual, France},
series = {MobileHCI '21}
}

@article{10.1145/3432942,
author = {Porcheron, Martin and Fischer, Joel E. and Reeves, Stuart},
title = {Pulling Back the Curtain on the Wizards of Oz},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {CSCW3},
url = {https://doi.org/10.1145/3432942},
doi = {10.1145/3432942},
abstract = {The Wizard of Oz method is an increasingly common practice in HCI and CSCW studies as part of iterative design processes for interactive systems. Instead of designing a fully-fledged system, the 'technical work' of key system components is completed by human operators yet presented to study participants as if computed by a machine. However, little is known about how Wizard of Oz studies are interactionally and collaboratively achieved in situ by researchers and participants. By adopting an ethnomethodological perspective, we analyse our use of the method in studies with a voice-controlled vacuum robot and two researchers present. We present data that reveals how such studies are organised and presented to participants and unpack the coordinated orchestration work that unfolds 'behind the scenes' to complete the study. We examine how the researchers attend to participant requests and technical breakdowns, and discuss the performative, collaborative, and methodological nature of their work. We conclude by offering insights from our application of the approach to others in the HCI and CSCW communities for using the method.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jan,
articleno = {243},
numpages = {22},
keywords = {woz, vuis, voice interfaces, robots, research practice, natural language interfaces, methodology, ethnomethodology, ethnography, cscw, coordination, collaboration}
}

@inproceedings{10.1145/3510003.3510057,
author = {Biswas, Sumon and Wardat, Mohammad and Rajan, Hridesh},
title = {The art and practice of data science pipelines: A comprehensive study of data science pipelines in theory, in-the-small, and in-the-large},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510057},
doi = {10.1145/3510003.3510057},
abstract = {Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large. Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {2091–2103},
numpages = {13},
keywords = {data science pipelines, data science processes, descriptive, predictive},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@article{10.1145/3440756,
author = {Chen, Xiaoxue and Jin, Lianwen and Zhu, Yuanzhi and Luo, Canjie and Wang, Tianwei},
title = {Text Recognition in the Wild: A Survey},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3440756},
doi = {10.1145/3440756},
abstract = {The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research topic in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising results in terms of innovation, practicality, and efficiency. This article aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition, (2) introduce new insights and ideas, (3) provide a comprehensive review of publicly available resources, and (4) point out directions for future work. In summary, this literature review attempts to present an entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field and could be helpful in inspiring future research. Related resources are available at our GitHub repository: https://github.com/HCIILAB/Scene-Text-Recognition.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {42},
numpages = {35},
keywords = {end-to-end systems, deep learning, Scene text recognition}
}

@article{10.1145/3528577,
author = {HeydariGorji, Ali and Rezaei, Siavash and Torabzadehkashi, Mahdi and Bobarshad, Hossein and Alves, Vladimir and Chou, Pai H.},
title = {Leveraging Computational Storage for Power-Efficient Distributed Data Analytics},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {1539-9087},
url = {https://doi.org/10.1145/3528577},
doi = {10.1145/3528577},
abstract = {This article presents a family of computational storage drives (CSDs) and demonstrates their performance and power improvements due to in-storage processing (ISP) when running big data analytics applications. CSDs are an emerging class of solid state drives that are capable of running user code while minimizing data transfer time and energy. Applications that can benefit from in situ processing include distributed training, distributed inferencing, and databases. To achieve the full advantage of the proposed ISP architecture, we propose software solutions for workload balancing before and at runtime for training and inferencing applications. Other applications such as sharding-based databases can readily take advantage of our ISP structure without additional tooling. Experimental results on different capacity and form factors of CSDs show up to 3.1\texttimes{} speedup in processing while reducing the energy consumption and data transfer by up to 67% and 68%, respectively, compared to regular enterprise solid state drives.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = oct,
articleno = {82},
numpages = {36},
keywords = {NLP, distributed processing, data analytics, near-data processing, in-storage processing, solid state drives, Computational storage drives}
}

