Authors,Author full names,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Open Access,Source,EID
Fujita K.; Ando A.; Ijima Y.,"Fujita, Kenichi (57340319300); Ando, Atsushi (57139809100); Ijima, Yusuke (34969290800)",57340319300; 57139809100; 34969290800,Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis,2024,IEICE Transactions on Information and Systems,E107.D,1,,93,104,11,0,10.1587/transinf.2023EDP7039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182603374&doi=10.1587%2ftransinf.2023EDP7039&partnerID=40&md5=069eac871cb0ab43ed5c2169d4a16143,"This paper proposes a speech rhythm-based method for speaker embeddings to model phoneme duration using a few utterances by the target speaker. Speech rhythm is one of the essential factors among speaker characteristics, along with acoustic features such as F0, for reproducing individual utterances in speech synthesis. A novel feature of the proposed method is the rhythm-based embeddings extracted from phonemes and their durations, which are known to be related to speaking rhythm. They are extracted with a speaker identification model similar to the conventional spectral feature-based one. We conducted three experiments, speaker embeddings generation, speech synthesis with generated embeddings, and embedding space analysis, to evaluate the performance. The proposed method demonstrated a moderate speaker identification performance (15.2% EER), even with only phonemes and their duration information. The objective and subjective evaluation results demonstrated that the proposed method can synthesize speech with speech rhythm closer to the target speaker than the conventional method. We also visualized the embeddings to evaluate the relationship between the distance of the embeddings and the perceptual similarity. The visualization of the embedding space and the relation analysis between the closeness indicated that the distribution of embeddings reflects the subjective and objective similarity. Copyright © 2024 The Institute of Electronics, Information and Communication Engineers.",phoneme duration; speaker embedding; speech rhythm; speech synthesis,Embeddings; Loudspeakers; Speech recognition; Acoustic features; Embeddings; Identification modeling; Phoneme duration; Speaker characteristics; Speaker embedding; Speaker identification; Spectral feature; Speech rhythm; Target speaker; Speech synthesis,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85182603374
Zheng R.-C.; Ai Y.; Ling Z.-H.,"Zheng, Rui-Chen (58202616600); Ai, Yang (57200444155); Ling, Zhen-Hua (24473163500)",58202616600; 57200444155; 24473163500,Incorporating Ultrasound Tongue Images for Audio-Visual Speech Enhancement,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,1430,1444,14,1,10.1109/TASLP.2024.3361376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184336978&doi=10.1109%2fTASLP.2024.3361376&partnerID=40&md5=ebe53014cc3f05542471d5c92295f65d,"Audio-visual speech enhancement (AV-SE) aims to enhance degraded speech along with extra visual information such as lip videos, and has been shown to be more effective than audio-only speech enhancement. This paper proposes the incorporation of ultrasound tongue images to improve the performance of lip-based AV-SE systems further. To address the challenge of acquiring ultrasound tongue images during inference, we first propose to employ knowledge distillation during training to investigate the feasibility of leveraging tongue-related information without directly inputting ultrasound tongue images. Specifically, we guide an audio-lip speech enhancement student model to learn from a pre-trained audio-lip-tongue speech enhancement teacher model, thus transferring tongue-related knowledge. To better model the alignment between the lip and tongue modalities, we further propose the introduction of a lip-tongue key-value memory network into the AV-SE model. This network enables the retrieval of tongue features based on readily available lip features, thereby assisting the subsequent speech enhancement task. Experimental results demonstrate that both methods significantly improve the quality and intelligibility of the enhanced speech compared to traditional lip-based AV-SE baselines. Moreover, both proposed methods exhibit strong generalization performance on unseen speakers and in the presence of unseen noises. Furthermore, phone error rate (PER) analysis of automatic speech recognition (ASR) reveals that while all phonemes benefit from introducing ultrasound tongue images, palatal and velar consonants benefit most.  © 2014 IEEE.",Audio-visual speech enhancement; knowledge distillation; memory network; ultrasound tongue image,Distillation; Image enhancement; Knowledge management; Speech enhancement; Speech intelligibility; Speech recognition; Teaching; Audio-visual speech; Audio-visual speech enhancement; Knowledge distillation; Lip; Memory network; Performance; Tongue; Ultrasound tongue image; Video; Visual information; Ultrasonic imaging,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85184336978
Borgstrom B.J.; Brandstein M.S.,"Borgstrom, Bengt J. (36109984700); Brandstein, Michael S. (6603951674)",36109984700; 6603951674,A Multiscale Autoencoder (MSAE) Framework for End-to-End Neural Network Speech Enhancement,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2418,2431,13,1,10.1109/TASLP.2024.3389632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190787904&doi=10.1109%2fTASLP.2024.3389632&partnerID=40&md5=44c0c7a61a39351e6b8b288840e454af,"Neural network approaches to single-channel speech enhancement have received much recent attention. In particular, mask-based architectures have achieved significant performance improvements over conventional methods. This paper proposes a multiscale autoencoder (MSAE) for mask-based end-to-end neural network speech enhancement. The MSAE performs spectral decomposition of an input waveform within separate band-limited branches, each operating with a different rate and scale, to extract a sequence of multiscale embeddings. The proposed framework features intuitive parameterization of the autoencoder, including a flexible spectral band design based on the Constant-Q transform. Additionally, the MSAE is constructed entirely of differentiable operators, allowing it to be implemented within an end-to-end neural network, and be discriminatively trained. The MSAE draws motivation both from recent multiscale network topologies and from traditional multiresolution transforms in speech processing. Experimental results show the MSAE to provide clear performance benefits relative to conventional single-branch autoencoders. Additionally, the proposed framework is shown to outperform a variety of state-of-the-art enhancement systems, both in terms of objective speech quality metrics and automatic speech recognition accuracy. © 2014 IEEE.",end-to-end neural networks; multiscale representations; mutliresolution transforms; Speech enhancement,Learning systems; Neural networks; Speech recognition; Decoding; Ear; End to end; End-to-end neural network; Multiscale representations; Mutliresolution transform; Neural-networks; Signal resolution; Time-frequency Analysis; Speech enhancement,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85190787904
Lu J.; Zhang Q.; Cao J.; Tian H.,"Lu, Jing (57155247800); Zhang, Qiang (59707744400); Cao, Jialu (59708678800); Tian, Hui (45861574300)",57155247800; 59707744400; 59708678800; 45861574300,DDFNet: A Dual-Domain Fusion Network for Robust Synthetic Speech Detection,2025,Big Data and Cognitive Computing,9,3,58,,,,0,10.3390/bdcc9030058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000949520&doi=10.3390%2fbdcc9030058&partnerID=40&md5=1dad5723b9e40458ae9e97c7f90e7eac,"The detection of synthetic speech has become a pressing challenge due to the potential societal risks posed by synthetic speech technologies. Existing methods primarily focus on either the time or frequency domain of speech, limiting their ability to generalize to new and diverse speech synthesis algorithms. In this work, we present a novel and scientifically grounded approach, the Dual-domain Fusion Network (DDFNet), which synergistically integrates features from both the time and frequency domains to capture complementary information. The architecture consists of two specialized single-domain feature extraction networks, each optimized for the unique characteristics of its respective domain, and a feature fusion network that effectively combines these features at a deep level. Moreover, we incorporate multi-task learning to simultaneously capture rich, multi-faceted representations, further enhancing the model’s generalization capability. Extensive experiments on the ASVspoof 2019 Logical Access corpus and ASVspoof 2021 tracks demonstrate that DDFNet achieves strong performance, maintaining competitive results despite the challenges posed by channel changes and compression coding, highlighting its robust generalization ability. © 2025 by the authors.",Dual-domain fusion; multi-task learning; speech forensics; synthetic speech detection,Frequency domain analysis; Speech recognition; Dual domain; Dual-domain fusion; Multitask learning; Pressung; Societal risks; Speech detection; Speech forensics; Speech technology; Synthetic speech; Synthetic speech detection; Multi-task learning,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-105000949520
Wang Q.; Lopez Moreno I.,"Wang, Quan (57204051224); Lopez Moreno, Ignacio (56303939400)",57204051224; 56303939400,Version control of speaker recognition systems,2024,Journal of Systems and Software,216,,112122,,,,0,10.1016/j.jss.2024.112122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195877611&doi=10.1016%2fj.jss.2024.112122&partnerID=40&md5=e28a280f3d5efa9749d9a3ed11ba28ae,"This paper discusses one of the most challenging practical engineering problems in speaker recognition systems — the version control of models and user profiles. A typical speaker recognition system consists of two stages: the enrollment stage, where a profile is generated from user-provided enrollment audio; and the runtime stage, where the voice identity of the runtime audio is compared against the stored profiles. As technology advances, the speaker recognition system needs to be updated for better performance. However, if the stored user profiles are not updated accordingly, version mismatch will result in meaningless recognition results. In this paper, we describe different version control strategies for speaker recognition systems that had been carefully studied at Google from years of engineering practice. These strategies are categorized into three groups according to how they are deployed in the production environment: device-side deployment, server-side deployment, and hybrid deployment. To compare different strategies with quantitative metrics under various network configurations, we present SpeakerVerSim, an easily-extensible Python-based simulation framework for different server-side deployment strategies of speaker recognition systems. © 2024 Elsevier Inc.",Biometrics; Deployment; Simulation; Speaker recognition; Version control,Information management; Python; Speech recognition; User profile; Deployment; Practical engineering problems; Runtimes; Server sides; Simulation; Speaker recognition; Speaker recognition system; User's profiles; Version control; Voice identities; Computer software,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85195877611
Klüber K.; Schwaiger K.; Onnasch L.,"Klüber, Kim (57373544000); Schwaiger, Katharina (59563352600); Onnasch, Linda (35111634600)",57373544000; 59563352600; 35111634600,Affect-Enhancing Speech Characteristics for Robotic Communication,2025,International Journal of Social Robotics,17,2,922503,315,333,18,0,10.1007/s12369-025-01221-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218190084&doi=10.1007%2fs12369-025-01221-w&partnerID=40&md5=c96950a4b1893e6a9a26527c13808434,"The attribution of mind to others, either humans or artificial agents, can be conceptualized along two dimensions: experience and agency. These dimensions are crucial in interactions with robots, influencing how they are perceived and treated by humans. Specifically, a higher attribution of agency to robots is associated with greater perceived responsibility, while a higher attribution of experience enhances sympathy towards them. One potential strategy to increase the attribution of experience to robots is the application of affective communication induced via prosody and verbal content such as emotional words and speech style. In two online studies (NI = 30, NII = 60), participants listened to audio recordings in which robots introduced themselves. In study II, robot pictures were additionally presented to investigate potential matching effects between appearance and speech. Our results showed that both the use of emotional words and speaking expressively significantly increased the attributed experience of robots, whereas the attribution of agency remained unaffected. Findings further indicate that speaking expressively and using emotional words enhanced the perception of human-like qualities in artificial communication partners, with a more pronounced effect observed for technical robots compared to human-like robots. These insights can be used to improve the affective impact of synthesized robot speech and thus potentially increase the acceptance of robots to ensure long-term use. © The Author(s) 2025.",Affect; Human–robot interaction; Mind perception; Speech perception; Trust,Human robot interaction; Microrobots; Nanorobots; Sensory perception; Speech communication; Speech enhancement; Speech recognition; Affect; Artificial agents; Attribution of mind; Emotional words; Human agent; Humans-robot interactions; Mind perception; Speech perception; Trust; Two-dimensions; Audio recordings,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85218190084
Shen S.; Liu F.; Wang H.; Wang Y.; Zhou A.,"Shen, Siyuan (57371563200); Liu, Feng (57218659451); Wang, Hanyang (57258596800); Wang, Yunlong (59370554900); Zhou, Aimin (36648271300)",57371563200; 57218659451; 57258596800; 59370554900; 36648271300,Temporal Shift Module with Pretrained Representations for Speech Emotion Recognition,2024,Intelligent Computing,3,,0073,,,,6,10.34133/icomputing.0073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199612576&doi=10.34133%2ficomputing.0073&partnerID=40&md5=4daebe05fe2538860f8cdde2141a8c06,"Recent advances in self-supervised models have led to effective pretrained speech representations in downstream speech emotion recognition tasks. However, previous research has primarily focused on exploiting pretrained representations by simply adding a linear head on top of the pretrained model, while overlooking the design of the downstream network. In this paper, we propose a temporal shift module with pretrained representations to integrate channel-wise information without introducing additional parameters or floating-point operations per second. By incorporating the temporal shift module, we developed corresponding shift variants for 3 baseline building blocks: ShiftCNN, ShiftLSTM, and Shiftformer. Furthermore, we propose 2 technical strategies, placement and proportion of shift, to balance the trade-off between mingling and misalignment. Our family of temporal shift models outperforms state-of-the-art methods on the benchmark Interactive Emotional Dyadic Motion Capture dataset in fine-tuning and feature-extraction scenarios. In addition, through comprehensive experiments using wav2vec 2.0 and Hidden-Unit Bidirectional Encoder Representations from Transformers representations, we identified the behavior of the temporal shift module in downstream models, which may serve as an empirical guideline for future exploration of channel-wise shift and downstream network design. © 2024 Siyuan Shen et al.",,Integrated circuit design; Speech recognition; Building blockes; Down-stream; Downstream networks; Floating point operations per seconds; Shift-variant; Speech emotion recognition; State-of-the-art methods; Technical strategy; Temporal shifts; Trade off; Emotion Recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85199612576
Jung S.; Kim H.; Park H.; Choi A.,"Jung, Sunghoon (55740726600); Kim, Hanmoe (59564813600); Park, Hyunseo (59564608100); Choi, Ahyoung (57193550851)",55740726600; 59564813600; 59564608100; 57193550851,"Integrated AI System for Real-Time Sports Broadcasting: Player Behavior, Game Event Recognition, and Generative AI Commentary in Basketball Games",2025,Applied Sciences (Switzerland),15,3,1543,,,,0,10.3390/app15031543,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218257267&doi=10.3390%2fapp15031543&partnerID=40&md5=363b7360bb323b0b82945c1c5118eb1d,"This study presents an AI-based sports broadcasting system capable of real-time game analysis and automated commentary. The model first acquires essential background knowledge, including the court layout, game rules, team information, and player details. YOLO model-based segmentation is applied for a local camera view to enhance court recognition accuracy. Player’s actions and ball tracking is performed through YOLO algorithms. In each frame, the YOLO detection model is used to detect the bounding boxes of the players. Then, we proposed our tracking algorithm, which computed the IoU from previous frames and linked together to track the movement paths of the players. Player behavior is achieved via the R(2+1)D action recognition model including player actions such as running, dribbling, shooting, and blocking. The system demonstrates high performance, achieving an average accuracy of 97% in court calibration, 92.5% in player and object detection, and 85.04% in action recognition. Key game events are identified based on positional and action data, with broadcast lines generated using GPT APIs and converted to natural audio commentary via Text-to-Speech (TTS). This system offers a comprehensive framework for automating sports broadcasting with advanced AI techniques. © 2025 by the authors.",commentary generation; court segmentation; sports activity recognition,Human computer interaction; Image segmentation; Metadata; Program debugging; Speech enhancement; Speech recognition; Action recognition; Activity recognition; AI systems; Commentary generation; Court segmentation; Event recognition; Player behavior; Real- time; Sport activity recognition; Sports activity; Basketball,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85218257267
Chutia T.; Baruah N.,"Chutia, Tulika (59212816100); Baruah, Nomi (57188715299)",59212816100; 57188715299,A review on emotion detection by using deep learning techniques,2024,Artificial Intelligence Review,57,8,203,,,,4,10.1007/s10462-024-10831-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198075954&doi=10.1007%2fs10462-024-10831-1&partnerID=40&md5=1999b284ee9ecdbab6f364c987dca143,"Along with the growth of Internet with its numerous potential applications and diverse fields, artificial intelligence (AI) and sentiment analysis (SA) have become significant and popular research areas. Additionally, it was a key technology that contributed to the Fourth Industrial Revolution (IR 4.0). The subset of AI known as emotion recognition systems facilitates communication between IR 4.0 and IR 5.0. Nowadays users of social media, digital marketing, and e-commerce sites are increasing day by day resulting in massive amounts of unstructured data. Medical, marketing, public safety, education, human resources, business, and other industries also use the emotion recognition system widely. Hence it provides a large amount of textual data to extract the emotions from them. The paper presents a systematic literature review of the existing literature published between 2013 to 2023 in text-based emotion detection. This review scrupulously summarized 330 research papers from different conferences, journals, workshops, and dissertations. This paper explores different approaches, methods, different deep learning models, key aspects, description of datasets, evaluation techniques, Future prospects of deep learning, challenges in existing studies and presents limitations and practical implications. © The Author(s) 2024.",BERT; Bi-LSTM; Deep learning; Emotion detection; LSTM,Accident prevention; Commerce; Learning systems; Long short-term memory; Marketing; Speech recognition; Application fields; BERT; Bi-LSTM; Deep learning; Diverse fields; Emotion detection; Emotion recognition; Learning techniques; LSTM; Recognition systems; Emotion Recognition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85198075954
Behdad M.; Gharavian D.,"Behdad, Majid (58482353400); Gharavian, Davood (24075697400)",58482353400; 24075697400,MetricCycleGAN-VC: Forcing CycleGAN-Based Voice Conversion Systems to Associate with Objective Quality Metrics,2024,IEEE Access,12,,,154631,154649,18,0,10.1109/ACCESS.2024.3471926,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205868859&doi=10.1109%2fACCESS.2024.3471926&partnerID=40&md5=e75105ee89aec7b2c8208ce55f9e3fa0,"In CycleGAN-based voice conversion (VC) systems, such as MaskCycleGAN-VC, similar to many other GAN-based VC systems, adversarial losses are not explicitly designed to optimize VC evaluation metrics. Consequently, the generator may not consistently improve the metric scores related to VC assessments, such as the Mel-Cepstral Distance (MCD). To address this, we propose MetricCycleGAN-VC, which optimizes discriminators based on one or more VC evaluation metrics in a black-box strategy. Our approach covers various metrics related to VC quality triple aspects: speaker similarity, speech naturalness, and intelligibility. We Examine different well-known speech-related metrics such as MCD, Short-Time Objective of Intelligibility (STOI), neural tools such as Non-Intrusive objective Speech Quality Assessment (NISQA), and a newly proposed metric called Speaker Embedding Cosine Similarity (SECS). We also leverage full-reference image quality metrics like Feature-based similarity index Measure (FSIM) and combine all these intrusive, and non-intrusive metrics Within a unified framework for non-parallel voice conversion. While our method allows the utilization of all existing or newly developed quality metrics, this will be done without compromising their structural integrity or requiring knowledge of their formulation. Experimental results demonstrate improved desired metrics in both inter- and intra-gender conversions, showcasing the superiority of our approach over MaskCycleGAN-VC a typical baseline and many state-of-the-art VC systems. A key strength of our proposed method is its adaptability to evolving VC quality metrics, promising even better results as more precise metrics are introduced and implemented.  © 2013 IEEE.",CycleGAN-VC; DEM problem; FSIM metric; Mel-Cepstral distance (MCD); MetricGAN; NISQA tool; Resemblyzer; speaker similarity; speech perceptual evaluation; STOI measure,Arts computing; Speech enhancement; Speech recognition; Assessment tool; Cepstral; Cyclegan-voice conversion; Feature-based similarities; Feature-based similarity index measure metric; Mel-cepstral distance; Metricgan; Non-intrusive; Non-intrusive objective speech quality assessment tool; Objective speech quality assessment; Perceptual evaluation; Perceptual optimization; Resemblyzer; Short-time objective of intelligibility measure; Similarity indices; Speaker similarity; Speech perceptual evaluation; Voice conversion; Discriminators,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85205868859
Kurbanazarova N.; Shavkidinova D.; Khaydarov M.; Mukhitdinova N.; Khudoymurodova K.; Toshniyozova D.; Karimov N.; Alimova R.,"Kurbanazarova, Nargis (59380502400); Shavkidinova, Dilnavoz (59380199000); Khaydarov, Murodilla (57221666532); Mukhitdinova, Nazmiya (59380653600); Khudoymurodova, Khuriyat (59380199100); Toshniyozova, Dilfuza (59380653700); Karimov, Nodir (57211402498); Alimova, Rakhima (59166027600)",59380502400; 59380199000; 57221666532; 59380653600; 59380199100; 59380653700; 57211402498; 59166027600,Development of Speech Recognition in Wireless Mobile Networks for An Intelligent Learning System in Language Education,2024,"Journal of Wireless Mobile Networks, Ubiquitous Computing, and Dependable Applications",15,3,,298,311,13,13,10.58346/JOWUA.2024.I3.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207276363&doi=10.58346%2fJOWUA.2024.I3.020&partnerID=40&md5=56a9682015a847038cb966d3797de5cf,"Communication has been crucial to human existence, society, and globalization for millennia. Speech Recognition (SR) technologies include biometric evaluation, security, safety, medical care, and smart cities. Most research has primarily focused on English; others neglect other lower-asset dialects like Uzbek, neglecting its research unaddressed. This study examines the efficacy of peer and ASR response in wireless mobile networks-assisted pronouncing training. This study proposes a Deep Neural Network (DNN) and Hidden Markov Method (HMM) based ASR model to develop a voice recognition system utilizing a combination of Connected Time-based Categorization (CTC)-attending networks for the Uzbek words and their variants. The suggested method diminishes training duration and enhances SR precision by efficiently employing the CTC goal function in attentiveness modeling. The research assessed the results of both linguistic experts and native speakers on the Uzbek database, which was compiled for this research. The data were gathered through a pronunciation assessment and a discussion. The participant was further instructed in the classroom. Test outcomes indicate that the suggested method attained a word error rate of 13.1%, utilizing 210 hours of records as a learning dataset for the Uzbek dialect. The proposed technique can significantly enhance students' pronunciation qualities. It might inspire pupils to participate in pronunciation learning. © 2024, Innovative Information Science and Technology Research Group. All rights reserved.",Language Education; Smart Learning; Speech Recognition; Wireless Mobile Networks,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85207276363
Wieczorkowska A.,"Wieczorkowska, Alicja (6603646989)",6603646989,Methodology for Obtaining High-Quality Speech Corpora,2025,Applied Sciences (Switzerland),15,4,1848,,,,0,10.3390/app15041848,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218465551&doi=10.3390%2fapp15041848&partnerID=40&md5=fa3b5912f007cb8f0017f5e54a9e5d30,"Featured Application: Creating speech corpora. Speech-based communication between users and machines is a very lively branch of research that covers speech recognition, synthesis, and, generally, natural language processing. Speech corpora are needed for training algorithms for human–machine communication, especially for automatic speech recognition and for speech synthesis. Generative artificial intelligence models also need corpora for training for every language implemented. Therefore, speech corpora are constantly being created. In this paper, we discuss how to create high-quality corpora. The technical parameters of the recordings and audio files are addressed, and a methodology is proposed for planning speech corpus creation with an emphasis on usability. The proposed methodology draws the attention of potential creators of speech corpora to often neglected aspects of the corpus creation process. The criteria for a quality assessment of particular components are also discussed. The author recommends not combining all quality metrics into one (or at least allowing users to adjust particular weights), as different users might be interested in different quality components. The presented guidelines lead to obtaining high-quality corpora that meet the needs of their end users and are easy to use. © 2025 by the author.",automatic speech recognition; natural language processing; speech corpora,Audio recordings; Natural language processing systems; Audio files; Automatic speech recognition; High quality; Human-machine communication; Intelligence models; Language processing; Natural language processing; Natural languages; Speech corpora; Training algorithms; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85218465551
Luo X.; Takamichi S.; Saito Y.; Koriyama T.; Saruwatari H.,"Luo, Xuan (57538500000); Takamichi, Shinnosuke (55751754900); Saito, Yuki (57194868285); Koriyama, Tomoki (42061816900); Saruwatari, Hiroshi (7003918936)",57538500000; 55751754900; 57194868285; 42061816900; 7003918936,"Emotion-controllable Speech Synthesis Using Emotion Soft Label, Utterance-level Prosodic Factors, and Word-level Prominence",2024,APSIPA Transactions on Signal and Information Processing,13,1,e2,,,,0,10.1561/116.00000242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189864509&doi=10.1561%2f116.00000242&partnerID=40&md5=0a9bb6b21397c5db1fa9e545135672f2,"We propose a two-stage emotion-controllable text-to-speech (TTS) model that can increase the diversity of intra-emotion variation and also preserve inter-emotion controllability in synthesized speech. Conventional emotion-controllable TTS models increase the diversity of intra-emotion variation by controlling fine-grained emotion strengths; however, such models cannot control various prosodic factors (e.g., pitch). While other methods directly condition TTS models on intuitive prosodic factors, they cannot control emotions. Our proposed two-stage emotion-controllable TTS model extends the Tacotron2 model with a speech emotion recognizer (SER) and a prosodic factor generator (PFG) to solve this problem. In the first stage, we condition our model on emotion soft labels predicted by the SER model to enable inter-emotion controllability. In the second stage, we fine-condition our model on utterance-level prosodic factors and word-level prominence generated by the PFG model from emotion soft labels, which provides intra-emotion diversity. Due to this two-stage control design, we can increase intra-emotion diversity at both the utterance and word levels, and also preserve inter-emotion controllability. The experiments achieved 1) 51% emotion-distinguishable accuracy on average when conditioning on soft labels of three emotions, 2) average linear controllability scores of 0.95 when fine-conditioning on prosodic factors and prominence, respectively, and 3) comparable audio quality to conventional models. © 2024 X. Luo, S. Takamichi, Y. Saito, T. Koriyama and H. Saruwatari.",controllable speech synthesis; Emotion-controllable speech synthesis; expressive speech synthesis; speech emotion recognition; text to speech,Character recognition; Controllability; Emotion Recognition; Speech recognition; Condition; Controllable speech synthesis; Emotion-controllable speech synthesis; Expressive speech synthesis; Prosodics; Soft labels; Speech emotion recognition; Speech models; Text to speech; Word level; Speech synthesis,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85189864509
Yao J.; Wang Q.; Guo P.; Ning Z.; Xie L.,"Yao, Jixun (57222567951); Wang, Qing (57020582000); Guo, Pengcheng (57204213650); Ning, Ziqian (57971417000); Xie, Lei (35294300000)",57222567951; 57020582000; 57204213650; 57971417000; 35294300000,Distinctive and Natural Speaker Anonymization via Singular Value Transformation-Assisted Matrix,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2944,2956,12,1,10.1109/TASLP.2024.3407600,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196614913&doi=10.1109%2fTASLP.2024.3407600&partnerID=40&md5=d9c5139f55f34592775f264ced7d63e4,"Speaker anonymization is an effective privacy protection solution that aims to conceal speaker's identity while preserving the naturalness and distinctiveness of the original speech. Mainstream approaches use an utterance-level vector from a pre-trained automatic speaker verification (ASV) model to represent speaker identity, which is then averaged or modified for anonymization. However, these systems suffer from deterioration in the naturalness of anonymized speech, degradation in speaker distinctiveness, and severe privacy leakage against powerful attackers. To address these issues and especially generate more natural and distinctive anonymized speech, we propose a novel speaker anonymization approach that models a matrix related to speaker identity and transforms it into an anonymized singular value transformation-assisted matrix to conceal the original speaker identity. Our approach extracts frame-level speaker vectors from a pre-trained ASV model and employs an attention mechanism to create a speaker-score matrix and speaker-related tokens. Notably, the speaker-score matrix acts as the weight for the corresponding speaker-related token, representing the speaker's identity. The singular value transformation-assisted matrix is generated through the recomposition of the decomposed orthonormal eigenvectors matrix and non-linear transformed singular through Singular Value Decomposition (SVD). This process prevents the degradation of speaker distinctiveness caused by the introduction of other speakers' identity information. By multiplying the singular value transformation-assisted matrix and speaker-related tokens, we generate the anonymized speaker identity representation, thereby producing anonymized speech that is both natural and distinctive. Experiments on VoicePrivacy Challenge datasets demonstrate the effectiveness of our approach in protecting speaker privacy under all attack scenarios while maintaining speech naturalness and distinctiveness.  © 2014 IEEE.",privacy protection; singular value decomposition (SVD); Speaker anonymization; VoicePrivacy challenge,Linear transformations; Speech recognition; Anonymization; Automatic speaker verification; matrix; Privacy protection; Score matrixes; Singular value decomposition; Singular values; Speaker anonymization; Verification model; Voiceprivacy challenge; Singular value decomposition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85196614913
Li K.; Baird C.; Lin D.,"Li, Ke (57899964700); Baird, Cameron (57900169800); Lin, Dan (58597787900)",57899964700; 57900169800; 58597787900,Defend Data Poisoning Attacks on Voice Authentication,2024,IEEE Transactions on Dependable and Secure Computing,21,4,,1754,1769,15,1,10.1109/TDSC.2023.3289446,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163729077&doi=10.1109%2fTDSC.2023.3289446&partnerID=40&md5=6223bf004486aebd1cd10ef3f027b254,"With the advances in deep learning, speaker verification has achieved very high accuracy and is gaining popularity as a type of biometric authentication option in many scenes of our daily life, especially the growing market of web services. Compared to traditional passwords, 'vocal passwords' are much more convenient as they relieve people from memorizing different passwords. However, new machine learning attacks are putting these voice authentication systems at risk. Without a strong security guarantee, attackers could access legitimate users' web accounts by fooling the deep neural network (DNN) based voice recognition models. In this article, we demonstrate an easy-to-implement data poisoning attack to the voice authentication system, which cannot be captured effectively by existing defense mechanisms. Thus, we also propose a more robust defense method called Guardian, a convolutional neural network-based discriminator. The Guardian discriminator integrates a series of novel techniques including bias reduction, input augmentation, and ensemble learning. Our approach is able to distinguish about 95% of attacked accounts from normal accounts, which is much more effective than existing approaches with only 60% accuracy. © 2004-2012 IEEE.",data poisoning attacks; deep neural networks; Voice authentication,Authentication; Deep neural networks; Network security; Web services; Websites; Authentication systems; Data poisoning attack; High-accuracy; Network-based; Neural-networks; Password; Poisoning attacks; Speaker verification; Voice authentication; Webs services; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85163729077
Wang Z.; Chen Y.; Wang X.; Xie L.; Wang Y.,"Wang, Zhichao (57221097923); Chen, Yuanzhe (57305371600); Wang, Xinsheng (57204884630); Xie, Lei (35294300000); Wang, Yuping (57225157860)",57221097923; 57305371600; 57204884630; 35294300000; 57225157860,StreamVoice+: Evolving Into End-to-End Streaming Zero-Shot Voice Conversion,2024,IEEE Signal Processing Letters,31,,,3000,3004,4,0,10.1109/LSP.2024.3483012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207704882&doi=10.1109%2fLSP.2024.3483012&partnerID=40&md5=7eefdae62c2d267f0ab9f93c4d3cb761,"StreamVoice has recently pushed the boundaries of zero-shot voice conversion (VC) in the streaming domain. It uses a streamable language model (LM) with a context-aware approach to convert semantic features from automatic speech recognition (ASR) into acoustic features with the desired speaker timbre. Despite its innovations, StreamVoice faces challenges due to its dependency on a streaming ASR within a cascaded framework, which complicates system deployment and optimization, affects VC system's design and performance based on the choice of ASR, and struggles with conversion stability when faced with low-quality semantic inputs. To overcome these limitations, we introduce StreamVoice+, an enhanced LM-based end-to-end streaming framework that operates independently of streaming ASR. StreamVoice+ integrates a semantic encoder and a connector with the original StreamVoice framework, now trained using a non-streaming ASR. This model undergoes a two-stage training process: initially, the StreamVoice backbone is pre-trained for voice conversion and the semantic encoder for robust semantic extraction. Subsequently, the system is fine-tuned end-to-end, incorporating a LoRA matrix to activate comprehensive streaming functionality. Furthermore, StreamVoice+ mainly introduces two strategic enhancements to boost conversion quality: a residual compensation mechanism in the connector to ensure effective semantic transmission and a self-refinement strategy that leverages pseudo-parallel speech pairs generated by the conversion backbone to improve speech decoupling. Experiments demonstrate that StreamVoice+ not only achieves higher naturalness and speaker similarity in voice conversion than its predecessor but also provides versatile support for both streaming and non-streaming conversion scenarios. © 1994-2012 IEEE.",end-to-end; language model; parameter-efficient fine-tuning; Streaming voice conversion; zero-shot,Audio streaming; Encoding (symbols); Image coding; Integrated circuit design; Signal encoding; Speech enhancement; Speech recognition; Automatic speech recognition; Context-aware approaches; End to end; Fine tuning; Language model; Parameter-efficient fine-tuning; Streaming domains; Streaming voice conversion; Voice conversion; Zero-shot; Semantics,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85207704882
Wagner D.; Baumann I.; Bocklet T.,"Wagner, Dominik (57610138600); Baumann, Ilja (57606401800); Bocklet, Tobias (23395938500)",57610138600; 57606401800; 23395938500,Generative adversarial networks for whispered to voiced speech conversion: a comparative study,2024,International Journal of Speech Technology,27,4,,1093,1110,17,0,10.1007/s10772-024-10161-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207004430&doi=10.1007%2fs10772-024-10161-1&partnerID=40&md5=737839f3e749358806f840193c9db3a6,"Generative Adversarial Networks (GANs) have demonstrated promising results as end-to-end models for whispered to voiced speech conversion. Leveraging non-autoregressive systems like GANs capable of performing conditional waveform generation eliminates the need for separate models to estimate voiced speech features, and leads to faster inference compared to autoregressive methods. This study aims to identify the optimal GAN architecture for the whispered to voiced speech conversion task by comparing six state-of-the-art models. Furthermore, we present a method for evaluating the preservation of speaker identity and local accent, using embeddings obtained from speaker- and language identification systems. Our experimental results show that building the speech conversion system based on the HiFi-GAN architecture yields the best objective evaluation scores, outperforming the baseline by ∼ 9% relative using frequency-weighted Signal-to-Noise Ratio and Log Likelihood Ratio, as well as by ∼ 29% relative using Root Mean Squared Error. In subjective tests, HiFi-GAN yielded a mean opinion score of 2.9, significantly outperforming the baseline with a score of 1.4. Furthermore, HiFi-GAN enhanced ASR performance and preserved speaker identity and accent, with correct language detection rates of up to ∼ 98%. © The Author(s) 2024.",Generative adversarial networks; Speech conversion; Voiced speech; Whispered speech,Mean square error; Speech recognition; Subjective testing; Adversarial networks; Autoregressive systems; Comparatives studies; End-to-end models; Fast inference; Speech conversion; Speech features; Voiced speech; Waveform generation; Whispered speech; Generative adversarial networks,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85207004430
Ueno S.; Lee A.; Kawahara T.,"Ueno, Sei (57204047250); Lee, Akinobu (7405629175); Kawahara, Tatsuya (7202414284)",57204047250; 7405629175; 7202414284,Refining Synthesized Speech Using Speaker Information and Phone Masking for Data Augmentation of Speech Recognition,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,3924,3933,9,0,10.1109/TASLP.2024.3451982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203498256&doi=10.1109%2fTASLP.2024.3451982&partnerID=40&md5=97bc5ae5716799a60dafcef05865fce0,"While end-to-end automatic speech recognition (ASR) has shown impressive performance, it requires a huge amount of speech and transcription data. The conversion of domain-matched text to speech (TTS) has been investigated as one approach to data augmentation. The quality and diversity of the synthesized speech are critical in this approach. To ensure quality, a neural vocoder is widely used to generate speech waveforms in conventional studies, but it requires a huge amount of computation and another conversion to spectral-domain features such as the log-Mel filterbank (lmfb) output typically used for ASR. In this study, we explore the direct refinement of these features. Unlike conventional speech enhancement, we can use information on the ground-truth phone sequences of the speech and designated speaker to improve the quality and diversity. This process is realized as a Mel-to-Mel network, which can be placed after a text-to-Mel synthesis system such as FastSpeech 2. These two networks can be trained jointly. Moreover, semantic masking is applied to the lmfb features for robust training. Experimental evaluations demonstrate the effect of phone information, speaker information, and semantic masking. For speaker information, x-vector performs better than the simple speaker embedding. The proposed method achieves even better ASR performance with a much shorter computation time than the conventional method using a vocoder. © 2014 IEEE.",Data augmentation; domain adaptation; speech recognition; speech synthesis,Direct process refining; Speech enhancement; Speech recognition; Automatic speech recognition; Data augmentation; Domain adaptation; Domain feature; End to end; Performance; Spectral domains; Speech waveforms; Synthesized speech; Text to speech; Semantics,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85203498256
Mehrish A.; Majumder N.; Bharadwaj R.; Mihalcea R.; Poria S.,"Mehrish, Ambuj (57189058040); Majumder, Navonil (57203239752); Bharadwaj, Rishabh (59098910400); Mihalcea, Rada (8619220500); Poria, Soujanya (55316592700)",57189058040; 57203239752; 59098910400; 8619220500; 55316592700,A review of deep learning techniques for speech processing,2023,Information Fusion,99,,101869,,,,153,10.1016/j.inffus.2023.101869,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162051266&doi=10.1016%2fj.inffus.2023.101869&partnerID=40&md5=90a58330b1ebc9e04605b2cf2b3d9351,"The field of speech processing has undergone a transformative shift with the advent of deep learning. The use of multiple processing layers has enabled the creation of models capable of extracting intricate features from speech data. This development has paved the way for unparalleled advancements in speech recognition, text-to-speech synthesis, automatic speech recognition, and emotion recognition, propelling the performance of these tasks to unprecedented heights. The power of deep learning techniques has opened up new avenues for research and innovation in the field of speech processing, with far-reaching implications for a range of industries and applications. This review paper provides a comprehensive overview of the key deep learning models and their applications in speech-processing tasks. We begin by tracing the evolution of speech processing research, from early approaches, such as MFCC and HMM, to more recent advances in deep learning architectures, such as CNNs, RNNs, transformers, conformers, and diffusion models. We categorize the approaches and compare their strengths and weaknesses for solving speech-processing tasks. Furthermore, we extensively cover various speech-processing tasks, datasets, and benchmarks used in the literature and describe how different deep-learning networks have been utilized to tackle these tasks. Additionally, we discuss the challenges and future directions of deep learning in speech processing, including the need for more parameter-efficient, interpretable models and the potential of deep learning for multimodal speech processing. By examining the field's evolution, comparing and contrasting different approaches, and highlighting future directions and challenges, we hope to inspire further research in this exciting and rapidly advancing field. © 2023 Elsevier B.V.",Deep learning; Speech processing; Survey; Transformers; Trends,Character recognition; Emotion Recognition; Learning algorithms; Learning systems; Speech processing; Speech recognition; Speech synthesis; Automatic speech recognition; Deep learning; Emotion recognition; Learning techniques; Multiple processing; Processing layer; Speech data; Speech emotions; Transformer; Trend; Deep learning,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85162051266
Panariello M.; Tomashenko N.; Wang X.; Miao X.; Champion P.; Nourtel H.; Todisco M.; Evans N.; Vincent E.; Yamagishi J.,"Panariello, Michele (57223775680); Tomashenko, Natalia (55900552200); Wang, Xin (57196088126); Miao, Xiaoxiao (57203930009); Champion, Pierre (57222111913); Nourtel, Hubert (57212379933); Todisco, Massimiliano (24482053500); Evans, Nicholas (56207371100); Vincent, Emmanuel (14010158800); Yamagishi, Junichi (7004695833)",57223775680; 55900552200; 57196088126; 57203930009; 57222111913; 57212379933; 24482053500; 56207371100; 14010158800; 7004695833,The VoicePrivacy 2022 Challenge: Progress and Perspectives in Voice Anonymisation,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,3477,3491,14,1,10.1109/TASLP.2024.3430530,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199391105&doi=10.1109%2fTASLP.2024.3430530&partnerID=40&md5=5c1b44ab99e6593fb516e37577803654,"—The VoicePrivacy Challenge promotes the development of voice anonymisation solutions for speech technology. In this paper we present a systematic overview and analysis of the second edition held in 2022. We describe the voice anonymisation task and datasets used for system development and evaluation, present the different attack models used for evaluation, and the associated objective and subjective metrics. We describe three anonymisation baselines, provide a summary description of the anonymisation systems developed by challenge participants, and report objective and subjective evaluation results for all. In addition, we describe post-evaluation analyses and a summary of related work reported in the open literature. Results show that solutions based on voice conversion better preserve utility, that an alternative which combines automatic speech recognition with synthesis achieves greater privacy, and that a privacy-utility trade-off remains inherent to current anonymisation solutions. Finally, we present our ideas and priorities for future VoicePrivacy Challenge editions. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Anonymisation; attack model; pseudonymisation; speech synthesis; voice conversion; voice privacy,Data privacy; Economic and social effects; Job analysis; Speech processing; Speech recognition; Anonymization; Attack modeling; Privacy; Pseudonymisation; Recording; Speech technology; System development; Task analysis; Voice conversion; Voice privacy; Speech synthesis,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85199391105
Bekarystankyzy A.; Mamyrbayev O.; Anarbekova T.,"Bekarystankyzy, Akbayan (57226655446); Mamyrbayev, Orken (55967630400); Anarbekova, Tolganay (59202233000)",57226655446; 55967630400; 59202233000,Integrated End-to-End Automatic Speech Recognition for Languages for Agglutinative Languages,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,23,6,79,,,,2,10.1145/3663568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197432635&doi=10.1145%2f3663568&partnerID=40&md5=bbbbfda75487cbac165deaec6aaaae67,"The relevance of the problem of automatic speech recognition lies in the lack of research for low-resource languages, stemming from limited training data and the necessity for new technologies to enhance efficiency and performance. The purpose of this work was to study the main aspects of integrated end-to-end speech recognition and the use of modern technologies in the natural processing of agglutinative languages, including Kazakh. In this article, the study of language models was carried out using comparative, graphic, statistical, and analytical-synthetic methods, which were used in combination. This article addresses automatic speech recognition (ASR) in agglutinative languages, particularly Kazakh, through a unified neural network model that integrates both acoustic and language modeling. Employing advanced techniques like connectionist temporal classification and attention mechanisms, the study focuses on effective speech-to-text transcription for languages with complex morphologies. Transfer learning from high-resource languages helps mitigate data scarcity in languages such as Kazakh, Kyrgyz, Uzbek, Turkish, and Azerbaijani. The research assesses model performance, underscores ASR challenges, and proposes advancements for these languages. It includes a comparative analysis of phonetic and word-formation features in agglutinative Turkic languages, using statistical data. The findings aid further research in linguistics and technology for enhancing speech recognition and synthesis, contributing to voice identification and automation processes. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",data corpus; Language model; scarcity of resources; system learning,Classification (of information); Computational linguistics; Learning systems; Modeling languages; Natural language processing systems; Text processing; Agglutinative language; Automatic speech recognition; Data corpus; Efficiency and performance; End to end; Language model; Limited training data; Low resource languages; Scarcity of resource; System learning; Speech recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85197432635
Cheng S.; Zhu P.; Liu J.; Wang Z.,"Cheng, Shiyang (57192397354); Zhu, Pengcheng (58139770000); Liu, Jueting (57215220843); Wang, Zehua (56084199700)",57192397354; 58139770000; 57215220843; 56084199700,A Survey of Grapheme-to-Phoneme Conversion Methods,2024,Applied Sciences (Switzerland),14,24,11790,,,,0,10.3390/app142411790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213283103&doi=10.3390%2fapp142411790&partnerID=40&md5=39aad91fd0c0b0397b8f01737c7e992c,"Grapheme-to-phoneme conversion (G2P) is the task of converting letters (grapheme sequences) into their pronunciations (phoneme sequences). It plays a crucial role in natural language processing, text-to-speech synthesis, and automatic speech recognition systems. This paper provides a systematical overview of the G2P conversion from different perspectives. The conversion methods are first presented in the paper; detailed discussions are conducted on methods based on deep learning technology. For each method, the key ideas, advantages, disadvantages, and representative models are summarized. This paper then mentioned the learning strategies and multilingual G2P conversions. Finally, this paper summarized the commonly used monolingual and multilingual datasets, including Mandarin, Japanese, Arabic, etc. Two tables illustrated the performance of various methods with relative datasets. After making a general overall of G2P conversion, this paper concluded with the current issues and the future directions of deep learning-based G2P conversion. © 2024 by the authors.",deep learning; grapheme-to-phoneme conversion; machine learning; speech synthesis,Adversarial machine learning; Audio signal processing; Character recognition; Contrastive Learning; Deep learning; Speech recognition; Automatic speech recognition system; Conversion methods; Deep learning; Grapheme-to-phoneme conversion; Language processing; Learning strategy; Learning technology; Machine-learning; Natural languages; Text to speech; Natural language processing systems,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85213283103
Chaiwongyen A.; Duangpummet S.; Karnjana J.; Kongprawechnon W.; Unoki M.,"Chaiwongyen, Anuwat (36663505800); Duangpummet, Suradej (57201274405); Karnjana, Jessada (57190762441); Kongprawechnon, Waree (6507950956); Unoki, Masashi (6604048482)",36663505800; 57201274405; 57190762441; 6507950956; 6604048482,Potential of Speech-Pathological Features for Deepfake Speech Detection,2024,IEEE Access,12,,,121958,121970,12,0,10.1109/ACCESS.2024.3447582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201756721&doi=10.1109%2fACCESS.2024.3447582&partnerID=40&md5=b400aae40c896eeaac74fa91485f7715,"There is a great concern regarding the misuse of deepfake speech technology to synthesize a real person's voice. Therefore, developing speech-security systems capable of detecting deepfake speech remains paramount in safeguarding against such misuse. Although various speech features and methods have been proposed, their potential for distinguishing between genuine and deepfake speech remains unclear. Since speech-pathological features with deep learning are widely used to assess unnaturalness in disordered voices associated with voice-production mechanisms, we investigated the potential of eleven speech-pathological features for distinguishing between genuine and deepfake speech, i.e., jitter (three types), shimmer (four types), harmonics-to-noise ratio, cepstral-harmonics-to-noise ratio, normalized noise energy, and glottal-to-noise excitation ratio. This paper proposes a method of combining two models on the basis of two different dimensions of speech-pathological features to greatly improve the effectiveness of deepfake speech detection, along with mel-spectrogram features, to enhance detection efficiency. We evaluated the proposed method on the datasets of the Automatic Speaker Verification Spoofing and Countermeasures Challenges ASVspoof 2019 and 2021. The results indicate that the proposed method outperforms the baselines in terms of accuracy, recall, F1-score, and F2-score, achieving 95.06, 99.46, 97.30, and 98.59%, respectively, on the ASVspoof 2019 dataset. It also surpasses the baselines on the ASVspoof 2021 dataset in terms of recall, F1-score, F2-score, and equal error rate, achieving 99.96, 96.65, 98.18, and 15.97%, respectively.  © 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",cepstral-harmonics-to-noise ratio; Deepfake speech detection; glottal-to-noise; harmonics-to-noise ratio; jitter and shimmer; normalized noise energy; speech-pathological feature,Signal to noise ratio; Speech enhancement; Timing jitter; Cepstral; Cepstral-harmonic-to-noise ratio; Deepfake; Deepfake speech detection; Features extraction; Glottal-to-noise; Harmonic to noise ratios; Harmonics-to-noise ratios; Jitter and shimmer; Noise; Noise energy; Normalized noise; Normalized noise energy; Speech detection; Speech-pathological feature; Voice-activity detections; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85201756721
Kolekar S.V.; Agnihotri S.; Rao D.,"Kolekar, Sucheta V. (35119211500); Agnihotri, Shreevasta (59175825700); Rao, Divya (57489748000)",35119211500; 59175825700; 57489748000,Enhancing the Experience and Accessibility of Users with Disability by Integrating Voice Navigation into a Telemedicine Website,2024,"International Journal of Mathematical, Engineering and Management Sciences",9,4,,801,820,19,0,10.33889/IJMEMS.2024.9.4.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196183903&doi=10.33889%2fIJMEMS.2024.9.4.041&partnerID=40&md5=9540ba7b774bdd8f0a7cb1e027414daf,"As per the universal principles of usability and sustainable development goal defined for reducing inequality, each individual will have special abilities and challenges. Individuals with visual challenges have trouble interacting with digital platforms. In order to achieve inclusivity, there is a need to integrate universal accessibility into the web portals which are used as the most popular digital platforms. This study mainly focuses on the requirements of visually impaired users. The research work discusses the proposed approach for a telemedicine web platform by integrating voice navigation system. With this system, users can orally interact with the platform by using defined commands. Users can also receive the audio feedback from the computer for the specific command. The proposed speech recognition engine is implemented using deep learning models and tested on various browsers. The engine captures the commands in the form of user inputs and generates the proper audio feedback after executing the commands. Users with visual impairment have been involved in the evaluation by allowing them to interact on the telemedicine platform with verbal commands. The evaluation questions have been asked after each interaction to capture the response time, accuracy, experience and satisfaction. The outcome of the evaluation shows that individuals are showing significant progress in accessing required information and navigating the web pages on their choice of browsers. The study also observed that speech recognition engine along with speech grammar, acoustic model and synthesis has improved the usage of the system for all types of users. Ultimately, integrating voice navigation into web platforms provides a satisfactory experience to the users and improves accessibility, inclusivity and reduces inequality. © 2024 Ram Arti Publishers. All rights reserved.",Acoustic modelling; Language model; Natural language processing; Speech recognition; Viterbi algorithm,Air navigation; Computational linguistics; Deep learning; Engines; Modeling languages; Natural language processing systems; Navigation systems; Portals; Speech recognition; Telemedicine; Acoustics model; Audio feedbacks; Digital platforms; Language model; Language processing; Natural language processing; Natural languages; Speech recognition engine; Users with disabilities; Visually-impaired users; Viterbi algorithm,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85196183903
Yan X.,"Yan, Xin (59242138200)",59242138200,A Teaching Mode of College English Listening in Intelligent Phonetic Environments,2024,International Journal of e-Collaboration,20,1,,,,,0,10.4018/IJeC.347986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200158511&doi=10.4018%2fIJeC.347986&partnerID=40&md5=f39d48cfec90bb10d8aa7f561ac98c07,"This paper discusses the integration of cutting-edge technologies, especially artificial intelligence (AI) and speech synthesis in UETL environment. By using methods based on artificial intelligence, such as Fuzzy Convolutional Neural Network (FCNN) and Improved Hidden Markov Model (MHMM), this study aims to reform the traditional teaching paradigm. Through the in-depth study of the experiment, it illustrates how these innovations can enhance students' autonomous learning, understanding and participation in English language education. The implementation of speech synthesis mechanism realizes the conversion from real-time speech to text, and promotes interactive learning experience and personalized feedback. The comparative analysis before and after adopting advanced teaching methods shows that students' learning achievements and the overall effectiveness of UETL process have been significantly improved. This study emphasizes the revolutionary potential of integrating artificial intelligence and speech synthesis technology to optimize college English education. © 2024 IGI Global. All rights reserved.",Artificial Intelligence; Fuzzy based convolutional neural networks; Modeling; Speech recognition; Speech synthesis,Convolution; Education computing; Fuzzy inference; Fuzzy neural networks; Hidden Markov models; Learning systems; Speech synthesis; Students; College English; Convolutional neural network; Cutting edge technology; Fuzzy based convolutional neural network; Hidden-Markov models; In-depth study; Listening-in; Modeling; Teaching modes; Teaching paradigm; Speech recognition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85200158511
Chen G.; Zhao Z.; Song F.; Chen S.; Fan L.; Wang F.; Wang J.,"Chen, Guangke (57219623947); Zhao, Zhe (57207780865); Song, Fu (50562054300); Chen, Sen (57190395316); Fan, Lingling (57197024797); Wang, Feng (57747373200); Wang, Jiashui (57250802800)",57219623947; 57207780865; 50562054300; 57190395316; 57197024797; 57747373200; 57250802800,Towards Understanding and Mitigating Audio Adversarial Examples for Speaker Recognition,2023,IEEE Transactions on Dependable and Secure Computing,20,5,,3970,3987,17,15,10.1109/TDSC.2022.3220673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141615192&doi=10.1109%2fTDSC.2022.3220673&partnerID=40&md5=23a5c5ecf58fb2d60cb63435739db126,"Speaker recognition systems (SRSs) have recently been shown to be vulnerable to adversarial attacks, raising significant security concerns. In this work, we systematically investigate transformation and adversarial training based defenses for securing SRSs. According to the characteristic of SRSs, we present 22 diverse transformations and thoroughly evaluate them using 7 recent promising adversarial attacks (4 white-box and 3 black-box) on speaker recognition. With careful regard for best practices in defense evaluations, we analyze the strength of transformations to withstand adaptive attacks. We also evaluate and understand their effectiveness against adaptive attacks when combined with adversarial training. Our study provides thirteen useful insights and findings, many of them are new or inconsistent with the conclusions in the image and speech recognition domains, e.g., variable and constant bit rate speech compressions have different performance, and some non-differentiable transformations remain effective against current promising evasion techniques which often work well in the image domain. We demonstrate that the proposed novel feature-level transformation combined with adversarial training is rather effective compared to the sole adversarial training in a complete white-box setting, e.g., increasing the accuracy by 13.62% and attack cost by two orders of magnitude, while other transformations do not necessarily improve the overall defense capability. This work sheds further light on the research directions in this field. We also release our evaluation platform SpeakerGuard to foster further research.  © 2004-2012 IEEE.",Adversarial defenses; adversarial examples; adversarial training; input transformation; speaker recognition,Audio systems; Feature extraction; Image recognition; Job analysis; Network security; Adversarial defense; Adversarial example; Adversarial training; Features extraction; Input transformation; Speaker recognition; Speaker recognition system; Task analysis; White box; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85141615192
Azizah K.,"Azizah, Kurniawati (57221672866)",57221672866,Zero-Shot Voice Cloning Text-to-Speech for Dysphonia Disorder Speakers,2024,IEEE Access,12,,,63528,63547,19,1,10.1109/ACCESS.2024.3396377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192200067&doi=10.1109%2fACCESS.2024.3396377&partnerID=40&md5=3343dc5891853ee67188426f964aebf3,"Deep neural network based text-to-speech (TTS) technology has brought advances in speech synthesis approaching the quality of human speech sounds. Zero-shot voice cloning TTS is a system that accepts input in the form of text and a few seconds of a sample of the target speaker's voice to produce speech sound waves similar to the target speaker's voice. Some of the latest zero-shot voice cloning TTS studies still focuses on normal human voices. However, this technology still has limitations for individuals with speech disorders such as dysphonia. We observe that our baseline zero-shot TTS model applied to the dysphonia domain still has poor performance on the following aspects: speaker similarity, intelligibility or clarity of speech, and speech sound quality. This research develops 24 zero-shot voice cloning TTS models to observe which models can improve the baseline model performance on the dysphonia domain. We propose four categories to change the baseline model architecture and setting: input-level text sequences (grapheme, phoneme, or combination of grapheme-phoneme), speaker embedding type (speaker encoder or speaker model), speaker embedding position (at TTS encoder of at TTS encoder and decoder), and loss function (without or with speaker consistency loss). The experimental results show that the best model is the one uses the following configuration settings: a combination of grapheme-phoneme-level text sequences, speaker model as the speaker embedding, placing the speaker embedding at the TTS encoder only, and adding speaker consistency loss to the frame-level speech loss. Compared to the baseline model, our proposed best model is able to improve speaker cosine similarity (COS), speech intelligibility (CER), and speech sound quality (MOS) performance in the domain of dysphonia speech disorders by 0.197, 0.55%, and 0.244, respectively. When compared with the original voice of dysphonia disorder speakers, the best model also increases the speech intelligibility and quality of the speech sounds by 13.45% and 0.22, respectively.  © 2013 IEEE.",Deep neural network; dysphonia; speaker consistency loss; speaker encoder; speaker model; text-to-speech; voice cloning; zero-shot learning,Cloning; Decision trees; Speech enhancement; Speech intelligibility; Speech recognition; Speech synthesis; Adaptation models; Dysphonia; Recording; Speaker consistency loss; Speaker encoder; Speaker model; Speech sounds; Task analysis; Text to speech; Voice cloning; Deep neural networks,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85192200067
Ilgaz H.; Akkoyun B.; Alpay Ö.; Ali Akcayol M.,"Ilgaz, Hilal (59329715900); Akkoyun, Beyza (59329443400); Alpay, Özlem (57207456436); Ali Akcayol, M. (6507254369)",59329715900; 59329443400; 57207456436; 6507254369,CNN Based Automatic Speech Recognition: A Comparative Study,2024,Advances in Distributed Computing and Artificial Intelligence Journal,13,,e29191,,,,3,10.14201/adcaij.29191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204054259&doi=10.14201%2fadcaij.29191&partnerID=40&md5=4b36769ea2fd1b9314e41c4bd1108395,"Recently, one of the most common approaches used in speech recognition is deep learning. The most advanced results have been obtained with speech recognition systems created using convolutional neural network (CNN) and recurrent neural networks (RNN). Since CNNs can capture local features effectively, they are applied to tasks with relatively short-term dependencies, such as keyword detection or phoneme- level sequence recognition. This paper presents the development of a deep learning and speech command recognition system. The Google Speech Commands Dataset has been used for training. The dataset contained 65.000 one-second-long words of 30 short English words. That is, %80 of the dataset has been used in the training and %20 of the dataset has been used in the testing. The data set consists of one-second voice commands that have been converted into a spectrogram and used to train different artificial neural network (ANN) models. Various variants of CNN are used in deep learning applications. The performance of the proposed model has reached %94.60. © 2024 University of Salamanca. All rights reserved.",Artificial neural networks; Deep learning; Speech recognition,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85204054259
Mukhamadiyev A.; Khujayarov I.; Cho J.,"Mukhamadiyev, Abdinabi (57209690294); Khujayarov, Ilyos (58250907600); Cho, Jinsoo (55474141500)",57209690294; 58250907600; 55474141500,Voice-Controlled Intelligent Personal Assistant for Call-Center Automation in the Uzbek Language,2023,Electronics (Switzerland),12,23,4850,,,,1,10.3390/electronics12234850,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179330529&doi=10.3390%2felectronics12234850&partnerID=40&md5=3a47cc65a09ee52059dada3dc8c3defc,"The demand for customer support call centers has surged across various sectors due to the pandemic. Yet, the constraints of round-the-clock human services and fluctuating wait times pose challenges in fully meeting customer needs. In response, there’s a growing need for automated customer service systems that can provide responses tailored to specific domains and in the native languages of customers, particularly in developing nations like Uzbekistan where call center usage is on the rise. Our system, “UzAssistant,” is designed to recognize user voices and accurately present customer issues in standardized Uzbek, as well as vocalize the responses to voice queries. It employs feature extraction and recurrent neural network (RNN)-based models for effective automatic speech recognition, achieving an impressive 96.4% accuracy in real-time tests with 56 participants. Additionally, the system incorporates a sentence similarity assessment method and a text-to-speech (TTS) synthesis feature specifically for the Uzbek language. The TTS component utilizes the WaveNet architecture to convert text into speech in Uzbek. © 2023 by the authors.",call center; IVR; public services; speech corpus; speech recognition; speech synthesis; speech technologies; speech-to-text; text-to-speech; Uzbek language,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85179330529
Madan C.; Diddee H.; Kumar D.; Mittal M.,"Madan, Chetan (57938120000); Diddee, Harshita (57221469679); Kumar, Deepika (57760130700); Mittal, Mamta (57194916918)",57938120000; 57221469679; 57760130700; 57194916918,CodeFed: Federated Speech Recognition for Low-Resource Code-Switching Detection,2024,ACM Transactions on Asian and Low-Resource Language Information Processing,23,1,2,,,,0,10.1145/3571732,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183331471&doi=10.1145%2f3571732&partnerID=40&md5=46868697ac5ae75122c39577eabe437b,"One common constraint in the practical application of speech recognition is Code Switching. The issue of code-switched languages is especially aggravated in the context of Indian languages -since most massively multilingual models are trained on corpora that are not representative of the diverse set of Indian languages. An associated constraint with such systems is the privacy-intrusive nature of the applications that aim to collate such representative data. To collectively mitigate both problems, this work presents CodeFed: A fed- erated learning-based code-switching detection model that can be deployed to collaboratively be trained by leveraging private data from multiple users, without compromising their privacy. Using a representative low- resource Indic dataset, we demonstrate the superior performance of a collaboratively trained global model that is trained using federated learning on three low-resource Indic languages -Gujarati, Tamil and Telugu and draw a comparison of the model with respect to the most current work in the field. Finally, to evaluate the practical realizability of the proposed system, CodeFed also discusses the system overview of the label generation architecture which may accompany CodeFed's possible real-time deployment.  © 2024 Association for Computing Machinery.",Code-switching; fed- erated learning; low resource Indian languages; mobile computing; speech processing,Data privacy; Learning systems; Speech processing; Code-switching; Detection models; Feed- erated learning; Global models; Indian languages; Low resource indian language; Mobile-computing; Multiple user; Performance; Private data; Speech recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85183331471
Ali A.; Khusro S.,"Ali, Amjad (58268846700); Khusro, Shah (35302658300)",58268846700; 35302658300,SA-MEAS: Sympy-based automated mathematical equations analysis and solver,2024,SoftwareX,25,,101596,,,,2,10.1016/j.softx.2023.101596,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183578676&doi=10.1016%2fj.softx.2023.101596&partnerID=40&md5=12b5b9bd4c1d66127e7ea82e2c91aa44,"Students with blindness and visual disabilities encounter challenges learning mathematics, particularly mathematical equations (MEs), which contributes to their underrepresentation in STEM fields. MEs are complex because they have a two-dimensional structure, which makes them difficult to access. In addition, inconsistencies in Braille codes across regions, lack of collaborative math platforms, and a lack of accessible mathematics literature exacerbate their difficulties. Technology and tools currently available limit MEs analysis and solution. This study proposes a mathematical equation analysis and solving software application called SA-MEAS (Sympy-based Automated Mathematical Equation Analysis and Solver) for mathematical linear and quadratic equations and expressions. The proposed solution provides a quick overview and solutions for MEs. The overview provides terms, factors, variables and constants, types of equations, structure and a detailed summary of MEs. Also, it provides correct solutions. Students correlate the summary and solution and build a mental model of the MEs. To develop this application a Python symbolic mathematics library (sympy) was utilized to automate MEs analysis and solution. Similarly, a speech synthesis library verbally summarized the analysis and solution. Furthermore, a Python speech recognition library was used for interactive communication. Finally, a user-friendly interface was designed by utilizing the Tkinter library. The proposed software was empirically evaluated with 32 blind and visually impaired students, and significant improvements were found in math accessibility and learning. © 2023 The Author(s)",Blind and Visually impaired students; Math Accessibility; Mathematical Equations Solver; Symbolic Mathematics in Python (SYMPY),Application programs; High level languages; Speech communication; Speech recognition; Speech synthesis; Students; Blind and visually impaired; Blind and visually impaired student; Equation solvers; Math accessibility; Mathematical equation solv; Mathematical equations; Symbolic mathematic in python; Symbolic mathematics; Visual disability; Visually impaired students; Python,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85183578676
Zhang K.; Hua Z.; Zhang Y.; Guo Y.; Xiang T.,"Zhang, Kuiyuan (57221953060); Hua, Zhongyun (56022005600); Zhang, Yushu (55508709300); Guo, Yifang (59469464100); Xiang, Tao (57213003210)",57221953060; 56022005600; 55508709300; 59469464100; 57213003210,Robust AI-Synthesized Speech Detection Using Feature Decomposition Learning and Synthesizer Feature Augmentation,2025,IEEE Transactions on Information Forensics and Security,20,,,871,885,14,0,10.1109/TIFS.2024.3520001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212768798&doi=10.1109%2fTIFS.2024.3520001&partnerID=40&md5=e4085182dfe8960f2f10820e1aff1205,"AI-synthesized speech, also known as deepfake speech, has recently raised significant concerns due to the rapid advancement of speech synthesis and speech conversion techniques. Previous works often rely on distinguishing synthesizer artifacts to identify deepfake speech. However, excessive reliance on these specific synthesizer artifacts may result in unsatisfactory performance when addressing speech signals created by unseen synthesizers. In this paper, we propose a robust deepfake speech detection method that employs feature decomposition to learn synthesizer-independent content features as complementary for detection. Specifically, we propose a dual-stream feature decomposition learning strategy that decomposes the learned speech representation using a synthesizer stream and a content stream. The synthesizer stream specializes in learning synthesizer features through supervised training with synthesizer labels. Meanwhile, the content stream focuses on learning synthesizer-independent content features, enabled by a pseudo-labeling-based supervised learning method. This method randomly transforms speech to generate speed and compression labels for training. Additionally, we employ an adversarial learning technique to reduce the synthesizer-related components in the content stream. The final classification is determined by concatenating the synthesizer and content features. To enhance the model's robustness to different synthesizer characteristics, we further propose a synthesizer feature augmentation strategy that randomly blends the characteristic styles within real and fake audio features and randomly shuffles the synthesizer features with the content features. This strategy effectively enhances the feature diversity and simulates more feature combinations. Experimental results on four deepfake speech benchmark datasets demonstrate that our model achieves state-of-the-art robust detection performance across various evaluation scenarios, including cross-method, cross-dataset, and cross-language evaluations.  © 2005-2012 IEEE.",Deepfake speech detection; feature augmentation; feature decomposition; robust detection,Audio streaming; Benchmarking; Contents-stream; Detection methods; Feature decomposition; Learn+; Learning strategy; Performance; Speech conversion; Speech detection; Speech signals; Synthesized speech; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85212768798
Vidal A.; Busso C.,"Vidal, Andrea (57213269074); Busso, Carlos (35742852700)",57213269074; 35742852700,Multimodal attention for lip synthesis using conditional generative adversarial networks,2023,Speech Communication,153,,102959,,,,1,10.1016/j.specom.2023.102959,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166219032&doi=10.1016%2fj.specom.2023.102959&partnerID=40&md5=df70b00dc0473d90d89581ce993ba503,"The synthesis of lip movements is an important problem for a socially interactive agent (SIA). It is important to generate lip movements that are synchronized with speech and have realistic co-articulation. We hypothesize that combining lexical information (i.e., sequence of phonemes) and acoustic features can lead not only to models that generate the correct lip movements matching the articulatory movements, but also to trajectories that are well synchronized with the speech emphasis and emotional content. This work presents attention-based frameworks that use acoustic and lexical information to enhance the synthesis of lip movements. The lexical information is obtained from automatic speech recognition (ASR) transcriptions, broadening the range of applications of the proposed solution. We propose models based on conditional generative adversarial networks (CGAN) with self-modality attention and cross-modalities attention mechanisms. These models allow us to understand which frames are considered more in the generation of lip movements. We animate the synthesized lip movements using blendshapes. These animations are used to compare our proposed multimodal models with alternative methods, including unimodal models implemented with either text or acoustic features. We rely on subjective metrics using perceptual evaluations and an objective metric based on the LipSync model. The results show that our proposed models with attention mechanisms are preferred over the baselines on the perception of naturalness. The addition of cross-modality attentions and self-modality attentions has a significant positive impact on the performance of the generated sequences. We observe that lexical information provides valuable information even when the transcriptions are not perfect. The improved performance observed by the multimodal system confirms the complementary information provided by the speech and text modalities. © 2023",Attention mechanism; Conditional GAN; Cross-modal attention; Lip movements; Socially interactive agents; Speech-driven animations,Emotion Recognition; Speech recognition; Acoustic features; Attention mechanisms; Conditional GAN; Cross-modal; Cross-modal attention; Interactive agents; Lexical information; Lip movements; Socially interactive agent; Speech-driven animation; Generative adversarial networks,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85166219032
Hu G.; Ruan Z.; Guo W.; Quan Y.,"Hu, Guoqiang (58690572500); Ruan, Zhuofan (58691682200); Guo, Wenqiu (58692239000); Quan, Yujuan (23493745700)",58690572500; 58691682200; 58692239000; 23493745700,A multi-task learning speech synthesis optimization method based on CWT: a case study of Tacotron2,2024,Eurasip Journal on Advances in Signal Processing,2024,1,4,,,,2,10.1186/s13634-023-01096-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181231642&doi=10.1186%2fs13634-023-01096-x&partnerID=40&md5=5e2511748915e29a8d27529654eb282f,"Text-to-speech synthesis plays an essential role in facilitating human-computer interaction. Currently, the predominant approach in Text-to-speech acoustic models selects only the Mel spectrum as an intermediate feature for converting text to speech. However, the Mel spectrograms obtained may exhibit ambiguity in some aspects owing to the limited capability of the Fourier transform to capture mutation signals during the acquisition of the Mel spectrograms. With the aim of improving the clarity of synthesized speech, this study proposes a multi-task learning optimization method and conducts experiments on the Tacotron2 speech synthesis system to demonstrate the effectiveness of the proposed method. The method in the study introduces an additional task: wavelet spectrograms. The continuous wavelet transform has gained significant popularity in various applications, including speech enhancement and speech recognition, which is primarily attributed to its capability to adaptively vary the time-frequency resolution and its excellent performance in capturing non-stationary signals. This study highlights that the clarity of Tacotron2 synthesized speech can be improved by introducing Wavelet-spectrogram as an auxiliary task through theoretical and experimental analysis: a feature extraction network is added, and Wavelet-spectrogram features are extracted from the Mel spectrum output generated by the decoder. Experimental findings indicate that the Mean Opinion Score achieved for the speech synthesized by the model using multi-task learning is 0.17 higher compared to the baseline model. Furthermore, by analyzing the factors contributing to the success of the continuous wavelet transform-based multi-task learning method in the Tacotron2 model, as well as the effectiveness of multi-task learning, the study conjectures that the proposed method has the potential to enhance the performance of other acoustic models. © 2023, The Author(s).",Clarity; Continuous wavelet transform; Multi-task learning; Speech synthesis; Tacotron2,Human computer interaction; Learning systems; Spectrographs; Speech enhancement; Speech recognition; Wavelet transforms; Acoustics model; Clarity; Continuous Wavelet Transform; Multitask learning; Optimization method; Spectra's; Spectrograms; Synthesized speech; Tacotron2; Text to speech; Speech synthesis,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85181231642
Shanthamallappa M.,"Shanthamallappa, Mahadevaswamy (57193573397)",57193573397,Robust Speech Enhancement Using Dabauchies Wavelet Based Adaptive Wavelet Thresholding for the Development of Robust Automatic Speech Recognition: A Comprehensive Review,2024,Wireless Personal Communications,137,4,,2085,2119,34,2,10.1007/s11277-024-11448-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200567669&doi=10.1007%2fs11277-024-11448-x&partnerID=40&md5=566b76a74c90b5c62080db8801223d08,"Developing a robust Automatic Speech Recognition (ASR) system is a major challenge in speech signal processing research. These systems perform exceedingly well in clean environments. However, the performance of these systems is not acceptable when the spoken signal is corrupted by several environmental and other artificial noises. The efficiency of any ASR system depends on several factors such as size of the vocabulary, native language influences, transmission channel, emotional and health state of the speaker, age of the speaker, designed speech corpus, size of the dataset, training and testing strategy and its preprocessing and other challenges. It is well known fact that the presence of noise in speech signal degrades its perceptual quality and intelligibility and hence ASR system performance is also affected. So, in this paper Dabauchies Wavelet based time adaptive Bayes thresholding algorithm is proposed with a custom Wavelet Packet Decomposition and Reconstruction Tree. The proposed system performance is evaluated on the Private Kannada Dataset and TIMIT dataset. The results reveal the effectiveness of the proposed system in various SNR levels such as − 10, − 5, 0, 5, 10, 15, 20, 25 and 30 dB. The article begins with introductory insights on ASR, Physiological process of speech production and perception in Humans, ASR jorgans, the architecture of ASR, and barriers associated with the ASR design. The work also focus on dataset design, baseline speech enhancement methods. This work provides comprehensive review to Wavelet based speech enhancement approach to the research scholars pursuing research in the area of speech signal processing. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",Automatic Speech Recognition; Daubechies wavelet; Deep neural network; Orthogonal wavelets,Discrete wavelet transforms; Signal to noise ratio; Speech communication; Speech enhancement; Speech recognition; Statistical tests; Trees (mathematics); Wavelet decomposition; Adaptive wavelet thresholding; Automatic speech recognition; Automatic speech recognition system; Clean environment; Daubechies Wavelet; Orthogonal wavelets; Robust Speech Enhancement; Signal processing research; Speech signal processing; Systems performance; Deep neural networks,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85200567669
Sun G.; Zhang C.; Woodland P.C.,"Sun, Guangzhi (57209889643); Zhang, Chao (56303967500); Woodland, Philip C. (7003878437)",57209889643; 56303967500; 7003878437,Graph Neural Networks for Contextual ASR With the Tree-Constrained Pointer Generator,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2407,2417,10,2,10.1109/TASLP.2024.3389645,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190746046&doi=10.1109%2fTASLP.2024.3389645&partnerID=40&md5=37f585f05e9506836cb0732bb39c61e6,"Incorporating biasing words obtained through contextual knowledge is paramount in automatic speech recognition (ASR) applications. This paper proposes an innovative method for achieving end-to-end contextual ASR using graph neural network (GNN) encodings based on the tree-constrained pointer generator method. GNN node encodings facilitate lookahead for future word pieces in the process of ASR decoding at each tree node by incorporating information about all word pieces on the tree branches rooted from it. This results in a more precise prediction of the generation probability of the biasing words. The study explores three GNN encoding techniques: namely the tree recursive neural network (Tree-RNN), the graph convolutional network (GCN), and GraphSAGE, along with different combinations of the complementary GCN and GraphSAGE structures. The performance of the systems was evaluated using both Librispeech and the AMI corpus with a visual-grounded contextual ASR pipeline. The findings indicate that using GNN encodings achieved consistent and significant reductions in word error rate (WER), particularly for words that are rare or have not been seen during the training process. Notably, on LibriSpeech test sets, the combined GNN proposed in this paper achieved a 20% relative rare word error rate reduction compared to Tree-RNN, 30%-40% compared to standard TCPGen and 60% compared to standard ASR systems without TCPGen. © 2014 IEEE.",audio-visual; contextual speech recognition; end-to-end; graph neural networks; Pointer generator,Encoding (symbols); Graph neural networks; Network coding; Trees (mathematics); Audio-visual; Automatic speech recognition; Context models; Contextual speech recognition; Encodings; End to end; Generator; Graph neural networks; Pointer generator; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85190746046
ZainEldin H.; Gamel S.A.; Talaat F.M.; Aljohani M.; Baghdadi N.A.; Malki A.; Badawy M.; Elhosseini M.A.,"ZainEldin, Hanaa (56677719700); Gamel, Samah A. (57192639629); Talaat, Fatma M. (57194720343); Aljohani, Mansourah (58001097400); Baghdadi, Nadiah A. (57219014401); Malki, Amer (57375133400); Badawy, Mahmoud (24723484700); Elhosseini, Mostafa A. (56050513100)",56677719700; 57192639629; 57194720343; 58001097400; 57219014401; 57375133400; 24723484700; 56050513100,"Silent no more: a comprehensive review of artificial intelligence, deep learning, and machine learning in facilitating deaf and mute communication",2024,Artificial Intelligence Review,57,7,188,,,,9,10.1007/s10462-024-10816-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196841702&doi=10.1007%2fs10462-024-10816-0&partnerID=40&md5=87b8311b9f0ad56073e472d7fd60c123,"People who often communicate via sign language are essential to our society and significantly contribute. They struggle with communication mostly because other people, who often do not understand sign language, cannot interact with them. It is necessary to develop a dependable system for automatic sign language recognition. This paper aims to provide a comprehensive review of the advancements in artificial intelligence (AI), deep learning (DL), and machine learning (ML) technologies that have been used to facilitate communication for individuals who are deaf and mute (D–M). This study explores various applications of these technologies, including sign language interpretation, speech recognition, and text-to-speech synthesis. By examining the current state of research and development in AI, ML, and DL for the D–M field, the survey sheds light on the potential and challenges faced in utilizing AI, deep learning, and ML to bridge the communication gap for the D–M community. The findings of this survey will contribute to a greater understanding of the potential impact of these technologies in improving access to communication for individuals who are D–M, thereby aiding in the development of more inclusive and accessible solutions. © The Author(s) 2024.",Artificial intelligence (AI); Convolutional neural networks (CNN); Deaf and mute (D–M); Deep learning (DL); Machine learning; Optimization,Character recognition; Convolutional neural networks; Speech recognition; Speech synthesis; Artificial intelligence; Convolutional neural network; Deaf and mute; Deep learning; Dependable systems; Machine-learning; Optimisations; Sign language; Sign Language recognition; Deep learning,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85196841702
Mallol-Ragolta A.; Schuller B.,"Mallol-Ragolta, Adria (57204679167); Schuller, Bjorn (6603767415)",57204679167; 6603767415,Coupling Sentiment and Arousal Analysis Towards an Affective Dialogue Manager,2024,IEEE Access,12,,,20654,20662,8,0,10.1109/ACCESS.2024.3361750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184336537&doi=10.1109%2fACCESS.2024.3361750&partnerID=40&md5=ebf82a13e6648917befea6d97879acd3,"We present the technologies and host components developed to power a speech-based dialogue manager with affective capabilities. The overall goal is that the system adapts its response to the sentiment and arousal level of the user inferred by analysing the linguistic and paralinguistic information embedded in his or her interaction. A linguistic-based, dedicated sentiment analysis component determines the body of the system response. A paralinguistic-based, dedicated arousal recognition component adjusts the energy level to convey in the affective system response. The sentiment analysis model is trained using the CMU-MOSEI dataset and implements a hierarchical contextual attention fusion network, which scores an Unweighted Average Recall (UAR) of 79.04% on the test set when tackling the task as a binary classification problem. The arousal recognition model is trained using the MSP-Podcast corpus. This model extracts the Mel-spectrogram representations of the speech signals, which are exploited with a Convolutional Neural Network (CNN) trained from scratch, and scores a UAR of 61.11% on the test set when tackling the task as a three-class classification problem. Furthermore, we highlight two sample dialogues implemented at the system back-end to detail how the sentiment and arousal inferences are coupled to determine the affective system response. These are also showcased in a proof of concept demonstrator. We publicly release the trained models to provide the research community with off-the-shelf sentiment analysis and arousal recognition tools. © 2013 IEEE.",Affective dialogue manager; arousal recognition; emotional artificial intelligence; human-computer interaction; sentiment analysis,Classification (of information); Data mining; Emotion Recognition; Linguistics; Neural networks; Speech recognition; Statistical tests; Virtual reality; Affective dialog manager; Annotation; Arousal recognition; Dialogue manager; Emotional artificial intelligence; Sentiment analysis; System response; Task analysis; Virtual assistants; Human computer interaction,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85184336537
Qu L.; Li T.; Weber C.; Pekarek-Rosin T.; Ren F.; Wermter S.,"Qu, Leyuan (57189588967); Li, Taihao (57274306300); Weber, Cornelius (7402376952); Pekarek-Rosin, Theresa (57212390115); Ren, Fuji (57197721795); Wermter, Stefan (7003826680)",57189588967; 57274306300; 7402376952; 57212390115; 57197721795; 7003826680,Disentangling Prosody Representations With Unsupervised Speech Reconstruction,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,39,54,15,3,10.1109/TASLP.2023.3320864,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174829083&doi=10.1109%2fTASLP.2023.3320864&partnerID=40&md5=52fa01c29edcf01f55fbf9beab5b2bb7,"Human speech can be characterized by different components, including semantic content, speaker identity and prosodic information. Significant progress has been made in disentangling representations for semantic content and speaker identity in speech recognition and speaker verification tasks respectively. However, it is still an open challenging question to extract prosodic information because of the intrinsic association of different attributes, such as timbre and rhythm, and because of the need for supervised training schemes to achieve robust speech recognition. The aim of this article is to address the disentanglement of emotional prosody based on unsupervised reconstruction. Specifically, we identify, design, implement and integrate three crucial components in our proposed model Prosody2Vec: (1) a unit encoder that transforms speech signals into discrete units for semantic content, (2) a pretrained speaker verification model to generate speaker identity embeddings, and (3) a trainable prosody encoder to learn prosody representations. We first pretrain Prosody2Vec on unlabelled emotional speech corpora, then fine-tune the model on specific datasets to perform Speech Emotion Recognition (SER) and Emotional Voice Conversion (EVC) tasks. Both objective and subjective evaluations on the EVC task suggest that Prosody2Vec effectively captures general prosodic features that can be smoothly transferred to other emotional speech. In addition, our SER experiments on the IEMOCAP dataset reveal that the prosody features learned by Prosody2Vec are complementary and beneficial for the performance of widely used speech pretraining models and surpass the state-of-the-art methods when combining Prosody2Vec with HuBERT representations. Audio samples can be found on our demo website. © 2023 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",emotional voice conversion; Prosody disentanglement; speech emotion recognition,Emotion Recognition; Job analysis; Quantization (signal); Semantics; Signal encoding; Speech communication; Emotion recognition; Emotional voice conversion; Emotional voices; Prosody disentanglement; Quantization (signal); Rhythm; Semantic content; Speech emotion recognition; Task analysis; Voice conversion; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85174829083
Jain R.; Barcovschi A.; Yiwere M.Y.; Corcoran P.; Cucu H.,"Jain, Rishabh (57568296800); Barcovschi, Andrei (58135704300); Yiwere, Mariam Yahayah (57204890157); Corcoran, Peter (57190839462); Cucu, Horia (36439147500)",57568296800; 58135704300; 57204890157; 57190839462; 36439147500,Exploring Native and Non-Native English Child Speech Recognition With Whisper,2024,IEEE Access,12,,,41601,41610,9,1,10.1109/ACCESS.2024.3378738,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188422344&doi=10.1109%2fACCESS.2024.3378738&partnerID=40&md5=7353fc3facfb112bc6da009384e5e868,"Modern end-to-end Automatic Speech Recognition (ASR) systems struggle to recognise children's speech. This challenge is due to the high acoustic variability in children's voices and the scarcity of child speech training data, particularly for accented or low-resource languages. This study focuses on improving the performance of ASR on native and non-native English child speech using publicly available datasets. We evaluate how the large-scale whisper models (trained with a large amount of adult speech data) perform with child speech. In addition, we perform finetuning experiments using different child speech datasets to investigate the performance of whisper ASR on non-native English-speaking children's speech. Our findings indicate relative Word Error Rate (WER) improvements ranging from 29% to 89% over previous benchmarks on the same datasets. Notably, these gains were achieved by finetuning with only a 10% sample of unseen non-native datasets. These results demonstrate the potential of whisper for improving ASR in a low-resource scenario for non-native child speech.  © 2013 IEEE.",Child automatic speech recognition; CMU_Kids; large-scale supervision; MyST; non-native child speech; PFSTAR; speechocean762; whisper,Electric transformer testing; Adaptation models; Automatic speech recognition; Child automatic speech recognition; Children's speech; CMU_kid; Decoding; Large-scale supervision; Large-scales; MyST; Non-native; Non-native child speech; PFSTAR; Speechocean762; Transformer; Whisper; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85188422344
Kwon H.,"Kwon, Hyun (57197769092)",57197769092,Friend-Guard Textfooler Attack on Text Classification System,2025,IEEE Access,13,,,3841,3848,7,6,10.1109/ACCESS.2021.3080680,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107215269&doi=10.1109%2fACCESS.2021.3080680&partnerID=40&md5=c2a07deec16798f91ac1a20db1aa9691,"Deep neural networks provide good performance for image classification, text classification, speech classification, and pattern analysis. However, such neural networks are vulnerable to adversarial examples. An adversarial example is a sample created by adding a little noise to the original sample data and that, although presenting no change identifiable to human perception, will be misclassified by a deep neural network. Most studies on adversarial examples have focused on images, but research is expanding to include the field of text. Textual adversarial examples can be useful in certain situations, such as when models of both friend and enemy coexist, as in a military scenario. Here, a specific message may be generated as an adversarial example such that no grammatical or semantic problems are apparent to human perception and it will be correctly classified by the friend model but incorrectly classified by the enemy model. In this paper, I propose a ""friend-guard""textual adversarial example for a text classification system. Unlike the existing methods for generating image adversarial examples, the proposed method creates adversarial examples designed to be misclassified by an enemy model and correctly classified by a friend model while retaining the meaning and grammar of the original sentence by replacing words of importance with substitutions. Experiments were conducted using a movie review dataset and the TensorFlow library. The experimental results show that the proposed method can generate an adversarial example that will be correctly classified with 88.2% accuracy by the friend model and 26.1% accuracy by the enemy model. © 2021 The Authors.",deep neural network (DNN); evasion attack; Machine learning; text adversarial example; text classification,Bit error rate; Classification (of information); Distortion (waves); Semantics; Speech recognition; Text processing; Bit-error rate; Context models; Deep neural network; Evasion attack; Grammar; Machine-learning; Neural-networks; Text adversarial example; Text classification; Text recognition; Tokenization; Deep neural networks,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85107215269
Ko M.; Kim E.; Choi Y.-H.,"Ko, Myeongjin (57444361000); Kim, Euiyeon (58829118700); Choi, Yong-Hoon (7404777616)",57444361000; 58829118700; 7404777616,Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS,2024,IEEE Open Journal of Signal Processing,5,,,577,587,10,0,10.1109/OJSP.2024.3386495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190167342&doi=10.1109%2fOJSP.2024.3386495&partnerID=40&md5=68e8b754aed97b3c9809ab88698224be,"The diffusion model is capable of generating high-quality data through a probabilistic approach. However, it suffers from the drawback of slow generation speed due to its requirement for many time steps. To address this limitation, recent models such as denoising diffusion implicit models (DDIM) focus on sample generation without explicitly modeling the entire probability distribution, while models like denoising diffusion generative adversarial networks (GAN) combine diffusion processes with GANs. In the field of speech synthesis, a recent diffusion speech synthesis model called DiffGAN-TTS, which utilizes the structure of GANs, has been introduced and demonstrates superior performance in both speech quality and generation speed. In this paper, to further enhance the performance of DiffGAN-TTS, we propose a speech synthesis model with two discriminators: a diffusion discriminator to learn the distribution of the reverse process, and a spectrogram discriminator to learn the distribution of the generated data. Objective metrics such as the structural similarity index measure (SSIM), mel-cepstral distortion (MCD), F0 root mean squared error (F0- RMSE), phoneme error rate (PER), word error rate (WER), as well as subjective metrics like mean opinion score (MOS), are used to evaluate the performance of the proposed model. The evaluation results demonstrate that our model matches or exceeds recent state-of-the-art models like FastSpeech 2 and DiffGAN-TTS across various metrics. Our code and audio samples are available on GitHub. © 2020 IEEE.",Denoising diffusion model; generative adversarial network; mel-spectrogram discriminator; speech synthesis; text-to-speech,Diffusion; Discriminators; Errors; Hidden Markov models; Mean square error; Noise abatement; Probability distributions; Spectrographs; Speech enhancement; Speech recognition; Speech synthesis; Computational modelling; De-noising; Denoising diffusion model; Diffusion model; Hidden-Markov models; Mel-spectrogram discriminator; Performance; Spectrograms; Text to speech; Generative adversarial networks,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85190167342
Teixeira F.; Abad A.; Raj B.; Trancoso I.,"Teixeira, Francisco (57203978875); Abad, Alberto (35172483000); Raj, Bhiksha (7102615577); Trancoso, Isabel (6603959337)",57203978875; 35172483000; 7102615577; 6603959337,Privacy-Oriented Manipulation of Speaker Representations,2024,IEEE Access,12,,,82949,82971,22,0,10.1109/ACCESS.2024.3409067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195378702&doi=10.1109%2fACCESS.2024.3409067&partnerID=40&md5=739d87453f51c85776eb5197365654b9,"Speaker embeddings are ubiquitous, with applications ranging from speaker recognition and diarization to speech synthesis and voice anonymization. The amount of information held by these embeddings lends them versatility but also raises privacy concerns. Speaker embeddings have been shown to contain sensitive information, including the speaker's age, sex, health state and more - in other words, information that speakers may want to keep private, especially when it is not required for the target task. In this work, we propose a method for removing and manipulating private attribute information in speaker representations that leverages a Vector-Quantized Variational Autoencoder architecture combined with an adversarial classifier and a novel mutual information loss. We validate our model on two attributes, sex and age, and perform experiments to remove or manipulate this information using ignorant and informed attackers. The model is tested with in-domain and out-of-domain data to assess its robustness, and the resulting speaker representations are used in a speaker verification scenario to validate their utility. Our results show that our model obtains a strong trade-off between utility and privacy, achieving age and sex classification results near chance level for both attackers and yielding little impact on speaker verification performance. © 2013 IEEE.",Age information removal; attribute-based privacy; privacy-oriented manipulation; sex information removal; speaker embeddings; speaker recognition,Economic and social effects; Job analysis; Speech recognition; Speech synthesis; Age information removal; Attribute-based; Attribute-based privacy; Embeddings; Market researches; Privacy; Privacy-oriented manipulation; Sex information removal; Solid modelling; Speaker embedding; Speaker recognition; Task analysis; Classification (of information),Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85195378702
Xin D.; Jiang J.; Takamichi S.; Saito Y.; Aizawa A.; Saruwatari H.,"Xin, Detai (57221101232); Jiang, Junfeng (57224216773); Takamichi, Shinnosuke (55751754900); Saito, Yuki (57194868285); Aizawa, Akiko (6701312731); Saruwatari, Hiroshi (7003918936)",57221101232; 57224216773; 55751754900; 57194868285; 6701312731; 7003918936,JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions,2024,IEEE Access,12,,,19752,19764,12,5,10.1109/ACCESS.2024.3360885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184315600&doi=10.1109%2fACCESS.2024.3360885&partnerID=40&md5=0bf1571bd5d530e3b32b82c27c58dd8d,"We present the JVNV, a Japanese emotional speech corpus with verbal content and nonverbal vocalizations whose scripts are generated by a large-scale language model. Existing emotional speech corpora lack not only proper emotional scripts but also nonverbal vocalizations (NVs) that are essential expressions in spoken language to express emotions. We propose an automatic script generation method to produce emotional scripts by providing seed words with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using prompt engineering. We select 514 scripts with balanced phoneme coverage from the generated candidate scripts with the assistance of emotion confidence scores and language fluency scores. Experimental results show that JVNV has better phoneme coverage and emotion recognizability than previous Japanese emotional speech corpora. We then benchmark JVNV on emotional text-to-speech synthesis using discrete codes to represent NVs. The results demonstrate that there still exists a gap between the performance of synthesizing read-aloud speech and emotional speech, and adding NVs in the speech makes the task even harder, which brings new challenges for this task and makes JVNV a valuable resource for relevant works in the future. To our best knowledge, JVNV is the first speech corpus that generates scripts automatically using large language models. © 2013 IEEE.",Corpus design; emotional speech corpus; Japanese corpus; large-scale language model; nonverbal expression; nonverbal vocalization,Computational linguistics; Emotion Recognition; Speech communication; Speech synthesis; Corpus design; Emotion recognition; Emotional speech; Emotional speech corpus; Japanese corpus; Language model; Language processing; Large-scale language model; Large-scales; Natural language processing; Natural languages; Nonverbal expression; Nonverbal vocalizations; Nonverbals; Oral communication; Speech corpora; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85184315600
Gonzales M.G.; Corcoran P.; Harte N.; Schukat M.,"Gonzales, Michael Gian (57221830735); Corcoran, Peter (57190839462); Harte, Naomi (6602143215); Schukat, Michael (14016517800)",57221830735; 57190839462; 6602143215; 14016517800,Joint Speech-Text Embeddings for Multitask Speech Processing,2024,IEEE Access,12,,,145955,145967,12,0,10.1109/ACCESS.2024.3473743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206853385&doi=10.1109%2fACCESS.2024.3473743&partnerID=40&md5=d538a72769b3204094661446227bd83b,"Devices that use speech as the communication medium between human and computer have been emerging for the past few years. The technologies behind this interface are called Automatic Speech Recognition (ASR) and Text-to-Speech (TTS). The two are distinct fields in speech signal processing that have independently made great strides in recent years. This paper proposes an architecture that takes advantage of the two modalities present in ASR and TTS, speech and text, while simultaneously training three tasks, adding speaker recognition to the underlying ASR and TTS tasks. This architecture not only reduces the memory footprint required to run all tasks, but also has performance comparable to single-task models. The dataset used to train and evaluate the model is the CSTR VCTK Corpus. Results show a 97.64% accuracy in the speaker recognition task, word and character error rates of 18.18% and 7.95% for the ASR task, a mel cepstral distortion of 4.31 and two predicted MOS of 2.98 and 3.28 for the TTS task. While voice conversion is not part of the training tasks, the architecture is capable of doing this and was evaluated to have 5.22, 2.98, and 2.73 for mel cepstral distortion and predicted MOS, respectively.  © 2024 The Authors.",Automatic speech recognition; joint speech-text; speaker recognition; speech processing; text-to-speech; voice conversion,Audio signal processing; Automatic speech recognition; Cepstral; Communication media; Embeddings; Joint speech-text; Memory footprint; Speaker recognition; Speech signal processing; Text to speech; Voice conversion; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85206853385
Wan G.; Mao T.; Zhang J.; Chen H.; Gao J.; Ye Z.,"Wan, Genshun (57207995145); Mao, Tingzhi (57219692707); Zhang, Jingxuan (57204049170); Chen, Hang (58113784400); Gao, Jianqing (56428194700); Ye, Zhongfu (7401957018)",57207995145; 57219692707; 57204049170; 58113784400; 56428194700; 7401957018,Grammar-Supervised End-to-End Speech Recognition with Part-of-Speech Tagging and Dependency Parsing,2023,Applied Sciences (Switzerland),13,7,4243,,,,4,10.3390/app13074243,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152521704&doi=10.3390%2fapp13074243&partnerID=40&md5=5ba61dddeb39032d5b190542f8f96a09,"For most automatic speech recognition systems, many unacceptable hypothesis errors still make the recognition results absurd and difficult to understand. In this paper, we introduce the grammar information to improve the performance of the grammatical deviation distance and increase the readability of the hypothesis. The reinforcement of word embedding with grammar embedding is presented to intensify the grammar expression. An auxiliary text-to-grammar task is provided to improve the performance of the recognition results with the downstream task evaluation. Furthermore, the multiple evaluation methodology of grammar is used to explore an expandable usage paradigm with grammar knowledge. Experiments on the small open-source Mandarin speech corpus AISHELL-1 and large private-source Mandarin speech corpus TRANS-M tasks show that our method can perform very well with no additional data. Our method achieves relative character error rate reductions of 3.2% and 5.0%, a relative grammatical deviation distance reduction of 4.7% and 5.9% on AISHELL-1 and TRANS-M tasks, respectively. Moreover, the grammar-based mean opinion score of our method is about 4.29 and 3.20, significantly superior to the baseline of 4.11 and 3.02. © 2023 by the authors.",grammar knowledge; grammatical deviation distance; multiple evaluation methodology of grammar; speech recognition,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85152521704
Liu R.; Yuan H.; Gao G.; Li H.,"Liu, Rui (57193863352); Yuan, Hongyu (59395143700); Gao, Guanglai (7403171213); Li, Haizhou (8615868400)",57193863352; 59395143700; 7403171213; 8615868400,Listening and seeing again: Generative error correction for audio-visual speech recognition,2025,Information Fusion,120,,103077,,,,0,10.1016/j.inffus.2025.103077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000070062&doi=10.1016%2fj.inffus.2025.103077&partnerID=40&md5=3906315009998a58ad9572f2002b7dc0,"Unlike traditional Automatic Speech Recognition (ASR), Audio-Visual Speech Recognition (AVSR) takes audio and visual signals simultaneously to infer the transcription. Recent studies have shown that Large Language Models (LLMs) can be effectively used for Generative Error Correction (GER) in ASR by predicting the best transcription from ASR-generated N-best hypotheses. However, these LLMs lack the ability to simultaneously understand audio and visual, making the GER approach challenging to apply in AVSR. In this work, we propose a novel GER paradigm for AVSR, termed AVGER, that follows the concept of “listening and seeing again”. Specifically, we first use the powerful AVSR system to read the audio and visual signals to get the N-Best hypotheses, and then use the Q-former-based Multimodal Synchronous Encoder to read the audio and visual information again and convert them into an audio and video compression representation respectively that can be understood by LLM. Afterward, the audio-visual compression representation and the N-Best hypothesis together constitute a Cross-modal Prompt to guide the LLM in producing the best transcription. In addition, we also proposed a Multi-Level Consistency Constraint training criterion, including logits-level, utterance-level and representations-level, to improve the correction accuracy while enhancing the interpretability of audio and visual compression representations. The experimental results on the LRS3 dataset show that our method outperforms current mainstream AVSR systems. The proposed AVGER can reduce the Word Error Rate (WER) by 27.59% compared to them. Code and models can be found at: https://github.com/AI-S2-Lab/AVGER. © 2025 Elsevier B.V.",Audio-visual speech recognition; Generative error correction; Multimodal large language models,Speech recognition; Audio signal; Audiovisual speech recognition; Automatic speech recognition; Errors correction; Generative error correction; Language model; Multi-modal; Multimodal large language model; N-best hypothesis; Visual signals; Visual languages,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-105000070062
Reza S.; Ferreira M.C.; Machado J.J.M.; Tavares J.M.R.S.,"Reza, Selim (57345004700); Ferreira, Marta Campos (55583974700); Machado, J.J.M. (57193344480); Tavares, João Manuel R.S. (36538110300)",57345004700; 55583974700; 57193344480; 36538110300,A customized residual neural network and bi-directional gated recurrent unit-based automatic speech recognition model,2023,Expert Systems with Applications,215,,119293,,,,16,10.1016/j.eswa.2022.119293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143545962&doi=10.1016%2fj.eswa.2022.119293&partnerID=40&md5=f8ccfeed2a9055109461c0ffa0624704,"Speech recognition aims to convert human speech into text and has applications in security, healthcare, commerce, automobiles, and technology, just to name a few. Inserting residual neural networks before recurrent neural network cells improves accuracy and cuts training time by a good margin. Furthermore, layer normalization instead of batch normalization is more effective in model training and performance enhancement. Also, the size of the datasets presents tremendous influences in achieving the best performance. Leveraging these tricks, this article proposes an automatic speech recognition model with a stacked five layers of customized Residual Convolution Neural Network and seven layers of Bi-Directional Gated Recurrent Units, including a logarithmic softmax for the model output. Each of them incorporates a learnable per-element affine parameter-based layer normalization technique. The training and testing of the new model were conducted on the LibriSpeech corpus and LJ Speech dataset. The experimental results demonstrate a character error rate (CER) of 4.7 and 3.61% on the two datasets, respectively, with only 33 million parameters without the requirement of any external language model. © 2022 The Authors",Character error rate; Deep learning; Layer normalization; LibriSpeech corpus; LJ speech dataset; Word error rate,Character recognition; Errors; Multilayer neural networks; Network layers; Recurrent neural networks; Statistical tests; Automatic speech recognition; Bi-directional; Character error rates; Deep learning; Layer normalization; Librispeech corpus; LJ speech dataset; Neural-networks; Normalisation; Word error rate; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85143545962
Kefalas T.; Panagakis Y.; Pantic M.,"Kefalas, Triantafyllos (57219052732); Panagakis, Yannis (35503932300); Pantic, Maja (56259551600)",57219052732; 35503932300; 56259551600,Large-Scale Unsupervised Audio Pre-Training for Video-to-Speech Synthesis,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2255,2268,13,0,10.1109/TASLP.2024.3382500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189343241&doi=10.1109%2fTASLP.2024.3382500&partnerID=40&md5=8f4265c935f4699a7c05cb3c3c35dac6,"Video-to-speech synthesis is the task of reconstructing the speech signal from a silent video of a speaker. Previous approaches train on data from almost exclusively audio-visual datasets, i.e., every audio sample has a corresponding video sample. This precludes the use of abundant audio-only datasets which may not have a corresponding visual modality such as audiobooks, radio podcasts, and speech recognition datasets. In this paper we propose to train encoder-decoder models on more than 3,500 hours of audio data at 24 kHz, and then use the pre-trained decoders to initialize the audio decoders for the video-to-speech synthesis task. The pre-training step uses audio samples only and does not require labels or corresponding samples from other modalities (visual, text). We demonstrate that this improves the reconstructed speech and that it is an unexplored way to improve the quality of the generator in a cross-modal task while only requiring samples from one of the modalities. We conduct experiments using both raw audio and mel spectrograms as target outputs and benchmark our models with existing work.  © 2014 IEEE.",conformer; generative adversarial networks (GANs); pre-training; speech synthesis; Video-to-speech,Decoding; Hidden Markov models; Spectrographs; Speech recognition; Speech synthesis; Audio samples; Conformer; Decoding; Generative adversarial network; Hidden-Markov models; Large-scales; Pre-training; Predictive models; Spectrograms; Video-to-speech; Generative adversarial networks,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85189343241
Ziani A.; Adouane A.M.; Amiri M.N.; Smail S.,"Ziani, Amel (56878800700); Adouane, Amine Mohamed (56323436700); Amiri, Mohamed Nassim (59418621000); Smail, Sabiha (59418621100)",56878800700; 56323436700; 59418621000; 59418621100,New Proposed Solution for Speech Recognition without Labeled Data: Tutoring System for Children with Autism Spectrum Disorder,2024,Informatica (Slovenia),48,18,,109,122,13,0,10.31449/inf.v48i18.5204,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209715130&doi=10.31449%2finf.v48i18.5204&partnerID=40&md5=dfb68be908b3253a1075ed9b2b9643ac,"Children diagnosed with autism spectrum disorder (ASD) face challenges in understanding situations, verbal communication, and social interactions. Autism can manifest differently in each child, and it can be characterized by various degrees of severity. Some common behaviors observed in children with ASD include poor skills, repetitive behaviors, delayed speech, reasoning difficulties, narrow interests, and challenges with social interactions and communication, such as recognizing social cues. As every child with ASD has unique educational needs, there is no universal solution for treating the condition. This paper aims to introduce an adaptive educational system that will help children with ASD acquire new skills and improve their communication abilities, enabling them to better integrate into society. The proposed system will be based on therapist-researched and -analyzed activities that utilize speech recognition technology. To address the resource requirements of labeled datasets, we propose a new approach that leverages generative adversarial networks (MelGAN) to produce responses that closely resemble a child's voice. This allows for the comparison of the generated response with the correct answer using similarity metrics. The system was tested on Algerian children with ASD who speak Algerian dialect, and the results were promising and this can open a new direction for developing educational systems that do not rely on labeled datasets. © 2024 Slovene Society Informatika. All rights reserved.",arabic speech recognition; autism spectrum disorder (ASD); generative adversarial networks; MelGAN-VC generators; wav2vec,Adversarial machine learning; Diseases; Speech recognition; Teaching; Adversarial networks; Arabic speech recognition; Autism spectrum disorder; Autism spectrum disorders; Children with autisms; Educational systems; Labeled dataset; MelGAN-VC generator; Social interactions; Wav2vec; Generative adversarial networks,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85209715130
Yu X.; Guo D.; Zhang J.; Lin Y.,"Yu, Xincheng (58547908300); Guo, Dongyue (57213190082); Zhang, Jianwei (57195437554); Lin, Yi (57191432858)",58547908300; 57213190082; 57195437554; 57191432858,ROSE: A Recognition-Oriented Speech Enhancement Framework in Air Traffic Control Using Multi-Objective Learning,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,3365,3378,13,0,10.1109/TASLP.2024.3423652,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198396484&doi=10.1109%2fTASLP.2024.3423652&partnerID=40&md5=8830c0fcb95cad0d73232dd833fa143d,"—Radio speech echo is a specific phenomenon in the air traffic control (ATC) domain, which degrades speech quality and further impacts automatic speech recognition (ASR) accuracy. In this work, a time-domain recognition-oriented speech enhancement (ROSE) framework is proposed to improve speech intelligibility and also advance ASR accuracy based on convolutional encoder-decoder-based U-Net framework, which serves as a plug-and-play tool in ATC scenarios and does not require additional retraining of the ASR model. Specifically, 1) In the U-Net architecture, an attention-based skip-fusion (ABSF) module is applied to mine shared features from encoders using an attention mask, which enables the model to effectively fuse the hierarchical features. 2) A channel and sequence attention (CSAtt) module is innovatively designed to guide the model to focus on informative features in dual parallel attention paths, aiming to enhance the effective representations and suppress the interference noises. 3) Based on the handcrafted features, ASR-oriented optimization targets are designed to improve recognition performance in the ATC environment by learning robust feature representations. By incorporating both the SE-oriented and ASR-oriented losses, ROSE is implemented in a multi-objective learning manner by optimizing shared representations across the two task objectives. The experimental results show that the ROSE significantly outperforms other state-of-the-art methods for both the SE and ASR tasks, in which all the proposed improvements are confirmed by designed experiments. In addition, the proposed approach can contribute to the desired performance improvements on public datasets. © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",attention-based skip-fusion; automatic speech recognition; channel and sequence attention; multi-objective learning; Speech enhancement,Air navigation; Air traffic control; Control towers; Quality control; Signal encoding; Speech recognition; Time domain analysis; Attention-based skip-fusion; Automatic speech recognition; Channel and sequence attention; Enhancement framework; Multi-objective learning; Noise; Noise measurements; Optimisations; Task analysis; Time-domain analysis; Speech enhancement,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85198396484
Avdeeva A.; Gusev A.; Andzhukaev T.; Ivanov A.,"Avdeeva, Anastasia (57200257783); Gusev, Aleksei (57211643521); Andzhukaev, Tseren (57210788227); Ivanov, Artem (57213403201)",57200257783; 57211643521; 57210788227; 57213403201,Streaming ASR Encoder for Whisper-to-Speech Online Voice Conversion,2024,IEEE Open Journal of Signal Processing,5,,,160,167,7,2,10.1109/OJSP.2023.3343342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180281381&doi=10.1109%2fOJSP.2023.3343342&partnerID=40&md5=7de112ea4bc83c192c33161a8671b61a,"Whispered speech is a quiet voice without vocalization. One of the common cases of using whispered speech is a technique that can help overcome stuttering. But whispered speech can be uncomfortable and difficult to understand in everyday communication. To address these problems, we propose a method of low-delayed whisper-to-speech voice conversion, which can be useful in real life communication of people with disordered speech. As part of our research, we study the impact of streaming Automatic Speech Recognition models on the quality of voice conversion, comparing different streaming models and methods for model adaptation to streaming settings, and showing the importance of using such models in cases of low-delayed voice conversion. © 2023 The Authors.",disordered speech; Speech recognition; voice conversion; whisper-to-speech processing,Speech communication; Speech processing; Adaptation models; Automatic speech recognition; Context models; Decoding; Disordered speech; Recognition models; Streaming model; Voice conversion; Whisper-to-speech processing; Whispered speech; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85180281381
Fu H.; Li Q.; Tao H.; Zhu C.; Xie Y.; Guo R.,"Fu, Hongliang (7402948092); Li, Qianqian (57219973763); Tao, Huawei (55029891400); Zhu, Chunhua (56328317700); Xie, Yue (57189250869); Guo, Ruxue (57193567340)",7402948092; 57219973763; 55029891400; 56328317700; 57189250869; 57193567340,Cross-Corpus Speech Emotion Recognition Based on Causal Emotion Information Representation,2024,IEICE Transactions on Information and Systems,E107.D,8,,1097,1100,3,0,10.1587/transinf.2023EDL8087,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200479111&doi=10.1587%2ftransinf.2023EDL8087&partnerID=40&md5=56b0291271b95fc27787af19733240c3,"Speech emotion recognition (SER) is a key research technology to realize the third generation of artificial intelligence, which is widely used in human-computer interaction, emotion diagnosis, interpersonal communication and other fields. However, the aliasing of language and semantic information in speech tends to distort the alignment of emotion features, which affects the performance of cross-corpus SER system. This paper proposes a cross-corpus SER model based on causal emotion information representation (CEIR). The model uses the reconstruction loss of the deep autoencoder network and the source domain label information to realize the preliminary separation of causal features. Then, the causal correlation matrix is constructed, and the local maximum mean difference (LMMD) feature alignment technology is combined to make the causal features of different dimensions jointly distributed independent. Finally, the supervised fine-tuning of labeled data is used to achieve effective extraction of causal emotion information. The experimental results show that the average unweighted average recall (UAR) of the proposed algorithm is increased by 3.4% to 7.01% compared with the latest partial algorithms in the field. Copyright © 2024 The Institute of Electronics, Information and Communication Engineers.",causal representation learning; cross-corpus speech emotion recognition; domain adaptation,Emotion Recognition; Semantics; Speech communication; Speech recognition; Aliasing; Causal representation learning; Cross-corpus speech emotion recognition; Domain adaptation; Information representation; Inter-personal communications; Language informations; Research technologies; Speech emotion recognition; Third generation; Human computer interaction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85200479111
Kawade R.; Jagtap S.,"Kawade, Rupali (56592787100); Jagtap, Sonal (55532222700)",56592787100; 55532222700,Comprehensive Study of Automatic Speech Emotion Recognition Systems,2023,International Journal on Recent and Innovation Trends in Computing and Communication,11,9s,,709,717,8,0,10.17762/ijritcc.v11i9s.7743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173650781&doi=10.17762%2fijritcc.v11i9s.7743&partnerID=40&md5=f1a7332dafe8db3e65dec65b36111215,"Speech emotion recognition (SER) is the technology that recognizes psychological characteristics and feelings from the speech signals through techniques and methodologies. SER is challenging because of more considerable variations in different languages arousal and valence levels. Various technical developments in artificial intelligence and signal processing methods have encouraged and made it possible to interpret emotions.SER plays a vital role in remote communication. This paper offers a recent survey of SER using machine learning (ML) and deep learning (DL)-based techniques. It focuses on the various feature representation and classification techniques used for SER. Further, it describes details about databases and evaluation metrics used for speech emotion recognition. © 2023 Auricle Global Society of Education and Research.",Affective Computing; Deep Learning; Machine Learning; Speech Emotion Recognition; Speech Recognition,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85173650781
Fan C.; Ding M.; Tao J.; Fu R.; Yi J.; Wen Z.; Lv Z.,"Fan, Cunhang (57202112890); Ding, Mingming (58553063300); Tao, Jianhua (55898465800); Fu, Ruibo (57202920878); Yi, Jiangyan (57191923020); Wen, Zhengqi (54413415000); Lv, Zhao (24314514500)",57202112890; 58553063300; 55898465800; 57202920878; 57191923020; 54413415000; 24314514500,Dual-Branch Knowledge Distillation for Noise-Robust Synthetic Speech Detection,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2453,2466,13,4,10.1109/TASLP.2024.3389643,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190794258&doi=10.1109%2fTASLP.2024.3389643&partnerID=40&md5=b73ef6010391682d6c9b69699057e92a,"Most research in synthetic speech detection (SSD) focuses on improving performance on standard noise-free datasets. However, in actual situations, noise interference is usually present, causing significant performance degradation in SSD systems. To improve noise robustness, this paper proposes a dual-branch knowledge distillation synthetic speech detection (DKDSSD) method. Specifically, a parallel data flow of the clean teacher branch and the noisy student branch is designed, and interactive fusion module and response-based teacher-student paradigms are proposed to guide the training of noisy data from both the data distribution and decision-making perspectives. In the noisy student branch, speech enhancement is introduced initially for denoising, aiming to reduce the interference of strong noise. The proposed interactive fusion combines denoised features and noisy features to mitigate the impact of speech distortion and ensure consistency with the data distribution of the clean branch. The teacher-student paradigm maps the student's decision space to the teacher's decision space, enabling noisy speech to behave similarly to clean speech. Additionally, a joint training method is employed to optimize both branches for achieving global optimality. Experimental results based on multiple datasets demonstrate that the proposed method performs effectively in noisy environments and maintains its performance in cross-dataset experiments. © 2014 IEEE.",interactive fusion; knowledge distillation; noise-robust; Synthetic speech detection,Decision making; Parallel flow; Personnel training; Speech synthesis; Students; Data distribution; Decision space; Improving performance; Interactive fusion; Knowledge distillation; Noise robust; Speech detection; Synthetic speech; Synthetic speech detection; Teachers'; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85190794258
Weng Z.; Qin Z.; Tao X.; Pan C.; Liu G.; Li G.Y.,"Weng, Zhenzi (57221773884); Qin, Zhijin (55370178400); Tao, Xiaoming (57208894708); Pan, Chengkang (8536403200); Liu, Guangyi (12797049200); Li, Geoffrey Ye (57211345986)",57221773884; 55370178400; 57208894708; 8536403200; 12797049200; 57211345986,Deep Learning Enabled Semantic Communications With Speech Recognition and Synthesis,2023,IEEE Transactions on Wireless Communications,22,9,,6227,6240,13,120,10.1109/TWC.2023.3240969,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148415022&doi=10.1109%2fTWC.2023.3240969&partnerID=40&md5=23c4eb22fa051a41d0f94da9e61fb979,"In this paper, we develop a deep learning based semantic communication system for speech transmission, named DeepSC-ST. We take the speech recognition and speech synthesis as the transmission tasks of the communication system, respectively. First, the speech recognition-related semantic features are extracted for transmission by a joint semantic-channel encoder and the text is recovered at the receiver based on the received semantic features, which significantly reduces the required amount of data transmission without performance degradation. Then, we perform speech synthesis at the receiver, which dedicates to re-generate the speech signals by feeding the recognized text and the speaker information into a neural network module. To enable the DeepSC-ST adaptive to dynamic channel environments, we identify a robust model to cope with different channel conditions. According to the simulation results, the proposed DeepSC-ST significantly outperforms conventional communication systems and existing DL-enabled communication systems, especially in the low signal-to-noise ratio (SNR) regime. A software demonstration is further developed as a proof-of-concept of the DeepSC-ST. © 2002-2012 IEEE.",Deep learning; semantic communication; speech recognition; speech synthesis,Character recognition; Deep learning; Hidden Markov models; Job analysis; Semantics; Signal to noise ratio; Speech communication; Speech recognition; Speech transmission; Channel encoder; Communications systems; Data-transmission; Deep learning; Hidden-Markov models; Performance degradation; Receiver; Semantic communication; Semantic features; Task analysis; Speech synthesis,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85148415022
Lima Florido F.J.; Corpas Pastor G.,"Lima Florido, Francisco Javier (59711961800); Corpas Pastor, Gloria (36696863000)",59711961800; 36696863000,"Advanced Identification of Prosodic Boundaries, Speakers, and Accents Through Multi-Task Audio Pre-Processing and Speech Language Models",2025,Computers,14,3,102,,,,0,10.3390/computers14030102,https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001119107&doi=10.3390%2fcomputers14030102&partnerID=40&md5=59eac573f277ea5c2db0961b59573b3c,"In recent years, the advances in deep neural networks (DNNs) and large language models (LLMs) have led to major breakthroughs and new levels of performance in Natural Language Processing (NLP), including tasks related to speech processing. Based on these new trends, new models such as Whisper and Wav2Vec 2.0 achieve robust performance in speech processing tasks, even in speech-to-text translation and end-to-end speech translation, far exceeding all previous results. Although these models have shown excellent results in real-time speech processing, they still have some accuracy issues for some tasks and high latency problems when working with large amounts of audio data. In addition, many of them need audio to be segmented and labelled for speech synthesis and annotation tasks. Speaker diarisation, background noise detection, prosodic boundary detection and accent classification are some of the pre-processing tasks required in these cases. In this study, we will fine-tune a small Wav2Vec 2.0 base model for multi-task classification and audio segmentation. A corpus of spoken American English will be used for the experiments. We intend to explore this new approach and, more specifically, the performance of the model with regard to prosodic boundaries detection for audio segmentation, and advanced accent identification. © 2025 by the authors.",accent classification; multi-task classification; NLP; prosodic boundaries detection; speaker change detection; speech processing; transformer architecture; Wav2Vec2,Background noise; Change detection; Image coding; Image thinning; Natural language processing systems; Speech recognition; Translation (languages); Accent classifications; Language processing; Multi tasks; Multi-task classification; Natural language processing; Natural languages; Prosodic boundary detection; Speaker change detection; Task classification; Transformer architecture; Wav2vec2; Deep neural networks,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-105001119107
Zhang Z.; Chen S.; Zhou L.; Wu Y.; Ren S.; Liu S.; Yao Z.; Gong X.; Dai L.; Li J.; Wei F.,"Zhang, Ziqiang (57221424487); Chen, Sanyuan (57219737215); Zhou, Long (57221148819); Wu, Yu (57148827400); Ren, Shuo (57208219575); Liu, Shujie (56181265100); Yao, Zhuoyuan (57221042451); Gong, Xun (57220842525); Dai, Lirong (55552203600); Li, Jinyu (35488220000); Wei, Furu (23995914700)",57221424487; 57219737215; 57221148819; 57148827400; 57208219575; 56181265100; 57221042451; 57220842525; 55552203600; 35488220000; 23995914700,SpeechLM: Enhanced Speech Pre-Training With Unpaired Textual Data,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2177,2187,10,6,10.1109/TASLP.2024.3379877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188664684&doi=10.1109%2fTASLP.2024.3379877&partnerID=40&md5=6b9d7c607afd71fb5071d8b60558b4a8,"How to boost speech pre-training with textual data is an unsolved problem due to the fact that speech and text are very different modalities with distinct characteristics. In this paper, we propose a cross-modal Speech and Language Model (SpeechLM) to explicitly align speech and text pre-training with a pre-defined unified discrete representation. Specifically, we introduce two alternative discrete tokenizers to bridge the speech and text modalities, including phoneme-unit and hidden-unit tokenizers, which can be trained using unpaired speech or a small amount of paired speech-text data. Based on the trained tokenizers, we convert the unlabeled speech and text data into tokens of phoneme units or hidden units. The pre-training objective is designed to unify the speech and the text into the same discrete semantic space with a unified Transformer network. We evaluate SpeechLM on various spoken language processing tasks including speech recognition, speech translation, and universal representation evaluation framework SUPERB, demonstrating significant improvements on content-related tasks.  © 2014 IEEE.",discrete tokenization; speech recognition; speech translation; Speech-text joint pre-training,Character recognition; Job analysis; Semantics; Speech processing; Discrete tokenization; Pre-training; Representation learning; Speech translation; Speech-text joint pre-training; Task analysis; Textual data; Tokenization; Tokenizer; Transformer; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85188664684
Violeta L.P.; Ma D.; Huang W.-C.; Toda T.,"Violeta, Lester Phillip (57311066800); Ma, Ding (57536014400); Huang, Wen-Chin (57208822576); Toda, Tomoki (7202683282)",57311066800; 57536014400; 57208822576; 7202683282,Pretraining and Adaptation Techniques for Electrolaryngeal Speech Recognition,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2777,2789,12,1,10.1109/TASLP.2024.3402557,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193513850&doi=10.1109%2fTASLP.2024.3402557&partnerID=40&md5=8b758349802db45e6544592485c1e033,"We investigate state-of-the-art automatic speech recognition (ASR) systems and provide thorough investigations on training methods to adapt them to low-resourced electrolaryngeal (EL) datasets. Transfer learning is often sufficient to resolve low-resourced problems; however, in EL speech, the domain shift between the pretraining and fine-tuning data is too large to overcome, limiting the ASR performance. We propose a method of reducing the domain shift gap during transfer learning between the healthy and EL datasets by conducting an intermediate fine-tuning task that uses imperfectly synthesized EL speech. Although using imperfect synthetic speech is non-intuitive, we proved the effectiveness of this method by decreasing the character error rate by up to 6.1% compared to the baselines using naive transfer learning. To further understand the model's behavior, we further analyze the produced latent spaces in each task through linguistic and identity proxy tasks and find that the intermediate fine-tuning focuses on identifying the voicing characteristics of the EL speakers. Moreover, we also show the differences between a simulated EL speaker from a real EL speaker and find that simulated EL data has pronunciation differences from real EL data, showing the huge domain gap between real EL and other speech data. © 2014 IEEE.",Automatic speech recognition (ASR); domain adaptation; electrolaryngeal (EL) speech; low-resourced ASR; model pretraining,Job analysis; Linguistics; Neural networks; Speech synthesis; Tuning; Automatic speech recognition; Domain adaptation; Electrolaryngeal speech; Low-resourced automatic speech recognition; Model pretraining; Neural-networks; Pre-training; Task analysis; Transfer learning; Speech recognition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85193513850
Calderón-González D.; Ábalos N.; Bayo B.; Cánovas P.; Griol D.; Muñoz-Romero C.; Pérez C.; Vila P.; Callejas Z.,"Calderón-González, Daniel (59400768800); Ábalos, Nieves (36602047600); Bayo, Blanca (59400768900); Cánovas, Pedro (59400769000); Griol, David (15765332600); Muñoz-Romero, Carlos (57201156605); Pérez, Carmen (57212759755); Vila, Pere (58545952000); Callejas, Zoraida (55991672400)",59400768800; 36602047600; 59400768900; 59400769000; 15765332600; 57201156605; 57212759755; 58545952000; 55991672400,Deep Speech Synthesis and Its Implications for News Verification: Lessons Learned in the RTVE-UGR Chair,2024,Applied Sciences (Switzerland),14,21,9916,,,,1,10.3390/app14219916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208496160&doi=10.3390%2fapp14219916&partnerID=40&md5=469118625a8d973640f6116e15be77c6,"This paper presents the multidisciplinary work carried out in the RTVE-UGR Chair within the IVERES project, whose main objective is the development of a tool for journalists to verify the veracity of the audios that reach the newsrooms. In the current context, voice synthesis has both beneficial and detrimental applications, with audio deepfakes being a significant concern in the world of journalism due to their ability to mislead and misinform. This is a multifaceted problem that can only be tackled adopting a multidisciplinary perspective. In this article, we describe the approach we adopted within the RTVE-UGR Chair to successfully address the challenges derived from audio deepfakes involving a team with different backgrounds and a specific methodology of iterative co-creation. As a result, we present several outcomes including the compilation and generation of audio datasets, the development and deployment of several audio fake detection models, and the development of a web audio verification tool addressed to journalists. As a conclusion, we highlight the importance of this systematic collaborative work in the fight against misinformation and the future potential of audio verification technologies in various applications. © 2024 by the authors.",audio deepfake; misinformation; news verification; speech synthesis; voice conversion,Audio acoustics; Audio signal processing; Newsprint; Speech recognition; Verification; 'current; Audio deepfake; Co-creation; Detection models; Fake detection; Misinformation; Multidisciplinary perspectives; News verification; Verification tools; Voice conversion; Seats,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85208496160
Alqadasi A.M.A.; Zeki A.M.; Sunar M.S.; Salam M.S.B.H.; Abdulghafor R.; Khaled N.A.,"Alqadasi, Ammar Mohammed Ali (58796174700); Zeki, Akram M. (24780175900); Sunar, Mohd Shahrizal (7004071446); Salam, Md. Sah Bin Hj (25825625300); Abdulghafor, Rawad (55776630200); Khaled, Nashwan Abdo (58796174800)",58796174700; 24780175900; 7004071446; 25825625300; 55776630200; 58796174800,Improving Automatic Forced Alignment for Phoneme Segmentation in Quranic Recitation,2024,IEEE Access,12,,,229,244,15,1,10.1109/ACCESS.2023.3345843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181575250&doi=10.1109%2fACCESS.2023.3345843&partnerID=40&md5=76e4eb26cd1baf863030b5ed3c11b6bc,"Segmentation plays a crucial role in speech processing applications, where high accuracy is essential. The quest for improved accuracy in automatic segmentation, particularly in the context of the Arabic language, has garnered substantial attention. However, the differences between Qur'an recitation and normal Arabic speech, especially with regard to intonation rules affecting the lengthening of long vowels, pose challenges in segmentation especially for Qur'an recitation. This research endeavors to address these challenges by delving into the domain of automatic segmentation for Qur'an recitation recognition. The proposed scheme employs a hidden Markov models (HMMs) forced alignment algorithm. To enhance the precision of segmentation, several refinements have been introduced, with a primary emphasis on the phonetic model of the Qur'an and Tajweed, particularly the intricate rules governing elongation. These enhancements encompass the adaptation of an acoustic model tailored for Qur'anic recitation as preprocessing and culminate in the development of an algorithm aimed at refining forced alignment based on the phonetic nuances of the Qur'an. These enhancements are seamlessly integrated as post-processing components for the classic HMM-based forced alignment. The research utilizes a comprehensive database featuring recordings from 100 renowned Qur'an reciters, encompassing the recitation of 21 Qur'anic verses (Ayat). Additionally, 30 reciters were asked to record the same verses, incorporating various recitation speed patterns. To facilitate the evaluation process, a Random sample of the Qur'anic database was manually segmented, comprised 21 Ayats, totaling 19,800 words, with 89 unique words (14 verses x 3 recitation levels: fast, slow and normal x 6 readers). The outcomes of this study manifest notable advancements in the alignment of long vowels within Qur'an recitation, all while maintaining the precise alignment of vowels and consonants. Objective comparisons between the proposed automatic methods and manual segmentation were conducted to ascertain the superior approach. The findings affirm that the classic forced alignment method produces satisfactory outcomes when employed on verses lacking long vowels. However, its performance diminishes when confronted with verses containing long vowels. Therefore, the test samples were categorized into three groups based on the presence of long vowels, resulting in a Correct Classification Rate (CCR) that ranged from 6% to 57%, contingent on whether the verse includes long vowels or not. The average CCR across all test samples was 23%. In contrast, the proposed algorithm significantly enhances audio segmentation. It achieved CCR values ranging from 16% to 70% within the same database categories, with an average CCR of 45% across all test samples. This marks a notable advancement of 22% in segmented speech accuracy, particularly within a 30 ms tolerance, for verses containing long vowels.  © 2023 The Authors.",Arabic phoneme segmentation; forced alignment; Phoneme alignment; phoneme duration; phoneme recognition; phoneme segmentation; recitation recognition; Tajweed recognition,Hidden Markov models; Job analysis; Linguistics; Speech processing; Speech recognition; Arabic phoneme segmentation; Forced alignment; Hidden-Markov models; Manual; Phoneme alignments; Phoneme duration; Phoneme segmentation; Phonemes recognition; Recitation recognition; Tajweed recognition; Task analysis; Alignment,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85181575250
Zi Y.; Xiong S.,"Zi, Yunfei (57203882690); Xiong, Shengwu (57203905556)",57203882690; 57203905556,Improving Short-Duration Speaker Recognition by Joint Bark-Wavelet Acoustic Feature Coupling and Triplet Dual-Attention Mechanism Network,2024,Wireless Personal Communications,135,3,,1725,1746,21,0,10.1007/s11277-024-11149-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192367377&doi=10.1007%2fs11277-024-11149-5&partnerID=40&md5=a19d0f2fe2692d749abee5ca09d602b1,"Speaker recognition methods are negatively affected by the short duration of the input audio signal. In this paper, we address the problem of speaker recognition from short-duration speech data by coupling two proposed acoustic features: Bark-scaled Gaussian Filter Cepstral Coefficients (BGCC) and Perceptual Wavelet Packet Entropy (PWPE). Our approach is based on the observation that BGCC and PWPE capture comprehensive information related to speech, such as speech perception and high time–frequency representation. This information enhances the diversity of speaker characteristics and thus improves the accuracy of speaker discrimination. To effectively integrate these two features, we propose a Triplet Dual Attention Mechanism as a creative solution. By using this mechanism, the limited features extracted from short utterances can be reused, while simultaneously enhancing the discriminative features for improved performance in speaker recognition tasks with short-duration audio signals. Extensive analysis conducted on various datasets containing speech samples of different types and lengths confirms the superiority of our proposed feature engineering and method over existing acoustic feature extraction and speaker recognition algorithms. These include approaches based on MFCCs, LPCCs features, GMM-UBM, iVector-PLDA, and ResCNN-Triplet. The experimental results show that our proposed method achieves a significant improvement over existing approaches in the area of short-duration speaker recognition. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.",BGCC; Coupled feature; PWPE; Short duration audio; Speaker recognition; Triplet-DAM,Bark-scaled gaussian filter cepstral coefficient; Cepstral coefficients; Coupled feature; Gaussian filters; Perceptual wavelet packet entropy; Short duration audio; Short durations; Speaker recognition; Triplet-DAM; Wavelet packet entropies; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85192367377
Mamieva D.; Abdusalomov A.B.; Kutlimuratov A.; Muminov B.; Whangbo T.K.,"Mamieva, Dilnoza (58054839100); Abdusalomov, Akmalbek Bobomirzaevich (57194333152); Kutlimuratov, Alpamis (57220041924); Muminov, Bahodir (57215928123); Whangbo, Taeg Keun (35617849900)",58054839100; 57194333152; 57220041924; 57215928123; 35617849900,Multimodal Emotion Detection via Attention-Based Fusion of Extracted Facial and Speech Features,2023,Sensors,23,12,5475,,,,36,10.3390/s23125475,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164018247&doi=10.3390%2fs23125475&partnerID=40&md5=6a5ea5fdb94545bd02226910fe9002f5,"Methods for detecting emotions that employ many modalities at the same time have been found to be more accurate and resilient than those that rely on a single sense. This is due to the fact that sentiments may be conveyed in a wide range of modalities, each of which offers a different and complementary window into the thoughts and emotions of the speaker. In this way, a more complete picture of a person’s emotional state may emerge through the fusion and analysis of data from several modalities. The research suggests a new attention-based approach to multimodal emotion recognition. This technique integrates facial and speech features that have been extracted by independent encoders in order to pick the aspects that are the most informative. It increases the system’s accuracy by processing speech and facial features of various sizes and focuses on the most useful bits of input. A more comprehensive representation of facial expressions is extracted by the use of both low- and high-level facial features. These modalities are combined using a fusion network to create a multimodal feature vector which is then fed to a classification layer for emotion recognition. The developed system is evaluated on two datasets, IEMOCAP and CMU-MOSEI, and shows superior performance compared to existing models, achieving a weighted accuracy WA of 74.6% and an F1 score of 66.1% on the IEMOCAP dataset and a WA of 80.7% and F1 score of 73.7% on the CMU-MOSEI dataset. © 2023 by the authors.",attention mechanism; CNN; facial feature; multimodal emotion recognition; speech feature,"Emotions; Facial Expression; Humans; Recognition, Psychology; Speech; Face recognition; Feature extraction; Speech recognition; Analysis of data; Attention mechanisms; Emotion detection; Emotional state; F1 scores; Facial Expressions; Facial feature; Multi-modal; Multimodal emotion recognition; Speech features; emotion; facial expression; human; recognition; speech; Emotion Recognition",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85164018247
Zhou R.; Koshikawa T.; Ito A.; Nose T.; Chen C.-P.,"Zhou, Rui (59175813500); Koshikawa, Takaki (59399322600); Ito, Akinori (7403722531); Nose, Takashi (35769706500); Chen, Chia-Ping (14017560500)",59175813500; 59399322600; 7403722531; 35769706500; 14017560500,Multilingual Meta-Transfer Learning for Low-Resource Speech Recognition,2024,IEEE Access,12,,,158493,158504,11,1,10.1109/ACCESS.2024.3486711,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208363215&doi=10.1109%2fACCESS.2024.3486711&partnerID=40&md5=06fd2739a8c954c2272d6029f9f06983,"This paper proposes a novel meta-transfer learning method to improve automatic speech recognition (ASR) performance in low-resource languages. Nowadays, we are witnessing high interest in low-resource ASR tasks aiming at delivering feasible and reliable systems with very limited data. The main challenge is the design and development of a methodology to address the issue of data scarcity. Our proposed meta-transfer learning approach combines two well-known machine-learning methods: transfer learning and meta-learning. We propose their integration that can ameliorate the training bottlenecks and overfitting issues with pre-training models on low-resource speech data. For evaluation, we conduct extensive multilingual ASR experiments on the Common Voice Corpus and Globalphone Corpus and compare the performance of the meta-transfer learning, meta-learning, and transfer learning methods. The proposed meta-transfer learning achieves a relative character error rate (CER) reduction of 11.62% over meta-learning and a relative CER reduction of 10.86% over transfer learning in low-resource experiments. We used less than 15 minutes of data for each target language in near-zero resource language experiments. Our meta-transfer learning approach achieved an average CER of 25.25% less than meta-learning and transfer learning. These results clearly demonstrate that the proposed integration works well in ASR tasks in languages with very limited data resources. © 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",end-to-end speech recognition; low-resource languages; Meta-transfer learning,Adversarial machine learning; Federated learning; Speech enhancement; Speech recognition; Transfer learning; Automatic speech recognition; Character error rates; End to end; End-to-end speech recognition; Limited data; Low resource languages; Meta-transfer learning; Metalearning; Transfer learning; Transfer learning methods; Contrastive Learning,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85208363215
Qu L.; Weber C.; Wermter S.,"Qu, Leyuan (57189588967); Weber, Cornelius (7402376952); Wermter, Stefan (7003826680)",57189588967; 7402376952; 7003826680,LipSound2: Self-Supervised Pre-Training for Lip-to-Speech Reconstruction and Lip Reading,2024,IEEE Transactions on Neural Networks and Learning Systems,35,2,,2772,2782,10,10,10.1109/TNNLS.2022.3191677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184518948&doi=10.1109%2fTNNLS.2022.3191677&partnerID=40&md5=55efc65ded26a2e803609e2e42fb9b14,"The aim of this work is to investigate the impact of crossmodal self-supervised pre-training for speech reconstruction (video-to-audio) by leveraging the natural co-occurrence of audio and visual streams in videos. We propose LipSound2 that consists of an encoder-decoder architecture and location-aware attention mechanism to map face image sequences to mel-scale spectrograms directly without requiring any human annotations. The proposed LipSound2 model is first pre-trained on ∼ 2400-h multilingual (e.g., English and German) audio-visual data (VoxCeleb2). To verify the generalizability of the proposed method, we then fine-tune the pre-trained model on domain-specific datasets (GRID and TCD-TIMIT) for English speech reconstruction and achieve a significant improvement on speech quality and intelligibility compared to previous approaches in speaker-dependent and speaker-independent settings. In addition to English, we conduct Chinese speech reconstruction on the Chinese Mandarin Lip Reading (CMLR) dataset to verify the impact on transferability. Finally, we train the cascaded lip reading (video-to-text) system by fine-tuning the generated audios on a pre-trained speech recognition system and achieve the state-of-the-art performance on both English and Chinese benchmark datasets.  © 2012 IEEE.",Lip reading; self-supervised pre-training; speech recognition; speech reconstruction,"Humans; Lipreading; Neural Networks, Computer; Speech; Speech Perception; Benchmarking; Character recognition; Personnel training; Speech intelligibility; Attention mechanisms; Co-occurrence; Cross-modal; Encoder-decoder architecture; Lip reading; Location-aware; Natural CO; Pre-training; Self-supervised pre-training; Speech reconstruction; artificial neural network; human; lip reading; speech; speech perception; Speech recognition",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85184518948
Lu C.; Zheng W.; Lian H.; Zong Y.; Tang C.; Li S.; Zhao Y.,"Lu, Cheng (57194429246); Zheng, Wenming (7403566545); Lian, Hailun (57207733912); Zong, Yuan (57038101500); Tang, Chuangao (57193794835); Li, Sunan (57215313619); Zhao, Yan (57254405800)",57194429246; 7403566545; 57207733912; 57038101500; 57193794835; 57215313619; 57254405800,Speech Emotion Recognition via an Attentive Time-Frequency Neural Network,2023,IEEE Transactions on Computational Social Systems,10,6,,3159,3168,9,15,10.1109/TCSS.2022.3219825,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144800792&doi=10.1109%2fTCSS.2022.3219825&partnerID=40&md5=74d0900a86d473ef8f21bc202a339eee,"Spectrogram is commonly used as the input feature of deep neural networks to learn the high(er)-level time-frequency pattern of speech signal for speech emotion recognition (SER). Generally, different emotions correspond to specific energy activations both within frequency bands and time frames on spectrogram, which indicates the frequency and time domains are both essential to represent the emotion for SER. However, recent spectrogram-based works mainly focus on modeling the long-term dependency in time domain, which makes these methods suffer from the following issues: 1) neglecting to model the emotion-related correlations within frequency domain during the time-frequency joint learning and 2) ignoring to capture the specific frequency bands associated with emotions. To cope with the issues, we propose an attentive time-frequency neural network (ATFNN) for SER, including a time-frequency neural network (TFNN) and time-frequency attention. Specifically, aiming at the first issue, we design a TFNN with a frequency-domain encoder (F-Encoder) based on the Transformer encoder and a time-domain encoder (T-Encoder) based on the bidirectional long short-term memory (Bi-LSTM). The F-Encoder and T-Encoder model the correlations within frequency bands and time frames, respectively, and they are embedded into a time-frequency joint learning strategy to obtain the time-frequency patterns of speech emotions. Moreover, to handle the second issue, we adopt the time-frequency attention with a frequency-attention network (F-Attention) and a time-attention network (T-Attention) to focus on the emotion-related long-range dependencies between frequency bands and across time frames, which can enhance the emotional discrimination of speech features. Extensive experimental results on three public emotional databases, i.e., IEMOCAP, ABC, and CASIA, show that our proposed ATFNN outperforms the state-of-the-art methods.  © 2014 IEEE.",Attention mechanism; spectrogram; speech emotion recognition (SER); time-frequency neural network (TFNN),Activation energy; Deep neural networks; Emotion Recognition; Encoding (symbols); Frequency domain analysis; Long short-term memory; Signal encoding; Spectrographs; Time domain analysis; Attention mechanisms; Correlation; Encodings; Features extraction; Neural-networks; Spectrograms; Speech emotion recognition; Time frequency; Time-domain analysis; Time-frequency Analysis; Time–frequency neural network; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85144800792
Park K.; Hong J.,"Park, Kwanghyun (59552887200); Hong, Jungpyo (57225132073)",59552887200; 57225132073,Real-Time Communication Aid System for Korean Dysarthric Speech,2025,Applied Sciences (Switzerland),15,3,1416,,,,0,10.3390/app15031416,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217616426&doi=10.3390%2fapp15031416&partnerID=40&md5=2dfee7bde53f1ab3e82bb4c35fa89c1a,"Dysarthria is a speech disorder characterized by difficulties in articulation and vocalization due to impaired control of the articulatory system. Around 30% of individuals with speech disorders have dysarthria, facing significant communication challenges. Existing assistive tools for dysarthria either require additional manipulation or only provide word-level speech support, limiting their ability to support effective communication in real-world situations. Thus, this paper proposes a real-time communication aid system that converts sentence-level Korean dysarthric speech to non-dysarthric normal speech. The proposed system consists of two main parts in cascading form. Specifically, a Korean Automatic Speech Recognition (ASR) model is trained with dysarthric utterances using a conformer-based architecture and the graph transducer network–connectionist temporal classification algorithm, significantly enhancing recognition performance over previous models. Subsequently, a Korean Text-To-Speech (TTS) model based on Jointly Training FastSpeech2 and HiFi-GAN for end-to-end Text-to-Speech (JETS) is pipelined to synthesize high-quality non-dysarthric normal speech. These models are integrated into a single system on an app server, which receives 5–10 s of dysarthric speech and converts it to normal speech after 2–3 s. This can provide a practical communication aid for people with dysarthria. © 2025 by the authors.",communication aid system; conformer-based automatic speech recognition; dysarthria; JETS-based text-to-speech; Korean,Character recognition; Speech communication; Speech enhancement; Automatic speech recognition; Communication aid system; Communication aids; Conformer-based automatic speech recognition; Dysarthria; JETS-based text-to-speech; Korean; Real-time communication; Speech disorders; Text to speech; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85217616426
Abdulatif S.; Cao R.; Yang B.,"Abdulatif, Sherif (57194649144); Cao, Ruizhe (57573108300); Yang, Bin (55584795030)",57194649144; 57573108300; 55584795030,CMGAN: Conformer-Based Metric-GAN for Monaural Speech Enhancement,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2477,2493,16,17,10.1109/TASLP.2024.3393718,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191795840&doi=10.1109%2fTASLP.2024.3393718&partnerID=40&md5=11e249720244d420663686f7afc5c26d,"In this work, we further develop the conformer-based metric generative adversarial network (CMGAN) model1 for speech enhancement (SE) in the time-frequency (TF) domain. This paper builds on our previous work but takes a more in-depth look by conducting extensive ablation studies on model inputs and architectural design choices. We rigorously tested the generalization ability of the model to unseen noise types and distortions. We have fortified our claims through DNS-MOS measurements and listening tests. Rather than focusing exclusively on the speech denoising task, we extend this work to address the dereverberation and super-resolution tasks. This necessitated exploring various architectural changes, specifically metric discriminator scores and masking techniques. It is essential to highlight that this is among the earliest works that attempted complex TF-domain super-resolution. Our findings show that CMGAN outperforms existing state-of-the-art methods in the three major speech enhancement tasks: denoising, dereverberation, and super-resolution. For example, in the denoising task using the Voice Bank+DEMAND dataset, CMGAN notably exceeded the performance of prior models, attaining a PESQ score of 3.41 and an SSNR of 11.10 dB. Audio samples and CMGAN implementations are available online2 © 2014 IEEE.",attention models; deep learning; generative adversarial networks; metric discriminator; Speech enhancement,Acoustic noise; Deep learning; Discriminators; Frequency domain analysis; Noise abatement; Optical resolving power; Speech enhancement; Speech recognition; Time domain analysis; Unsupervised learning; Attention model; De-noising; Deep learning; Metric discriminator; Network models; Study on models; Superresolution; Task analysis; Time frequency domain; Time-domain analysis; Generative adversarial networks,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85191795840
Zhou K.; Sisman B.; Rana R.; Schuller B.W.; Li H.,"Zhou, Kun (57220568300); Sisman, Berrak (57201983856); Rana, Rajib (35318333200); Schuller, Bjorn W. (6603767415); Li, Haizhou (8615868400)",57220568300; 57201983856; 35318333200; 6603767415; 8615868400,Speech Synthesis With Mixed Emotions,2023,IEEE Transactions on Affective Computing,14,4,,3120,3134,14,21,10.1109/TAFFC.2022.3233324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146218167&doi=10.1109%2fTAFFC.2022.3233324&partnerID=40&md5=c4f58e7d2075a73d8fd01742e4414a8a,"Emotional speech synthesis aims to synthesize human voices with various emotional effects. The current studies are mostly focused on imitating an averaged style belonging to a specific emotion type. In this paper, we seek to generate speech with a mixture of emotions at run-time. We propose a novel formulation that measures the relative difference between the speech samples of different emotions. We then incorporate our formulation into a sequence-to-sequence emotional text-to-speech framework. During the training, the framework does not only explicitly characterize emotion styles but also explores the ordinal nature of emotions by quantifying the differences with other emotions. At run-time, we control the model to produce the desired emotion mixture by manually defining an emotion attribute vector. The objective and subjective evaluations have validated the effectiveness of the proposed framework. To our best knowledge, this research is the first study on modelling, synthesizing, and evaluating mixed emotions in speech. © 2010-2012 IEEE.",emotion attribute vector; Emotional speech synthesis; mixed emotions; relative difference; sequence-to-sequence; the ordinal nature of emotions,Electronic mail; Hidden Markov models; Speech recognition; Speech synthesis; Attribute vectors; Emotion attribute vector; Emotion recognition; Emotional speech synthesis; Hidden-Markov models; Mixed emotion; Psychology; Relative difference; Sequence-to-sequence; The ordinal nature of emotion; Emotion Recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85146218167
Zmolikova K.; Pedersen M.S.; Jensen J.,"Zmolikova, Katerina (57189593906); Pedersen, Michael Syskind (57212107278); Jensen, Jesper (57199943098)",57189593906; 57212107278; 57199943098,Masked Spectrogram Prediction for Unsupervised Domain Adaptation in Speech Enhancement,2024,IEEE Open Journal of Signal Processing,5,,,274,283,9,3,10.1109/OJSP.2023.3343343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180283179&doi=10.1109%2fOJSP.2023.3343343&partnerID=40&md5=bc8abc1363a84177175c3e3e89df6559,"Supervised learning-based speech enhancement methods often work remarkably well in acoustic situations represented in the training corpus but generalize poorly to out-of-domain situations, i.e. situations not seen during training. This stands in the way of further improvement of these methods in realistic scenarios, as collecting paired noisy-clean recordings in the target application domain is typically not feasible. Recording noisy-only in-domain data is, though, much more practical. In this article, we tackle the problem of unsupervised domain adaptation in speech enhancement. Specifically, we propose a way to use in-domain noisy-only data in the training of a neural network to improve upon a model trained solely on out-of-domain paired data. For this, we make use of masked spectrogram prediction, a technique from self-supervised learning that aims to interpolate masked regions of a spectrogram. We hypothesize that masked spectrogram prediction encourages learning of features that represent well both speech and noise components of the noisy signals. These features can then be used to train a more robust speech enhancement system. We evaluate the proposed method on the VoiceBank-DEMAND and LibriFSD50k databases, with WSJ0-CHiME3 serving as the out-of-domain database. We show that the proposed method outperforms both the out-of-domain system and the baseline approaches, i.e. RemixIT and noisy-target training, and also combines well with the previously proposed RemixIT method.  © 2020 IEEE.",Masked spectrogram prediction; speech enhancement; unsupervised domain adaptation,Audio recordings; Spectrographs; Speech enhancement; Speech recognition; Supervised learning; Domain adaptation; Features extraction; Masked spectrogram prediction; Noise measurements; Recording; Spectrograms; Speech enhancement methods; Training corpus; Unsupervised domain adaptation; Forecasting,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85180283179
Mira R.; Vougioukas K.; Ma P.; Petridis S.; Schuller B.W.; Pantic M.,"Mira, Rodrigo (57224467437); Vougioukas, Konstantinos (56103995500); Ma, Pingchuan (57207853930); Petridis, Stavros (24470463000); Schuller, Bjorn W. (6603767415); Pantic, Maja (56259551600)",57224467437; 56103995500; 57207853930; 24470463000; 6603767415; 56259551600,End-to-End Video-to-Speech Synthesis Using Generative Adversarial Networks,2023,IEEE Transactions on Cybernetics,53,6,,3454,3466,12,18,10.1109/TCYB.2022.3162495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128695037&doi=10.1109%2fTCYB.2022.3162495&partnerID=40&md5=722bcacbbfd3474b4e6c10c7ff524d11,"Video-to-speech is the process of reconstructing the audio speech from a video of a spoken utterance. Previous approaches to this task have relied on a two-step process where an intermediate representation is inferred from the video and is then decoded into waveform audio using a vocoder or a waveform reconstruction algorithm. In this work, we propose a new end-to-end video-to-speech model based on generative adversarial networks (GANs) which translates spoken video to waveform end-to-end without using any intermediate representation or separate waveform synthesis algorithm. Our model consists of an encoder-decoder architecture that receives raw video as input and generates speech, which is then fed to a waveform critic and a power critic. The use of an adversarial loss based on these two critics enables the direct synthesis of the raw audio waveform and ensures its realism. In addition, the use of our three comparative losses helps establish direct correspondence between the generated audio and the input video. We show that this model is able to reconstruct speech with remarkable realism for constrained datasets such as GRID, and that it is the first end-to-end model to produce intelligible speech for Lip Reading in the Wild (LRW), featuring hundreds of speakers recorded entirely 'in the wild.' We evaluate the generated samples in two different scenarios - seen and unseen speakers - using four objective metrics which measure the quality and intelligibility of artificial speech. We demonstrate that the proposed approach outperforms all previous works in most metrics on GRID and LRW.  © 2013 IEEE.",Computer vision; deep learning; end-to-end; generative adversarial networks (GANs); speech synthesis; video-to-speech,Decoding; Deep learning; Generative adversarial networks; Hidden Markov models; Quality control; Speech recognition; Speech synthesis; Deep learning; End to end; Features extraction; Generative adversarial network; Hidden-Markov models; Predictive models; Spectrograms; Task analysis; Video-to-speech; Waveforms; algorithm; article; human; human experiment; lip reading; speech; speech intelligibility; synthesis; videorecording; waveform; writing; Computer vision,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85128695037
Zhang J.; Tu G.; Liu S.; Cai Z.,"Zhang, Jiachen (58242842400); Tu, Guoqing (8924786600); Liu, Shubo (24721973100); Cai, Zhaohui (7402902112)",58242842400; 8924786600; 24721973100; 7402902112,Audio Anti-Spoofing Based on Audio Feature Fusion,2023,Algorithms,16,7,317,,,,5,10.3390/a16070317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165966543&doi=10.3390%2fa16070317&partnerID=40&md5=7642148177e8a77aef80038fe702208e,"The rapid development of speech synthesis technology has significantly improved the naturalness and human-likeness of synthetic speech. As the technical barriers for speech synthesis are rapidly lowering, the number of illegal activities such as fraud and extortion is increasing, posing a significant threat to authentication systems, such as automatic speaker verification. This paper proposes an end-to-end speech synthesis detection model based on audio feature fusion in response to the constantly evolving synthesis techniques and to improve the accuracy of detecting synthetic speech. The model uses a pre-trained wav2vec2 model to extract features from raw waveforms and utilizes an audio feature fusion module for back-end classification. The audio feature fusion module aims to improve the model accuracy by adequately utilizing the audio features extracted from the front end and fusing the information from timeframes and feature dimensions. Data augmentation techniques are also used to enhance the performance generalization of the model. The model is trained on the training and development sets of the logical access (LA) dataset of the ASVspoof 2019 Challenge, an international standard, and is tested on the logical access (LA) and deep-fake (DF) evaluation datasets of the ASVspoof 2021 Challenge. The equal error rate (EER) on ASVspoof 2021 LA and ASVspoof 2021 DF are 1.18% and 2.62%, respectively, achieving the best results on the DF dataset. © 2023 by the authors.",ASVspoof Challenge; automatic speaker verification; deep learning; deep-fake detection; wav2vec 2.0,Crime; Deep learning; Fake detection; Feature extraction; Speech recognition; Asvspoof challenge; Audio features; Automatic speaker verification; Deep learning; Deep-fake detection; Fake detection; Features fusions; Fusion modules; Synthetic speech; Wav2vec 2.0; Speech synthesis,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85165966543
Kheddar H.; Himeur Y.; Al-Maadeed S.; Amira A.; Bensaali F.,"Kheddar, Hamza (57207988595); Himeur, Yassine (55636199800); Al-Maadeed, Somaya (36467897100); Amira, Abbes (23003399100); Bensaali, Faycal (6506260453)",57207988595; 55636199800; 36467897100; 23003399100; 6506260453,Deep transfer learning for automatic speech recognition: Towards better generalization,2023,Knowledge-Based Systems,277,,110851,,,,61,10.1016/j.knosys.2023.110851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168645014&doi=10.1016%2fj.knosys.2023.110851&partnerID=40&md5=5c266678841cf633ada6d521b2966f50,"Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which cannot meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments and helps academics and professionals understand current challenges. Specifically, after presenting the DTL background, a well-designed taxonomy is adopted to inform the state-of-the-art. A critical analysis is then conducted to identify the limitations and advantages of each framework. Moving on, a comparative study is introduced to highlight the current challenges before deriving opportunities for future research. © 2023 Elsevier B.V.",Automatic speech recognition; Deep transfer learning; Domain adaptation; Fine-tuning; Large language model; Models fusion,Deep learning; Large dataset; Learning systems; Speech recognition; Transfer learning; 'current; Automatic speech recognition; Deep transfer learning; Domain adaptation; Fine tuning; Language model; Large language model; Model fusion; Training data; Transfer learning; Digital storage,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85168645014
Li Q.; Mai Q.; Wang M.; Ma M.,"Li, Qiang (55694488200); Mai, Qianyu (57216692028); Wang, Mandou (58860317400); Ma, Mingjuan (55430733600)",55694488200; 57216692028; 58860317400; 55430733600,Chinese dialect speech recognition: a comprehensive survey,2024,Artificial Intelligence Review,57,2,25,,,,4,10.1007/s10462-023-10668-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183905725&doi=10.1007%2fs10462-023-10668-0&partnerID=40&md5=2098da499f378e481966e75ed54ea273,"As a multi-ethnic country with a large population, China is endowed with diverse dialects, which brings considerable challenges to speech recognition work. In fact, due to geographical location, population migration, and other factors, the research progress and practical application of Chinese dialect speech recognition are currently at different stages. Therefore, exploring the significant regional heterogeneities in specific recognition approaches and effects, dialect corpus, and other resources is of vital importance for Chinese speech recognition work. Based on this, we first start with the regional classification of dialects and analyze the pivotal acoustic characteristics of dialects, including specific vowels and tones patterns. Secondly, we comprehensively summarize the existing dialect phonetic corpus in China, which is of some assistance in exploring the general construction methods of dialect phonetic corpus. Moreover, we expound on the general process of dialect recognition. Several critical dialect recognition approaches are summarized and introduced in detail, especially the hybrid method of Artificial Neural Network (ANN) combined with the Hidden Markov Model(HMM), as well as the End-to-End (E2E). Thirdly, through the in-depth comparison of their principles, merits, disadvantages, and recognition performance for different dialects, the development trends and challenges in dialect recognition in the future are pointed out. Finally, some application examples of dialect speech recognition are collected and discussed. © 2024, The Author(s).",Automatic speech recognition; Chinese dialect; Deep neural network; Dialect corpus; Dialectal acoustic modeling; End-to-end,Acoustic Modeling; Hidden Markov models; Linguistics; Population dynamics; Speech recognition; Acoustics model; Automatic speech recognition; Chinese dialects; Dialect corpus; Dialectal acoustic modeling; Different stages; End to end; Geographical locations; Large population; Specific recognition; Deep neural networks,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85183905725
Xiao Y.; Wu L.; Guo J.; Li J.; Zhang M.; Qin T.; Liu T.-Y.,"Xiao, Yisheng (57654053400); Wu, Lijun (57196120683); Guo, Junliang (57202367651); Li, Juntao (57195358513); Zhang, Min (56368409100); Qin, Tao (57223630731); Liu, Tie-Yan (57221068510)",57654053400; 57196120683; 57202367651; 57195358513; 56368409100; 57223630731; 57221068510,A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond,2023,IEEE Transactions on Pattern Analysis and Machine Intelligence,45,10,,11407,11427,20,55,10.1109/TPAMI.2023.3277122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160224915&doi=10.1109%2fTPAMI.2023.3277122&partnerID=40&md5=13a05806006e3f28466d11c1cd7e23e3,"Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models beyond machine translation, such as grammatical error correction, text summarization, text style transfer, dialogue, semantic parsing, automatic speech recognition, and so on. In addition, we also discuss potential directions for future exploration, including releasing the dependency of KD, reasonable training objectives, pre-training for NAR, and wider applications, etc. We hope this survey can help researchers capture the latest progress in NAR generation, inspire the design of advanced NAR models and algorithms, and enable industry practitioners to choose appropriate solutions for their applications.  © 1979-2012 IEEE.",natural language processing; neural machine translation; Non-autoregressive; sequence generation; transformer,Character recognition; Computer aided language translation; Decoding; Error correction; Modeling languages; Natural language processing systems; Neural machine translation; Neural networks; Semantics; Speech recognition; Speech transmission; Text processing; Websites; Auto-regressive; Computational modelling; Decoding; Language processing; Machine translations; Natural language processing; Natural languages; Non-autoregressive; Sequence generation; Transformer; algorithm; article; automatic speech recognition; controlled study; human; machine learning; physician; Computational linguistics,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85160224915
Amangeldy N.; Ukenova A.; Bekmanova G.; Razakhova B.; Milosz M.; Kudubayeva S.,"Amangeldy, Nurzada (57209981737); Ukenova, Aru (57730406100); Bekmanova, Gulmira (55701229100); Razakhova, Bibigul (56373445300); Milosz, Marek (14320553400); Kudubayeva, Saule (56414935700)",57209981737; 57730406100; 55701229100; 56373445300; 14320553400; 56414935700,Continuous Sign Language Recognition and Its Translation into Intonation-Colored Speech,2023,Sensors,23,14,6383,,,,8,10.3390/s23146383,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166016070&doi=10.3390%2fs23146383&partnerID=40&md5=d22013e93196a7604dcf0c1038a9113e,"This article is devoted to solving the problem of converting sign language into a consistent text with intonation markup for subsequent voice synthesis of sign phrases by speech with intonation. The paper proposes an improved method of continuous recognition of sign language, the results of which are transmitted to a natural language processor based on analyzers of morphology, syntax, and semantics of the Kazakh language, including morphological inflection and the construction of an intonation model of simple sentences. This approach has significant practical and social significance, as it can lead to the development of technologies that will help people with disabilities to communicate and improve their quality of life. As a result of the cross-validation of the model, we obtained an average test accuracy of 0.97 and an average val_accuracy of 0.90 for model evaluation. We also identified 20 sentence structures of the Kazakh language with their intonational model. © 2023 by the authors.",intonational speech synthesis; long short-term memory; natural language processing; sign language recognition; spatiotemporal features,Humans; Language; Quality of Life; Sign Language; Speech; Speech Perception; Natural language processing systems; Semantics; Speech recognition; Continuous recognition; Intonation modeling; Intonational speech synthesis; Language processing; Language processors; Natural language processing; Natural languages; Sign language; Sign Language recognition; Spatiotemporal feature; human; language; quality of life; sign language; speech; speech perception; Speech synthesis,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85166016070
Qu L.; Weber C.; Wermter S.,"Qu, Leyuan (57189588967); Weber, Cornelius (7402376952); Wermter, Stefan (7003826680)",57189588967; 7402376952; 7003826680,Emphasizing unseen words: New vocabulary acquisition for end-to-end speech recognition,2023,Neural Networks,161,,,494,504,10,6,10.1016/j.neunet.2023.01.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148327640&doi=10.1016%2fj.neunet.2023.01.027&partnerID=40&md5=4d16c55b7afa07484258204a901372cb,"Due to the dynamic nature of human language, automatic speech recognition (ASR) systems need to continuously acquire new vocabulary. Out-Of-Vocabulary (OOV) words, such as trending words and new named entities, pose problems to modern ASR systems that require long training times to adapt their large numbers of parameters. Different from most previous research focusing on language model post-processing, we tackle this problem on an earlier processing level and eliminate the bias in acoustic modeling to recognize OOV words acoustically. We propose to generate OOV words using text-to-speech systems and to rescale losses to encourage neural networks to pay more attention to OOV words. Specifically, we enlarge the classification loss used for training neural networks’ parameters of utterances containing OOV words (sentence-level), or rescale the gradient used for back-propagation for OOV words (word-level), when fine-tuning a previously trained model on synthetic audio. To overcome catastrophic forgetting, we also explore the combination of loss rescaling and model regularization, i.e. L2 regularization and elastic weight consolidation (EWC). Compared with previous methods that just fine-tune synthetic audio with EWC, the experimental results on the LibriSpeech benchmark reveal that our proposed loss rescaling approach can achieve significant improvement on the recall rate with only a slight decrease on word error rate. Moreover, word-level rescaling is more stable than utterance-level rescaling and leads to higher recall rates and precision rates on OOV word recognition. Furthermore, our proposed combined loss rescaling and weight consolidation methods can support continual learning of an ASR system. © 2023 The Author(s)",Automatic speech recognition; Continual learning; End-to-end learning; Loss rescaling; Out-of-vocabulary word recognition,"Humans; Language; Neural Networks, Computer; Speech; Speech Perception; Vocabulary; Acoustic Modeling; Backpropagation; Modeling languages; Vocabulary control; Automatic speech recognition; Automatic speech recognition system; Continual learning; End to end; End-to-end learning; Loss rescaling; Out-of-vocabulary word recognition; Outof-vocabulary words (OOV); Rescaling; Word recognition; adult; Article; artificial neural network; attention; automatic speech recognition; back propagation; classification algorithm; clinical article; elastic weight consolidation; female; human; image reconstruction; learning; machine learning; male; nerve cell network; recall; recognition; speech discrimination; support vector machine; training; vocabulary; word recognition; language; speech; speech perception; Speech recognition",Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85148327640
Faruk L.I.D.; Babakerkhell M.D.; Mongkolnam P.; Chongsuphajaisiddhi V.; Funilkul S.; Pal D.,"Faruk, Lawal Ibrahim Dutsinma (58783653200); Babakerkhell, Mohammad Dawood (57368323300); Mongkolnam, Pornchai (7801593429); Chongsuphajaisiddhi, Vithida (6506778398); Funilkul, Suree (22333896400); Pal, Debajyoti (57188765356)",58783653200; 57368323300; 7801593429; 6506778398; 22333896400; 57188765356,A Review of Subjective Scales Measuring the User Experience of Voice Assistants,2024,IEEE Access,12,,,14893,14917,24,9,10.1109/ACCESS.2024.3358423,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183614783&doi=10.1109%2fACCESS.2024.3358423&partnerID=40&md5=2839419945fc8879579f11c2b8c064f1,"The use of Voice Assistants (VA) in both commercial and personal contexts has experienced significant growth, emphasizing the importance of assessing their user experience (UX) for long-term viability. Currently, the development of appropriate scales that capture user viewpoints after interacting with a system has become a popular method for measuring UX of the Graphical User Interface (GUI) systems. However, the applicability of these scales that are meant for GUI systems on VA is still questionable, hence the need for analyzing the nature of previous scales used for measuring UX of VA. Additionally, in order to keep track of the state of UX research in the VA domain, it is crucial to understand the dimensions of UX that are being utilized. In this study, a comprehensive Systematic Literature Review (SLR) was carried out to identify 21 individual scales used for measuring UX of VA. Furthermore, this study present the evaluation criteria for assessing the rigor of operationalization during the development of these scales. The study analysis reveals that the scales used for measuring UX of VA extends beyond the traditional VUDA (value, usability, desirability, adoptability) principles and incorporates novel aspects such as anthropomorphism and machine personality. Future VA UX researchers should also acknowledge the variations in the rigorous measures employed during scale development, notwithstanding some common and accepted practices. Consequently, an overview is provided, along with suggestions for prospective studies in the field of VA UX research. © 2013 IEEE.",Factor analysis; reliability; scale; user experience; voice assistant,Human computer interaction; Job analysis; Reliability analysis; Speech recognition; Anthropomorphism; Chatbots; Factors analysis; Personal voice assistant; Scale; Task analysis; Usability; Users' experiences; Voice assistant; Voice-activity detections; Graphical user interfaces,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85183614783
Bhanushali A.R.; Mun H.; Yun J.,"Bhanushali, Amisha Rajnikant (58105253800); Mun, Hyunjun (57221965927); Yun, Joobeom (56911637100)",58105253800; 57221965927; 56911637100,Adversarial Attacks on Automatic Speech Recognition (ASR): A Survey,2024,IEEE Access,12,,,88279,88302,23,2,10.1109/ACCESS.2024.3416965,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196756223&doi=10.1109%2fACCESS.2024.3416965&partnerID=40&md5=44ada81754e8039290a48e825bd0d2d1,"Automatic Speech Recognition (ASR) systems have improved and eased how humans interact with devices. ASR system converts an acoustic waveform into the relevant text form. Modern ASR inculcates deep neural networks (DNNs) to provide faster and better results. As the use of DNN continues to expand, there is a need for examination against various adversarial attacks. Adversarial attacks are synthetic samples crafted carefully by adding particular noise to legitimate examples. They are imperceptible, yet they prove catastrophic to DNNs. Recently, adversarial attacks on ASRs have increased but previous surveys lack generalization of the different methods used for attacking ASR, and the scope of the study is narrowed to a particular application, making it difficult to determine the relationships and trade-offs between the attack techniques. Therefore, this survey provides a taxonomy illustrating the classification of the adversarial attacks on ASR based on their characteristics and behavior. Additionally, we have analyzed the existing methods for generating adversarial attacks and presented their comparative analysis. We have clearly drawn the outline to indicate the efficiency of the adversarial techniques, and based on the lacunae found in the existing studies, we have stated the future scope.  © 2013 IEEE.",Adversarial attacks; adversarial samples; automatic speech recognition (ASR); deep neural network (DNN),Character recognition; Economic and social effects; Job analysis; Speech recognition; Taxonomies; Acoustic waveform; Adversarial attack; Adversarial sample; Automatic speech recognition; Automatic speech recognition system; Deep neural network; Task analysis; Text recognition; Deep neural networks,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85196756223
Jung J.-W.; Wu Y.; Wang X.; Kim J.-H.; Maiti S.; Matsunaga Y.; Shim H.-J.; Tian J.; Evans N.; Chung J.S.; Zhang W.; Um S.; Takamichi S.; Watanabe S.,"Jung, Jee-Weon (57200084441); Wu, Yihan (57639598900); Wang, Xin (57196088126); Kim, Ji-Hoon (56028059200); Maiti, Soumi (57200084612); Matsunaga, Yuta (57566368800); Shim, Hye-Jin (57204047137); Tian, Jinchuan (57218454065); Evans, Nicholas (56207371100); Chung, Joon Son (58305787800); Zhang, Wangyou (57211641276); Um, Seyun (57214790370); Takamichi, Shinnosuke (55751754900); Watanabe, Shinji (59357183200)",57200084441; 57639598900; 57196088126; 56028059200; 57200084612; 57566368800; 57204047137; 57218454065; 56207371100; 58305787800; 57211641276; 57214790370; 55751754900; 59357183200,SpoofCeleb: Speech Deepfake Detection and SASV in the Wild,2025,IEEE Open Journal of Signal Processing,6,,,68,77,9,0,10.1109/OJSP.2025.3529377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215396617&doi=10.1109%2fOJSP.2025.3529377&partnerID=40&md5=d668b7d45adf74c4d06daa32cae248df,"This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data. Robust recognition systems require speech data recorded in varied acoustic environments with different levels of noise to be trained. However, current datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read speech is typically necessary to train TTS models. Current SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. SpoofCeleb leverages a fully automated pipeline we developed that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. SpoofCeleb comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We present the baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at https://jungjee.github.io/spoofceleb.  © 2025 IEEE.",In the wild; speech deepfake detection; spoofing-robust automatic speaker verification,'current; Automatic speaker verification; Condition; In the wild; Real-world; Speech deepfake detection; Speech training; Spoofing-robust automatic speaker verification; Text to speech; Text-to-speech system; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85215396617
Li H.; Li J.; Liu H.; Liu T.; Chen Q.; You X.,"Li, Hui (59241359800); Li, Jiawen (59080341700); Liu, Hai (56450301400); Liu, Tingting (58385272500); Chen, Qiang (59326816400); You, Xinge (8937550100)",59241359800; 59080341700; 56450301400; 58385272500; 59326816400; 8937550100,MelTrans: Mel-Spectrogram Relationship-Learning for Speech Emotion Recognition via Transformers,2024,Sensors,24,17,5506,,,,2,10.3390/s24175506,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203867219&doi=10.3390%2fs24175506&partnerID=40&md5=c968de84d71612050cb2d43f75b2afe2,"Speech emotion recognition (SER) is not only a ubiquitous aspect of everyday communication, but also a central focus in the field of human–computer interaction. However, SER faces several challenges, including difficulties in detecting subtle emotional nuances and the complicated task of recognizing speech emotions in noisy environments. To effectively address these challenges, we introduce a Transformer-based model called MelTrans, which is designed to distill critical clues from speech data by learning core features and long-range dependencies. At the heart of our approach is a dual-stream framework. Using the Transformer architecture as its foundation, MelTrans deciphers broad dependencies within speech mel-spectrograms, facilitating a nuanced understanding of emotional cues embedded in speech signals. Comprehensive experimental evaluations on the EmoDB (92.52%) and IEMOCAP (76.54%) datasets demonstrate the effectiveness of MelTrans. These results highlight MelTrans’s ability to capture critical cues and long-range dependencies in speech data, setting a new benchmark within the context of these specific datasets. These results highlight the effectiveness of the proposed model in addressing the complex challenges posed by SER tasks. © 2024 by the authors.",deep learning; feature extraction; speech emotion recognition; Transformer,Deep learning; Emotion Recognition; Spectrographs; Speech communication; Speech recognition; Computer interaction; Deep learning; Features extraction; Long-range dependencies; Noisy environment; Spectrograms; Speech data; Speech emotion recognition; Speech emotions; Transformer,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85203867219
Jeong M.; Kim M.; Lee J.Y.; Kim N.S.,"Jeong, Myeonghun (57223769672); Kim, Minchan (57223733257); Lee, Joun Yeop (56493239600); Kim, Nam Soo (34770411700)",57223769672; 57223733257; 56493239600; 34770411700,Efficient Parallel Audio Generation Using Group Masked Language Modeling,2024,IEEE Signal Processing Letters,31,,,979,983,4,1,10.1109/LSP.2024.3381910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189509451&doi=10.1109%2fLSP.2024.3381910&partnerID=40&md5=1bfb5563618ef2843601f73f7188e104,"We present a fast and high-quality codec language model for parallel audio generation. While SoundStorm, a state-of-the-art parallel audio generation model, accelerates inference speed compared to autoregressive models, it still suffers from slow inference due to iterative sampling. To resolve this problem, we propose Group-Masked Language Modeling (G-MLM) and Group Iterative Parallel Decoding (G-IPD) for efficient parallel audio generation. Both the training and sampling schemes enable the model to synthesize high-quality audio with a small number of iterations by effectively modeling the group-wise conditional dependencies. In addition, our model employs a cross-attention-based architecture to capture the speaker style of the prompt voice and improves computational efficiency. Experimental results demonstrate that our proposed model outperforms the baselines in prompt-based audio generation. © 1994-2012 IEEE.",neural audio codec; Parallel audio generation,Audio acoustics; Audio signal processing; Computational efficiency; Computational linguistics; Modeling languages; Speech communication; Speech recognition; Audio codecs; Autoregressive modelling; Computational modelling; High quality; Iterative decodings; Language model; Neural audio codec; Parallel audio generation; State of the art; Tokenization; Semantics,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85189509451
Beigi H.; Liu X.Y.,"Beigi, Homayoon (6602416988); Liu, Xing Yi (58744105500)",6602416988; 58744105500,Efficient Ensemble of Deep Neural Networks for Multimodal Punctuation Restoration and the Spontaneous Informal Speech Dataset,2025,Electronics (Switzerland),14,5,973,,,,0,10.3390/electronics14050973,https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000490421&doi=10.3390%2felectronics14050973&partnerID=40&md5=8481f00a1775d47e339216db182c9b7b,"Punctuation restoration plays an essential role in the postprocessing procedure of automatic speech recognition, but model efficiency is a key requirement for this task. To that end, we present EfficientPunct, an ensemble method with a multimodal time-delay neural network that outperforms the current best model by 1.0 F1 point while using less than a tenth of its network parameters for inference. This work further streamlines a speech recognizer and a BERT implementation to efficiently output hidden layer acoustic embeddings and text embeddings in the context of punctuation restoration. Here, forced alignment and temporal convolutions are used to eliminate the need for attention-based fusion, greatly increasing computational efficiency and improving performance. EfficientPunct sets a new state of the art with an ensemble that weighs BERT’s purely language-based predictions slightly more than the multimodal network’s predictions. Although EfficientPunct shows great promise, from a different perspective, to date, another important challenge in the field has been the fact that punctuation restoration models have been evaluated almost solely on well-structured, scripted corpora. However, real-world ASR systems and postprocessing pipelines typically apply to spontaneous speech with significant irregularities, stutters, and deviations from perfect grammar. To address this important discrepancy, we also introduce SponSpeech, a punctuation restoration dataset derived from informal speech sources, which includes punctuation and casing information. In addition to publicly releasing the dataset, the authors have contributed by providing a filtering pipeline that can be used to generate more data. This filtering pipeline examines the quality of both the speech audio and the transcription text. A challenging test set is also carefully constructed, aimed at evaluating the models’ ability to leverage audio information to predict, otherwise grammatically ambiguous, punctuation. SponSpeech has been made available to the public, along with all code for dataset building and model runs. © 2025 by the authors.",corpus; dataset; multimodal learning; punctuation restoration; punctuation restoration; speech recognition; speech recognition postprocessing,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-86000490421
Go C.; Park N.I.; Jeon O.-Y.; Chun C.,"Go, Changhwan (58548251100); Park, Nam In (36718293000); Jeon, Oc-Yeub (6603333962); Chun, Chanjun (36026528300)",58548251100; 36718293000; 6603333962; 36026528300,A Pre-Training Framework Based on Multi-Order Acoustic Simulation for Replay Voice Spoofing Detection,2023,Sensors,23,16,7280,,,,1,10.3390/s23167280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168745112&doi=10.3390%2fs23167280&partnerID=40&md5=627557c353f00ec1f81ecfba501f3df2,"Voice spoofing attempts to break into a specific automatic speaker verification (ASV) system by forging the user’s voice and can be used through methods such as text-to-speech (TTS), voice conversion (VC), and replay attacks. Recently, deep learning-based voice spoofing countermeasures have been developed. However, the problem with replay is that it is difficult to construct a large number of datasets because it requires a physical recording process. To overcome these problems, this study proposes a pre-training framework based on multi-order acoustic simulation for replay voice spoofing detection. Multi-order acoustic simulation utilizes existing clean signal and room impulse response (RIR) datasets to generate audios, which simulate the various acoustic configurations of the original and replayed audios. The acoustic configuration refers to factors such as the microphone type, reverberation, time delay, and noise that may occur between a speaker and microphone during the recording process. We assume that a deep learning model trained on an audio that simulates the various acoustic configurations of the original and replayed audios can classify the acoustic configurations of the original and replay audios well. To validate this, we performed pre-training to classify the audio generated by the multi-order acoustic simulation into three classes: clean signal, audio simulating the acoustic configuration of the original audio, and audio simulating the acoustic configuration of the replay audio. We also set the weights of the pre-training model to the initial weights of the replay voice spoofing detection model using the existing replay voice spoofing dataset and then performed fine-tuning. To validate the effectiveness of the proposed method, we evaluated the performance of the conventional method without pre-training and proposed method using an objective metric, i.e., the accuracy and F1-score. As a result, the conventional method achieved an accuracy of 92.94%, F1-score of 86.92% and the proposed method achieved an accuracy of 98.16%, F1-score of 95.08%. © 2023 by the authors.",acoustic configuration; deep learning; voice spoofing,Audio acoustics; Audio recordings; Deep learning; Large dataset; Microphone array; Speech recognition; Acoustic configuration; Acoustic simulations; Conventional methods; Deep learning; F1 scores; Multi-ordering; Pre-training; Recording process; Training framework; Voice spoofing; Impulse response,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85168745112
Zhu Q.; Zhou L.; Zhang Z.; Liu S.; Jiao B.; Zhang J.; Dai L.; Jiang D.; Li J.; Wei F.,"Zhu, Qiushi (57340408800); Zhou, Long (57221148819); Zhang, Ziqiang (57221424487); Liu, Shujie (56181265100); Jiao, Binxing (57245739400); Zhang, Jie (55986519600); Dai, Lirong (55552203600); Jiang, Daxin (9041416500); Li, Jinyu (35488220000); Wei, Furu (23995914700)",57340408800; 57221148819; 57221424487; 56181265100; 57245739400; 55986519600; 55552203600; 9041416500; 35488220000; 23995914700,VatLM: Visual-Audio-Text Pre-Training With Unified Masked Prediction for Speech Representation Learning,2024,IEEE Transactions on Multimedia,26,,,1055,1064,9,5,10.1109/TMM.2023.3275873,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159799804&doi=10.1109%2fTMM.2023.3275873&partnerID=40&md5=697560dac95cc6c7405b93249bd8ad12,"Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this article, we propose a unified cross-modal representation learning framework VatLM (Visual-Audio-Text Language Model). The proposed VatLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VatLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VatLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), and visual speech recognition (VSR) tasks. Results show that the proposed VatLM outperforms previous state-of-the-art models, such as the audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VatLM is capable of aligning different modalities into the same space.  © 1999-2012 IEEE.",Speech representation learning; unified masked prediction; visual-audio-text pre-training,Job analysis; Learning systems; Semantics; Speech recognition; Visual languages; Audio-visual; Pre-training; Predictive models; Representation learning; Simple++; Speech representation learning; Task analysis; Transformer; Unified masked prediction; Visual-audio-text pre-training; Forecasting,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85159799804
Wang W.; Song Y.; Jha S.,"Wang, Wenbin (58441094100); Song, Yang (55722226800); Jha, Sanjay (55658382400)",58441094100; 55722226800; 55658382400,USAT: A Universal Speaker-Adaptive Text-to-Speech Approach,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2590,2604,14,5,10.1109/TASLP.2024.3393714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191805606&doi=10.1109%2fTASLP.2024.3393714&partnerID=40&md5=c3d73dcd51aa48a918f76f8d603cb9d2,"Conventional text-to-speech (TTS) research has predominantly focused on enhancing the quality of synthesized speech for speakers in the training dataset. The challenge of synthesizing lifelike speech for unseen, out-of-dataset speakers, especially those with limited reference data, remains a significant and unresolved problem. While zero-shot or few-shot speaker-adaptive TTS approaches have been explored, they have many limitations. Zero-shot approaches tend to suffer from insufficient generalization performance to reproduce the voice of speakers with heavy accents. While few-shot methods can reproduce highly varying accents, they bring a significant storage burden and the risk of overfitting and catastrophic forgetting. Besides, most current evaluations of speaker-adaptive TTS are conducted only on datasets of native speakers. Our proposed framework unifies both zero-shot and few-shot speaker adaptation strategies, which we term as 'instant' and 'fine-grained' adaptations, respectively, based on their merits. To alleviate the insufficient generalization performance observed in zero-shot speaker adaptation, we designed two innovative discriminators and introduced a memory mechanism for the speech decoder. To prevent catastrophic forgetting and reduce storage implications for few-shot speaker adaptation, we designed two adapters and a unique adaptation procedure. Additionally, we introduce a new TTS dataset that encompasses 44,000 English utterances from 134 non-native speakers, capturing a wide array of non-native English accents. This dataset is intended to enhance holistic evaluations of adaptive TTS capabilities. Through comprehensive experiments on multiple datasets comprising both native and non-native speakers, our approach outperforms contemporary methodologies across various subjective and objective metrics.  © 2014 IEEE.",few-shot learning; speaker-adaptive; Text-to-speech; zero-shot learning,Digital storage; Speech enhancement; Speech recognition; Zero-shot learning; Adaptation models; Catastrophic forgetting; Few-shot learning; Generalization performance; Hidden-Markov models; Non-native speakers; Speaker adaptation; Speaker-adaptive; Text to speech; Timbre; Hidden Markov models,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85191805606
Soleymanpour M.; Johnson M.T.; Soleymanpour R.; Berry J.,"Soleymanpour, Mohammad (6504600348); Johnson, Michael T. (55723917600); Soleymanpour, Rahim (55598720900); Berry, Jeffrey (7402634840)",6504600348; 55723917600; 55598720900; 7402634840,Accurate synthesis of dysarthric Speech for ASR data augmentation,2024,Speech Communication,164,,103112,,,,0,10.1016/j.specom.2024.103112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202341211&doi=10.1016%2fj.specom.2024.103112&partnerID=40&md5=b3e89d586ddff2411083b3061a0f2f1a,"Dysarthria is a motor speech disorder often characterized by reduced speech intelligibility through slow, uncoordinated control of speech production muscles. Automatic Speech Recognition (ASR) systems can help dysarthric talkers communicate more effectively. However, robust dysarthria-specific ASR requires a significant amount of training speech, which is not readily available for dysarthric talkers. This paper presents a new dysarthric speech synthesis method for the purpose of ASR training data augmentation. Differences in prosodic and acoustic characteristics of dysarthric spontaneous speech at varying severity levels are important components for dysarthric speech modeling, synthesis, and augmentation. For dysarthric speech synthesis, a modified neural multi-talker TTS is implemented by adding a dysarthria severity level coefficient and a pause insertion model to synthesize dysarthric speech for varying severity levels. To evaluate the effectiveness for synthesis of training data for ASR, dysarthria-specific speech recognition was used. Results show that a DNN–HMM model trained on additional synthetic dysarthric speech achieves relative Word Error Rate (WER) improvement of 12.2 % compared to the baseline, and that the addition of the severity level and pause insertion controls decrease WER by 6.5 %, showing the effectiveness of adding these parameters. Overall results on the TORGO database demonstrate that using dysarthric synthetic speech to increase the amount of dysarthric-patterned speech for training has significant impact on the dysarthric ASR systems. In addition, we have conducted a subjective evaluation to evaluate the dysarthricness and similarity of synthesized speech. Our subjective evaluation shows that the perceived dysarthricness of synthesized speech is similar to that of true dysarthric speech, especially for higher levels of dysarthria. Audio samples are available at https://mohammadelc.github.io/SpeechGroupUKY/ © 2024 Elsevier B.V.",Data augmentation; Dysarthria; Speech recognition; Speech-to-text; Synthesized speech,Speech recognition; Automatic speech recognition; Automatic speech recognition system; Data augmentation; Dysarthria; Dysarthric talkers; Speech-to-text; Subjective evaluations; Synthesized speech; Training data; Word error rate,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85202341211
Yanagi Y.; Orihara R.; Tahara Y.; Sei Y.; Alumäe T.; Ohsuga A.,"Yanagi, Yuta (57219421823); Orihara, Ryohei (6506331343); Tahara, Yasuyuki (7101617617); Sei, Yuichi (56344492700); Alumäe, Tanel (8220354300); Ohsuga, Akihiko (6603722508)",57219421823; 6506331343; 7101617617; 56344492700; 8220354300; 6603722508,The Proposal of Countermeasures for DeepFake Voices on Social Media Considering Waveform and Text Embedding,2024,Annals of Emerging Technologies in Computing,8,2,,15,31,16,0,10.33166/AETiC.2024.02.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199091432&doi=10.33166%2fAETiC.2024.02.002&partnerID=40&md5=6059889b432fb17adf5dcaa0abcbcc5a,"In recent times, advancements in text-to-speech technologies have yielded more natural-sounding voices. However, this has also made it easier to generate malicious fake voices and disseminate false narratives. ASVspoof stands out as a prominent benchmark in the ongoing effort to automatically detect fake voices, thereby playing a crucial role in countering illicit access to biometric systems. Consequently, there is a growing need to broaden our perspectives, particularly when it comes to detecting fake voices on social media platforms. Moreover, existing detection models commonly face challenges related to their generalization performance. This study sheds light on specific instances involving the latest speech generation models. Furthermore, we introduce a novel framework designed to address the nuances of detecting fake voices in the context of social media. This framework considers not only the voice waveform but also the speech content. Our experiments have demonstrated that the proposed framework considerably enhances classification performance, as evidenced by the reduction in equal error rate. This underscores the importance of considering the waveform and the content of the voice when tasked with identifying fake voices and disseminating false claims. © 2023 by the author(s).",natural language processing; neural networks; speech synthesis; voice processing,Artificial intelligence; Biometrics; Fake detection; Natural language processing systems; Social networking (online); Speech processing; Speech recognition; Embeddings; Language processing; Natural language processing; Natural languages; Neural-networks; Social media; Speech technology; Text to speech; Voice processing; Waveforms; Speech synthesis,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85199091432
Kim W.-H.; Kim G.-W.; Kim J.-C.,"Kim, Woo-Hyeon (58971472400); Kim, Geon-Woo (59275667200); Kim, Joo-Chang (56691999100)",58971472400; 59275667200; 56691999100,Multi-Modal Deep Learning based Metadata Extensions for Video Clipping,2024,"International Journal on Advanced Science, Engineering and Information Technology",14,1,,375,380,5,1,10.18517/ijaseit.14.1.19047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191782953&doi=10.18517%2fijaseit.14.1.19047&partnerID=40&md5=eda0347a943758fa02b6c6a0a5f1542c,"General video search and recommendation systems primarily rely on metadata and personal information. Metadata includes file names, keywords, tags, and genres, among others, and is used to describe the video's content. The video platform assesses the relevance of user search queries to the video metadata and presents search results in order of highest relevance. Recommendations are based on videos with metadata judged to be similar to the one the user is currently watching. Most platforms offer search and recommendation services by employing separate algorithms for metadata and personal information. Therefore, metadata plays a vital role in video search. Video service platforms develop various algorithms to provide users with more accurate search results and recommendations. Quantifying video similarity is essential to enhance the accuracy of search results and recommendations. Since content producers primarily provide basic metadata, it can be abused. Additionally, the resemblance between similar video segments may diminish depending on its duration. This paper proposes a metadata expansion model that utilizes object recognition and Speech-to- Text (STT) technology. The model selects key objects by analyzing the frequency of their appearance in the video, extracts audio separately, transcribes it into text, and extracts the script. Scripts are quantified by tokenizing them into words using text-mining techniques. By augmenting metadata with key objects and script tokens, various video content search and recommendation platforms are expected to deliver results closer to user search terms and recommend related content. © IJASEIT is licensed under a Creative Commons Attribution-Share Alike 4.0 International License.",contextualized data; metadata; recommendation; speech recognition; Video Analysis,,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85191782953
Wang Z.; Xue L.; Kong Q.; Xie L.; Chen Y.; Tian Q.; Wang Y.,"Wang, Zhichao (57221097923); Xue, Liumeng (57206470348); Kong, Qiuqiang (57194871313); Xie, Lei (35294300000); Chen, Yuanzhe (57305371600); Tian, Qiao (57288834300); Wang, Yuping (57225157860)",57221097923; 57206470348; 57194871313; 35294300000; 57305371600; 57288834300; 57225157860,Multi-Level Temporal-Channel Speaker Retrieval for Zero-Shot Voice Conversion,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,2926,2937,11,2,10.1109/TASLP.2024.3407577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194860848&doi=10.1109%2fTASLP.2024.3407577&partnerID=40&md5=12e61abe566835e7963181e32986910a,"Zero-shot voice conversion (VC) converts source speech into the voice of any desired speaker using only one utterance of the speaker without requiring additional model updates. Typical methods use a speaker representation from a pre-trained speaker verification (SV) model or learn speaker representation during VC training to achieve zero-shot VC. However, existing speaker modeling methods overlook the variation of speaker information richness in temporal and frequency channel dimensions of speech. This insufficient speaker modeling hampers the ability of the VC model to accurately represent unseen speakers who are not in the training dataset. In this study, we present a robust zero-shot VC model with multi-level temporal-channel retrieval, referred to as MTCR-VC. Specifically, to flexibly adapt to the dynamic-variant speaker characteristic in the temporal and channel axis of the speech, we propose a novel fine-grained speaker modeling method, called temporal-channel retrieval (TCR), to find out when and where speaker information appears in speech. It retrieves variable-length speaker representation from both temporal and channel dimensions under the guidance of a pre-trained SV model. Besides, inspired by the hierarchical process of human speech production, the MTCR speaker module stacks several TCR blocks to extract speaker representations from multi-granularity levels. Furthermore, we introduce a cycle-based training strategy to simulate zero-shot inference recurrently to achieve better speech disentanglement and reconstruction. To drive this process, we adopt perceptual constraints on three aspects: content, style, and speaker. Experiments demonstrate that MTCR-VC is superior to the previous zero-shot VC methods in modeling speaker timbre while maintaining good speech naturalness.  © 2014 IEEE.",attention mechanism; temporal-channel retrieval; Voice conversion; zero-shot,Speech recognition; Zero-shot learning; Attention mechanisms; Channel dimension; Model method; Speaker model; Speaker verification; Temporal-channel retrieval; Timbre; Verification model; Voice conversion; Zero-shot; Speech processing,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85194860848
Hung J.-W.; Huang P.-C.; Li L.-Y.,"Hung, Jeih-Weih (7201963613); Huang, Pin-Chen (59380563700); Li, Li-Yin (59380418300)",7201963613; 59380563700; 59380418300,Employing Huber and TAP Losses to Improve Inter-SubNet in Speech Enhancement,2024,Future Internet,16,10,360,,,,0,10.3390/fi16100360,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207249784&doi=10.3390%2ffi16100360&partnerID=40&md5=4e748b25fe61e18b719ed950464941df,"In this study, improvements are made to Inter-SubNet, a state-of-the-art speech enhancement method. Inter-SubNet is a single-channel speech enhancement framework that enhances the sub-band spectral model by integrating global spectral information, such as cross-band relationships and patterns. Despite the success of Inter-SubNet, one crucial aspect probably overlooked by Inter-SubNet is the unequal perceptual weighting of different spectral regions by the human ear, as it employs MSE as its loss function. In addition, MSE loss has a potential convergence concern for model learning due to gradient explosion. Hence, we propose further enhancing Inter-SubNet by either integrating perceptual loss with MSE loss or modifying MSE loss directly in the learning process. Among various types of perceptual loss, we adopt the temporal acoustic parameter (TAP) loss, which provides detailed estimation for low-level acoustic descriptors, thereby offering a comprehensive evaluation of speech signal distortion. In addition, we leverage Huber loss, a combination of L1 and L2 (MSE) loss, to avoid the potential convergence issue for the training of Inter-SubNet. By the evaluation conducted on the VoiceBank-DEMAND database and task, we see that Inter-SubNet with the modified loss function reveals improvements in speech enhancement performance. Specifically, replacing MSE loss with Huber loss results in increases of 0.057 and 0.38 in WB-PESQ and SI-SDR metrics, respectively. Additionally, integrating TAP loss with MSE loss yields improvements of 0.115 and 0.196 in WB-PESQ and CSIG metrics. © 2024 by the authors.",Huber loss; Inter-SubNet; speech enhancement; TAP loss,Acoustic variables measurement; Image coding; Speech recognition; Acoustic parameters; Enhancement framework; Hub loss; Inter-subnet; Loss functions; Single channels; Speech enhancement methods; State of the art; Subnets; Temporal acoustic parameter loss; Speech enhancement,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85207249784
Larcher T.; Gritsch P.; Nastic S.; Ristov S.,"Larcher, Thomas (58728765700); Gritsch, Philipp (57947226200); Nastic, Stefan (55549511800); Ristov, Sashko (49561774300)",58728765700; 57947226200; 55549511800; 49561774300,BaaSLess: Backend-as-a-Service (BaaS)-Enabled Workflows in Federated Serverless Infrastructures,2024,IEEE Transactions on Cloud Computing,12,4,,1088,1102,14,1,10.1109/TCC.2024.3439268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200819093&doi=10.1109%2fTCC.2024.3439268&partnerID=40&md5=2b8a815d2479f546297e602f64bb302f,"Serverless is a popular paradigm for expressing compute-intensive applications as serverless workflows. In practice, a significant portion of the computing is typically offloaded to various Backend-as-a-Service (BaaS) cloud services. The recent rise of federated serverless and Sky computing offers cost and performance advantages for these BaaS-enabled serverless workflows. However, due to vendor lock-in and lack of service interoperability, many challenges remain that impact the development, deployment, and scheduling of BaaS-enabled serverless workflows in federated serverless infrastructures. This paper introduces BaaSLess - a novel platform that delivers global and dynamic federated BaaS to serverless workflows. BaaSLess provides: i) a novel SDK for uniform and dynamic access to federated BaaS services, reducing the complexity associated with the development of BaaS-enabled serverless workflows, ii) a novel globally-federated serverless BaaS framework that delivers a suite of BaaS-less ML services, including text-to-speech, speech-to-text, translation, and OCR, together with a globally-federated storage infrastructure, comprising AWS and Google cloud providers, and iii) a novel model and an algorithm for scheduling BaaS-enabled serverless workflows to improve their performance. Experimental results using three complementary BaaS-enabled serverless workflows show that BaaSLess improves workflow execution time by up to 2.95× compared to the state-of-the-art serverless schedulers, often at a lower cost.  © 2013 IEEE.",Backend-as-a-Service (BaaS); federation; optimization; SDK; serverless,Costs; Optical character recognition; Scheduling algorithms; Speech recognition; Backend-as-a-service; Code; Federation; Optimisations; Scheduling; SDK; Serverless; Serverless computing; Speech to text; Work-flows; Interoperability,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85200819093
Hu S.; Xie X.; Geng M.; Jin Z.; Deng J.; Li G.; Wang Y.; Cui M.; Wang T.; Meng H.; Liu X.,"Hu, Shujie (57478147400); Xie, Xurong (56413498600); Geng, Mengzhe (57211633811); Jin, Zengrui (57226893594); Deng, Jiajun (57688639300); Li, Guinan (57478019600); Wang, Yi (57211646836); Cui, Mingyu (57222708129); Wang, Tianzi (57218455343); Meng, Helen (7202279063); Liu, Xunying (13608959800)",57478147400; 56413498600; 57211633811; 57226893594; 57688639300; 57478019600; 57211646836; 57222708129; 57218455343; 7202279063; 13608959800,Self-Supervised ASR Models and Features for Dysarthric and Elderly Speech Recognition,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,3561,3575,14,3,10.1109/TASLP.2024.3422839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197515369&doi=10.1109%2fTASLP.2024.3422839&partnerID=40&md5=096673cfe863aba1f5c26c4e11cda346,"—Self-supervised learning (SSL) based speech foundation models have been applied to a wide range of ASR tasks. However, their application to dysarthric and elderly speech via data-intensive parameter fine-tuning is confronted by in-domain data scarcity and mismatch. To this end, this paper explores a series of approaches to integrate domain fine-tuned SSL pre-trained models and their features into TDNN and Conformer ASR systems for dysarthric and elderly speech recognition. These include: a) input feature fusion between standard acoustic frontends and domain fine-tuned SSL speech representations; b) frame-level joint decoding between TDNN systems separately trained using standard acoustic features alone and those with additional domain fine-tuned SSL features; and c) multi-pass decoding involving the TDNN/Conformer system outputs to be rescored using domain fine-tuned pre-trained ASR models. In addition, fine-tuned SSL speech features are used in acoustic-to-articulatory (A2A) inversion to construct multi-modal ASR systems. Experiments are conducted on four tasks: the English UASpeech and TORGO dysarthric speech corpora; and the English DementiaBank Pitt and Cantonese JCCOCC MoCA elderly speech datasets. The TDNN systems constructed by integrating domain-adapted HuBERT, wav2vec2-conformer or multi-lingual XLSR models and their features consistently outperform the standalone fine-tuned SSL pre-trained models. These systems produced statistically significant WER or CER reductions of 6.53%, 1.90%, 2.04% and 7.97% absolute (24.10%, 23.84%, 10.14% and 31.39% relative) on the four tasks respectively. Consistent improvements in Alzheimer’s Disease detection accuracy are also obtained using the DementiaBank Pitt elderly speech recognition outputs. © 2024 The Authors. This work is licensed under a Creative Commons Attribution 4.0 License.",Dysarthric Speech; Elderly Speech; HuBERT; Multi-lingual XLSR; Pre-trained ASR System; Wav2vec2.0,Decoding; Linguistics; Speech communication; Supervised learning; Data intensive; Dysarthric speech; Elderly speech; Fine tuning; Foundation models; HuBERT; Intensive parameters; Multi-lingual XLSR; Pre-trained ASR system; Wav2vec2.0; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85197515369
Han M.; Zhu D.; Wen X.; Shu L.; Yao Z.,"Han, Min (58896939600); Zhu, Dekun (58896947300); Wen, Xintong (58896945000); Shu, Liang (58896938000); Yao, Zhiwei (58896938100)",58896939600; 58896947300; 58896945000; 58896938000; 58896938100,Research on Dialect Protection: Interaction Design of Chinese Dialects Based on BLSTM-CRF and FBM Theories,2024,IEEE Access,12,,,22059,22071,12,1,10.1109/ACCESS.2024.3364098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185547975&doi=10.1109%2fACCESS.2024.3364098&partnerID=40&md5=aa1819d6ed73ece32398702756d8940c,"This study aims to augment the impact of dialect-related cultural elements among adolescents, thereby facilitating a more effective inheritance and progression of Chinese dialect culture within the contemporary socio-cultural milieu.Firstly, based on the LSTM architecture, the bidirectional long short-term memory-conditional random field (BLSTM-CRF) model framework for dialect recognition was optimized and established. Subsequently, employing the BLSTM-CRF model for dialect recognition and the acquisition of dialectal voice samples, the natural language sequence input underwent conversion into word vectors or character vectors. The word vectors or character vectors were input into the BLSTM layer to obtain the hidden state vector corresponding to each position, representing the context information of that position. The hidden state vector was input into a fully connected layer, and the CRF layer was adopted to calculate and find the highest scoring label sequence as the final prediction result. Secondly, the FBM theoretical model was utilized to analyze the target user needs through user research methods. Finally, a design framework oriented by dialect recognition and user needs was constructed, and interactive product design was carried out. On the basis of BLSTM-CRF, the product design framework is improved to complete the interactive application design. Integrating user needs and Long Short-Term Memory (LSTM) algorithms into the design of dialect products, we explore ways to preserve and disseminate dialect culture among the younger generation. This provides new insights for the digital inheritance and development of dialects, and also serves as a reference for the design process of related cultural apps.  © 2013 IEEE.",design; Dialect protection; FBM theory; long and short-term neural network algorithm; user needs,Brain; Character recognition; Human computer interaction; Job analysis; Long short-term memory; Natural language processing systems; Product design; Random processes; User centered design; User interfaces; Vectors; Adaptation models; Cultural aspects; Dialect protection; FBM theory; Interaction design; Language processing; Long and short-term neural network algorithm; Natural language processing; Natural languages; Neural networks algorithms; Predictive models; Symbol; Task analysis; Terms-neural network; Text recognition; User need; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85185547975
Guo Z.; Wang Q.; Du J.; Pan J.; Liu Q.-F.; Lee C.-H.,"Guo, Zilu (57931999000); Wang, Qing (56413198200); Du, Jun (18041858900); Pan, Jia (55611394400); Liu, Qing-Feng (55712232200); Lee, Chin-Hui (56004336700)",57931999000; 56413198200; 18041858900; 55611394400; 55712232200; 56004336700,A Variance-Preserving Interpolation Approach for Diffusion Models with Applications to Single Channel Speech Enhancement and Recognition,2024,IEEE/ACM Transactions on Audio Speech and Language Processing,32,,,3025,3038,13,4,10.1109/TASLP.2024.3407533,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195364720&doi=10.1109%2fTASLP.2024.3407533&partnerID=40&md5=02b7f5c7ed12fa368aa52a3105cfb6db,"In this paper, we propose a variance-preserving interpolation framework to improve diffusion models for single-channel speech enhancement (SE) and automatic speech recognition (ASR). This new variance-preserving interpolation diffusion model (VPIDM) approach requires only 25 iterative steps and obviates the need for a corrector, an essential element in the existing variance-exploding interpolation diffusion model (VEIDM). Two notable distinctions between VPIDM and VEIDM are the scaling function of the mean of state variables and the constraint imposed on the variance relative to the mean's scale. We conduct a systematic exploration of the theoretical mechanism underlying VPIDM, and develop insights regarding VPIDM's applications in SE and ASR using VPIDM as a frontend. Our proposed approach, evaluated on two distinct data sets, demonstrates VPIDM's superior performances over conventional discriminative SE algorithms. Furthermore, we assess the performance of the proposed model under varying signal-to-noise ratio (SNR) levels. The investigation reveals VPIDM's improved robustness in target noise elimination when compared to VEIDM. Furthermore, utilizing the mid-outputs of both VPIDM and VEIDM results in enhanced ASR accuracies, thereby highlighting the practical efficacy of our proposed approach.  © 2014 IEEE.",diffusion model; interpolating diffusion model; score-based; speech denoising; Speech enhancement,Acoustic noise; Diffusion; Iterative methods; Signal to noise ratio; Speech enhancement; Speech recognition; Automatic speech recognition; Diffusion model; Interpolating diffusion model; Noise; Noise measurements; Score-based; Single channels; Speech denoising; Task analysis; Interpolation,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85195364720
Abu-Nowar H.; Sait A.; Al-Hadhrami T.; Al-Sarem M.; Qasem S.N.,"Abu-Nowar, Haya (58904525400); Sait, Adeeb (58904652000); Al-Hadhrami, Tawfik (55243400100); Al-Sarem, Mohammed (43461087100); Qasem, Sultan Noman (34771853500)",58904525400; 58904652000; 55243400100; 43461087100; 34771853500,SENSES-ASD: a social-emotional nurturing and skill enhancement system for autism spectrum disorder,2024,PeerJ Computer Science,10,,e1792,,,,0,10.7717/peerj-cs.1792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185881576&doi=10.7717%2fpeerj-cs.1792&partnerID=40&md5=bf9ab9488120cea8ca9cacafc6ead9bc,"This article introduces the Social-Emotional Nurturing and Skill Enhancement System (SENSES-ASD) as an innovative method for assisting individuals with autism spectrum disorder (ASD). Leveraging deep learning technologies, specifically convolutional neural networks (CNN), our approach promotes facial emotion recognition, enhancing social interactions and communication. The methodology involves the use of the Xception CNN model trained on the FER-2013 dataset. The designed system accepts a variety of media inputs, successfully classifying and predicting seven primary emotional states. Results show that our system achieved a peak accuracy rate of 71% on the training dataset and 66% on the validation dataset. The novelty of our work lies in the intricate combination of deep learning methods specifically tailored for high-functioning autistic adults and the development of a user interface that caters to their unique cognitive and sensory sensitivities. This offers a novel perspective on utilising technological advances for ASD intervention, especially in the domain of emotion recognition. © 2024 Abu-Nowar et al. All Rights Reserved.",Algorithms and Analysis of Algorithms; Artificial; Autism spectrum disorder; Computer Vision; Convolutional neural network; Data Science; Deep learning; Emotion recognition; Human-Computer Interaction; Intelligence,Computer vision; Convolution; Deep learning; Diseases; Emotion Recognition; Human computer interaction; Learning systems; Speech recognition; User interfaces; Algorithm and analyse of algorithm; Analysis of algorithms; Artificial; Autism spectrum disorders; Convolutional neural network; Deep learning; Emotion recognition; Innovative method; Intelligence; Learning technology; Convolutional neural networks,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85185881576
Ko K.; Kim D.; Oh K.; Ko H.,"Ko, Kyungdeuk (57204679493); Kim, Donghyeon (56109754000); Oh, Kyungseok (58525611500); Ko, Hanseok (35069749800)",57204679493; 56109754000; 58525611500; 35069749800,WaveVC: Speech and Fundamental Frequency Consistent Raw Audio Voice Conversion,2024,Neural Processing Letters,56,4,166,,,,1,10.1007/s11063-024-11613-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192220801&doi=10.1007%2fs11063-024-11613-0&partnerID=40&md5=b67f77987cc3bdc22fbf48c07f804754,"Voice conversion (VC) is a task for changing the speech of a source speaker to the target voice while preserving linguistic information of the source speech. The existing VC methods typically use mel-spectrogram as both input and output, so a separate vocoder is required to transform mel-spectrogram into waveform. Therefore, the VC performance varies depending on the vocoder performance, and noisy speech can be generated due to problems such as train-test mismatch. In this paper, we propose a speech and fundamental frequency consistent raw audio voice conversion method called WaveVC. Unlike other methods, WaveVC does not require a separate vocoder and can perform VC directly on raw audio waveform using 1D convolution. This eliminates the issue of performance degradation caused by the train-test mismatch of the vocoder. In the training phase, WaveVC employs speech loss and F0 loss to preserve the content of the source speech and generate F0 consistent speech using the pre-trained networks. WaveVC is capable of converting voices while maintaining consistency in speech and fundamental frequency. In the test phase, the F0 feature of the source speech is concatenated with a content embedding vector to ensure the converted speech follows the fundamental frequency flow of the source speech. WaveVC achieves higher performances than baseline methods in both many-to-many VC and any-to-any VC. The converted samples are available online. © The Author(s) 2024.",Adversarial training; Deep learning; Voice conversion,Deep learning; Natural frequencies; Speech recognition; Vocoders; Adversarial training; Conversion methods; Deep learning; Fundamental frequencies; Input and outputs; Linguistic information; Performance; Spectrograms; Voice conversion; Waveforms; Spectrographs,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85192220801
Sen O.; Fuad M.; Islam M.D.N.; Rabbi J.; Masud M.; Hasan K.; Awal M.D.A.; Fime A.A.; Sikder D.; Iftee M.D.A.R.,"Sen, Ovishake (57222815675); Fuad, Mohtasim (57222815493); Islam, M.D. Nazrul (57220973650); Rabbi, Jakaria (56606267600); Masud, Mehedi (17338820600); Hasan, Kamrul (55058828800); Awal, M.D. Abdul (59024320500); Fime, Awal Ahmed (57222812624); Sikder, Delowar (57222813695); Iftee, M.D. Akil Raihan (57226305655)",57222815675; 57222815493; 57220973650; 56606267600; 17338820600; 55058828800; 59024320500; 57222812624; 57222813695; 57226305655,"Bangla natural language processing: A comprehensive analysis of classical, machine learning, and deep learning-based methods",2022,IEEE Access,10,,,38999,39044,45,39,10.1109/ACCESS.2022.3165563,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128284333&doi=10.1109%2fACCESS.2022.3165563&partnerID=40&md5=5eb4749ea8fb13cad005c11885e42286,"The Bangla language is the seventh most spoken language, with 265 million native and non-native speakers worldwide. However, English is the predominant language for online resources and technical knowledge, journals, and documentation. Consequently, many Bangla-speaking people, who have limited command of English, face hurdles to utilize English resources. To bridge the gap between limited support and increasing demand, researchers conducted many experiments and developed valuable tools and techniques to create and process Bangla language materials. Many efforts are also ongoing to make it easy to use the Bangla language in the online and technical domains. There are some review papers to understand the past, previous, and future Bangla Natural Language Processing (BNLP) trends. The studies are mainly concentrated on the specific domains of BNLP, such as sentiment analysis, speech recognition, optical character recognition, and text summarization. There is an apparent scarcity of resources that contain a comprehensive review of the recent BNLP tools and methods. Therefore, in this paper, we present a thorough analysis of 75 BNLP research papers and categorize them into 11 categories, namely Information Extraction, Machine Translation, Named Entity Recognition, Parsing, Parts of Speech Tagging, Question Answering System, Sentiment Analysis, Spam and Fake Detection, Text Summarization, Word Sense Disambiguation, and Speech Processing and Recognition. We study articles published between 1999 to 2021, and 50% of the papers were published after 2015. Furthermore, we discuss Classical, Machine Learning and Deep Learning approaches with different datasets while addressing the limitations and current and future trends of the BNLP. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Artificial neural network; Bangla natural language processing; Convolutional neural network; Gated recurrent unit; Long short-term memory; Sentiment analysis; Speech recognition; Support vector machine,Computational linguistics; Computer aided language translation; Data mining; Hidden Markov models; Job analysis; Learning algorithms; Long short-term memory; Neural machine translation; Sentiment analysis; Speech processing; Speech transmission; Bangla natural language processing; Convolutional neural network; Gated recurrent unit; Hidden-Markov models; Machine-learning; Sentiment analysis; Support vectors machine; Task analysis; Text Summarisation; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85128284333
Dalmia S.; Okhonko D.; Lewis M.; Edunov S.; Watanabe S.; Metze F.; Zettlemoyer L.; Mohamed A.,"Dalmia, Siddharth (57188986824); Okhonko, Dmytro (57218451461); Lewis, Mike (57677553600); Edunov, Sergey (57046063800); Watanabe, Shinji (35373073400); Metze, Florian (6505996325); Zettlemoyer, Luke (57204370628); Mohamed, Abdelrahman (56215658600)",57188986824; 57218451461; 57677553600; 57046063800; 35373073400; 6505996325; 57204370628; 56215658600,LegoNN: Building Modular Encoder-Decoder Models,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,3112,3126,14,6,10.1109/TASLP.2023.3296019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165302817&doi=10.1109%2fTASLP.2023.3296019&partnerID=40&md5=f7b1b3f4b02c352875caca541f42c3d4,"State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or automatic speech recognition (ASR)) are constructed and trained end-to-end as an atomic unit. No component of the model can be (re-)used without the others, making it impossible to share parts, e.g. a high resourced decoder, across tasks. We describe LegoNN, a procedure for building encoder-decoder architectures in a way so that its parts can be applied to other tasks without the need for any fine-tuning. To achieve this reusability, the interface between encoder and decoder modules is grounded to a sequence of marginal distributions over a pre-defined discrete vocabulary. We present two approaches for ingesting these marginals; one is differentiable, allowing the flow of gradients across the entire network, and the other is gradient-isolating. To enable the portability of decoder modules between MT tasks for different source languages and across other tasks like ASR, we introduce a modality agnostic encoder which consists of a length control mechanism to dynamically adapt encoders' output lengths in order to match the expected input length range of pre-trained decoders. We present several experiments to demonstrate the effectiveness of LegoNN models: a trained language generation LegoNN decoder module from German-English (De-En) MT task can be reused without any fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT tasks, matching or beating the performance of baseline. After fine-tuning, LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5% relative WER reduction on the Europarl ASR task. To show how the approach generalizes, we compose a LegoNN ASR model from three modules - each has been learned within different end-to-end trained models on three different datasets - achieving an overall WER reduction of 19.5%.  © 2014 IEEE.",encoder-decoder models; End-to-end; machine translation; modularity; speech recognition,Computational linguistics; Computer aided language translation; Decoding; Job analysis; Machine translation; Reusability; Signal encoding; Speech transmission; Decoding; Encoder-decoder; Encoder-decoder model; End to end; Machine translations; Modularity; Predictive models; Task analysis; Vocabulary; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85165302817
Atitallah A.B.; Ben Atitallah M.A.; Said Y.; Albekairi M.; Boudabous A.; Alanazi T.M.; Kaaniche K.; Atri M.,"Atitallah, Ahmed Ben (8576049700); Ben Atitallah, Mohamed Amin (57217948619); Said, Yahia (53867137900); Albekairi, Mohammed (57763190200); Boudabous, Anis (24470015100); Alanazi, Turki M. (57201554813); Kaaniche, Khaled (57217928562); Atri, Mohamed (23017853700)",8576049700; 57217948619; 53867137900; 57763190200; 24470015100; 57201554813; 57217928562; 23017853700,An Efficient Text Recognition System from Complex Color Image for Helping the Visually Impaired Persons,2023,Computer Systems Science and Engineering,46,1,,701,717,16,1,10.32604/csse.2023.035871,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147451178&doi=10.32604%2fcsse.2023.035871&partnerID=40&md5=539bd2d4f963dfa4d5ec3110c694c425,"The challenge faced by the visually impaired persons in their day-today lives is to interpret text from documents. In this context, to help these people, the objective of this work is to develop an efficient text recognition system that allows the isolation, the extraction, and the recognition of text in the case of documents having a textured background, a degraded aspect of colors, and of poor quality, and to synthesize it into speech. This system basically consists of three algorithms: a text localization and detection algorithm based on mathematical morphology method (MMM); a text extraction algorithm based on the gamma correction method (GCM); and an optical character recognition (OCR) algorithm for text recognition. A detailed complexity study of the different blocks of this text recognition system has been realized. Following this study, an acceleration of the GCM algorithm (AGCM) is proposed. The AGCM algorithm has reduced the complexity in the text recognition system by 70% and kept the same quality of text recognition as that of the original method. To assist visually impaired persons, a graphical interface of the entire text recognition chain has been developed, allowing the capture of images from a camera, rapid and intuitive visualization of the recognized text from this image, and text-to-speech synthesis. Our text recognition system provides an improvement of 6.8% for the recognition rate and 7.6% for the F-measure relative to GCM and AGCM algorithms. © 2023 CRL Publishing. All rights reserved.",AGCM; color images; GCM; graphical interface; OCR; Text recognition system,Color; Extraction; Optical character recognition; Speech recognition; Speech synthesis; Textures; Acceleration of the GCM algorithm; Colour image; Correction method; Gamma correction; Gamma correction method; Graphical interface; Recognition systems; Text recognition; Text recognition system; Visually impaired persons; Mathematical morphology,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85147451178
Nagano D.; Nakazawa K.,"Nagano, Daichi (58022462500); Nakazawa, Kazuo (7202920914)",58022462500; 7202920914,"Simultaneous Execution of Dereverberation, Denoising, and Speaker Separation Using a Neural Beamformer for Adapting Robots to Real Environments",2022,Journal of Robotics and Mechatronics,34,6,,1399,1410,11,0,10.20965/jrm.2022.p1399,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144285978&doi=10.20965%2fjrm.2022.p1399&partnerID=40&md5=6e09aa2eec3258fac7665d76de32cf34,"It remains challenging for robots to accurately per-form sound source localization and speech recognition in a real environment with reverberation, noise, and the voices of multiple speakers. Accordingly, we propose “U-TasNet-Beam,” a speech extraction method for extracting only the target speaker’s voice from all ambient sounds in a real environment. U-TasNet-Beam is a neural beamformer comprising three ele-ments: a neural network for removing reverberation and noise, a second neural network for separating the voices of multiple speakers, and a minimum variance distortionless response (MVDR) beamformer. Experiments with simulated data and recorded data show that the proposed U-TasNet-Beam can improve the accuracy of sound source localization and speech recognition in robots compared to the conventional methods in a noisy, reverberant, and multi-speaker environ-ment. In addition, we propose the spatial correlation matrix loss (SCM loss) as a loss function for the neural network learning the spatial information of the sound. By using the SCM loss, we can improve the speech extraction performance of the neural beamformer. © Fuji Technology Press Ltd.",communication robot; denoising; dereverberation; neural beamformer; speech extraction,Acoustic generators; Acoustic noise; Beamforming; Extraction; Robots; Speech communication; Speech recognition; Beam formers; Communication robot; De-noising; Dereverberation; Neural beamformer; Neural-networks; Real environments; Sound source localization; Spatial correlation matrix; Speech extraction; Reverberation,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144285978
Komatsu R.; Gao S.; Hou W.; Zhang M.; Tanaka T.; Toyoda K.; Kimura Y.; Hino K.; Iwamoto Y.; Mori K.; Okamoto T.; Shinozaki T.,"Komatsu, Ryota (57419670600); Gao, Shengzhou (57218453071); Hou, Wenxin (57218454558); Zhang, Mingxin (57221088680); Tanaka, Tomohiro (55624473409); Toyoda, Keisuke (57461605800); Kimura, Yusuke (57219284740); Hino, Kent (57219284569); Iwamoto, Yu (57536378700); Mori, Kosuke (57461745600); Okamoto, Takuma (16245575200); Shinozaki, Takahiro (18042804100)",57419670600; 57218453071; 57218454558; 57221088680; 55624473409; 57461605800; 57219284740; 57219284569; 57536378700; 57461745600; 16245575200; 18042804100,Automatic Spoken Language Acquisition Based on Observation and Dialogue,2022,IEEE Journal on Selected Topics in Signal Processing,16,6,,1480,1492,12,3,10.1109/JSTSP.2022.3189279,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134194969&doi=10.1109%2fJSTSP.2022.3189279&partnerID=40&md5=88b6715a0c38cb14b1a3411d7c9a9a63,"Human babies are born without knowledge of any specific language. They acquire language directly from observation and dialogue without being limited by the availability of labeled data. We propose spoken language acquisition agents that simulate the process. Such an ability requires multiple types of learning, including 1) word discovery, 2) symbol grounding, 3) message generation, and 4) pronunciation generation. Several studies have targeted one or combined learning types to elucidate human intelligence and aimed to equip spoken dialogue systems with human-like flexible language learning ability. However, their language ability was partially lacking some of the components. Our agents are the first to integrate them all. Our key concept is to design an architecture to integrate unsupervised, self-supervised, and reinforcement learning to utilize clues naturally existing in raw sensory signals and drive the learning based on the agent's intrinsic motivation. Experimental results show agents successfully acquire spoken language from scratch by interacting with an environment to act by speaking. Our proposed focusing mechanism significantly improves learning efficiency. We also demonstrate that our agents can learn neural vocoder and the concept of logical negation as a part of language acquisition.  © 2007-2012 IEEE.",Autonomous agent; reinforcement learning; self-supervised learning; spoken language acquisition; unsupervised learning,Autonomous agents; Digital storage; Reinforcement learning; Routing algorithms; Signal processing; Speech processing; Language acquisition; Reinforcement learnings; Routings; Self-supervised learning; Signal processing algorithms; Specific languages; Spoken language acquisition; Spoken languages; Vocabulary; Speech recognition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85134194969
Zhou Y.; Wu Z.; Zhang M.; Tian X.; Li H.,"Zhou, Yi (58940556100); Wu, Zhizheng (55500643500); Zhang, Mingyang (57193807130); Tian, Xiaohai (56022979300); Li, Haizhou (8615868400)",58940556100; 55500643500; 57193807130; 56022979300; 8615868400,TTS-Guided Training for Accent Conversion Without Parallel Data,2023,IEEE Signal Processing Letters,30,,,533,537,4,4,10.1109/LSP.2023.3270079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159721330&doi=10.1109%2fLSP.2023.3270079&partnerID=40&md5=0fdcacbf61b2ee160f836c353cd2b5b0,"Accent Conversion (AC) seeks to change the accent of speech from one (source) to another (target) while preserving the speech content and speaker identity. However, many existing AC approaches rely on source-target parallel speech data during training or reference speech at run-time. We propose a novel accent conversion framework without the need for either parallel data or reference speech. Specifically, a text-to-speech (TTS) system is first pretrained with target-accented speech data. This TTS model and its hidden representations are expected to be associated only with the target accent. Then, a speech encoder is trained to convert the accent of the speech under the supervision of the pretrained TTS model. In doing so, the source-accented speech and its corresponding transcription are forwarded to the speech encoder and the pretrained TTS, respectively. The output of the speech encoder is optimized to be the same as the text embedding in the TTS system. At run-time, the speech encoder is combined with the pretrained speech decoder to convert the source-accented speech toward the target. In the experiments, we converted English with two source accents (Chinese/Indian) to the target accent (American/British/Canadian). Both objective metrics and subjective listening tests successfully validate that the proposed approach generates speech samples that are close to the target accent with high speech quality.  © 1994-2012 IEEE.",Accent conversion (AC); text-to-speech (TTS),Data handling; Signal encoding; Speech recognition; Accent conversion; Accented speech; Decoding; Features extraction; Parallel data; Runtimes; Speech data; Text to speech; Text-to-speech; Text-to-speech system; Decoding,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85159721330
Koo S.; Park C.; Lee S.; Seo J.; Eo S.; Moon H.; Lim H.,"Koo, Seonmin (57877154200); Park, Chanjun (57217203519); Lee, Seolhwa (57191290060); Seo, Jaehyung (57226366837); Eo, Sugyeong (57226367768); Moon, Hyeonseok (57226355716); Lim, Heuiseok (36028297500)",57877154200; 57217203519; 57191290060; 57226366837; 57226367768; 57226355716; 36028297500,Uncovering the Risks and Drawbacks Associated with the Use of Synthetic Data for Grammatical Error Correction,2023,IEEE Access,11,,,95747,95756,9,5,10.1109/ACCESS.2023.3310257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169679091&doi=10.1109%2fACCESS.2023.3310257&partnerID=40&md5=fc256c38746de749d2b035129b6196a0,"In a Data-Centric AI paradigm, the model performance is enhanced without altering the model architecture, as evidenced by real-world and benchmark dataset demonstrations. With the advancements of large language models (LLM), it has become increasingly feasible to generate high-quality synthetic data, while considering the need to construct fully synthetic datasets for real-world data containing numerous personal information. However, in-depth validation of the solely synthetic data setting has yet to be conducted, despite the increased possibility of models trained exclusively on fully synthetic data emerging in the future. Therefore, we examined the question, 'Do data quality control techniques (known to positively impact data-centric AI) consistently aid models trained exclusively on synthetic datasets?'. To explore this query, we performed detailed analyses using synthetic datasets generated for speech recognition postprocessing using the BackTranScription (BTS) approach. Our study primarily addressed the potential adverse effects of data quality control measures (e.g., noise injection and balanced data) and training strategies in the context of synthetic-only experiments. As a result of the experiment, we observed the negative effect that the data-centric methodology drops by a maximum of 44.03 points in the fully synthetic data setting.  © 2013 IEEE.",balanced data; Korean grammatical error correction; noise injection; synthetic data,Benchmarking; Computational linguistics; Perturbation techniques; Quality control; Speech recognition; Balanced data; Chatbots; Data integrity; Errors correction; Grammatical errors; Korean grammatical error correction; Noise injection; Perturbation method; Synthetic data; Error correction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85169679091
Himawan I.; Wang R.; Sridharan S.; Fookes C.,"Himawan, Ivan (23967723700); Wang, Ruizhe (58017930400); Sridharan, Sridha (7102172665); Fookes, Clinton (7003338437)",23967723700; 58017930400; 7102172665; 7003338437,Jointly Trained Conversion Model With LPCNet for Any-to-One Voice Conversion Using Speaker-Independent Linguistic Features,2022,IEEE Access,10,,,134029,134037,8,1,10.1109/ACCESS.2022.3226350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144044189&doi=10.1109%2fACCESS.2022.3226350&partnerID=40&md5=55593968e8c4524ff06840768f65e8e8,"We propose a joint training scheme of an any-to-one voice conversion (VC) system with LPCNet to improve the speech naturalness, speaker similarity, and intelligibility of the converted speech. Recent advancements in neural-based vocoders, such as LPCNet, have enabled the production of more natural and clear speech. However, other components in typical VC systems are often designed independently, such as the conversion model. Hence, separate training strategies are used for each component that is not in direct correlation to the training objective of the vocoder preventing exploitation of the full potential of LPCNet. This problem is addressed by proposing a jointly trained conversion model and LPCNet. To accurately capture the linguistic contents of the given utterance, we use speaker-independent (SI) features derived from an automatic speech recognition (ASR) model trained using a mixed-language speech corpus. Subsequently, a conversion model maps the SI features to the acoustic representations used as input features to LPCNet. The possibility to synthesize cross-language speech using the proposed approach is also explored in this paper. Experimental results show that the proposed model can achieve real-time VC, unlocking the full potential of LPCNet and outperforming the state of the art.  © 2013 IEEE.",Automatic speech recognition; conversion model; joint training; neural vocoder; voice conversion,Linguistics; Speech communication; Speech intelligibility; Speech processing; Speech recognition; Automatic speech recognition; Conversion model; Conversion systems; Joint training; Linguistic features; Natural speech; Neural vocod; Speaker independents; Training schemes; Voice conversion; Vocoders,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144044189
Carofilis A.; Fernandez-Robles L.; Alegre E.; Fidalgo E.,"Carofilis, Andres (58498987200); Fernandez-Robles, Laura (36918096900); Alegre, Enrique (55901820900); Fidalgo, Eduardo (36092799000)",58498987200; 36918096900; 55901820900; 36092799000,MeWEHV: Mel and Wave Embeddings for Human Voice Tasks,2023,IEEE Access,11,,,80089,80104,15,1,10.1109/ACCESS.2023.3300973,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166743802&doi=10.1109%2fACCESS.2023.3300973&partnerID=40&md5=30d7283ccd4aea749912b4ddb12a9b58,"A recent trend in speech processing is the use of embeddings created through machine learning models trained on a specific task with large datasets. By leveraging the knowledge already acquired, these models can be reused in new tasks where the amount of available data is small. This paper proposes a pipeline to create a new model, called Mel and Wave Embeddings for Human Voice Tasks (MeWEHV), capable of generating robust embeddings for speech processing. MeWEHV combines the embeddings generated by a pre-trained raw audio waveform encoder model, and deep features extracted from Mel Frequency Cepstral Coefficients (MFCCs) using Convolutional Neural Networks (CNNs). We evaluate the performance of MeWEHV on three tasks: speaker, language, and accent identification. For the first one, we use the VoxCeleb1, and VBHIR datasets and present YouSpeakers204, a new and publicly available dataset for English speaker identification that contains 19607 audio clips from 204 persons speaking in six different accents, allowing other researchers to work with a very balanced dataset, and to create new models that are robust to multiple accents. For evaluating the language identification task, we use the VoxForge, Common Language, and the LRE17 datasets. Finally, for accent identification, we use the Latin American Spanish Corpora (LASC), Common Voice, and the NISP datasets. Our approach allows a significant increase in the performance of state-of-the-art embedding generation models on all the tested datasets, with a low additional computational cost.  © 2013 IEEE.",Embeddings; HuBERT; speech classification; WavLM; XLSR-Wav2Vec2; YouSpeakers204,Classification (of information); Computer architecture; Job analysis; Knowledge management; Learning systems; Neural networks; Speech processing; Adaptation models; Computational modelling; Embeddings; Features extraction; HuBERT; Human voice; Machine-learning; Speech classification; Task analysis; Transfer learning; WavLM; XLSR-wav2vec2; Youspeakers204; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85166743802
Ye J.-X.; Wen X.-C.; Wang X.-Z.; Xu Y.; Luo Y.; Wu C.-L.; Chen L.-Y.; Liu K.-H.,"Ye, Jia-Xin (57485104000); Wen, Xin-Cheng (57226098194); Wang, Xuan-Ze (57549853600); Xu, Yong (57188828963); Luo, Yan (57717020800); Wu, Chang-Li (57549853700); Chen, Li-Yan (35730725600); Liu, Kun-Hong (34973260300)",57485104000; 57226098194; 57549853600; 57188828963; 57717020800; 57549853700; 35730725600; 34973260300,GM-TCNet: Gated Multi-scale Temporal Convolutional Network using Emotion Causality for Speech Emotion Recognition,2022,Speech Communication,145,,,21,35,14,37,10.1016/j.specom.2022.07.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139868003&doi=10.1016%2fj.specom.2022.07.005&partnerID=40&md5=b2270e7d89c6f48640652e7a40dc0e53,"In human-computer interaction, Speech Emotion Recognition (SER) plays an essential role in understanding the user's intent and improving the interactive experience. While similar sentimental speeches own diverse speaker characteristics but share common antecedents and consequences, an essential challenge for SER is how to produce robust and discriminative representations through causality between speech emotions. In this paper, we propose a Gated Multi-scale Temporal Convolutional Network (GM-TCNet) to construct a novel emotional causality repre- sentation learning component with a multi-scale receptive field. GM-TCNet deploys a novel emotional causality representation learning component to capture the dynamics of emotion across the time domain, constructed with dilated causal convolutions layer and gating mechanism. Besides, it utilizes skip connection fusing high-level fea- tures from different Gated Convolution Blocks (GCB) to capture abundant and subtle emotion changes in human speech. GM-TCNet first uses a single type of feature, Mel-Frequency Cepstral Coefficients (MFCC), as inputs and then passes them through the Gated Temporal Convolutional Module (GTCM) to generate the high-level fea- tures. Finally, the features are fed to the emotion classifier to accomplish the SER task. The experimental results show that our model maintains the highest performance in most cases, with +0.90% to +18.50% and +0.55% to +20.15% average relative improvement on the weighted average recall and unweighted average recall compared to state-of-the-art techniques. The source code is available at: https://github.com/Jiaxin-Ye/GM-TCNet for SER. © 2022",Emotion Causality; Gating Mechanism; Multi-Scale; Speech Emotion Recognition; Temporal Convolution Network,Emotion Recognition; Human computer interaction; Speech recognition; Time domain analysis; Convolutional networks; Emotion causality; Gating mechanisms; Multi-scales; Receptive fields; Speaker characteristics; Speech emotion recognition; Speech emotions; Temporal convolution network; Time domain; Convolution,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85139868003
Iizuka T.; Mori H.,"Iizuka, Takahisa (57702600300); Mori, Hiroki (57214629379)",57702600300; 57214629379,How Does a Spontaneously Speaking Conversational Agent Affect User Behavior?,2022,IEEE Access,10,,,111042,111051,9,5,10.1109/ACCESS.2022.3214977,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140883832&doi=10.1109%2fACCESS.2022.3214977&partnerID=40&md5=6fa79e83cd5c4223784d9eb40d4f088d,"People treat conversational agents as mere tools and not as human-like social actors. While there has been much research on human-like agents, few studies have approached the realization of a conversational agent as a social actor from the viewpoint of speech synthesis. This study investigated the effect of synthetic voice of conversational agent trained with spontaneous speech on human interactants. Specifically, we hypothesized that humans will exhibit more social responses when interacting with conversational agent that has a synthetic voice built on spontaneous speech. Typically, speech synthesizers are built on a speech corpus where voice professionals read a set of written sentences. The synthesized speech is clear as if a newscaster were reading the news or a voice actor were playing an anime character. However, this is quite different from spontaneous speech we speak in everyday conversation. Recent advances in speech synthesis enabled us to build a speech synthesizer on a spontaneous speech corpus, and to obtain a near-conversational synthesized speech with reasonable quality. By making use of these technologies, we examined whether humans produce more social responses to a spontaneously speaking conversational agent. We conducted a large-scale conversation experiment with a conversational agent whose utterances were synthesized with the model trained either with spontaneous speech or read speech. The result showed that the subjects who interacted with the agent whose utterances were synthesized from spontaneous speech tended to show shorter response time and a larger number of backchannels. The result of a questionnaire showed that subjects who interacted with the agent whose utterances were synthesized from spontaneous speech tended to rate their conversation with the agent as closer to a human conversation. These results suggest that speech synthesis built on spontaneous speech is essential to realize a conversational agent as a social actor.  © 2013 IEEE.","Conversational agents; humanâÂ€Â""computer interaction; spontaneous speech synthesis",Behavioral research; Human computer interaction; Speech recognition; Conversational agents; Human like; Social actors; Speech synthesizer; Spontaneous speech; Spontaneous speech synthesis; Synthesised; Synthesized speech; Synthetic voices; User behaviors; Speech synthesis,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85140883832
Kim S.; Kim M.,"Kim, Sunwoo (57209886467); Kim, Minje (13204925700)",57209886467; 13204925700,"Boosted Locality Sensitive Hashing: Discriminative, Efficient, and Scalable Binary Codes for Source Separation",2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2659,2672,13,1,10.1109/TASLP.2022.3196877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135755271&doi=10.1109%2fTASLP.2022.3196877&partnerID=40&md5=9b3dc331673bebb5b385e6df051d17a2,"We propose a novel adaptive boosting approach to learn discriminative binary hash codes, boosted locality sensitive hashing (BLSH), that can represent audio spectra efficiently. We aim to use the learned hash codes in the single-channel speech denoising task by designing a nearest neighborhood search method that operates in the hashed feature space. To achieve the optimal denoising results given the highly compact binary feature representation, our proposed BLSH algorithm learns simple logistic regressors as the weak learners in an incremental way (i.e., one by one) so that each weak learner is trained to complement the mistake its predecessors have made. Upon testing, their binary classification results transform each spectrum of noisy speech into a bit string, where the bits are ordered based on their significance, adding scalability to the denoising system. Simple bitwise operations calculate Hamming distance to find the \boldsymbol{K}-nearest matching hashed frames in the dictionary of training noisy speech spectra, whose associated ideal binary masks are averaged to estimate the denoising mask for that test mixture. In contrast to the locality sensitive hashing method's random projections, our proposed supervised learning algorithm trains the projections such that the distance between the self-similarity matrix of the hash codes and that of the original spectra is minimized. Likewise, the process conceptually aligns to the Adaboost algorithm, although ours is specialized in learning binary features for source separation rather than classification. Experimental results on speech denoising suggest that the BLSH algorithm learns more discriminative representations than Fourier or mel spectra and the nonlinear kernels derived from them. Our compact binary representation is expected to facilitate model deployment onto resource-constrained environments, where comprehensive models (e.g., deep neural networks) are unaffordable.  © 2014 IEEE.",AdaBoost; Kernel Methods; Locality Sensitive Hashing; Speech Enhancement,Adaptive boosting; Audio signal processing; Codes (symbols); Deep neural networks; Hamming distance; Noise abatement; Optimization; Separation; Source separation; Speech recognition; Binary features; Code; De-noising; Hashing algorithms; Kernel-methods; Learn+; Locality sensitive hashing; Simple++; Spectra's; Speech denoising; Speech enhancement,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85135755271
Chen Y.-W.; Wang H.-M.; Tsao Y.,"Chen, Yu-Wen (57222274025); Wang, Hsin-Min (8297293300); Tsao, Yu (13608047100)",57222274025; 8297293300; 13608047100,BASPRO: A Balanced Script Producer for Speech Corpus Collection Based on the Genetic Algorithm,2023,APSIPA Transactions on Signal and Information Processing,12,3,e15,,,,0,10.1561/116.00000155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159203841&doi=10.1561%2f116.00000155&partnerID=40&md5=3efcd37e4c5c41b4db8e1bf631795ed1,"performance of speech-processing models is heavily influenced by the speech corpus that is used for training and evaluation. In this study, we propose BAlanced Script PROducer (BASPRO) system, which can automatically construct a phonetically balanced and rich set of Chinese sentences for collecting Mandarin Chinese speech data. First, we used pretrained natural language processing systems to extract ten-character candidate sentences from a large corpus of Chinese news texts. Then, we applied a genetic algorithm-based method to select 20 phonetically balanced sentence sets, each containing 20 sentences, from the candidate sentences. Using BASPRO, we obtained a recording script called TMNews, which contains 400 ten-character sentences. TMNews covers 84% of the syllables used in the real world. Moreover, the syllable distribution has 0.96 cosine similarity to the real-world syllable distribution. We converted the script into a speech corpus using two text-to-speech systems. Using the designed speech corpus, we tested the performances of speech enhancement (SE) and automatic speech recognition (ASR), which are one of the most important regression- and classification-based speech processing tasks, respectively. The experimental results show that the SE and ASR models trained on the designed speech corpus outperform their counterparts trained on a randomly composed speech corpus. © 2023 Cambridge University Press. All rights reserved.",Corpus design; genetic algorithm; Mandarin Chinese speech corpus; phonetically balanced and rich corpus; recording script design,Audio recordings; Natural language processing systems; Speech enhancement; Speech recognition; Chinese speech; Corpus design; Mandarin Chinese; Mandarin chinese speech corpus; Performance; Phonetically balanced; Phonetically balanced and rich corpus; Real-world; Recording script design; Speech corpora; Genetic algorithms,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85159203841
Ferrer L.; Castan D.; Mclaren M.; Lawson A.,"Ferrer, Luciana (7006111690); Castan, Diego (55350758700); Mclaren, Mitchell (25646483500); Lawson, Aaron (34969412300)",7006111690; 55350758700; 25646483500; 34969412300,A Discriminative Hierarchical PLDA-Based Model for Spoken Language Recognition,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2396,2410,14,4,10.1109/TASLP.2022.3190736,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135576715&doi=10.1109%2fTASLP.2022.3190736&partnerID=40&md5=aaa36c27cfb79b5a2f3be6c3a1329954,"Spoken language recognition (SLR) refers to the automatic process used to determine the language present in a speech sample. SLR is an important task in its own right, for example, as a tool to analyze or categorize large amounts of multi-lingual data. Further, it is also an essential tool for selecting downstream applications in a work flow, for example, to chose appropriate speech recognition or machine translation models. SLR systems are usually composed of two stages, one where an embedding representing the audio sample is extracted and a second one which computes the final scores for each language. In this work, we approach the SLR task as a detection problem and implement the second stage as a probabilistic linear discriminant analysis (PLDA) model. We show that discriminative training of the PLDA parameters gives large gains with respect to the usual generative training. Further, we propose a novel hierarchical approach where two PLDA models are trained, one to generate scores for clusters of highly-related languages and a second one to generate scores conditional to each cluster. The final language detection scores are computed as a combination of these two sets of scores. The complete model is trained discriminatively to optimize a cross-entropy objective. We show that this hierarchical approach consistently outperforms the non-hierarchical one for detection of highly related languages, in many cases by large margins. We train our systems on a collection of datasets including over 100 languages, and test them both on matched and mismatched conditions, showing that the gains are robust to condition mismatch.  © 2014 IEEE.",discriminative training; probabilistic linear discriminant analysis; Spoken language recognition,Audio systems; Discriminant analysis; Discriminant analysis model; Discriminative training; Downstream applications; Hierarchical approach; Large amounts; Machine translation models; Probabilistic linear discriminant analysis; Recognition systems; Spoken language recognition; Work-flows; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85135576715
Ambili A.R.; Roy R.C.,"Ambili, A.R. (57200418604); Roy, Rajesh Cherian (36940156400)",57200418604; 36940156400,The Effect of Synthetic Voice Data Augmentation on Spoken Language Identification on Indian Languages,2023,IEEE Access,11,,,102391,102407,16,6,10.1109/ACCESS.2023.3316142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173030860&doi=10.1109%2fACCESS.2023.3316142&partnerID=40&md5=3378ad9eb37133e162902f52b2438a43,"Multilingual based voice activated human computer interaction systems are currently in high demand. The Spoken Language Identification Unit (SPLID) is an inevitable front end unit of such a multilingual system. These systems will be a great boon to a country like India where around 24 official languages are spoken. Deep learning architectures for spoken language identification have progressed to the point that they can now perform well, even in the presence of various background noises. However, a strong phonetic relationship across various Indian languages leads to increased confusion in the SPLID unit. Therefore, the goal of this study is to propose a synthetic voice data augmentation method based on speech synthesis to improve the spoken Indian language identification system. Here the research attempts to determine how well pre-trained computer vision models recognize spoken languages in synthetic and classical audio augmentation environments. The accuracy of the models was compared using bottleneck features extracted from three different pre-trained models VGG16, RESNET50, and Inception-v3 while using an Artificial Neural Network (ANN), Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), Naive Bayes (NB), Decision Tree (DT) and KNN (K-Nearest Neighbors) as classifiers. The proposed system was tested on three Indian language datasets - two comprising seven Indian languages (Hindi, Malayalam, Tamil, Telugu, Marathi, Kannada and Bengali), one containing five Indian languages (Tamil, Hindi, Malayalam, Oria and Assamese), and on a foreign language dataset. It was found that the addition of synthetic audio samples improved the accuracy by 17%. Among the pre-trained models, VGG16 and Inception-v3 combined with PCA and ANN were found to have the maximum accuracy of 97%.  © 2013 IEEE.",Classifiers; data augmentation; inception-v3; RESNET50; speech synthesis; SPLID; VGG16,Classification (of information); Decision trees; Deep learning; Human computer interaction; Natural language processing systems; Nearest neighbor search; Neural networks; Speech recognition; Speech synthesis; Support vector machines; Classification algorithm; Computational modelling; Data augmentation; Inception-v3; Language identification; Neural-networks; Principal-component analysis; Residual neural network; RESNET50; Spoken language identification unit; Spoken languages; Support vectors machine; VGG16; Principal component analysis,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85173030860
Aggarwal S.; Vasukidevi G.; Selvakanmani S.; Pant B.; Kaur K.; Verma A.; Binegde G.N.,"Aggarwal, Shruti (57201487047); Vasukidevi, G. (57222657121); Selvakanmani, S. (56156565700); Pant, Bhaskar (35316278700); Kaur, Kiranjeet (57542481600); Verma, Amit (57304749700); Binegde, Geleta Negasa (58026126200)",57201487047; 57222657121; 56156565700; 35316278700; 57542481600; 57304749700; 58026126200,Audio Segmentation Techniques and Applications Based on Deep Learning,2022,Scientific Programming,2022,,7994191,,,,8,10.1155/2022/7994191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144493279&doi=10.1155%2f2022%2f7994191&partnerID=40&md5=231780439a6c2155777f6972130a1227,"Audio processing has become an inseparable part of modern applications in domains ranging from health care to speech-controlled devices. In automated audio segmentation, deep learning plays a vital role. In this article, we are discussing audio segmentation based on deep learning. Audio segmentation divides the digital audio signal into a sequence of segments or frames and then classifies these into various classes such as speech recognition, music, or noise. Segmentation plays an important role in audio signal processing. The most important aspect is to secure a large amount of high-quality data when training a deep learning network. In this study, various application areas, citation records, documents published year-wise, and source-wise analysis are computed using Scopus and Web of Science (WoS) databases. The analysis presented in this paper supports and establishes the significance of the deep learning techniques in audio segmentation. Copyright © 2022 Shruti Aggarwal et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.",,Audio acoustics; Audio signal processing; Deep learning; Learning systems; Music; Application area; Audio processing; Audio segmentation; Digital audio signals; High quality data; Large amounts; Learning network; Modern applications; Segmentation techniques; Web of Science; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144493279
Park H.-Y.; Lee Y.H.; Chun C.,"Park, Hwa-Young (58764609000); Lee, Young Han (57203798688); Chun, Chanjun (36026528300)",58764609000; 57203798688; 36026528300,Perturbation AUTOVC: Voice Conversion From Perturbation and Autoencoder Loss,2023,IEEE Access,11,,,140174,140185,11,1,10.1109/ACCESS.2023.3341434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179782544&doi=10.1109%2fACCESS.2023.3341434&partnerID=40&md5=b4fb007a9898b10141b864cd2a5deb91,"AUTOVC is a voice-conversion method that performs self-reconstruction using an autoencoder structure for zero-shot voice conversion. AUTOVC has the advantage of being easy and simple to learn because it only uses the autoencoder loss for learning. However, it performs voice conversion by disentangling speech information from speakers and linguistic information by adjusting the bottleneck dimension; this requires highly meticulous fine tuning of the bottleneck dimension and involves a tradeoff between speech quality and speaker similarity. To address these issues, neural analysis and synthesis (NANSY) - a fully self-supervised learning system that uses perturbations to extract speech features - is proposed. NANSY solves the problem of the adjustment of the bottleneck dimension by utilizing perturbation and exhibits high-reconstruction performance. In this study, we propose perturbation AUTOVC, a voice conversion method that utilizes the structure of AUTOVC and the perturbation of NANSY. The proposed method applies perturbations to speech signals (such as NANSY signals) to solve the problem of the voice conversion method using bottleneck dimensions. Perturbation is applied to remove the speaker-dependent information present in the speech, leaving only the linguistic information, which is then passed through a content encoder and modeled as a content embedding containing only the linguistic information. To obtain speaker information, we used x-vectors, which are extensively used in pretrained speaker recognition. The concatenated linguistic and speaker information extracted from the encoder and additional energy information is used as input to the decoder to perform self-reconstruction. Similar to AUTOVC, it is easy and simple to learn using only the autoencoder loss. For the evaluation, we measured three objective evaluation metrics: character error rate (%), cosine similarity, and short-time objective intelligibility, as well as a subjective evaluation metric: mean opinion score. The experimental results demonstrate that our proposed method outperforms other voice conversion techniques and demonstrated robust performance in zero-shot conversion.  © 2013 IEEE.",Autoencoder; information perturbation; speech signal processing; voice conversion,Audio signal processing; Learning systems; Linguistics; Perturbation techniques; Signal encoding; Speech communication; Speech intelligibility; Speech recognition; Analysis and synthesis; Auto encoders; Conversion methods; Features extraction; Information perturbation; Linguistic information; Perturbation method; Speaker recognition; Speech signal processing; Voice conversion; Data mining,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85179782544
Mahum R.; Irtaza A.; Javed A.,"Mahum, Rabbia (57224467375); Irtaza, Aun (54882450900); Javed, Ali (57190125008)",57224467375; 54882450900; 57190125008,EDL-Det: A Robust TTS Synthesis Detector Using VGG19-Based YAMNet and Ensemble Learning Block,2023,IEEE Access,11,,,134701,134716,15,11,10.1109/ACCESS.2023.3332561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177061259&doi=10.1109%2fACCESS.2023.3332561&partnerID=40&md5=53dbd872d661031c6577a7bfe78b9a38,"Various audio deep fake synthesis algorithms exist, such as deep voice, tacotron, fastspeech, and imitation techniques. Despite the existence of various spoofing speech detectors, they are not ready to distinguish unseen audio samples with high precision. In this study, we suggest a robust model, namely an Ensemble Deep Learning Detector (EDL-Det), to detect text-to-speech (TTS) and categorize it into spoofed and bonafide classes. Our proposed model is an improved method based on Yet Another Multi-scale Convolutional Neural Network (YAMNet) employing VGG19 as a base network combined with two other deep learning(DL) techniques. Our proposed system effectively analyzes the audio to extract better artifacts. We have added an ensemble learning block that consists of ResNet50 and InceptionNetv2. First, we convert speech into mel-spectrograms that consist of time-frequency representations. Second, we train our model using the ASVspoof-2019 dataset. Ultimately, we classified the audios, transforming them into mel-spectrograms using our trained binary classifier and a majority voting scheme by three networks. Due to ensemble architecture, our proposed model effectively extracts the most representative features from the mel-spectrograms. Furthermore, we have performed extensive experiments to assess the performance of the suggested model using the ASVspoof 2019 corpus. Additionally, our proposed model is robust enough to identify the unseen spoofed audios and accurately classify the attacks based on cloning algorithms.  © 2013 IEEE.",Deep learning; DeepFake audios; fake speech; mel-spectrograms; text-to-speech detection; VGG19,Character recognition; Cloning; Convolution; Deep learning; Electronic mail; Hidden Markov models; Network architecture; Neural networks; Spectrographs; Speech recognition; Text messaging; Convolutional neural network; Deep learning; Deepfake; Deepfake audio; Electronic messaging; Fake speech; Features extraction; Hidden-Markov models; Mel-spectrogram; Spectrograms; Speech detection; Text analysis; Text to speech; Text-to-speech detection; VGG19; Feature extraction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85177061259
Mukhamadiyev A.; Khujayarov I.; Djuraev O.; Cho J.,"Mukhamadiyev, Abdinabi (57209690294); Khujayarov, Ilyos (57215926099); Djuraev, Oybek (36650571500); Cho, Jinsoo (55474141500)",57209690294; 57215926099; 36650571500; 55474141500,Automatic Speech Recognition Method Based on Deep Learning Approaches for Uzbek Language,2022,Sensors,22,10,3683,,,,52,10.3390/s22103683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130092279&doi=10.3390%2fs22103683&partnerID=40&md5=aea2ecc4c55f1b7fc3b57ac15b0e8f9e,"Communication has been an important aspect of human life, civilization, and globalization for thousands of years. Biometric analysis, education, security, healthcare, and smart cities are only a few examples of speech recognition applications. Most studies have mainly concentrated on English, Spanish, Japanese, or Chinese, disregarding other low-resource languages, such as Uzbek, leaving their analysis open. In this paper, we propose an End-To-End Deep Neural Network-Hidden Markov Model speech recognition model and a hybrid Connectionist Temporal Classification (CTC)-attention network for the Uzbek language and its dialects. The proposed approach reduces training time and improves speech recognition accuracy by effectively using CTC objective function in attention model training. We evaluated the linguistic and lay-native speaker performances on the Uzbek language dataset, which was collected as a part of this study. Experimental results show that the proposed model achieved a word error rate of 14.3% using 207 h of recordings as an Uzbek language training dataset. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",convolutional neural network; CTC-attention; deep learning; end-to-end speech recognition; hidden Markov model; transformers; Uzbek language,Deep Learning; Humans; Language; Linguistics; Speech; Speech Perception; Classification (of information); Convolutional neural networks; Deep neural networks; Speech; Speech recognition; Automatic speech recognition; Connectionist temporal classification-attention; Convolutional neural network; Deep learning; End to end; End-to-end speech recognition; Hidden-Markov models; Temporal classification; Transformer; Uzbek language; human; language; linguistics; speech; speech perception; Hidden Markov models,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85130092279
Novitasari S.; Sakti S.; Nakamura S.,"Novitasari, Sashi (57205401384); Sakti, Sakriani (12807978200); Nakamura, Satoshi (55628545896)",57205401384; 12807978200; 55628545896,A Machine Speech Chain Approach for Dynamically Adaptive Lombard TTS in Static and Dynamic Noise Environments,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2673,2688,15,8,10.1109/TASLP.2022.3196879,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135766291&doi=10.1109%2fTASLP.2022.3196879&partnerID=40&md5=fb3d90192b72f379071fdd2a14a47a0f,"Recent end-to-end text-to-speech synthesis (TTS) systems have successfully synthesized high-quality speech. However, TTS speech intelligibility degrades in noisy environments because most of these systems were not designed to handle noisy environments. Several works attempted to address this problem by using offline fine-tuning to adapt their TTS to noisy conditions. Unlike machines, humans never perform offline fine-tuning. Instead, they speak with the Lombard effect in noisy places, where they dynamically adjust their vocal effort to improve the audibility of their speech. This ability is supported by the speech chain mechanism, which involves auditory feedback passing from speech perception to speech production. This paper proposes an alternative approach to TTS in noisy environments that is closer to the human Lombard effect. Specifically, we implement Lombard TTS in a machine speech chain framework to synthesize speech with dynamic adaptation. Our TTS performs adaptation by generating speech utterances based on the auditory feedback that consists of the automatic speech recognition (ASR) loss as the speech intelligibility measure and the speech-to-noise ratio (SNR) prediction as power measurement. Two versions of TTS are investigated: non-incremental TTS with utterance-level feedback and incremental TTS (ITTS) with short-term feedback to reduce the delay without significant performance loss. Furthermore, we evaluate the TTS systems in both static and dynamic noise conditions. Our experimental results show that auditory feedback enhanced the TTS speech intelligibility in noise.  © 2014 IEEE.",dynamic adaptation; Lombard effect; machine speech chain inference; Text-to-speech,Chains; Hidden Markov models; Mechanisms; Speech enhancement; Speech intelligibility; Speech recognition; Speech synthesis; Tuning; Auditory feedback; Dynamic adaptations; Hidden-Markov models; Lombard effects; Machine speech chain inference; Noise measurements; Noisy environment; Static noise; Statics and dynamics; Text to speech; Signal to noise ratio,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85135766291
Serai P.; Sunder V.; Fosler-Lussier E.,"Serai, Prashant (57209884266); Sunder, Vishal (57208495885); Fosler-Lussier, Eric (8578109100)",57209884266; 57208495885; 8578109100,Hallucination of Speech Recognition Errors with Sequence to Sequence Learning,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,890,900,10,15,10.1109/TASLP.2022.3145313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123786822&doi=10.1109%2fTASLP.2022.3145313&partnerID=40&md5=dcef09f284698b261449e49368c83fe8,"Prior work in this domain has focused on modeling errors at the phonetic level, while using a lexicon to convert the phones to words, usually accompanied by an FST Language model. We present novel end-to-end models to directly predict hallucinated ASR word sequence outputs, conditioning on an input word sequence as well as a corresponding phoneme sequence. This improves prior published results for recall of errors from an in-domain ASR system's transcription of unseen data, as well as an out-of-domain ASR system's transcriptions of audio from an unrelated task, while additionally exploring an in-between scenario when limited characterization data from the test ASR system is obtainable. To verify the extrinsic validity of the method, we also use our hallucinated ASR errors to augment training for a spoken question classifier, finding that they enable robustness to real ASR errors in a downstream task, when scarce or even zero task-specific audio was available at train-time.  © 2014 IEEE.",Error prediction; Hallucinated asr errors; Low resource; Sequence to sequence neural networks; Speech recognition,Character recognition; Errors; Job analysis; Modeling languages; Neural networks; Speech; Automatic speech recognition; Context models; Error prediction; Hallucinated automatic speech recognition error; Low resource; Neural-networks; Predictive models; Recognition error; Sequence to sequence neural network; Task analysis; Text recognition; Speech recognition,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85123786822
Xu X.; Shi L.; Chen X.; Lin P.; Lian J.; Chen J.; Zhang Z.; Hancock E.R.,"Xu, Xuexin (57219204043); Shi, Liang (57199772885); Chen, Xunquan (57343720500); Lin, Pingyuan (57343140700); Lian, Jie (57265078300); Chen, Jinhui (55925218800); Zhang, Zhihong (55644004034); Hancock, Edwin R. (7202876234)",57219204043; 57199772885; 57343720500; 57343140700; 57265078300; 55925218800; 55644004034; 7202876234,Any-to-Any Voice Conversion With Multi-Layer Speaker Adaptation and Content Supervision,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,3431,3445,14,1,10.1109/TASLP.2023.3306716,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168735615&doi=10.1109%2fTASLP.2023.3306716&partnerID=40&md5=c2a933550e17e196b5e64cf41a3c4595,"Any-to-any voice conversion can be performed among arbitrary speakers, even with a single reference utterance. Many related studies have demonstrated that it can be effectively implemented by speech representation disentanglement. However, most existing solutions fuse the speaker representations into the content features globally without considering their distribution difference. Additionally, in the any-to-any scenario, there is no effective method ensuring the consistency of linguistic content without text transcription or additional information extracted from additional modules (e.g., automatic speech recognition). Hence, to alleviate the above problems, this paper proposes SACS-VC, a novel any-to-any voice conversion method that combines two principal modules: Speaker Adaptation and Content Supervision. Specifically, we rearrange the timbre representations according to the content distribution using a temporal attention mechanism to obtain finer-grained speaker timbre information for each content feature. Meanwhile, we associate the converted outputs and source utterances directly to supervise the consistency of the semantic content in an unsupervised manner. This is achieved using contrastive learning based on the corresponding and non-corresponding locations of content features. It should be noted that SACS-VC can be implemented using a non-parallel speech corpus without any pertaining. The experimental results demonstrate that the proposed method outperforms current state-of-the-art any-to-any voice conversion systems in objective and subjective evaluation settings. © 2014 IEEE.",attention mechanism; contrastive learning; feature disentanglement; Voice conversion,Character recognition; Speech processing; Speech recognition; Attention mechanisms; Automatic speech recognition; Contrastive learning; Feature disentanglement; Features extraction; Multi-layers; Mutual informations; Speaker adaptation; Timbre; Voice conversion; Semantics,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85168735615
Melnik-Leroy G.A.; Bernatavičiene J.; Korvel G.; Navickas G.; Tamulevičius G.; Treigys P.,"Melnik-Leroy, Gerda Ana (57205330081); Bernatavičiene, Jolita (14010111600); Korvel, Grazina (56719707600); Navickas, Gediminas (58112818200); Tamulevičius, Gintautas (36562459300); Treigys, Povilas (12807782400)",57205330081; 14010111600; 56719707600; 58112818200; 36562459300; 12807782400,An Overview of Lithuanian Intonation: A Linguistic and Modelling Perspective,2022,Informatica (Netherlands),33,4,,795,832,37,7,10.15388/22-INFOR502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148598339&doi=10.15388%2f22-INFOR502&partnerID=40&md5=2f5e739094b6c72e97a920d829c9db3e,"Intonation is a complex suprasegmental phenomenon essential for speech processing. However, it is still largely understudied, especially in the case of under-resourced languages, such as Lithuanian. The current paper focuses on intonation in Lithuanian, a Baltic pitch-accent language with free stress and tonal variations on accented heavy syllables. Due to historical circumstances, the description and analysis of Lithuanian intonation were carried out within different theoretical frameworks and in several languages, which makes them hardly accessible to the international research community. This paper is the first attempt to gather research on Lithuanian intonation from both the Lithuanian and the Western traditions, the structuralist and generativist points of view, and the linguistic and modelling perspectives. The paper identifies issues in existing research that require special attention and proposes directions for future investigations both in linguistics and modelling. © 2022 Vilnius University.",intonation; intonation modelling; Lithuanian; Pitch accent; speech recognition; stress; under-resourced language,Continuous speech recognition; Linguistics; Speech processing; 'current; International researches; Intonation; Intonation modeling; Lithuanian; Pitch accents; Research communities; Theoretical framework; Under-resourced languages; Modeling languages,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85148598339
Sheng C.; Liu L.; Deng W.; Bai L.; Liu Z.; Lao S.; Kuang G.; Pietikainen M.,"Sheng, Changchong (57201460334); Liu, Li (57054497500); Deng, Wanxia (57190974421); Bai, Liang (37861161000); Liu, Zhong (57291529100); Lao, Songyang (7004254256); Kuang, Gangyao (7006353231); Pietikainen, Matti (7006291661)",57201460334; 57054497500; 57190974421; 37861161000; 57291529100; 7004254256; 7006353231; 7006291661,Importance-Aware Information Bottleneck Learning Paradigm for Lip Reading,2023,IEEE Transactions on Multimedia,25,,,6563,6574,11,6,10.1109/TMM.2022.3210761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139481343&doi=10.1109%2fTMM.2022.3210761&partnerID=40&md5=180c04d133a0bc80d8b80bb9c36f6531,"Lip reading is the task of decoding text from speakers' mouth movements. Numerous deep learning-based methods have been proposed to address this task. However, these existing deep lip reading models suffer from poor generalization due to overfitting the training data. To resolve this issue, we present a novel learning paradigm that aims to improve the interpretability and generalization of lip reading models. In specific, a Variational Temporal Mask (VTM) module is customized to automatically analyze the importance of frame-level features. Furthermore, the prediction consistency constraints of global information and local temporal important features are introduced to strengthen the model generalization. We evaluate the novel learning paradigm with multiple lip reading baseline models on the LRW and LRW-1000 datasets. Experiments show that the proposed framework significantly improves the generalization performance and interpretability of lip reading models.  © 2022 IEEE.",Deep learning; information bottleneck; lip reading; visual speech recognition,Character recognition; Deep learning; Hidden Markov models; Job analysis; Speech recognition; Deep learning; Features extraction; Hidden-Markov models; Information bottleneck; Learning paradigms; Lip; Lip reading; Noise measurements; Task analysis; Visual speech recognition; Feature extraction,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85139481343
Wang R.; Wei Z.; Duan H.; Ji S.; Long Y.; Hong Z.,"Wang, Rui (57226734635); Wei, Zhihua (36653333200); Duan, Haoran (57210123129); Ji, Shouling (36918358000); Long, Yang (57089680300); Hong, Zhen (57204328582)",57226734635; 36653333200; 57210123129; 36918358000; 57089680300; 57204328582,EfficientTDNN: Efficient Architecture Search for Speaker Recognition,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2267,2279,12,14,10.1109/TASLP.2022.3182856,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132634329&doi=10.1109%2fTASLP.2022.3182856&partnerID=40&md5=131d6b512219b3e336ae37c8cf424541,"Convolutional neural networks (CNNs), such as the time-delay neural network (TDNN), have shown their remarkable capability in learning speaker embedding. However, they meanwhile bring a huge computational cost in storage size, processing, and memory. Discovering the specialized CNN that meets a specific constraint requires a substantial effort of human experts. Compared with hand-designed approaches, neural architecture search (NAS) appears as a practical technique in automating the manual architecture design process and has attracted increasing interest in spoken language processing tasks such as speaker recognition. In this paper, we propose EfficientTDNN, an efficient architecture search framework consisting of a TDNN-based supernet and a TDNN-NAS algorithm. The proposed supernet introduces temporal convolution of different ranges of the receptive field and feature aggregation of various resolutions from different layers to TDNN. On top of it, the TDNN-NAS algorithm quickly searches for the desired TDNN architecture via weight-sharing subnets, which surprisingly reduces computation while handling the vast number of devices with various resources requirements. Experimental results on the VoxCeleb dataset show the proposed EfficientTDNN enables approximate 1013 architectures concerning depth, kernel, and width. Considering different computation constraints, it achieves a 2.20% equal error rate (EER) with 204 M multiply-accumulate operations (MACs), 1.41% EER with 571 M MACs as well as 0.94% EER with 1.45 G MACs. Comprehensive investigations suggest that the trained supernet generalizes subnets not sampled during training and obtains a favorable trade-off between accuracy and efficiency.  © 2014 IEEE.",Efficient search; neural architecture search; progressive learning; speaker recognition; time-delay neural network,Computer architecture; Economic and social effects; Network architecture; Neural networks; Speech recognition; Supernovae; Time delay; Efficient search; Equal error rate; Kernel; Neural architecture search; Neural architectures; Neural-networks; Progressive learning; Speaker recognition; Time delay neural networks; Convolution,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85132634329
Niu Z.,"Niu, Zhenyu (57828378500)",57828378500,Voice Detection and Deep Learning Algorithms Application in Remote English Translation Classroom Monitoring,2022,Mobile Information Systems,2022,,3340999,,,,3,10.1155/2022/3340999,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135284239&doi=10.1155%2f2022%2f3340999&partnerID=40&md5=4c5badf25581d17700045341a649f085,"With the continuous development of cellular networks, the traffic from voice services increases gradually. The wireless sensor network (WSN) is a distributed network consisting of a large number of peripheral nodes distributed in the surveillance area. The nodes in the network complete it in a self-organizing form, and the sink node collects the data from each sensor node. When sending data, the nodes near the receiver will quickly run out of energy and cannot perform further transmission tasks. The resulting ""power supply emptiness""problem has a great impact on network performance. Therefore, the power consumption of the network must be considered when designing the WSN routing algorithm. In order to effectively improve students' academic performance and monitor students' teaching conditions, the classroom remote monitoring system places two cameras in the university's English translation classroom and uses technology to merge the information to execute the entire process. By recording the course, we can save the teacher's classroom content and the student's classroom performance and upload the recorded video in real time. In addition, the classroom remote monitoring system is a multiclient system, divided into teacher and student terminals. The user can log in, watch the video, and perform other necessary operations. © 2022 Zhenyu Niu.",,Curricula; Deep learning; Monitoring; Remote control; Sensor nodes; Speech recognition; Teaching; Cellular network; Continuous development; Distributed networks; Peripheral nodes; Remote monitoring system; Self-organising; Sink nodes; Teachers'; Voice detection; Voice services; Students,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85135284239
Han T.; Yang Q.; Shi Z.; He S.; Zhang Z.,"Han, Tianxiao (57456650900); Yang, Qianqian (55695176800); Shi, Zhiguo (8446518800); He, Shibo (55837432600); Zhang, Zhaoyang (55701343800)",57456650900; 55695176800; 8446518800; 55837432600; 55701343800,Semantic-Preserved Communication System for Highly Efficient Speech Transmission,2023,IEEE Journal on Selected Areas in Communications,41,1,,245,259,14,112,10.1109/JSAC.2022.3221952,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142787226&doi=10.1109%2fJSAC.2022.3221952&partnerID=40&md5=b5969222ccd097aa8799d3994fcf7724,"Deep learning (DL) based semantic communication methods have been explored for the efficient transmission of images, text, and speech in recent years. In contrast to traditional wireless communication methods that focus on the transmission of abstract symbols, semantic communication approaches attempt to achieve better transmission efficiency by only sending the semantic-related information of the source data. In this paper, we consider semantic-oriented speech transmission which transmits only the semantic-relevant information over the channel for the speech recognition task, and a compact additional set of semantic-irrelevant information for the speech reconstruction task. We propose a novel end-to-end DL-based transceiver which extracts and encodes the semantic information from the input speech spectrums at the transmitter and outputs the corresponding transcriptions from the decoded semantic information at the receiver. In particular, we employ a soft alignment module and a redundancy removal module to extract only the text-related semantic features while dropping semantically redundant content, greatly reducing the amount of semantic redundancy compared to existing methods. We also propose a semantic correction module to further correct the predicted transcription with semantic knowledge by leveraging a pretrained language model. For the speech to speech transmission, we further include a CTC alignment module that extracts a small number of additional semantic-irrelevant but speech-related information, such as duration, pitch, power and speaker identification of the speech for the better reconstruction of the original speech signals at the receiver. We also introduce a two-stage training scheme which speeds up the training of the proposed DL model. The simulation results confirm that our proposed method outperforms current methods in terms of the accuracy of the predicted text for the speech to text transmission and the quality of the recovered speech signals for the speech to speech transmission, and significantly improves transmission efficiency. More specifically, the proposed method only sends 16% of the amount of the transmitted symbols required by the existing methods while achieving about a 10% reduction in WER for the speech to text transmission. For the speech to speech transmission, it results in an even more remarkable improvement in terms of transmission efficiency with only 0.2% of the amount of the transmitted symbols required by the existing method while preserving the comparable quality of the reconstructed speech signals.  © 1983-2012 IEEE.",Deep learning; end-to-end communication; semantic communication; speech transmission,Abstracting; Data mining; Deep learning; Efficiency; Job analysis; Knowledge management; Radio transceivers; Redundancy; Semantics; Speech communication; Speech recognition; Speech transmission; Transcription; Communication method; Communications systems; Deep learning; End-to-End communication; Features extraction; Receiver; Semantic communication; Speech signals; Task analysis; Transmission efficiency; Feature extraction,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85142787226
Zhang R.; Yan Z.; Wang X.; Deng R.H.,"Zhang, Rui (56874696300); Yan, Zheng (56517487500); Wang, Xuerui (57226132050); Deng, Robert H. (57203710994)",56874696300; 56517487500; 57226132050; 57203710994,VOLERE: Leakage Resilient User Authentication Based on Personal Voice Challenges,2023,IEEE Transactions on Dependable and Secure Computing,20,2,,1002,1016,14,4,10.1109/TDSC.2022.3147504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124222055&doi=10.1109%2fTDSC.2022.3147504&partnerID=40&md5=da3857170179b4e7d9a294cda1fa2fc5,"Voiceprint Authentication as a Service (VAaS) offers great convenience due to ubiquity, generality, and usability. Despite its attractiveness, it suffers from user voiceprint leakage over the air or at the cloud, which intrudes user voice privacy and retards its wide adoption. The literature still lacks an effective solution on this issue. Traditional methods based on cryptography are too complex to be practically deployed while other approaches distort user voiceprints, which hinders accurate user identification. In this article, we propose a leakage resilient user authentication cloud service with privacy preservation based on random personal voice challenges, named VOLERE (VOice LEakage REsilient). It applies a novel voiceprint synthesis method based on a Log Magnitude Approximate (LMA) vocal tract model to fuse original voices of different speaking modes in order to generate a synthesized voiceprint for authentication. Thus, raw voiceprints of users can be well protected. We implement VOLERE and conduct a series of user tests. Experimental results show sound performance of VOLERE regarding authentication accuracy, efficiency, stability, leakage resilience and user acceptance. In particular, its authentication accuracy is reasonably stable regardless user nationality, gender, age, elapsed time, and environment, as well as variance of speaking modes. © 2004-2012 IEEE.",biometric authentication; leakage resilience; privacy preservation; User authentication; voiceprint,Speech recognition; Biometric authentication; Correlation; Features extraction; Leakage-resilience; Privacy; Privacy preservation; Resilience; Spectrograms; User authentication; Voiceprint; Authentication,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85124222055
Zhang Y.; Li Z.; Lu J.; Hua H.; Wang W.; Zhang P.,"Zhang, Yuxiang (57839710600); Li, Zhuo (57279446700); Lu, Jingze (57954587100); Hua, Hua (57451161600); Wang, Wenchao (57208538745); Zhang, Pengyuan (55491517000)",57839710600; 57279446700; 57954587100; 57451161600; 57208538745; 55491517000,The Impact of Silence on Speech Anti-Spoofing,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,3374,3389,15,11,10.1109/TASLP.2023.3306711,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168745396&doi=10.1109%2fTASLP.2023.3306711&partnerID=40&md5=5aa9894d3620db8a87911375501224c6,"The current speech anti-spoofing countermeasures (CMs) show excellent performance on specific datasets. However, removing the silence of test speech through Voice Activity Detection (VAD) can severely degrade performance. In this article, the impact of silence on speech anti-spoofing is analyzed. First, the reasons for the impact are explored, including the proportion of silence duration and the content of silence. The proportion of silence duration in spoof speech generated by text-to-speech (TTS) algorithms is lower than that in bonafide speech. And the content of silence generated by different waveform generators varies compared to bonafide speech. Then the impact of silence on model prediction is explored. Even after retraining, the spoof speech generated by neural network based end-to-end TTS algorithms suffers a significant rise in error rates when the silence is removed. To demonstrate the reasons for the impact of silence on CMs, the attention distribution of a CM is visualized through class activation mapping (CAM). Furthermore, the implementation and analysis of the experiments masking silence or non-silence demonstrates the significance of the proportion of silence duration for detecting TTS and the importance of silence content for detecting voice conversion (VC). Based on the experimental results, improving the robustness of CMs against unknown spoofing attacks by masking silence is also proposed. Finally, the attacks on anti-spoofing CMs through concatenating silence, and the mitigation of VAD and silence attack through low-pass filtering are introduced. © 2014 IEEE.",Anti-spoofing; content of silence; proportion fo silence duration; speech synthesis; visual explanations,Low pass filters; Robustness (control systems); Speech processing; Speech recognition; Speech synthesis; Antispoofing; Content of silence; Convolutional neural network; Partitioning algorithms; Performance; Proportion fo silence duration; Robustness; Text to speech; Visual explanation; Voice-activity detections; Recurrent neural networks,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85168745396
Wang Z.; Wang X.; Xie Q.; Li T.; Xie L.; Tian Q.; Wang Y.,"Wang, Zhichao (57221097923); Wang, Xinsheng (57204884630); Xie, Qicong (57223904409); Li, Tao (57221124942); Xie, Lei (35294300000); Tian, Qiao (57288834300); Wang, Yuping (57225157860)",57221097923; 57204884630; 57223904409; 57221124942; 35294300000; 57288834300; 57225157860,MSM-VC: High-Fidelity Source Style Transfer for Non-Parallel Voice Conversion by Multi-Scale Style Modeling,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,3883,3895,12,3,10.1109/TASLP.2023.3313414,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171581156&doi=10.1109%2fTASLP.2023.3313414&partnerID=40&md5=c13290445a3d7b45d99ee32d74611149,"In addition to conveying the linguistic content from source speech to converted speech, maintaining the speaking style of source speech also plays an important role in the voice conversion (VC) task, which is essential in many scenarios with highly expressive source speech, such as dubbing and data augmentation. Previous work generally took explicit prosodic features or fixed-length style embedding extracted from source speech to model the speaking style of source speech, which is insufficient to achieve comprehensive style modeling and target speaker timbre preservation. Inspired by the style's multi-scale nature of human speech, a multi-scale style modeling method for the VC task, referred to as MSM-VC, is proposed in this article. MSM-VC models the speaking style of source speech from different levels, i.e., global, local, and frame levels. To effectively convey the speaking style and meanwhile prevent timbre leakage from source speech to converted speech, each level's style is modeled by specific representation. Specifically, prosodic features, pre-trained ASR model's bottleneck features, and features extracted by a model trained with a self-supervised strategy are adopted to model the frame, local, and global-level styles, respectively. Besides, to balance the performance of source style modeling and target speaker timbre preservation, an explicit constraint module consisting of a pre-trained speech emotion recognition model and a speaker classifier is introduced to MSM-VC. This explicit constraint module also makes it possible to simulate the style transfer inference process during the training to improve the disentanglement ability and alleviate the mismatch between training and inference. Experiments performed on the highly expressive speech corpus demonstrate that MSM-VC is superior to the state-of-the-art VC methods for modeling source speech style while maintaining good speech quality and speaker similarity. Furthermore, ablation analysis indicates the indispensable of every style level's modeling and the effectiveness of each module. © 2014 IEEE.",Multi-scale; style modeling; voice conversion,Emotion Recognition; Job analysis; Speech processing; Features extraction; Multi-scales; Predictive models; Prosodic features; Speaking styles; Style modeling; Target speaker; Task analysis; Timbre; Voice conversion; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85171581156
Jain R.; Barcovschi A.; Yiwere M.Y.; Bigioi D.; Corcoran P.; Cucu H.,"Jain, Rishabh (57568296800); Barcovschi, Andrei (58135704300); Yiwere, Mariam Yahayah (57204890157); Bigioi, Dan (57223669237); Corcoran, Peter (57190839462); Cucu, Horia (36439147500)",57568296800; 58135704300; 57204890157; 57223669237; 57190839462; 36439147500,A WAV2VEC2-Based Experimental Study on Self-Supervised Learning Methods to Improve Child Speech Recognition,2023,IEEE Access,11,,,46938,46948,10,15,10.1109/ACCESS.2023.3275106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159797544&doi=10.1109%2fACCESS.2023.3275106&partnerID=40&md5=1c0a7d12a2de7292b277e3c7b19b5eb1,"Despite recent advancements in deep learning technologies, Child Speech Recognition remains a challenging task. Current Automatic Speech Recognition (ASR) models require substantial amounts of annotated data for training, which is scarce. In this work, we explore using the ASR model, wav2vec2, with different pretraining and finetuning configurations for self-supervised learning (SSL) toward improving automatic child speech recognition. The pretrained wav2vec2 models were finetuned using different amounts of child speech training data, adult speech data, and a combination of both, to discover the optimum amount of data required to finetune the model for the task of child ASR. Our trained model achieves the best Word Error Rate (WER) of 7.42 on the MyST child speech dataset, 2.91 on the PFSTAR dataset and 12.77 on the CMU KIDS dataset using cleaned variants of each dataset. Our models outperformed the unmodified wav2vec2 BASE 960 on child speech using as little as 10 hours of child speech data in finetuning. The analysis of different types of training data and their effect on inference is provided by using a combination of custom datasets in pretraining, finetuning and inference. These 'cleaned' datasets are provided for use by other researchers to provide comparisons with our results.  © 2013 IEEE.",automatic speech recognition; Child speech recognition; CMU-kids dataset; MyST dataset; PFSTAR dataset; self-supervised learning; wav2vec2,Deep learning; Job analysis; Perturbation techniques; Speech recognition; Automatic speech recognition; Children's speech recognition; CMU_kid dataset; MyST dataset; Perturbation method; PFSTAR dataset; Self-supervised learning; Task analysis; Transformer; Wav2vec2; Supervised learning,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85159797544
Oglic D.; Cvetkovic Z.; Sollich P.; Renals S.; Yu B.,"Oglic, Dino (56358144100); Cvetkovic, Zoran (7004204914); Sollich, Peter (7004107468); Renals, Steve (7003714982); Yu, Bin (56933322900)",56358144100; 7004204914; 7004107468; 7003714982; 56933322900,Towards Robust Waveform-Based Acoustic Models,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,1977,1992,15,2,10.1109/TASLP.2022.3172632,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132523640&doi=10.1109%2fTASLP.2022.3172632&partnerID=40&md5=304ef25330401d8cf9417d889ec20e54,"We study the problem of learning robust acoustic models in adverse environments, characterized by a significant mismatch between training and test conditions. This problem is of paramount importance for the deployment of speech recognition systems that need to perform well in unseen environments. First, we characterize data augmentation theoretically as an instance of vicinal risk minimization, which aims at improving risk estimates during training by replacing the delta functions that define the empirical density over the input space with an approximation of the marginal population density in the vicinity of the training samples. More specifically, we assume that local neighborhoods centered at training samples can be approximated using a mixture of Gaussians, and demonstrate theoretically that this can incorporate robust inductive bias into the learning process. We then specify the individual mixture components implicitly via data augmentation schemes, designed to address common sources of spurious correlations in acoustic models. To avoid potential confounding effects on robustness due to information loss, which has been associated with standard feature extraction techniques (e.g., fbank and mfcc features), we focus on the waveform-based setting. Our empirical results show that the approach can generalize to unseen noise conditions, with 150% relative improvement in out-of-distribution generalization compared to training using the standard risk minimization principle. Moreover, the results demonstrate competitive performance relative to models learned using a training sample designed to match the acoustic conditions characteristic of test utterances.  © 2014 IEEE.",data augmentation; out-of-distribution generalization; Vicinal risk minimization; waveform-based models,Delta functions; Extraction; Feature extraction; Mixtures; Population statistics; Risk management; Speech processing; Speech recognition; Acoustics model; Data augmentation; Features extraction; Generalisation; Out-of-distribution generalization; Risk minimization; Risks management; Vicinal risk minimization; Waveform-based model; Waveforms; Sampling,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85132523640
Zhang Q.; Qian X.; Ni Z.; Nicolson A.; Ambikairajah E.; Li H.,"Zhang, Qiquan (57195618443); Qian, Xinyuan (57194871236); Ni, Zhaoheng (57196045442); Nicolson, Aaron (57204212732); Ambikairajah, Eliathamby (6602483257); Li, Haizhou (8615868400)",57195618443; 57194871236; 57196045442; 57204212732; 6602483257; 8615868400,A Time-Frequency Attention Module for Neural Speech Enhancement,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,462,475,13,40,10.1109/TASLP.2022.3225649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144066492&doi=10.1109%2fTASLP.2022.3225649&partnerID=40&md5=7f66a79b9599e00747e4232227aabfd2,"Speech enhancement plays an essential role in a wide range of speech processing applications. Recent studies on speech enhancement tend to investigate how to effectively capture the long-term contextual dependencies of speech signals to boost performance. However, these studies generally neglect the time-frequency (T-F) distribution information of speech spectral components, which is equally important for speech enhancement. In this paper, we propose a simple yet very effective network module, which we term the T-F attention (TFA) module, that uses two parallel attention branches, i.e., time-frame attention and frequency-channel attention, to explicitly exploit position information to generate a 2-D attention map to characterise the salient T-F speech distribution. We validate our TFA module as part of two widely used backbone networks (residual temporal convolution network and Transformer) and conduct speech enhancement with four most popular training objectives. Our extensive experiments demonstrate that our proposed TFA module consistently leads to substantial enhancement performance improvements in terms of the five most widely used objective metrics, with negligible parameter overheads. In addition, we further evaluate the efficacy of speech enhancement as a front-end for a downstream speech recognition task. Our evaluation results show that the TFA module significantly improves the robustness of the system to noisy conditions.  © 2014 IEEE.",ResTCN; Speech enhancement; time-frequency attention; transformer,Speech recognition; Performance; Processing applications; ResTCN; Simple++; Spectral components; Speech signals; Time frequency; Time-frequency attention; Time-frequency distributions; Transformer; Speech enhancement,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85144066492
Korzekwa D.; Lorenzo-Trueba J.; Drugman T.; Kostek B.,"Korzekwa, Daniel (57207859634); Lorenzo-Trueba, Jaime (36986671800); Drugman, Thomas (24479674000); Kostek, Bozena (6603685068)",57207859634; 36986671800; 24479674000; 6603685068,Computer-assisted pronunciation training—Speech synthesis is almost all you need,2022,Speech Communication,142,,,22,33,11,27,10.1016/j.specom.2022.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132963718&doi=10.1016%2fj.specom.2022.06.003&partnerID=40&md5=4a6a8f0d10617ce6f4f89f7f1e461762,"The research community has long studied computer-assisted pronunciation training (CAPT) methods in non-native speech. Researchers focused on studying various model architectures, such as Bayesian networks and deep learning methods, as well as on the analysis of different representations of the speech signal. Despite significant progress in recent years, existing CAPT methods are not able to detect pronunciation errors with high accuracy (only 60% precision at 40%–80% recall). One of the key problems is the low availability of mispronounced speech that is needed for the reliable training of pronunciation error detection models. If we had a generative model that could mimic non-native speech and produce any amount of training data, then the task of detecting pronunciation errors would be much easier. We present three innovative techniques based on phoneme-to-phoneme (P2P), text-to-speech (T2S) and speech-to-speech (S2S) conversion to generate correctly pronounced and mispronounced synthetic speech. We show that these techniques not only improve the accuracy of three machine learning models for detecting pronunciation errors, but also help establish a new state-of-the-art in the field. Earlier studies have used simple speech generation techniques such as P2P conversion, but only as an additional mechanism to improve the accuracy of pronunciation error detection. We, on the other hand, consider speech generation to be the first-class method of detecting pronunciation errors. The effectiveness of these techniques is assessed in the tasks of detecting pronunciation and lexical stress errors. Non-native English speech corpora of German, Italian, and Polish speakers are used in the evaluations. The best proposed S2S technique improves the accuracy of detecting pronunciation errors in AUC metric by 41% from 0.528 to 0.749 compared to the state-of-the-art approach. © 2022 The Author(s)",Automated lexical stress error detection; Automated pronunciation error detection; Computer-assisted pronunciation training; Deep learning; Speech synthesis; Voice conversion,Bayesian networks; Computer aided instruction; Deep learning; Error detection; Learning systems; Speech recognition; Automated lexical stress error detection; Automated pronunciation error detection; Computer assisted; Computer-assisted pronunciation training; Deep learning; Non-native speech; Pronunciation error detection; Pronunciation trainings; Training methods; Voice conversion; Speech synthesis,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85132963718
Perero-Codosero J.M.; Espinoza-Cuadros F.M.; Hernández-Gómez L.A.,"Perero-Codosero, Juan M. (57216417919); Espinoza-Cuadros, Fernando M. (55536144400); Hernández-Gómez, Luis A. (6701850865)",57216417919; 55536144400; 6701850865,X-vector anonymization using autoencoders and adversarial training for preserving speech privacy,2022,Computer Speech and Language,74,,101351,,,,20,10.1016/j.csl.2022.101351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123594307&doi=10.1016%2fj.csl.2022.101351&partnerID=40&md5=1f0cec393996e7aaafc2a1a6c2bdb8f6,"The rapid increase in web services and mobile apps, which collect personal data from users, has also increased the risk that their privacy may be severely compromised. In particular, the increasing variety of spoken language interfaces and voice assistants empowered by the vertiginous breakthroughs in deep learning have prompted important concerns in the European Union in terms of preserving the privacy of speech data. For instance, an attacker can record speech from users and impersonate them to obtain access to systems that require voice identification. By extracting speaker, linguistic (e.g., dialect), and paralinguistic features (e.g., age) from a speech signal, the speaker profiles can also be hacked from users through existing technology. To mitigate these weaknesses, in this study, we present a speech anonymization method based on autoencoders and adversarial training. Given an utterance, we first extract an x-vector, which is a powerful utterance-level embedding used in state-of-the-art speaker recognition. This original x-vector is transformed by an autoencoder producing a new x-vector, where speaker, gender, and accent information are suppressed through adversarial training. The anonymized speech is finally generated through a neural speech synthesizer driven by the anonymized x-vector, fundamental frequency, and phoneme information extracted from the input speech. For the evaluation, we followed the VoicePrivacy Challenge framework, where anonymization or privacy is measured using automatic speaker verification and the preservation of the intelligibility is evaluated through automatic speech recognition. Our experimental results show that the proposed method achieves higher privacy than the VoicePrivacy baseline (i.e., a higher speaker verification error) while preserving a similar intelligibility for the spoken content (i.e., a similar word error rate). © 2022 The Authors",Adversarial neural networks; Adversarial training; Autoencoders; Automatic speaker verification; Automatic speech recognition; Speaker anonymization,Deep learning; Linguistics; Privacy-preserving techniques; Speech intelligibility; Speech synthesis; Vectors; Web services; Websites; Adversarial neural network; Adversarial training; Anonymization; Auto encoders; Automatic speaker verification; Automatic speech recognition; Mobile app; Neural-networks; Speaker anonymization; Speech privacy; Speech recognition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85123594307
Kłosowski P.,"Kłosowski, Piotr (6604003542)",6604003542,A Rule-Based Grapheme-to-Phoneme Conversion System,2022,Applied Sciences (Switzerland),12,5,2758,,,,6,10.3390/app12052758,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126276254&doi=10.3390%2fapp12052758&partnerID=40&md5=80a0fb22548c760756db2a561db558ef,"This article presents a rule-based grapheme-to-phoneme conversion method and algorithm for Polish. It should be noted that the fundamental grapheme-to-phoneme conversion rules have been developed by Maria Steffen-Batóg and presented in her set of monographs dedicated to the automatic grapheme-to-phoneme conversion of texts in Polish. The author used previously developed rules and independently developed the grapheme-to-phoneme conversion algorithm.The algorithm has been implemented as a software application called TransFon, which allows the user to convert any text in Polish orthography to corresponding strings of phonemes, in phonemic transcription. Using TransFon, a phonemic Polish language corpus was created out of an orthographic corpus. The phonemic language corpusallows statistical analysis of the Polish language, as well as the development of phoneme-and word-based language models for automatic speech recognition using statistical methods. The developed phonemic language corpus opens up further opportunities for research to improve automatic speech recognition in Polish. The development of statistical methods for speech recognition and language modelling requires access to large language corpora, including phonemic corpora. The method presented here enables the creation of such corpora. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Grapheme-to-phoneme conversion; Language corpus; Language modelling; Language statistical analysis; Speech recognition,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85126276254
Deshpande M.; Veena M.B.; Ferede A.W.,"Deshpande, Meena (57209569227); Veena, M.B. (37011489200); Ferede, Alachew Wubie (57870948300)",57209569227; 37011489200; 57870948300,Auditory Speech Based Alerting System for Detecting Dummy Number Plate via Video Processing Data sets,2022,Computational Intelligence and Neuroscience,2022,,4423744,,,,1,10.1155/2022/4423744,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137682756&doi=10.1155%2f2022%2f4423744&partnerID=40&md5=46e9e0d2df130fef2899c495f07fffd6,"Spectrum of applications in computer vision use object detection algorithms driven by the power of AI and ML algorithms. State of art detection models like faster Region based convolutional Neural Network (RCNN), Single Shot Multibox Detector (SSD), and You Only Look Once (YOLO) demonstrated a good performance for object detection, but many failed in detecting small objects. In view of this an improved network structure of YOLOv4 is proposed in this paper. This work presents an algorithm for small object detection trained using real-time high-resolution data for porting it on embedded platforms. License plate recognition, which is a small object in a car image, is considered for detection and an auditory speech signal is generated for detecting fake license plates. The proposed network is improved in the following aspects: Training the classifier by using positive data set formed from the core patterns of an image. Training YOLOv4 by the features obtained by decomposing the image into low frequency and high frequency. The resultant values are processed and demonstrated via a speech alerting signals and messages. This contributes to reducing the computation load and increasing the accuracy. Algorithm was tested on eight real-time video data sets. The results show that our proposed method greatly reduces computing effort while maintaining comparable accuracy. It takes 45 fps to detect one image when the input size is 1280 × 960, which could keep a real-time speed. Proposed algorithm works well in case of tilted, blurred, and occluded license plates. Also, an auditory traffic monitoring system can reduce criminal attacks by detecting suspicious license plates. The proposed algorithm is highly applicable for autonomous driving applications. © 2022 Meena Deshpande et al.",,"Algorithms; Neural Networks, Computer; Speech; Classification (of information); Convolutional neural networks; Data handling; Image enhancement; License plates (automobile); Object recognition; Optical character recognition; Plates (structural components); Signal detection; Speech recognition; Video signal processing; Alerting systems; Data set; Detection models; Number plates; Object detection algorithms; Power; Real- time; Small objects; Spectra's; Video processing; algorithm; speech; Object detection",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85137682756
Qi J.; Yang C.-H.H.; Chen P.-Y.; Tejedor J.,"Qi, Jun (57214261512); Yang, Chao-Han Huck (57212483912); Chen, Pin-Yu (36930105800); Tejedor, Javier (6602125897)",57214261512; 57212483912; 36930105800; 6602125897,Exploiting Low-Rank Tensor-Train Deep Neural Networks Based on Riemannian Gradient Descent With Illustrations of Speech Processing,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,633,642,9,10,10.1109/TASLP.2022.3231714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147143215&doi=10.1109%2fTASLP.2022.3231714&partnerID=40&md5=d9e1bba346e59de1dafdccfa0ab1b2e7,"This work focuses on designing low-complexity hybrid tensor networks by considering trade-offs between the model complexity and practical performance. Firstly, we exploit a low-rank tensor-train deep neural network (TT-DNN) to build an end-to-end deep learning pipeline, namely LR-TT-DNN. Secondly, a hybrid model combining LR-TT-DNN with a convolutional neural network (CNN), which is denoted as CNN+(LR-TT-DNN), is set up to boost the performance. Instead of randomly assigning large TT-ranks for TT-DNN, we leverage Riemannian gradient descent to determine a TT-DNN associated with small TT-ranks. Furthermore, CNN+(LR-TT-DNN) consists of convolutional layers at the bottom for feature extraction and several TT layers at the top to solve regression and classification problems. We separately assess the LR-TT-DNN and CNN+(LR-TT-DNN) models on speech enhancement and spoken command recognition tasks. Our empirical evidence demonstrates that the LR-TT-DNN and CNN+(LR-TT-DNN) models with fewer model parameters can outperform the TT-DNN and CNN+(TT-DNN) counterparts.  © 2014 IEEE.",low-rank tensor-train decomposition; Riemannian gradient descent; speech enhancement; spoken command recognition; tensor-train deep neural network; Tensor-train network,Complex networks; Convolution; Economic and social effects; Speech enhancement; Speech recognition; Tensors; Command recognition; Gradient-descent; Low-rank tensor-train decomposition; Riemannian gradient; Riemannian gradient descent; Spoken command recognition; Tensor trains; Tensor-train deep neural network; Tensor-train network; Deep neural networks,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85147143215
Maharjan R.; Doherty K.; Rohani D.A.; Bækgaard P.; Bardram J.E.,"Maharjan, Raju (57204704004); Doherty, Kevin (57192910271); Rohani, Darius Adam (57008994100); Bækgaard, Per (56377241200); Bardram, Jakob E. (6603307974)",57204704004; 57192910271; 57008994100; 56377241200; 6603307974,Experiences of a Speech-enabled Conversational Agent for the Self-report of Well-being among People Living with Affective Disorders: An In-the-Wild Study,2022,ACM Transactions on Interactive Intelligent Systems,12,2,10,,,,15,10.1145/3484508,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135017535&doi=10.1145%2f3484508&partnerID=40&md5=a136fca75b44262fcfb87b9e8e1c518b,"The growing commercial success of smart speaker devices following recent advancements in speech recognition technology has surfaced new opportunities for collecting self-reported health and well-being data. Speech-enabled conversational agents (CAs) in particular, deployed in home environments using just such systems, may offer increasingly intuitive and engaging means of self-report. To date, however, few real-world studies have examined users' experiences of engaging in the self-report of mental health using such devices or the challenges of deploying these systems in the home context. With these aims in mind, this article recounts findings from a 4-week ""in-the-wild""study during which 20 individuals with depression or bipolar disorder used a speech-enabled CA named ""Sofia""to maintain a daily diary log, responding also to the World Health Organization-Five Well-Being Index WHO-5 scale every 2 weeks. Thematic analysis of post-study interviews highlights actions taken by participants to overcome CAs' limitations, diverse personifications of a speech-enabled agent, and unique forms of valuing of this system among users' personal and social circles. These findings serve as initial evidence for the potential of CAs to support the self-report of mental health and well-being, while highlighting the need to address outstanding technical limitations in addition to design challenges of conversational pattern matching, filling unmet interpersonal gaps, and the use of self-report CAs in the at-home social context. Based on these insights, we discuss implications for the future design of CAs to support the self-report of mental health and well-being.  © 2022 Association for Computing Machinery.",conversational agent; Conversational user interface; mental health; self-reports; virtual assistant; virtual health assistant; voice user interface; who-5,Health; Pattern matching; Speech recognition; Conversational agents; Conversational user interface; Mental health; Self-report; Speech recognition technology; Virtual assistants; Virtual health assistant; Voice user interface; Well being; Who-5; User interfaces,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85135017535
Catania F.; Garzotto F.,"Catania, Fabio (57207913470); Garzotto, Franca (6701572830)",57207913470; 6701572830,A conversational agent for emotion expression stimulation in persons with neurodevelopmental disorders,2023,Multimedia Tools and Applications,82,9,,12797,12828,31,5,10.1007/s11042-022-14135-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141608661&doi=10.1007%2fs11042-022-14135-w&partnerID=40&md5=e477e9e09290056aa05212f5e24fe021,"Difficulty in emotion expression and recognition is typical of the personality trait known as alexithymia, which is often observed in persons with neurodevelopmental disorders (NDD). Past research has investigated various forms of conversational technology for people with NDD, but only a few studies have explored the use of conversational agents to reduce alexithymia. This paper presents Emoty, a speech-based conversational agent designed for people with NDD to train emotional communication skills. An original characteristic of this agent is that it exploits the emotional expression power of the voice. Emoty engages users in small conversations during which they are asked to repeat sentences and express specific emotions using the appropriate vocal tone. We ran an empirical study to evaluate the usability and effectiveness of our conversational agent. The study involved 19 Italian individuals with NDD and alexithymia aged from 29 to 45 (10 women and 9 men). They used Emoty in five individual sessions over two and a half months. The results showed that two subjects encountered problems using the system because they had difficulty verbalizing the sentences and were not understood by Emoty. The others performed the assigned tasks with the agent. Their capability to express emotions with the voice consistently improved, and other benefits were observed in other social and communication skills. © 2022, The Author(s).",Alexithymia; Computer-assisted therapy; Conversational technology; Human-computer interaction; Neurodevelopmental disorder; Speech emotion recognition,Emotion Recognition; Speech communication; Speech recognition; Alexithymia; Communication skills; Computer assisted; Computer-assisted therapy; Conversational agents; Conversational technologies; Emotion expression; Emotion recognition; Neurodevelopmental disorder; Speech emotion recognition; Human computer interaction,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85141608661
Zong Y.; Lian H.; Zhang J.; Feng E.; Lu C.; Chang H.; Tang C.,"Zong, Yuan (57038101500); Lian, Hailun (57207733912); Zhang, Jiacheng (57261879000); Feng, Ercui (57913227800); Lu, Cheng (57194429246); Chang, Hongli (57222146751); Tang, Chuangao (57193794835)",57038101500; 57207733912; 57261879000; 57913227800; 57194429246; 57222146751; 57193794835,Progressive distribution adapted neural networks for cross-corpus speech emotion recognition,2022,Frontiers in Neurorobotics,16,,987146,,,,1,10.3389/fnbot.2022.987146,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139166190&doi=10.3389%2ffnbot.2022.987146&partnerID=40&md5=9ba94af67fa9e1f29e2d25f355a6be70,"In this paper, we investigate a challenging but interesting task in the research of speech emotion recognition (SER), i.e., cross-corpus SER. Unlike the conventional SER, the training (source) and testing (target) samples in cross-corpus SER come from different speech corpora, which results in a feature distribution mismatch between them. Hence, the performance of most existing SER methods may sharply decrease. To cope with this problem, we propose a simple yet effective deep transfer learning method called progressive distribution adapted neural networks (PDAN). PDAN employs convolutional neural networks (CNN) as the backbone and the speech spectrum as the inputs to achieve an end-to-end learning framework. More importantly, its basic idea for solving cross-corpus SER is very straightforward, i.e., enhancing the backbone's corpus invariant feature learning ability by incorporating a progressive distribution adapted regularization term into the original loss function to guide the network training. To evaluate the proposed PDAN, extensive cross-corpus SER experiments on speech emotion corpora including EmoDB, eNTERFACE, and CASIA are conducted. Experimental results showed that the proposed PDAN outperforms most well-performing deep and subspace transfer learning methods in dealing with the cross-corpus SER tasks. Copyright © 2022 Zong, Lian, Zhang, Feng, Lu, Chang and Tang.",cross-corpus speech emotion recognition; deep learning; deep transfer learning; domain adaptation; speech emotion recognition,Convolutional neural networks; Deep learning; Emotion Recognition; Learning systems; Transfer learning; Cross-corpus speech emotion recognition; Deep learning; Deep transfer learning; Domain adaptation; Neural-networks; Speech corpora; Speech emotion recognition; Transfer learning; Transfer learning methods; article; convolutional neural network; deep learning; emotion; human; human experiment; learning; loss of function mutation; speech; transfer of learning; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85139166190
Wubet Y.A.; Lian K.-Y.,"Wubet, Yeshanew Ale (57417164200); Lian, Kuang-Yow (7006070817)",57417164200; 7006070817,Voice Conversion Based Augmentation and a Hybrid CNN-LSTM Model for Improving Speaker-Independent Keyword Recognition on Limited Datasets,2022,IEEE Access,10,,,89170,89180,10,17,10.1109/ACCESS.2022.3200479,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136735498&doi=10.1109%2fACCESS.2022.3200479&partnerID=40&md5=de2c17f3a30ea3f7475667b4f4e4880e,"Keyword recognition is the basis of speech recognition, and its application is rapidly increasing in keyword spotting, robotics, and smart home surveillance. Because of these advanced applications, improving the accuracy of keyword recognition is crucial. In this paper, we proposed voice conversion (VC) - based augmentation to increase the limited training dataset and a fusion of a convolutional neural network (CNN) and long-short term memory (LSTM) model for robust speaker-independent isolated keyword recognition. Collecting and preparing a sufficient amount of voice data for speaker-independent speech recognition is a tedious and bulky task. To overcome this, we generated new raw voices from the original voices using an auxiliary classifier conditional variational autoencoder (ACVAE) method. In this study, the main intention of voice conversion is to obtain numerous and various human-like keywords' voices that are not identical to the source and target speakers' pronunciation. Parallel VC was used to accurately maintain the linguistic content. We examined the performance of the proposed voice conversion augmentation techniques using robust deep neural network algorithms. Original training data, excluding generated voice using other data augmentation and regularization techniques, were considered as the baseline. The results showed that incorporating voice conversion augmentation into the baseline augmentation techniques and applying the CNN-LSTM model improved the accuracy of isolated keyword recognition.  © 2013 IEEE.",CNN-LSTM; data augmentation; speaker-independent keyword recognition; voice conversion,Automation; Deep neural networks; Speech processing; Speech recognition; Augmentation techniques; Convolutional neural network; Convolutional neural network-long-short term memory; Data augmentation; ITS applications; Keyword spotting; Memory modeling; Speaker independents; Speaker-independent keyword recognition; Voice conversion; Long short-term memory,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85136735498
Meng W.; Yolwas N.,"Meng, Weijing (57913056500); Yolwas, Nurmemet (55899531200)",57913056500; 55899531200,A Study of Speech Recognition for Kazakh Based on Unsupervised Pre-Training,2023,Sensors,23,2,870,,,,10,10.3390/s23020870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146639452&doi=10.3390%2fs23020870&partnerID=40&md5=3edeb61c76699e06f093fd26193e4769,"Building a good speech recognition system usually requires a lot of pairing data, which poses a big challenge for low-resource languages, such as Kazakh. In recent years, unsupervised pre-training has achieved good performance in low-resource speech recognition, but it is rarely used in Kazakh and other Central and West Asian languages. In this paper, wav2vec2.0 is improved by integrating a Factorized TDNN layer to better preserve the relationship between the voice and the time step before and after the quantization, which is called wav2vec-F. The unsupervised pre-training strategy was used to learn the potential speech representation from a large number of unlabeled audio data and was applied to the cross-language ASR task, which was optimized using the noise contrast binary classification task. At the same time, speech synthesis is used to promote the performance of speech recognition. The experiment shows that wav2vec-F can effectively utilize the unlabeled data from non-target languages, and the multi-language pre-training is obviously better than the single-language pre-training. The data enhancement method using speech synthesis can bring huge benefits. Compared with the baseline model, Librispeech’s test-clean dataset has an average reduction of 1.9% in the word error rate. On the Kazakh KSC test set, the pre-training using only Kazakh reduced the word error rate by 3.8%. The pre-training of a small amount of Kazakh speech data synthesized by multi-language combined with TTS achieved a word error rate of 8.6% on the KSC test set when the labeled data were only 10 h, which was comparable to the results of the previous end-to-end model when the labeled data were 30 times less. © 2023 by the authors.",automatic speech recognition; Factorized TDNN; speech synthesis; unsupervised pre-training,Language; Noise; Speech; Speech Perception; Speech Recognition Software; Audio acoustics; Errors; Speech recognition; Statistical tests; Automatic speech recognition; Factorized TDNN; Labeled data; Multi languages; Performance; Pre-training; Speech recognition systems; Test sets; Unsupervised pre-training; Word error rate; automatic speech recognition; language; noise; speech; speech perception; Speech synthesis,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85146639452
Huang W.-C.; Yang S.-W.; Hayashi T.; Toda T.,"Huang, Wen-Chin (57208822576); Yang, Shu-Wen (57219055235); Hayashi, Tomoki (57216552967); Toda, Tomoki (7202683282)",57208822576; 57219055235; 57216552967; 7202683282,A Comparative Study of Self-Supervised Speech Representation Based Voice Conversion,2022,IEEE Journal on Selected Topics in Signal Processing,16,6,,1308,1318,10,15,10.1109/JSTSP.2022.3193761,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135745767&doi=10.1109%2fJSTSP.2022.3193761&partnerID=40&md5=359a41b4101469a5fdc75ace49c1402d,"We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.  © 2007-2012 IEEE.",Self-supervised learning; self-supervised speech representation; voice conversion,K-means clustering; Linguistics; Open source software; Open systems; Speech recognition; Unsupervised learning; Comparatives studies; Conversion systems; Features extraction; Hidden-Markov models; Self-supervised learning; Self-supervised speech representation; State of the art; Synthesizer; Task analysis; Voice conversion; Hidden Markov models,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85135745767
Liu X.; Wang X.; Sahidullah M.; Patino J.; Delgado H.; Kinnunen T.; Todisco M.; Yamagishi J.; Evans N.; Nautsch A.; Lee K.A.,"Liu, Xuechen (57219785078); Wang, Xin (57196088126); Sahidullah, Md (58802043400); Patino, Jose (57196401950); Delgado, Hector (54402491600); Kinnunen, Tomi (23135344600); Todisco, Massimiliano (24482053500); Yamagishi, Junichi (7004695833); Evans, Nicholas (56207371100); Nautsch, Andreas (55489138700); Lee, Kong Aik (7501503733)",57219785078; 57196088126; 58802043400; 57196401950; 54402491600; 23135344600; 24482053500; 7004695833; 56207371100; 55489138700; 7501503733,ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,2507,2522,15,97,10.1109/TASLP.2023.3285283,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162920923&doi=10.1109%2fTASLP.2023.3285283&partnerID=40&md5=c4dce1e99abe2fd4a07d1d425392ee87,"Benchmarking initiatives support the meaningful comparison of competing solutions to prominent problems in speech and language processing. Successive benchmarking evaluations typically reflect a progressive evolution from ideal lab conditions towards to those encountered in the wild. ASVspoof, the spoofing and deepfake detection initiative and challenge series, has followed the same trend. This article provides a summary of the ASVspoof 2021 challenge and the results of 54 participating teams that submitted to the evaluation phase. For the logical access (LA) task, results indicate that countermeasures are robust to newly introduced encoding and transmission effects. Results for the physical access (PA) task indicate the potential to detect replay attacks in real, as opposed to simulated physical spaces, but a lack of robustness to variations between simulated and real acoustic environments. The Deepfake (DF) task, new to the 2021 edition, targets solutions to the detection of manipulated, compressed speech data posted online. While detection solutions offer some resilience to compression effects, they lack generalization across different source datasets. In addition to a summary of the top-performing systems for each task, new analyses of influential data factors and results for hidden data subsets, the article includes a review of post-challenge results, an outline of the principal challenge limitations and a road-map for the future of ASVspoof.  © 2014 IEEE.",ASVspoof; countermeasures; deepfakes; presentation attack detection; spoofing,Job analysis; Speech recognition; Asvspoof; Attack detection; Codec; Communications networks; Countermeasure; Deepfake; Distributed database; Presentation attack detection; Spoofing; Task analysis; Network coding,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85162920923
Zhang X.; Xu Y.; Zhang S.; Li X.,"Zhang, Xinyu (57959553800); Xu, Yang (55694981400); Zhang, Sicong (57208423796); Li, Xiaojian (57763251900)",57959553800; 55694981400; 57208423796; 57763251900,A Highly Stealthy Adaptive Decay Attack Against Speaker Recognition,2022,IEEE Access,10,,,118789,118805,16,7,10.1109/ACCESS.2022.3220639,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141590232&doi=10.1109%2fACCESS.2022.3220639&partnerID=40&md5=9543e9b5103629b231b1d5d23e41913e,"Speaker recognition based on deep learning is currently the most advanced and mainstream technology in the industry. Adversarial attacks, an emerging and powerful attack against neural network models, also posing serious security problems for speaker recognition. Common gradient-based attack methods such as FGSM (Fast Gradient Sign Method), PGD (Projected Gradient Descent), and MI-FGSM (Momentum Iteration-FGSM) generate adversarial examples that are poorly stealthy and easily perceived by the human ear. To improve the stealthiness of the adversarial examples, this paper proposes a new attack method called the Adaptive Decay Attack (ADA), whose stealth is very close to the CW2(Carlini&Wagner) method based on optimization attacks, with much less computation time than CW2. The method takes the set number of iterations as the termination condition, automatically adjusts the size of the maximum perturbation according to whether the attack is successful or not, and then uses the decay methods in learning rates such as exponential decay and cosine annealing to continuously reduce the step size. The experimental results show that under the two speaker recognition models x-vector, and i-vector, the proposed attack method improves the stealthiness metrics such as SNR and PESQ by at least 30% and 39%, respectively, compared with the best PGD attack under speaker identification of untargeted attacks. For the speaker identification task with targeted attacks, the average improvement is at least 20% and 25% compared to PGD. For the speaker verification task, the improvement is at least 29.5% and 33.4% compared to PGD. In addition, we also use this attack method for adversarial training to enhance the robustness of the model. Experimental results show that ADA-based adversarial training takes 28.31% less time than PGD-based adversarial training, and its improved robustness is generally superior to PGD-based adversarial training. Specifically, the attack success rate of PGD and ADA methods decreased from 50.88% to 36.47% and 64.74% to 45.82%, respectively. © 2013 IEEE.",adaptive decay attack; adversarial attacks; adversarial training; Deep learning; speaker recognition,Character recognition; Deep learning; Perturbation techniques; Adaptive decay attack; Adversarial attack; Adversarial training; Deep learning; Perturbation method; Speaker recognition; Target recognition; Task analysis; Text recognition; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85141590232
Alqadasi A.M.A.; Abdulghafor R.; Sunar M.S.; Salam M.S.B.H.J.,"Alqadasi, Ammar Mohammed Ali (58796174700); Abdulghafor, Rawad (55776630200); Sunar, Mohd Shahrizal (7004071446); Salam, Md. Sah Bin H. J. (25825625300)",58796174700; 55776630200; 7004071446; 25825625300,Modern Standard Arabic Speech Corpora: A Systematic Review,2023,IEEE Access,11,,,55771,55796,25,5,10.1109/ACCESS.2023.3282259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161563931&doi=10.1109%2fACCESS.2023.3282259&partnerID=40&md5=6cd780d22e6838a87950a24e6b2b8987,"Speech processing applications have become integral components across various domains of modern life. The design and preparation of a reliable recognition system rely heavily on the availability of suitable speech databases. While numerous speech databases exist for English and other languages, the availability of comprehensive resources for Arabic language remains limited. In light of this, we conducted a systematic review aiming to identify, analyse, and classify existing Modern Standard Arabic speech databases. Through our review, we identified 27 publicly available databases and analysed an additional 80 subjective databases. These databases were thoroughly studied, classified based on their characteristics, and subjected to a detailed analysis of research trends in the field. This paper provides a comprehensive discussion on the diverse speech databases developed for various speech processing applications. It sheds light on the purposes and unique characteristics of Arabic speech databases, enabling researchers to easily access suitable resources for their specific applications. The findings of this review contribute to bridging the gap in available Arabic speech databases and serve as a valuable resource for researchers in the field.  © 2013 IEEE.",Arabic recognition; modern standard Arabic; MSA corpora; Speech corpus; speech database; speech recognition,Classification (of information); Database systems; Market Research; Speech recognition; Arabic recognition; Distributed database; Market researches; Modern standard arabic; Modern standards; MSA corpus; Speech corpora; Speech database; Standard arabics; Systematic; Speech processing,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85161563931
Naowarat B.; Piansaddhayanon C.; Chuangsuwanich E.,"Naowarat, Burin (57219732667); Piansaddhayanon, Chawan (57542669900); Chuangsuwanich, Ekapol (36987854100)",57219732667; 57542669900; 36987854100,Context Conditioning via Surrounding Predictions for Non-Recurrent CTC Models,2023,IEEE Access,11,,,73531,73542,11,0,10.1109/ACCESS.2023.3291343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163754343&doi=10.1109%2fACCESS.2023.3291343&partnerID=40&md5=efd397a4a1088fa7cbd7cd9b968bbcf3,"Connectionist Temporal Classification (CTC) loss has become widely used in sequence modeling tasks such as Automatic Speech Recognition (ASR) and Handwritten Text Recognition (HTR) due to its ease of use. Recent sequence models that incorporate CTC loss have been focusing on speed by removing recurrent structures, hence losing important context information. This paper presents extensive studies of Contextualized Connectionist Temporal Classification (CCTC) framework, which induces prediction dependencies in non-recurrent and non-autoregressive neural networks for sequence modeling. CCTC allows the model to implicitly learn the language model by predicting neighboring labels via multi-task learning. Experiments on ASR and HTR tasks in two different languages show that CCTC models offer improvements over CTC models by 2.2-8.4% relative without incurring extra inference costs. We have also found that higher order of context information can potentially help the model produce better predictions.  © 2013 IEEE.",automatic speech recognition (ASR); contextualized CTC; CTC; handwritten text recognition (HTR); non-autoregressive; non-recurrent,Character recognition; Forecasting; Job analysis; Recurrent neural networks; Semantics; Speech recognition; Auto-regressive; Automatic speech recognition; Computational modelling; Connectionist temporal classification; Context models; Contextualized connectionist temporal classification; Hand-written text recognition; Handwritten text recognition; Hidden-Markov models; Non-autoregressive; Non-recurrent; Predictive models; Task analysis; Temporal classification; Text recognition; Hidden Markov models,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85163754343
Latif S.; Rana R.; Khalifa S.; Jurdak R.; Qadir J.; Schuller B.,"Latif, Siddique (57195632382); Rana, Rajib (35318333200); Khalifa, Sara (55604829500); Jurdak, Raja (23097482900); Qadir, Junaid (15058218600); Schuller, Bjorn (6603767415)",57195632382; 35318333200; 55604829500; 23097482900; 15058218600; 6603767415,Survey of Deep Representation Learning for Speech Emotion Recognition,2023,IEEE Transactions on Affective Computing,14,2,,1634,1654,20,54,10.1109/TAFFC.2021.3114365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115705598&doi=10.1109%2fTAFFC.2021.3114365&partnerID=40&md5=5d6e58a982dbf77e7283a3b25191ea3f,"Traditionally, speech emotion recognition (SER) research has relied on manually handcrafted acoustic features using feature engineering. However, the design of handcrafted features for complex SER tasks requires significant manual effort, which impedes generalisability and slows the pace of innovation. This has motivated the adoption of representation learning techniques that can automatically learn an intermediate representation of the input signal without any manual feature engineering. Representation learning has led to improved SER performance and enabled rapid innovation. Its effectiveness has further increased with advances in deep learning (DL), which has facilitated deep representation learning where hierarchical representations are automatically learned in a data-driven manner. This article presents the first comprehensive survey on the important topic of deep representation learning for SER. We highlight various techniques, related challenges and identify important future areas of research. Our survey bridges the gap in the literature since existing surveys either focus on SER with hand-engineered features or representation learning in the general setting without focusing on SER.  © 2010-2012 IEEE.",domain adaptation; multi task learning; representation learning; Speech emotion recognition; unsupervised learning,Deep learning; Emotion Recognition; Job analysis; Principal component analysis; Speech recognition; Surveys; Unsupervised learning; Australia; Deep learning; Domain adaptation; Emotion recognition; Feature engineerings; Multitask learning; Principal-component analysis; Representation learning; Speech emotion recognition; Task analysis; Neurons,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85115705598
Chen L.; Ren J.; Chen P.; Mao X.; Zhao Q.,"Chen, Lijiang (55213455200); Ren, Jie (57189658980); Chen, Pengfei (57217135075); Mao, Xia (8406400500); Zhao, Qi (55743332100)",55213455200; 57189658980; 57217135075; 8406400500; 55743332100,Limited text speech synthesis with electroglottograph based on Bi-LSTM and modified Tacotron-2,2022,Applied Intelligence,52,13,,15193,15209,16,9,10.1007/s10489-021-03075-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126203186&doi=10.1007%2fs10489-021-03075-x&partnerID=40&md5=61f00042ece7bae2b53e4062288b07da,"This paper proposes a framework of applying only the EGG signal for speech synthesis in the limited categories of contents scenario. EGG is a sort of physiological signal which can reflect the trends of the vocal cord movement. Note that EGG’s different acquisition method contrasted with speech signals, we exploit its application in speech synthesis under the following two scenarios. (1) To synthesize speeches under high noise circumstances, where clean speech signals are unavailable. (2) To enable dumb people who retain vocal cord vibration to speak again. Our study consists of two stages, EGG to text and text to speech. The first is a text content recognition model based on Bi-LSTM, which converts each EGG signal sample into the corresponding text with a limited class of contents. This model achieves 91.12% accuracy on the validation set in a 20-class content recognition experiment. Then the second step synthesizes speeches with the corresponding text and the EGG signal. Based on modified Tacotron-2, our model gains the Mel cepstral distortion (MCD) of 5.877 and the mean opinion score (MOS) of 3.87, which is comparable with the state-of-the-art performance and achieves an improvement by 0.42 and a relatively smaller model size than the origin Tacotron-2. Considering to introduce the characteristics of speakers contained in EGG to the final synthesized speech, we put forward a fine-grained fundamental frequency modification method, which adjusts the fundamental frequency according to EGG signals and achieves a lower MCD of 5.781 and a higher MOS of 3.94 than that without modification. © 2021, The Author(s).",Bi-LSTM; Electroglottograph (EGG); Speech Synthesis; Tacotron,Character recognition; Long short-term memory; Natural frequencies; Speech communication; Speech recognition; Bi-LSTM; Cepstral; Content recognition; Electro-glottograph signals; Electroglottograph; Mean opinion scores; Speech signals; Tacotra; Vocal cords; Speech synthesis,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85126203186
Zribi I.; Belguith L.H.,"Zribi, Inès (55633581400); Belguith, Lamia Hadrich (35791744800)",55633581400; 35791744800,TOWARD DEVELOPING AN INTELLIGENT PERSONAL ASSISTANT FOR TUNISIAN ARABIC,2022,Jordanian Journal of Computers and Information Technology,8,4,,318,335,17,1,10.5455/jjcit.71-1652434864,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143549251&doi=10.5455%2fjjcit.71-1652434864&partnerID=40&md5=0a95af46e77d485133b69801c6f02c3c,"Intelligent systems powered by artificial intelligence techniques have been massively proposed to help humans in performing various tasks. The intelligent personal assistant (IPA) is one of these smart systems. In this paper, we present an attempt to create an IPA that interacts with users via Tunisian Arabic (TA) (the colloquial form used in Tunisia). We propose and explore a simple-to-implement method for building the principal components of a TA IPA. We apply deep-learning techniques: CNN [1], RNN encoder-decoder [2] and end-to-end approaches for creating IPA speech components (speech recognition and speech synthesis). In addition, we explore the availability and free-dialog platform for understanding and generating the suitable response in TA for a request. For this proposal, we create and use TA transcripts for generating the corresponding models. Evaluation results are acceptable for the first attempt. © 2022, Scientific Research Support Fund of Jordan. All rights reserved.",Dialog management; Intelligent personal assistant; Natural-language understanding; Response generation; Speech recognition; Speech synthesis; Tunisian Arabic,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85143549251
Ullah M.; Li X.; Hassan M.A.; Ullah F.; Muhammad Y.; Granelli F.; Vilcekova L.; Sadad T.,"Ullah, Mahib (58054949000); Li, Xingmei (35194224800); Hassan, Muhammad Abul (57219412779); Ullah, Farhat (57219416400); Muhammad, Yar (57220392824); Granelli, Fabrizio (6701655006); Vilcekova, Lucia (57193434275); Sadad, Tariq (57204044966)",58054949000; 35194224800; 57219412779; 57219416400; 57220392824; 6701655006; 57193434275; 57204044966,"An Intelligent Multi-Floor Navigational System Based on Speech, Facial Recognition and Voice Broadcasting Using Internet of Things",2023,Sensors,23,1,275,,,,4,10.3390/s23010275,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145966238&doi=10.3390%2fs23010275&partnerID=40&md5=a5ca9f83733ace87d318eafba7b4294f,"Modern technologies such as the Internet of Things (IoT) and physical systems used as navigation systems play an important role in locating a specific location in an unfamiliar environment. Due to recent technological developments, users can now incorporate these systems into mobile devices, which has a positive impact on the acceptance of navigational systems and the number of users who use them. The system that is used to find a specific location within a building is known as an indoor navigation system. In this study, we present a novel approach to adaptable and changeable multistory navigation systems that can be implemented in different environments such as libraries, grocery stores, shopping malls, and official buildings using facial and speech recognition with the help of voice broadcasting. We chose a library building for the experiment to help registered users find a specific book on different building floors. In the proposed system, to help the users, robots are placed on each floor of the building, communicating with each other, and with the person who needs navigational help. The proposed system uses an Android platform that consists of two separate applications: one for administration to add or remove settings and data, which in turn builds an environment map, while the second application is deployed on robots that interact with the users. The developed system was tested using two methods, namely system evaluation, and user evaluation. The evaluation of the system is based on the results of voice and face recognition by the user, and the model’s performance relies on accuracy values obtained by testing out various values for the neural network parameters. The evaluation method adopted by the proposed system achieved an accuracy of 97.92% and 97.88% for both of the tasks. The user evaluation method using the developed Android applications was tested on multi-story libraries, and the results were obtained by gathering responses from users who interacted with the applications for navigation, such as to find a specific book. Almost all the users find it useful to have robots placed on each floor of the building for giving specific directions with automatic recognition and recall of what a person is searching for. The evaluation results show that the proposed system can be implemented in different environments, which shows its effectiveness. © 2022 by the authors.",autonomic computing; facial recognition; IoT; monitoring; robotics; smart services; voice recognition,Facial Recognition; Humans; Internet of Things; Speech; Voice; Air navigation; Buildings; Face recognition; Floors; Indoor positioning systems; Internet of things; Libraries; Location; Navigation systems; Robots; Autonomic Computing; Evaluation methods; Facial recognition; Modern technologies; Navigational systems; Physical systems; Smart services; Specific location; Technological development; User evaluations; facial recognition; human; speech; voice; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85145966238
Marras M.; Korus P.; Jain A.; Memon N.,"Marras, Mirko (57193883534); Korus, Pawel (35324909800); Jain, Anubhav (57208644562); Memon, Nasir (7005458042)",57193883534; 35324909800; 57208644562; 7005458042,Dictionary Attacks on Speaker Verification,2023,IEEE Transactions on Information Forensics and Security,18,,,773,788,15,8,10.1109/TIFS.2022.3229583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144748397&doi=10.1109%2fTIFS.2022.3229583&partnerID=40&md5=9452668341c51d5a52849500d0b22a91,"In this paper, we propose dictionary attacks against speaker verification-a novel attack vector that aims to match a large fraction of speaker population by chance. We introduce a generic formulation of the attack that can be used with various speech representations and threat models. The attacker uses adversarial optimization to maximize raw similarity of speaker embeddings between a seed speech sample and a proxy population. The resulting master voice successfully matches a non-trivial fraction of people in an unknown population. Adversarial waveforms obtained with our approach can match on average 69% of females and 38% of males enrolled in the target system at a strict decision threshold calibrated to yield false alarm rate of 1%. By using the attack with a black-box voice cloning system, we obtain master voices that are effective in the most challenging conditions and transferable between speaker encoders. We also show that, combined with multiple attempts, this attack opens even more to serious issues on the security of these systems.  © 2005-2012 IEEE.",adversarial machine learning; Authentication; biometrics (access control); impersonation attacks; speaker recognition,Acoustics; Perturbation techniques; Speech recognition; Attack vector; Dictionary attack; Embeddings; Fingerprint Recognition; Optimisations; Perturbation method; Psychoacoustic model; Representation model; Speaker verification; Threat modeling; Computer crime,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85144748397
Birkholz P.; Ossmann S.; Blandin R.; Wilbrandt A.; Krug P.K.; Fleischer M.,"Birkholz, Peter (23395660400); Ossmann, Steffen (57207687400); Blandin, Remi (56527211300); Wilbrandt, Alexander (57340098100); Krug, Paul Konstantin (57219818185); Fleischer, Mario (56421619900)",23395660400; 57207687400; 56527211300; 57340098100; 57219818185; 56421619900,Modeling Speech Sound Radiation With Different Degrees of Realism for Articulatory Synthesis,2022,IEEE Access,10,,,95008,95019,11,4,10.1109/ACCESS.2022.3204816,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137867195&doi=10.1109%2fACCESS.2022.3204816&partnerID=40&md5=428cef75c13a2497a1672db838bae62c,"Articulatory synthesis is based on modeling various physical phenomena of speech production, including sound radiation from the mouth. With regard to sound radiation, the most common approach is to approximate it in terms of a simple spherical source of strength equal to the mouth volume velocity. However, because this approximation is only valid at very low frequencies and does not account for the diffraction by the head and the torso, we simulated two alternative radiation characteristics that are potentially more realistic: the radiation from a vibrating piston in a spherical baffle, and the radiation from the mouth of a detailed model of the human head and torso. Using the articulatory speech synthesizer VocalTractLab, a corpus of 10 sentences was synthesized with the different radiation characteristics combined with three different phonation types. The synthesized sentences were acoustically compared with natural recordings of the same sentences in terms of their long-term average spectra (LTAS), and evaluated in terms of their naturalness and intelligibility. The intelligibility was not affected by the type of radiation characteristic. However, it was found that the more similar their LTAS was to real speech, the more natural the synthetic sentences were perceived to be. Hence, the naturalness was not directly determined by the realism of the radiation characteristic, but by the combined spectral effect of the radiation characteristic and the voice source. While the more realistic radiation models do not per se improve synthesis quality, they provide new insights in the study of speech production and articulatory synthesis.  © 2013 IEEE.",articulatory synthesis; Radiation characteristic,Acoustic generators; Acoustic properties; Acoustic wave scattering; Speech recognition; Speech synthesis; Articulatory synthesis; Mouth; Radiation characteristics; Solid modelling; Sound radiations; Spectra's; Speech production; Speech sounds; Synthesised; Synthesizer; Radiation,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85137867195
Wang Q.; Yao J.; Zhang L.; Guo P.; Xie L.,"Wang, Qing (57020582000); Yao, Jixun (57222567951); Zhang, Li (57221583918); Guo, Pengcheng (57204213650); Xie, Lei (35294300000)",57020582000; 57222567951; 57221583918; 57204213650; 35294300000,Timbre-Reserved Adversarial Attack in Speaker Identification,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,3848,3858,10,3,10.1109/TASLP.2023.3306714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168735318&doi=10.1109%2fTASLP.2023.3306714&partnerID=40&md5=d163390f8fa98e1777044a9ce0beb123,"As a type of biometric identification, speaker identification (SID) systems face various attacks. Spoofing attacks imitate target speakers' timbre, while adversarial attacks confuse SID systems with well-designed perturbations. Spoofing mimics victim timbre but fails to exploit SID model vulnerabilities, potentially not achieving the attacker.s goal. On the other hand, adversarial attacks can lead SID to a decision but may not meet specific text or speaker timbre requirements for certain attack scenarios. In this study, we propose a timbre-reserved adversarial attack in speaker identification to leverage SID model vulnerabilities while preserving the target speaker.s timbre. We generate timbre-reserved adversarial audio by adding an adversarial constraint during different training stages of the voice conversion (VC) model. This constraint utilizes the target speaker label to optimize adversarial perturbations in VC model representations and is implemented through a speaker classifier integrated into VC model training. This adversarial constraint helps control the VC model to generate speaker-wised audio. Ultimately, the VC model.s inference produces ideal timbre-reserved adversarial audio capable of deceiving SID system. Experimental results on the Audio deepfake detection (ADD) challenge dataset demonstrate that our method significantly improves attack success rate compared to the vanilla VC model, without introducing additional adversarial noise to the attack speech. Objective and subjective evaluations confirm the superior quality of fake audio generated by our approach compared to directly adding adversarial perturbation to VC-generated audio. Additionally, our analysis indicates that our generated adversarial fake audio meets the specified text and target speaker timbre requirements of the attacker. © 2014 IEEE.",Adversarial attack; speaker identification; timbre-reserved; voice conversion,Fake detection; Job analysis; Loudspeakers; Perturbation techniques; Quality control; Speech recognition; Adversarial attack; Conversion model; Perturbation method; Predictive models; Speaker identification; Speaker recognition; Task analysis; Timbre; Timbre-reserved; Voice conversion; Speech synthesis,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85168735318
Lu Y.-J.; Chang C.-Y.; Yu C.; Liu C.-F.; Hung J.-W.; Watanabe S.; Tsao Y.,"Lu, Yen-Ju (57219759991); Chang, Chia-Yu (57221101649); Yu, Cheng (57213355374); Liu, Ching-Feng (55661225600); Hung, Jeih-Weih (7201963613); Watanabe, Shinji (35373073400); Tsao, Yu (13608047100)",57219759991; 57221101649; 57213355374; 55661225600; 7201963613; 35373073400; 13608047100,Improving Speech Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,2738,2750,12,3,10.1109/TASLP.2023.3288418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166745459&doi=10.1109%2fTASLP.2023.3288418&partnerID=40&md5=bdcc734810a15b57c6e33e90df38fd28,"Previous studies have confirmed that by augmenting acoustic features with the place/manner of articulatory features, the speech enhancement (SE) process can be guided to consider the broad phonetic properties of the input speech when performing enhancement to attain performance improvements. In this article, we explore the contextual information of articulatory attributes as additional information to further benefit SE. More specifically, we propose to improve the SE performance by leveraging losses from an end-to-end automatic speech recognition (E2E-ASR) model that predicts the sequence of broad phonetic classes (BPCs). We also developed multi-objective training with ASR and perceptual losses to train the SE system based on a BPC-based E2E-ASR. Experimental results from speech denoising, speech dereverberation, and impaired speech enhancement tasks confirmed that contextual BPC information improves SE performance. Moreover, the SE model trained with the BPC-based E2E-ASR outperforms that with the phoneme-based E2E-ASR. The results suggest that objectives with misclassification of phonemes by the ASR system may lead to imperfect feedback, and BPC could be a potentially better choice. Finally, it is noted that combining the most-confusable phonetic targets into the same BPC when calculating the additional objective can effectively improve the SE performance.  © 2014 IEEE.",articulatory attribute; broad phonetic classes; end-to-end; robust automatic speech recognition; Speech enhancement,Linguistics; Speech recognition; Acoustic features; Articulatory attributes; Articulatory features; Automatic speech recognition; Broad phonetic class; Class information; Class-based; End to end; Performance; Robust automatic speech recognition; Speech enhancement,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85166745459
Guizzo E.; Weyde T.; Scardapane S.; Comminiello D.,"Guizzo, Eric (57210359479); Weyde, Tillman (24476899500); Scardapane, Simone (55772102700); Comminiello, Danilo (36444807900)",57210359479; 24476899500; 55772102700; 36444807900,Learning Speech Emotion Representations in the Quaternion Domain,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,1200,1212,12,23,10.1109/TASLP.2023.3250840,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149423370&doi=10.1109%2fTASLP.2023.3250840&partnerID=40&md5=450afa946719b0577f4f8b81051acb55,"The modeling of human emotion expression in speech signals is an important, yet challenging task. The high resource demand of speech emotion recognition models, combined with the general scarcity of emotion-labelled data are obstacles to the development and application of effective solutions in this field. In this paper, we present an approach to jointly circumvent these difficulties. Our method, named RH-emo, is a novel semi-supervised architecture aimed at extracting quaternion embeddings from real-valued monoaural spectrograms, enabling the use of quaternion-valued networks for speech emotion recognition tasks. RH-emo is a hybrid real/quaternion autoencoder network that consists of a real-valued encoder in parallel to a real-valued emotion classifier and a quaternion-valued decoder. On the one hand, the classifier permits to optimization of each latent axis of the embeddings for the classification of a specific emotion-related characteristic: valence, arousal, dominance, and overall emotion. On the other hand, quaternion reconstruction enables the latent dimension to develop intra-channel correlations that are required for an effective representation as a quaternion entity. We test our approach on speech emotion recognition tasks using four popular datasets: IEMOCAP, RAVDESS, EmoDB, and TESS, comparing the performance of three well-established real-valued CNN architectures (AlexNet, ResNet-50, VGG) and their quaternion-valued equivalent fed with the embeddings created with RH-emo. We obtain a consistent improvement in the test accuracy for all datasets, while drastically reducing the resources' demand of models. Moreover, we performed additional experiments and ablation studies that confirm the effectiveness of our approach.  © 2014 IEEE.",quaternion algebra; quaternion neural networks; Speech emotion recognition; transferable embeddings,Emotion Recognition; Job analysis; Network architecture; Speech processing; Embeddings; Emotion recognition; Features extraction; Neural-networks; Quaternion; Quaternion algebra; Quaternion neural network; Speech emotion recognition; Task analysis; Transferable embedding; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85149423370
Abdusalomov A.B.; Safarov F.; Rakhimov M.; Turaev B.; Whangbo T.K.,"Abdusalomov, Akmalbek Bobomirzaevich (57194333152); Safarov, Furkat (57959865500); Rakhimov, Mekhriddin (57194093135); Turaev, Boburkhon (57463089200); Whangbo, Taeg Keun (35617849900)",57194333152; 57959865500; 57194093135; 57463089200; 35617849900,Improved Feature Parameter Extraction from Speech Signals Using Machine Learning Algorithm,2022,Sensors,22,21,8122,,,,40,10.3390/s22218122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141630184&doi=10.3390%2fs22218122&partnerID=40&md5=e11d3c3db8ce63de1c9fe5202fa18cba,"Speech recognition refers to the capability of software or hardware to receive a speech signal, identify the speaker’s features in the speech signal, and recognize the speaker thereafter. In general, the speech recognition process involves three main steps: acoustic processing, feature extraction, and classification/recognition. The purpose of feature extraction is to illustrate a speech signal using a predetermined number of signal components. This is because all information in the acoustic signal is excessively cumbersome to handle, and some information is irrelevant in the identification task. This study proposes a machine learning-based approach that performs feature parameter extraction from speech signals to improve the performance of speech recognition applications in real-time smart city environments. Moreover, the principle of mapping a block of main memory to the cache is used efficiently to reduce computing time. The block size of cache memory is a parameter that strongly affects the cache performance. In particular, the implementation of such processes in real-time systems requires a high computation speed. Processing speed plays an important role in speech recognition in real-time systems. It requires the use of modern technologies and fast algorithms that increase the acceleration in extracting the feature parameters from speech signals. Problems with overclocking during the digital processing of speech signals have yet to be completely resolved. The experimental results demonstrate that the proposed method successfully extracts the signal features and achieves seamless classification performance compared to other conventional speech recognition algorithms. © 2022 by the authors.",distributed computing; feature extraction; multicore processor; parallel computing; spectral analysis; speech recognition,"Acoustics; Algorithms; Machine Learning; Recognition, Psychology; Speech; Audio signal processing; Cache memory; Classification (of information); Extraction; Interactive computer systems; Learning algorithms; Parameter estimation; Real time systems; Spectrum analysis; Speech communication; Speech recognition; Feature parameters; Features extraction; Machine learning algorithms; Multi-core processor; Parallel com- puting; Parameters extraction; Real - Time system; Recognition process; Speech recognize; Speech signals; acoustics; algorithm; machine learning; speech; Feature extraction",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85141630184
Tao H.; Wang Y.; Zhuang Z.; Fu H.; Guo X.; Zou S.,"Tao, Huawei (55029891400); Wang, Yang (57910181800); Zhuang, Zhihao (57909128700); Fu, Hongliang (7402948092); Guo, Xinying (56022929300); Zou, Shuguang (57306532900)",55029891400; 57910181800; 57909128700; 7402948092; 56022929300; 57306532900,Cross-Corpus Speech Emotion Recognition Based on Transfer Learning and Multi-Loss Dynamic Adjustment,2022,Computational Intelligence and Neuroscience,2022,,5019384,,,,3,10.1155/2022/5019384,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138980096&doi=10.1155%2f2022%2f5019384&partnerID=40&md5=725ce1d5ef49e658d189a2accd9c5498,"In this paper, we do research on cross-corpus speech emotion recognition (SER), in which the training and testing speech signals come from different speech corpus. The mismatched feature distribution between the training and testing sets makes many classical algorithms unable to achieve better results. To deal with this issue, a transfer learning and multi-loss dynamic adjustment (TLMLDA) algorithm is initiatively proposed in this paper. The proposed algorithm first builds a novel deep network model based on a deep auto-encoder and fully connected layers to improve the representation ability of features. Subsequently, global domain and subdomain adaptive algorithms are jointly adopted to implement features transfer. Finally, dynamic weighting factors are constructed to adjust the contribution of different loss functions to prevent optimization offset of model training, which effectively improve the generalization ability of the whole system. The results of simulation experiments on Berlin, eNTERFACE, and CASIA speech corpora show that the proposed algorithm can achieve excellent recognition results, and it is competitive with most of the state-of-the-art algorithms.  © 2022 Huawei Tao et al.",,Algorithms; Emotions; Learning; Machine Learning; Speech; Adaptive algorithms; Emotion Recognition; Adjustment algorithms; Dynamic adjustment; Feature distribution; Speech corpora; Speech emotion recognition; Speech signals; Testing sets; Training and testing; Training sets; Transfer learning; algorithm; emotion; learning; machine learning; speech; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85138980096
Byun K.; Um S.; Kang H.-G.,"Byun, Kyungguen (57038256300); Um, Seyun (57214790370); Kang, Hong-Goo (7404070814)",57038256300; 57214790370; 7404070814,Length-Normalized Representation Learning for Speech Signals,2022,IEEE Access,10,,,60362,60372,10,0,10.1109/ACCESS.2022.3181298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131738874&doi=10.1109%2fACCESS.2022.3181298&partnerID=40&md5=373fe2ce9901e92b16721c9931955c91,"In this study, we proposed a length-normalized representation learning method for speech and text to address the inherent problem of sequence-to-sequence models when the input and output sequences exhibit different lengths. To this end, the representations were constrained to a fixed-length shape by including length normalization and de-normalization processes in the pre- and post-network architecture of the transformer-based self-supervised learning framework. Consequently, this enabled the direct modelling of the relationships between sequences with different length without attention or recurrent network between representation domains. This method not only achieved the aforementioned regularized length effect but also achieved a data augmentation effect that effectively handled differently time-scaled input features. The performance of the proposed length-normalized representations on downstream tasks for speaker and phoneme recognition was investigated to verify the effectiveness of this method over conventional representation methods. In addition, to demonstrate the applicability of the proposed representation method to sequence-to-sequence modeling, a unified speech recognition and text-to-speech (TTS) system was developed. The unified system achieved a high accuracy on a frame-wise phoneme prediction and exhibited a promising potential for the generation of high-quality synthesized speech signals on the TTS.  © 2013 IEEE.",representation learning; Self-supervised learning; speech and text analysis,Character recognition; Speech; Speech communication; Speech recognition; Supervised learning; Features extraction; Representation learning; Representation method; Self-supervised learning; Sequence models; Speech and text analyse; Speech signals; Task analysis; Transformer; Network architecture,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85131738874
Tavi L.; Kinnunen T.; González Hautamäki R.,"Tavi, Lauri (57200114458); Kinnunen, Tomi (23135344600); González Hautamäki, Rosa (56331086500)",57200114458; 23135344600; 56331086500,Improving speaker de-identification with functional data analysis of f0 trajectories,2022,Speech Communication,140,,,1,10,9,14,10.1016/j.specom.2022.03.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127275560&doi=10.1016%2fj.specom.2022.03.010&partnerID=40&md5=10d7794d3e5e649a59e39eadda585bb4,"Due to a constantly increasing amount of speech data that is stored in different types of databases, voice privacy has become a major concern. To respond to such concern, speech researchers have developed various methods for speaker de-identification. The state-of-the-art solutions utilize deep learning solutions which can be effective but might be unavailable or impractical to apply for, for example, under-resourced languages. Formant modification is a simpler, yet effective method for speaker de-identification which requires no training data. Still, remaining intonational patterns in formant-anonymized speech may contain speaker-dependent cues. This study introduces a novel speaker de-identification method, which, in addition to simple formant shifts, manipulates f0 trajectories based on functional data analysis. The proposed speaker de-identification method will conceal plausibly identifying pitch characteristics in a phonetically controllable manner and improve formant-based speaker de-identification up to 25%. © 2022 The Authors",Functional data analysis; Functional principal component analysis; Pitch manipulation; Speaker de-identification,Data handling; Information analysis; Loudspeakers; Principal component analysis; Speech recognition; De-identification; Functional data analysis; Functional principal component analysis; Identification method; Pitch manipulation; Simple++; Speaker de-identification; Speech data; State of the art; Voice privacy; Deep learning,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85127275560
Latif S.; Cuayáhuitl H.; Pervez F.; Shamshad F.; Ali H.S.; Cambria E.,"Latif, Siddique (57195632382); Cuayáhuitl, Heriberto (15764590600); Pervez, Farrukh (57195401482); Shamshad, Fahad (57199999899); Ali, Hafiz Shehbaz (57222063165); Cambria, Erik (56140547500)",57195632382; 15764590600; 57195401482; 57199999899; 57222063165; 56140547500,A survey on deep reinforcement learning for audio-based applications,2023,Artificial Intelligence Review,56,3,,2193,2240,47,42,10.1007/s10462-022-10224-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133247161&doi=10.1007%2fs10462-022-10224-2&partnerID=40&md5=59663fd9ccec2b8bccbbb3623f03249c,"Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields including computer vision, natural language processing, healthcare, robotics, to name a few. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising applications in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together research studies across different but related areas in speech and music. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting important challenges faced by audio-based DRL agents and by highlighting open areas for future research and investigation. The findings of this paper will guide researchers interested in DRL for the audio domain. © 2022, The Author(s).",(Embodied) dialogue; Deep learning; Emotion recognition; Reinforcement learning; Speech recognition,Audio acoustics; Audio systems; Deep learning; Emotion Recognition; Learning systems; Music; Reinforcement learning; Surveys; (embodied) dialog; Audio-based; Deep learning; Emotion recognition; Healthcare robotics; Language processing; Natural languages; Real-world; Reinforcement learning algorithms; Reinforcement learnings; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85133247161
Yanagita T.; Sakti S.; Nakamura S.,"Yanagita, Tomoya (57204215469); Sakti, Sakriani (12807978200); Nakamura, Satoshi (55628545896)",57204215469; 12807978200; 55628545896,Japanese Neural Incremental Text-to-Speech Synthesis Framework With an Accent Phrase Input,2023,IEEE Access,11,,,22355,22363,8,0,10.1109/ACCESS.2023.3251657,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149371602&doi=10.1109%2fACCESS.2023.3251657&partnerID=40&md5=56529b282bfdb7b3763146a470237711,"Work in the development of neural incremental text-to-speech (iTTS), which is attracting increasing attention, has recently pursued low-latency processing by generating speech on the fly before reading complete sentences. Most current state-of-the-art iTTS systems use a prefix-to-prefix neural iTTS framework with look-ahead of 1-2 unit segments (i.e., phonemes or words). However, since the Japanese language is based on accent phrase units that are longer than words, using a prefix-to-prefix neural iTTS with a look-ahead approach increases latency. Here, we propose an alternative to the end-to-end neural iTTS architecture that does not apply look-ahead input when synthesizing speech chunks. We further propose a method to use information from the previous time step by connecting the synthesized vector and the model's internal state to the current time step. We experimentally investigated the latency of various iTTS systems with different modeling and synthesis chunks. The experimental results show that, for Japanese, the proposed iTTS is able to synthesize better speech quality, with a similar latency range, than the conventional baseline prefix-to-prefix neural iTTS with word units. Moreover, we found that our proposed approach improved the prosodic naturalness among synthesized units in the Japanese language. Subjective evaluations also revealed that the proposed approach with an incremental unit of two accent phrases achieved the best scores in Japanese iTTS systems.  © 2013 IEEE.",accent phrase unit; end-to-end; Incremental speech synthesis; Japanese language,Character recognition; Electronic mail; Hidden Markov models; Speech synthesis; Surface treatment; 'current; Accent phrase unit; End to end; Features extraction; Hidden-Markov models; Incremental speech synthesis; Japanese language; Text to speech; Text-to-speech system; Time step; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85149371602
Cho J.; Villalba J.; Moro-Velazquez L.; Dehak N.,"Cho, Jaejin (57204215118); Villalba, Jesús (36667707200); Moro-Velazquez, Laureano (56938696400); Dehak, Najim (16202714800)",57204215118; 36667707200; 56938696400; 16202714800,Non-Contrastive Self-Supervised Learning for Utterance-Level Information Extraction From Speech,2022,IEEE Journal on Selected Topics in Signal Processing,16,6,,1284,1295,11,12,10.1109/JSTSP.2022.3197315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136026605&doi=10.1109%2fJSTSP.2022.3197315&partnerID=40&md5=20d41708ebaecb351a4b71cb940b04ea,"In recent studies, self-supervised pre-trained models tend to outperform supervised pre-trained models in transfer learning. In particular, self-supervised learning of utterance-level speech representation can be used in speech applications that require discriminative representation of consistent attributes within an utterance: speaker, language, emotion, and age. Existing frame-level self-supervised speech representation, e.g., wav2vec, can be used as utterance-level representation with pooling, but the models are usually large. There are also self-supervised learning techniques to learn utterance-level representation. One of the most successful is a contrastive method, which requires negative sampling: selecting alternative samples to contrast with the current sample (anchor). However, this does not ensure that all the negative samples belong to classes different from the anchor class without labels. This paper applies a non-contrastive self-supervised method to learn utterance-level embeddings. We adapted DIstillation with NO labels (DINO) from computer vision to speech. Unlike contrastive methods, DINO does not require negative sampling. We compared DINO to x-vector trained in a supervised manner. When transferred to speaker verification, speech emotion recognition (SER), and Alzheimer's disease detection, DINO outperformed x-vector. We studied the influence of several aspects during transfer learning such as dividing the fine-tuning process into steps, chunk lengths, or augmentation. Fine-tuning the last affine layers first and then the whole network surpassed fine-tuning all at once. Using shorter chunk lengths, although they generate more diverse inputs, did not necessarily improve performance, implying speech segments at least with a specific length are required for better performance per application. Augmentation was helpful in SER.  © 2007-2012 IEEE.",Alzheimer's disease; distillation; emotion recognition; non-contrastive; Self-supervised learning; speaker verification; transfer learning,Distillation; Feature extraction; Job analysis; Neurodegenerative diseases; Speech processing; Speech recognition; Supervised learning; Adaptation models; Alzheimers disease; Emotion recognition; Features extraction; Non-contrastive; Self-supervised learning; Speaker verification; Task analysis; Transfer learning; Emotion Recognition,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85136026605
Bailly G.; Godde E.; Piat-Marchand A.-L.; Bosse M.-L.,"Bailly, Gérard (7003803108); Godde, Erika (57209853199); Piat-Marchand, Anne-Laure (57191528442); Bosse, Marie-Line (8775440100)",7003803108; 57209853199; 57191528442; 8775440100,Automatic assessment of oral readings of young pupils,2022,Speech Communication,138,,,67,79,12,2,10.1016/j.specom.2022.01.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125011276&doi=10.1016%2fj.specom.2022.01.008&partnerID=40&md5=050195c20c1e51f662d393bc712a2be1,"We propose a computational framework for estimating multidimensional subjective ratings of the reading performance of young readers from speech-based objective measures. We combine linguistic features (number of correct words, repetitions, deletions, insertions uttered per minute, etc.) with prosodic features. Expressivity is particularly difficult to predict since there is no unique gold standard. We propose a novel framework for performing such an estimation that exploits multiple references performed by adults and we demonstrate its effectiveness using recordings from a large data set of 1063 oral readings from 442 children (more than 30 h of speech), 84 oral readings from 42 adults and 6853 subjective scores delivered by 29 different human raters. We show that robust and accurate estimations of reading fluency can be achieved using combined features. This automatic assessment tool provides teachers and speech therapists with reliable estimates of the maturation of several reading skills. © 2022 The Authors",Automatic assessment; Children; Fluency; NAEP; Oral readings; Prosody; Speech recognition,Linguistics; Speech; Automatic assessment; Child; Computational framework; Fluency; NAEP; Oral reading; Prosody; Reading performance; Subjective rating; Young readers; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85125011276
Tomashenko N.; Wang X.; Vincent E.; Patino J.; Srivastava B.M.L.; Noé P.-G.; Nautsch A.; Evans N.; Yamagishi J.; O'Brien B.; Chanclu A.; Bonastre J.-F.; Todisco M.; Maouche M.,"Tomashenko, Natalia (55900552200); Wang, Xin (57196088126); Vincent, Emmanuel (14010158800); Patino, Jose (57196401950); Srivastava, Brij Mohan Lal (57191375068); Noé, Paul-Gauthier (57218454233); Nautsch, Andreas (55489138700); Evans, Nicholas (56207371100); Yamagishi, Junichi (7004695833); O'Brien, Benjamin (57216604021); Chanclu, Anaïs (57271268100); Bonastre, Jean-François (6603934225); Todisco, Massimiliano (24482053500); Maouche, Mohamed (57203225403)",55900552200; 57196088126; 14010158800; 57196401950; 57191375068; 57218454233; 55489138700; 56207371100; 7004695833; 57216604021; 57271268100; 6603934225; 24482053500; 57203225403,The VoicePrivacy 2020 Challenge: Results and findings,2022,Computer Speech and Language,74,,101362,,,,61,10.1016/j.csl.2022.101362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124461753&doi=10.1016%2fj.csl.2022.101362&partnerID=40&md5=8709587c992c0d69b4413d728092ba9d,"This paper presents the results and analyses stemming from the first VoicePrivacy 2020 Challenge which focuses on developing anonymization solutions for speech technology. We provide a systematic overview of the challenge design with an analysis of submitted systems and evaluation results. In particular, we describe the voice anonymization task and datasets used for system development and evaluation. Also, we present different attack models and the associated objective and subjective evaluation metrics. We introduce two anonymization baselines and provide a summary description of the anonymization systems developed by the challenge participants. We report objective and subjective evaluation results for baseline and submitted systems. In addition, we present experimental results for alternative privacy metrics and attack models developed as a part of the post-evaluation analysis. Finally, we summarize our insights and observations that will influence the design of the next VoicePrivacy challenge edition and some directions for future voice anonymization research. © 2022 Elsevier Ltd",Anonymization; Attack model; Automatic speech recognition; Metrics; Privacy; Speaker verification; Speech synthesis; Utility; Voice conversion,Speech processing; Speech synthesis; Anonymization; Attack modeling; Automatic speech recognition; Evaluation results; Metric; Objective and subjective evaluations; Privacy; Speaker verification; Utility; Voice conversion; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85124461753
Faisal M.; Alsulaiman M.; Mekhtiche M.; Abdelkader B.M.; Algabri M.; Alrayes T.B.S.; Muhammad G.; Mathkour H.; Abdul W.; Alohali Y.; Al-Hammadi M.; Altaheri H.; Alfakih T.,"Faisal, Mohammed (55354611700); Alsulaiman, Mansour (26667487200); Mekhtiche, Mohamed (56576441500); Abdelkader, Bencherif Mohamed (36056175300); Algabri, Mohammed (56073285900); Alrayes, Tariq Bin Saleh (58217997900); Muhammad, Ghulam (56605566900); Mathkour, Hassan (15925954200); Abdul, Wadood (36098480800); Alohali, Yousef (59157877500); Al-Hammadi, Muneer (55938159200); Altaheri, Hamdi (57192236880); Alfakih, Taha (56576325400)",55354611700; 26667487200; 56576441500; 36056175300; 56073285900; 58217997900; 56605566900; 15925954200; 36098480800; 59157877500; 55938159200; 57192236880; 56576325400,Enabling Two-Way Communication of Deaf Using Saudi Sign Language,2023,IEEE Access,11,,,135423,135434,11,8,10.1109/ACCESS.2023.3337514,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179092501&doi=10.1109%2fACCESS.2023.3337514&partnerID=40&md5=bf652b9cfe4e24caae0c7abd709d509b,"Disabled people are facing many difficulties communicating with others and involving in society. Modern societies have dedicated significant efforts to promote the integration of disabled individuals into their societies and services. Currently, smart healthcare systems are used to facilitate disabled people. The objective of this paper is to enable two-way communication of deaf individuals with the rest of society, thus enabling their migration from marginal elements of society to mainstream contributing elements. In the proposed system, we developed three modules; the sign recognition module (SRM) that recognizes the signs of a deaf individual, the speech recognition and synthesis module (SRSM) that processes the speech of a non-deaf individual and converts it to text, and an Avatar module (AM) to generate and perform the corresponding sign of the non-deaf speech, which were integrated into the sign translation companion system called Saudi deaf companion system (SDCS) to facilitate the communication from the deaf to the hearing and vice versa. This paper also contributes to the literature by utilizing our self-developed database, the largest Saudi Sign Language (SSL) database-the King Saud University Saudi-SSL (KSU-SSL). The proposed SDCS system performs 293 Saudi signs that are recommended by the Saudi Association for Hearing Impairment (SAHI) from 10 domains (healthcare, common, alphabets, verbs, pronouns and adverbs, numbers, days, kings, family, and regions).  © 2013 IEEE.",Avatar; Saudi Sig language; sign language recognition; speech recognition system,Audition; Character recognition; Gesture recognition; Health care; Speech communication; Assistive technology; Auditory systems; Avatar; Gestures recognition; Medical services; Saudi sig language; Sign Language recognition; Speech recognition systems; STEM; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85179092501
Jain R.; Yiwere M.Y.; Bigioi D.; Corcoran P.; Cucu H.,"Jain, Rishabh (57568296800); Yiwere, Mariam Yahayah (57204890157); Bigioi, Dan (57223669237); Corcoran, Peter (57190839462); Cucu, Horia (36439147500)",57568296800; 57204890157; 57223669237; 57190839462; 36439147500,"A Text-to-Speech Pipeline, Evaluation Methodology, and Initial Fine-Tuning Results for Child Speech Synthesis",2022,IEEE Access,10,,,47628,47642,14,17,10.1109/ACCESS.2022.3170836,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129669097&doi=10.1109%2fACCESS.2022.3170836&partnerID=40&md5=152018617127ee56e5129c6e0c384cfd,"Speech synthesis has come a long way as current text-to-speech (TTS) models can now generate natural human-sounding speech. However, most of the TTS research focuses on using adult speech data and there has been very limited work done on child speech synthesis. This study developed and validated a training pipeline for fine-tuning state-of-the-art (SOTA) neural TTS models using child speech datasets. This approach adopts a multi-speaker TTS retuning workflow to provide a transfer-learning pipeline. A publicly available child speech dataset was cleaned to provide a smaller subset of approximately 19 hours, which formed the basis of our fine-tuning experiments. Both subjective and objective evaluations were performed using a pretrained MOSNet for objective evaluation and a novel subjective framework for mean opinion score (MOS) evaluations. Subjective evaluations achieved the MOS of 3.95 for speech intelligibility, 3.89 for voice naturalness, and 3.96 for voice consistency. Objective evaluation using a pretrained MOSNet showed a strong correlation between real and synthetic child voices. Speaker similarity was also verified by calculating the cosine similarity between the embeddings of utterances. An automatic speech recognition (ASR) model is also used to provide a word error rate (WER) comparison between the real and synthetic child voices. The final trained TTS model was able to synthesize child-like speech from reference audio samples as short as 5 seconds.  © 2013 IEEE.",alternative WaveRNN; child speech synthesis; MOSNet; multi-speaker TTS; subjective MOS; tacotron; Text-to-speech,Hidden Markov models; Personnel training; Pipelines; Speech intelligibility; Speech recognition; Alternative WaveRNN; Annotation; Child speech synthesis; Children's speech; Hidden-Markov models; License; Mean opinion scores; MOSNet; Multi-speaker text-to-speech; Noise measurements; Subjective mean opinion score; Tacotra; Task analysis; Text to speech; Speech synthesis,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85129669097
Ooster J.; Tuschen L.; Meyer B.T.,"Ooster, Jasper (57140713100); Tuschen, Laura (57194096804); Meyer, Bernd T. (57214341543)",57140713100; 57194096804; 57214341543,Self-conducted speech audiometry using automatic speech recognition: Simulation results for listeners with hearing loss,2023,Computer Speech and Language,78,,101447,,,,9,10.1016/j.csl.2022.101447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137660361&doi=10.1016%2fj.csl.2022.101447&partnerID=40&md5=a1ede6204045ab869014ed042a8dd150,"Speech-in-noise tests are an important tool for assessing hearing impairment, the successful fitting of hearing aids, as well as for research in psychoacoustics. An important drawback of many speech-based tests is the requirement of an expert to be present during the measurement, in order to assess the listener's performance. This drawback may be largely overcome through the use of automatic speech recognition (ASR), which utilizes automatic response logging. However, such an unsupervised system may reduce the accuracy due to the introduction of potential errors. In this study, two different ASR systems are compared for automated testing: A system with a feed-forward deep neural network (DNN) from a previous study (Ooster et al., 2018), as well as a state-of-the-art system utilizing a time-delay neural network (TDNN). The dynamic measurement procedure of the speech intelligibility test was simulated considering the subjects’ hearing loss and selecting from real recordings of test participants. The ASR systems’ performance is investigated based on responses of 73 listeners, ranging from normal-hearing to severely hearing-impaired as well as read speech from cochlear implant listeners. The feed-forward DNN produced accurate testing results for NH and unaided HI listeners but a decreased measurement accuracy was found in the simulation of the adaptive measurement procedure when considering aided severely HI listeners, recorded in noisy environments with a loudspeaker setup. The TDNN system produces error rates of 0.6% and 3.0% for deletion and insertion errors, respectively. We estimate that the SRT deviation with this system is below 1.38 dB for 95% of the users. This result indicates that a robust unsupervised conduction of the matrix sentence test is possible with a similar accuracy as with a human supervisor even when considering noisy conditions and altered or disordered speech from elderly severely HI listeners and listeners with a CI. © 2022",Automatic speech recognition; Matrix sentence test; Speech audiometry; Unsupervised measurement,Cochlear implants; Deep neural networks; Errors; Feedforward neural networks; Psychoacoustic; Speech intelligibility; Speech recognition; Well testing; Automatic speech recognition; Automatic speech recognition system; Feed forward; Hearing loss; matrix; Matrix sentence test; Measurement procedures; Speech audiometry; Time delay neural networks; Unsupervised measurement; Audition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85137660361
Zhang G.; Qin Y.; Zhang W.; Wu J.; Li M.; Gai Y.; Jiang F.; Lee T.,"Zhang, Guangyan (57209883698); Qin, Ying (57194459041); Zhang, Wenjie (57808468800); Wu, Jialun (57809160100); Li, Mei (57808602100); Gai, Yutao (57808334100); Jiang, Feijun (35198283100); Lee, Tan (7501439194)",57209883698; 57194459041; 57808468800; 57809160100; 57808602100; 57808334100; 35198283100; 7501439194,iEmoTTS: Toward Robust Cross-Speaker Emotion Transfer and Control for Speech Synthesis Based on Disentanglement Between Prosody and Timbre,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,1693,1705,12,11,10.1109/TASLP.2023.3268571,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153801516&doi=10.1109%2fTASLP.2023.3268571&partnerID=40&md5=a8eba915eed2341ee0a78c2da993d125,"Cross-speaker emotion transfer is a common approach to generating emotional speech when speech data with emotion labels from target speakers is not available. This paper presents a novel cross-speaker emotion transfer system named iEmoTTS. The system is composed of an emotion encoder, a prosody predictor, and a timbre encoder. The emotion encoder extracts the identity of emotion type and the respective emotion intensity from the mel-spectrogram of input speech. The emotion intensity is measured by the posterior probability that the input utterance carries that emotion. Unlike many other studies which focus on disentangling speaker and style factors of speech, the iEmoTTS is designed to achieve cross-speaker emotion transfer via disentanglement between prosody and timbre. Prosody is considered the primary carrier of emotion-related speech characteristics, and timbre accounts for the essential characteristics for speaker identification. Zero-shot emotion transfer, meaning that the speech of target speakers is not seen in model training, is also realized with iEmoTTS. Extensive experiments of subjective evaluation have been carried out. The results demonstrate the effectiveness of iEmoTTS compared with other recently proposed systems of cross-speaker emotion transfer. It is shown that iEmoTTS can produce speech with designated emotion types and controllable emotion intensity. With appropriate information bottleneck capacity, iEmoTTS is able to transfer emotional information to a new speaker effectively. Audio samples are publicly available.  © 2014 IEEE.",cross-speaker; emotion intensity; Emotion transfer; prosody; timbre; zero-shot,Human computer interaction; Signal encoding; Speech recognition; Speech synthesis; Zero-shot learning; Cross-speaker; Emotion intensity; Emotion transfer; Features extraction; Hidden-Markov models; Labelings; Prosody; Timbre; Zero-shot; Hidden Markov models,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85153801516
Pavón-Pulido N.; Blasco-García J.D.; López-Riquelme J.A.; Feliu-Batlle J.; Oterino-Bono R.; Herrero M.T.,"Pavón-Pulido, Nieves (27267775600); Blasco-García, Jesús Damián (57904524900); López-Riquelme, Juan Antonio (58204487000); Feliu-Batlle, Jorge (6603060313); Oterino-Bono, Roberto (57904524800); Herrero, María Trinidad (59157654800)",27267775600; 57904524900; 58204487000; 6603060313; 57904524800; 59157654800,JUNO Project: Deployment and Validation of a Low-Cost Cloud-Based Robotic Platform for Reliable Smart Navigation and Natural Interaction with Humans in an Elderly Institution,2023,Sensors,23,1,483,,,,1,10.3390/s23010483,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145971097&doi=10.3390%2fs23010483&partnerID=40&md5=49e3bc56cd7b689373535dafb6289d4e,"This paper describes the main results of the JUNO project, a proof of concept developed in the Region of Murcia in Spain, where a smart assistant robot with capabilities for smart navigation and natural human interaction has been developed and deployed, and it is being validated in an elderly institution with real elderly users. The robot is focused on helping people carry out cognitive stimulation exercises and other entertainment activities since it can detect and recognize people, safely navigate through the residence, and acquire information about attention while users are doing the mentioned exercises. All the information could be shared through the Cloud, if needed, and health professionals, caregivers and relatives could access such information by considering the highest standards of privacy required in these environments. Several tests have been performed to validate the system, which combines classic techniques and new Deep Learning-based methods to carry out the requested tasks, including semantic navigation, face detection and recognition, speech to text and text to speech translation, and natural language processing, working both in a local and Cloud-based environment, obtaining an economically affordable system. The paper also discusses the limitations of the platform and proposes several solutions to the detected drawbacks in this kind of complex environment, where the fragility of users should be also considered. © 2023 by the authors.",autonomous navigation; digital transformation of health systems; human–robot natural interaction; smart assistant robots,Character recognition; Costs; Deep learning; Face recognition; Human robot interaction; Natural language processing systems; Semantics; Speech recognition; Assistant robot; Autonomous navigation; Cloud-based; Digital transformation; Digital transformation of health system; Health systems; Human robots; Human–robot natural interaction; Natural interactions; Smart assistant robot; Navigation,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85145971097
Srivastava B.M.L.; Maouche M.; Sahidullah M.; Vincent E.; Bellet A.; Tommasi M.; Tomashenko N.; Wang X.; Yamagishi J.,"Srivastava, Brij Mohan Lal (57191375068); Maouche, Mohamed (57203225403); Sahidullah, Md (58802043400); Vincent, Emmanuel (14010158800); Bellet, Aurelien (35490138400); Tommasi, Marc (57206488975); Tomashenko, Natalia (55900552200); Wang, Xin (57196088126); Yamagishi, Junichi (7004695833)",57191375068; 57203225403; 58802043400; 14010158800; 35490138400; 57206488975; 55900552200; 57196088126; 7004695833,Privacy and Utility of X-Vector Based Speaker Anonymization,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2383,2395,12,17,10.1109/TASLP.2022.3190741,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134333880&doi=10.1109%2fTASLP.2022.3190741&partnerID=40&md5=7713b22e4910c1fdd9d5df025628342b,"We study the scenario where individuals (speakers) contribute to the publication of an anonymized speech corpus. Data users leverage this public corpus for downstream tasks, e.g., training an automatic speech recognition (ASR) system, while attackers may attempt to de-anonymize it using auxiliary knowledge. Motivated by this scenario, speaker anonymization aims to conceal speaker identity while preserving the quality and usefulness of speech data. In this article, we study x-vector based speaker anonymization, the leading approach in the VoicePrivacy Challenge, which converts the speaker's voice into that of a random pseudo-speaker. We show that the strength of anonymization varies significantly depending on how the pseudo-speaker is chosen. We explore four design choices for this step: the distance metric between speakers, the region of speaker space where the pseudo-speaker is picked, its gender, and whether to assign it to one or all utterances of the original speaker. We assess the quality of anonymization from the perspective of the three actors involved in our threat model, namely the speaker, the user and the attacker. To measure privacy and utility, we use respectively the linkability score achieved by the attackers and the decoding word error rate achieved by an ASR model trained on the anonymized data. Experiments on LibriSpeech show that the best combination of design choices yields state-of-the-art performance in terms of both privacy and utility. Experiments on Mozilla Common Voice further show that it guarantees the same anonymization level against re-identification attacks among 50 speakers as original speech among 20,000 speakers. © 2014 IEEE.",Linkability; privacy; speaker anonymization; voice conversion,Knowledge management; Anonymization; Automatic speech recognition system; Down-stream; Linkability; Privacy; Pseudo speakers; Speaker anonymization; Speech corpora; Speech data; Voice conversion; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85134333880
Salvi D.; Hosler B.; Bestagini P.; Stamm M.C.; Tubaro S.,"Salvi, Davide (57222181353); Hosler, Brian (56825704000); Bestagini, Paolo (21638596100); Stamm, Matthew C. (34870520200); Tubaro, Stefano (7003411765)",57222181353; 56825704000; 21638596100; 34870520200; 7003411765,TIMIT-TTS: A Text-to-Speech Dataset for Multimodal Synthetic Media Detection,2023,IEEE Access,11,,,50851,50866,15,20,10.1109/ACCESS.2023.3276480,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160220716&doi=10.1109%2fACCESS.2023.3276480&partnerID=40&md5=c17fd54a46b59cc3d95bde6da0cd1b22,"With the rapid development of deep learning techniques, the generation and counterfeiting of multimedia material has become increasingly simple. Current technology enables the creation of videos where both the visual and audio contents are falsified. While the multimedia forensics community has begun to address this threat by developing fake media detectors. However, the vast majority existing forensic techniques only analyze one modality at a time. This is an important limitation when authenticating manipulated videos, because sophisticated forgeries may be difficult to detect without exploiting cross-modal inconsistencies (e.g., across the audio and visual tracks). One important reason for the lack of multimodal detectors is a similar lack of research datasets containing multimodal forgeries. Existing datasets typically contain only one falsified modality, such as deepfaked videos with authentic audio tracks, or synthetic audio with no associated video. Currently, datasets are needed that can be used to develop, train, and test these forensic algorithms. In this paper, we propose a new audio-visual deepfake dataset containing multimodal video forgeries. We present a general pipeline for synthesizing deepfake speech content from a given video, facilitating the creation of counterfeit multimodal material. The proposed method uses Text-to-Speech (TTS) and Dynamic Time Warping (DTW) techniques to achieve realistic speech tracks. We use this pipeline to generate and release TIMIT-TTS, a synthetic speech dataset containing the most cutting-edge methods in the TTS field. This can be used as a standalone audio dataset, or combined with DeepfakeTIMIT and VidTIMIT video datasets to perform multimodal research. Finally, we present numerous experiments to benchmark the proposed dataset in both monomodal (i.e., audio) and multimodal (i.e., audio and video) conditions. This highlights the need for multimodal forensic detectors and more multimodal deepfake data.  © 2013 IEEE.",Audio; deepfake; forensics; multimodal; synthetic speech; text-to-speech; TIMIT,Audio acoustics; Deep learning; Digital forensics; Pipelines; Speech recognition; Speech synthesis; Audio; Deepfake; Forensic; Medium; Multi-modal; Synthetic speech; Text to speech; TIMIT; Audio systems,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85160220716
Zhang L.; Wang X.; Cooper E.; Evans N.; Yamagishi J.,"Zhang, Lin (57223708730); Wang, Xin (57196088126); Cooper, Erica (57220570427); Evans, Nicholas (56207371100); Yamagishi, Junichi (7004695833)",57223708730; 57196088126; 57220570427; 56207371100; 7004695833,The PartialSpoof Database and Countermeasures for the Detection of Short Fake Speech Segments Embedded in an Utterance,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,813,825,12,40,10.1109/TASLP.2022.3233236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147227031&doi=10.1109%2fTASLP.2022.3233236&partnerID=40&md5=71a554f2407e6ff43faa71cb5f94fbd8,"Automatic speaker verification is susceptible to various manipulations and spoofing, such as text-to-speech synthesis, voice conversion, replay, tampering, adversarial attacks, and so on. We consider a new spoofing scenario called 'Partial Spoof' (PS) in which synthesized or transformed speech segments are embedded into a bona fide utterance. While existing countermeasures (CMs) can detect fully spoofed utterances, there is a need for their adaptation or extension to the PS scenario. We propose various improvements to construct a significantly more accurate CM that can detect and locate short-generated spoofed speech segments at finer temporal resolutions. First, we introduce newly developed self-supervised pre-trained models as enhanced feature extractors. Second, we extend our PartialSpoof database by adding segment labels for various temporal resolutions. Since the short spoofed speech segments to be embedded by attackers are of variable length, six different temporal resolutions are considered, ranging from as short as 20 ms to as large as 640 ms. Third, we propose a new CM that enables the simultaneous use of the segment-level labels at different temporal resolutions as well as utterance-level labels to execute utterance- and segment-level detection at the same time. We also show that the proposed CM is capable of detecting spoofing at the utterance level with low error rates in the PS scenario as well as in a related logical access (LA) scenario. The equal error rates of utterance-level detection on the PartialSpoof database and ASVspoof 2019 LA database were 0.77 and 0.90%, respectively.  © 2014 IEEE.",Anti-spoofing; countermeasure; deepfake; PartialSpoof; self-supervised learning; spoof localization,Character recognition; Database systems; Feature extraction; Speech processing; Speech synthesis; Antispoofing; Countermeasure; Deepfake; Features extraction; Forgery; Localisation; Partialspoof; Self-supervised learning; Splicing; Spoof localization; Speech recognition,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85147227031
Kąkol K.; Korvel G.; Tamulevičius G.; Kostek B.,"Kąkol, Krzysztof (57193491284); Korvel, Gražina (56719707600); Tamulevičius, Gintautas (36562459300); Kostek, Bożena (6603685068)",57193491284; 56719707600; 36562459300; 6603685068,Detecting Lombard Speech Using Deep Learning Approach,2023,Sensors,23,1,315,,,,2,10.3390/s23010315,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145874079&doi=10.3390%2fs23010315&partnerID=40&md5=22a9d50c99446acac404f53dac4c7d50,"Robust Lombard speech-in-noise detecting is challenging. This study proposes a strategy to detect Lombard speech using a machine learning approach for applications such as public address systems that work in near real time. The paper starts with the background concerning the Lombard effect. Then, assumptions of the work performed for Lombard speech detection are outlined. The framework proposed combines convolutional neural networks (CNNs) and various two-dimensional (2D) speech signal representations. To reduce the computational cost and not resign from the 2D representation-based approach, a strategy for threshold-based averaging of the Lombard effect detection results is introduced. The pseudocode of the averaging process is also included. A series of experiments are performed to determine the most effective network structure and the 2D speech signal representation. Investigations are carried out on German and Polish recordings containing Lombard speech. All 2D signal speech representations are tested with and without augmentation. Augmentation means using the alpha channel to store additional data: gender of the speaker, F0 frequency, and first two MFCCs. The experimental results show that Lombard and neutral speech recordings can clearly be discerned, which is done with high detection accuracy. It is also demonstrated that the proposed speech detection process is capable of working in near real-time. These are the key contributions of this work. © 2022 by the authors.",2D feature representations; Lombard speech; machine learning; speech recognition; threshold-based averaging strategy,Deep Learning; Language; Noise; Speech; Speech Perception; Audio recordings; Convolutional neural networks; Deep learning; Learning systems; Real time systems; Speech communication; 2d feature representation; Feature representation; Lombard effects; Lombard speech; Machine-learning; Near-real time; Signal representations; Speech detection; Speech signals; Threshold-based averaging strategy; language; noise; speech; speech perception; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85145874079
Gonzalez P.; Alstrom T.S.; May T.,"Gonzalez, Philippe (58090954300); Alstrom, Tommy Sonne (36117621200); May, Tobias (36545114400)",58090954300; 36117621200; 36545114400,Assessing the Generalization Gap of Learning-Based Speech Enhancement Systems in Noisy and Reverberant Environments,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,3390,3403,13,11,10.1109/TASLP.2023.3318965,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173394588&doi=10.1109%2fTASLP.2023.3318965&partnerID=40&md5=9c386fe5c3000c52dd18d5354ac7833d,"The acoustic variability of noisy and reverberant speech mixtures is influenced by multiple factors, such as the spectro-temporal characteristics of the target speaker and the interfering noise, the signal-to-noise ratio (SNR) and the room characteristics. This large variability poses a major challenge for learning-based speech enhancement systems, since a mismatch between the training and testing conditions can substantially reduce the performance of the system. Generalization to unseen conditions is typically assessed by testing the system with a new speech, noise or binaural room impulse response (BRIR) database different from the one used during training. However, the difficulty of the speech enhancement task can change across databases, which can substantially influence the results. The present study introduces a generalization assessment framework that uses a reference model trained on the test condition, such that it can be used as a proxy for the difficulty of the test condition. This allows to disentangle the effect of the change in task difficulty from the effect of dealing with new data, and thus to define a new measure of generalization performance termed the generalization gap. The procedure is repeated in a cross-validation fashion by cycling through multiple speech, noise, and BRIR databases to accurately estimate the generalization gap. The proposed framework is applied to evaluate the generalization potential of a feedforward neural network (FFNN), Conv-TasNet, DCCRN and MANNER. We find that for all models, the performance degrades the most in speech mismatches, while good noise and room generalization can be achieved by training on multiple databases. Moreover, while recent models show higher performance in matched conditions, their performance substantially decreases in mismatched conditions and can become inferior to that of the FFNN-based system.  © 2014 IEEE.",deep neural networks; generalization; Speech enhancement,Acoustic noise; Architectural acoustics; Database systems; Feedforward neural networks; Impulse response; Personnel training; Reverberation; Signal to noise ratio; Speech enhancement; Speech recognition; Binaural room impulse response; Condition; Generalisation; Noise measurements; Noisy environment; Performance; Response database; Speech enhancement system; Task analysis; Test condition; Deep neural networks,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85173394588
Adoptante A.J.M.; Baes A.M.; Catilo J.C.A.; Lucero P.K.L.; De Ocampo A.L.P.; Alon A.S.; Dellosa R.M.,"Adoptante, Aunhel John M. (57576420100); Baes, Arnie M. (57575142800); Catilo, John Carlo A. (57575359100); Lucero, Patrick Kendrex L. (57575778700); De Ocampo, Anton Louise P. (57202279546); Alon, Alvin S. (57212308862); Dellosa, Rhowel M. (57205327105)",57576420100; 57575142800; 57575359100; 57575778700; 57202279546; 57212308862; 57205327105,Spoken-Digit Classification using Artificial Neural Network,2023,ASEAN Engineering Journal,13,1,,93,99,6,1,10.11113/aej.V13.18388,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152118928&doi=10.11113%2faej.V13.18388&partnerID=40&md5=9d7c0a261945fd9a276cba23bc671c12,"Audio classification has been one of the most popular applications of Artificial Neural Networks. This process is at the center of modern AI technology, such as virtual assistants, automatic speech recognition, and text-to-speech applications. There have been studies about spoken digit classification and its applications. However, to the best of the author's knowledge, very few works focusing on English spoken digit recognition that implemented ANN classification have been done. In this study, the authors utilized the Mel-Frequency Cepstral Coefficients (MFCC) features of the audio recording and Artificial Neural Network (ANN) as the classifier to recognize the spoken digit by the speaker. The Audio MNIST dataset was used as training and test data while the Free-Spoken Digit Dataset was used as additional validation data. The model showed an F-1 score of 99.56% accuracy for the test data and an F1 score of 81.92% accuracy for the validation data. © 2023 Penerbit UTM Press. All rights reserved.",artificial neural networks; MFCC; signal processing; speech recognition,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85152118928
Farkhod A.; Abdusalomov A.B.; Mukhiddinov M.; Cho Y.-I.,"Farkhod, Akhmedov (57999853900); Abdusalomov, Akmalbek Bobomirzaevich (57194333152); Mukhiddinov, Mukhriddin (57215927861); Cho, Young-Im (15764374600)",57999853900; 57194333152; 57215927861; 15764374600,Development of Real-Time Landmark-Based Emotion Recognition CNN for Masked Faces,2022,Sensors,22,22,8704,,,,42,10.3390/s22228704,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142764943&doi=10.3390%2fs22228704&partnerID=40&md5=af7025d5d1f571bd808ad9659fea519b,"Owing to the availability of a wide range of emotion recognition applications in our lives, such as for mental status calculation, the demand for high-performance emotion recognition approaches remains uncertain. Nevertheless, the wearing of facial masks has been indispensable during the COVID-19 pandemic. In this study, we propose a graph-based emotion recognition method that adopts landmarks on the upper part of the face. Based on the proposed approach, several pre-processing steps were applied. After pre-processing, facial expression features need to be extracted from facial key points. The main steps of emotion recognition on masked faces include face detection by using Haar–Cascade, landmark implementation through a media-pipe face mesh model, and model training on seven emotional classes. The FER-2013 dataset was used for model training. An emotion detection model was developed for non-masked faces. Thereafter, landmarks were applied to the upper part of the face. After the detection of faces and landmark locations were extracted, we captured coordinates of emotional class landmarks and exported to a comma-separated values (csv) file. After that, model weights were transferred to the emotional classes. Finally, a landmark-based emotion recognition model for the upper facial parts was tested both on images and in real time using a web camera application. The results showed that the proposed model achieved an overall accuracy of 91.2% for seven emotional classes in the case of an image application. Image based emotion detection of the proposed model accuracy showed relatively higher results than the real-time emotion detection. © 2022 by the authors.",emotion recognition; face detection; facial expression detection; facial mask; landmark vectors application,COVID-19; Emotions; Face; Facial Expression; Humans; Pandemics; Emotion Recognition; Graphic methods; Navigation; Speech recognition; Emotion detection; Emotion recognition; Faces detection; Facial expression detections; Facial mask; Graph-based; Landmark vector application; Model training; Performance; Real- time; emotion; face; facial expression; human; pandemic; Face recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85142764943
Tian Z.; Yi J.; Tao J.; Zhang S.; Wen Z.,"Tian, Zhengkun (57211634774); Yi, Jiangyan (57191923020); Tao, Jianhua (55898465800); Zhang, Shuai (57196101312); Wen, Zhengqi (54413415000)",57211634774; 57191923020; 55898465800; 57196101312; 54413415000,Hybrid Autoregressive and Non-Autoregressive Transformer Models for Speech Recognition,2022,IEEE Signal Processing Letters,29,,,762,766,4,19,10.1109/LSP.2022.3152128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124822936&doi=10.1109%2fLSP.2022.3152128&partnerID=40&md5=6f675a0cdfab8489713dcff3bed1d7b1,"The autoregressive (AR) models, such as attention-based encoder-decoder models and RNN-Transducer, have achieved great success in speech recognition. They predict the output sequence conditioned on the previous tokens and acoustic encoded states, which is inefficient on GPUs. The non-autoregressive (NAR) models can get rid of the temporal dependency between the output tokens and predict the entire output tokens in one inference step. However, the NAR model still faces two major problems. Firstly, there is still a great gap in performance between the NAR models and the advanced AR models. Secondly, it's difficult for most of the NAR models to train and converge. We propose a hybrid autoregressive and non-autoregressive transformer (HANAT) model, which integrates AR and NAR models deeply by sharing parameters. We assume that the AR model will assist the NAR model to learn some linguistic dependencies and accelerate the convergence. Furthermore, the two-stage hybrid inference is applied to improve the model performance. All the experiments are conducted on a mandarin dataset ASIEHLL-1 and a english dataset librispeech-960 h. The results show that the HANAT can achieve a competitive performance with the AR model and outperform many complicated NAR models. Besides, the RTF is only 1/5 of the AR model.  © 1994-2012 IEEE.",Autoregressive; hybrid; non-autoregressive; speech recognition; transformer,Linguistics; Program processors; Speech; Speech recognition; Auto-regressive; Autoregressive modelling; Encoder-decoder; Hybrid; Non-autoregressive; Predictive models; Transformer; Transformer modeling; Decoding,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85124822936
Wang Y.; Fu H.; Tao H.; Yang J.; Ge H.; Xie Y.,"Wang, Yang (57910181800); Fu, Hongliang (7402948092); Tao, Huawei (55029891400); Yang, Jing (57192452047); Ge, Hongyi (35109057700); Xie, Yue (57189250869)",57910181800; 7402948092; 55029891400; 57192452047; 35109057700; 57189250869,Convolutional Auto-Encoder and Adversarial Domain Adaptation for Cross-Corpus Speech Emotion Recognition,2022,IEICE Transactions on Information and Systems,E105D,10,,1803,1806,3,3,10.1587/transinf.2022EDL8045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139430555&doi=10.1587%2ftransinf.2022EDL8045&partnerID=40&md5=fe75bc91c2486c8917385db8a3bb2bde,"This letter focuses on the cross-corpus speech emotion recognition (SER) task, in which the training and testing speech signals in cross-corpus SER belong to different speech corpora. Existing algorithms are incapable of effectively extracting common sentiment information between different corpora to facilitate knowledge transfer. To address this challenging problem, a novel convolutional auto-encoder and adversarial domain adaptation (CAEADA) framework for cross-corpus SER is proposed. The framework first constructs a one-dimensional convolutional auto-encoder (1D-CAE) for feature processing, which can explore the correlation among adjacent one-dimensional statistic features and the feature representation can be enhanced by the architecture based on encoderdecoder-style. Subsequently the adversarial domain adaptation (ADA) module alleviates the feature distributions discrepancy between the source and target domains by confusing domain discriminator, and specifically employs maximum mean discrepancy (MMD) to better accomplish feature transformation. To evaluate the proposed CAEADA, extensive experiments were conducted on EmoDB, eNTERFACE, and CASIA speech corpora, and the results show that the proposed method outperformed other approaches.  © 2022 The Institute of Electronics, Information and Communication Engineers.",adversarial domain adaptation; convolutional autoencoder; cross-corpus speech emotion recognition,Convolution; Knowledge management; Learning systems; Signal encoding; Speech recognition; Adversarial domain adaptation; Auto encoders; Convolutional autoencoder; Cross-corpus speech emotion recognition; Domain adaptation; One-dimensional; Speech corpora; Speech emotion recognition; Speech signals; Training and testing; Emotion Recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85139430555
Jaradat G.A.; Alzubaidi M.A.; Otoom M.,"Jaradat, Ghadeer A. (58018590300); Alzubaidi, Mohammad A. (35174245000); Otoom, Mwaffaq (26424765100)",58018590300; 35174245000; 26424765100,A Novel Human-Vehicle Interaction Assistive Device for Arab Drivers Using Speech Recognition,2022,IEEE Access,10,,,127514,127529,15,9,10.1109/ACCESS.2022.3226539,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144062423&doi=10.1109%2fACCESS.2022.3226539&partnerID=40&md5=004b6846211f85bf94a9e73575a6c23a,"About one-quarter of all car collisions in the United States are caused by distracted driving, and this ratio is expected to rise. As vehicles are equipped with more elaborate and complex technology, human-vehicle interaction via dashboard displays and controls will become more complex and distracting. Human-vehicle interaction via voice-based technology offers a less distracting alternative. In this study we aim to develop a voice-based car assistant, with a focus on Arabic language speech recognition. We prepare a new 4000-word domain-specific lexicon to comprehensively support driver-vehicle interactions, and we create corresponding text and speech corpora. Then we extract acoustic feature vectors and use various acoustic models to support speech recognition. The language model is created using an n-gram model. Then acoustic and language models, and the lexicon are combined to generate a decoding graph. The text corpus consists of 6110 elements, including words, phrases, and sentences. The speech corpus has more than 60000 recordings (almost 50 hours). For the decoding of noise-free audio, a Deep Neural Network + Hidden Markov Model provided 94.832% accuracy, a Subspace Gaussian Mixture Model + Hidden Markov Model provided 94.2% accuracy, and the best Gaussian Mixture Model + Hidden Markov Model provided 94.13% accuracy. For the decoding of noisy audio, a Deep Neural Network + Hidden Markov Model provided 93.316% accuracy, a Subspace Gaussian Mixture Model + Hidden Markov Model provided 92.62% accuracy, and the best Gaussian Mixture Model + Hidden Markov Model provided 91.82% accuracy. A usability study was conducted on the system with 10 participants. Almost all of the results of that study showed usability ratings of greater than 4.0 out of 5.0. These usability ratings indicate that the proposed system was seen by the participants as important, and useful for reducing driver distraction. © 2013 IEEE.",Arabic language; car assistant; human-vehicle interaction; speech recognition,Complex networks; Decoding; Deep neural networks; Gaussian distribution; Speech recognition; Trellis codes; Vectors; Vehicles; Arabic languages; Assistive devices; Car assistant; Car collisions; Gaussian Mixture Model; Hidden-Markov models; Human vehicle interactions; Speech corpora; Subspace Gaussian Mixture Models; Text corpora; Hidden Markov models,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85144062423
Li K.; Lu X.; Akagi M.; Unoki M.,"Li, Kai (57221174813); Lu, Xugang (22985891900); Akagi, Masato (16318490800); Unoki, Masashi (6604048482)",57221174813; 22985891900; 16318490800; 6604048482,Contributions of Jitter and Shimmer in the Voice for Fake Audio Detection,2023,IEEE Access,11,,,84689,84698,9,5,10.1109/ACCESS.2023.3301616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166756384&doi=10.1109%2fACCESS.2023.3301616&partnerID=40&md5=3dd58052528121a1112a8f32172dcf62,"Fake audio detection (FAD) aims to identify fraudulent speech generated through advanced speech-synthesis techniques. Most current FAD methods rely solely on a deep neural network (DNN) framework with either speech waveforms or commonly used acoustic features to extract high-level representations, overlooking the analysis of prosody differences between genuine and fake speech. Prosody carries important cues about the naturalness of speech and emotional content, which can be leveraged in the detection of fake audio. This paper explicitly investigates the differences in prosody information between genuine and fake speech represented by the jitter and shimmer features. On the basis of our investigation, we found strong evidence that obvious differences exist in the level of jitter and shimmer between fake and real speech, particularly on the shimmer feature that has a large dynamic variation for fake speech. To ensure accurate estimation of F{0} for better jitter and shimmer feature representations, we propose using two additional F{0} estimation methods, YIN and SWIPE, in place of the IRAPT algorithm in the feature extraction process. Moreover, we design a DNN-FAD system by explicitly combining the shimmer and Mel-spectrogram features. The effectiveness of the proposed method for FAD is evaluated in the datasets of Audio Deep Synthesis Detection (ADD) 2022 and 2023 challenges. The experimental results show that both the static and dynamic continuous shimmer features, especially that extracted with the YIN and SWIPE algorithms, can provide complementary knowledge to the traditional spectrum-based FAD systems. The optimal results effectively reduce the equal error rate from 41.29 % to 35.77 % in the ADD2023 challenge, achieving a relative improvement of 13.37 %.  © 2013 IEEE.",amplitude perturbation; Fake audio detection; frequency perturbation; jitter and shimmer features; prosody information,Audio acoustics; Computer architecture; Deep neural networks; Extraction; Frequency estimation; Heuristic algorithms; Heuristic methods; Jitter; Perturbation techniques; Spectrographs; Speech recognition; Speech synthesis; Amplitude perturbation; Audio detection; Fake audio detection; Features extraction; Frequency perturbation; Heuristics algorithm; Jitter and shimmer feature; Perturbation method; Prosody information; Signal processing algorithms; Feature extraction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85166756384
Almutairi Z.M.; Elgibreen H.,"Almutairi, Zaynab M. (57697707300); Elgibreen, Hebah (36701319400)",57697707300; 36701319400,Detecting Fake Audio of Arabic Speakers Using Self-Supervised Deep Learning,2023,IEEE Access,11,,,72134,72147,13,8,10.1109/ACCESS.2023.3286864,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162721139&doi=10.1109%2fACCESS.2023.3286864&partnerID=40&md5=ec70b9f25c4f2d0cc7d86d67037d2df8,"One of the most significant discussions in forensics is Audio Deepfake, where AI-generated tools are used to clone audio content of people's voices. Although it was intended to improve people's lives, attackers utilized it maliciously, compromising the public's safety. Thus, Machine Learning (ML) and Deep Learning (DL) methods have been developed to detect imitated or synthetically faked voices. However, the developed methods suffered from massive training data or excessive pre-processing. To the author's best knowledge, Arabic speech has not yet been explored with synthetic fake audio, and it is very limited to the challenged fakeness, which is imitation. This paper proposed a new Audio Deepfake detection method called Arabic-AD based on self-supervised learning techniques to detect both synthetic and imitated voices. Additionally, it contributed to the literature by creating the first synthetic dataset of a single speaker who perfectly speaks Modern Standard Arabic (MSA). Besides, the accent was also considered by collecting Arabic recordings from non-Arabic speakers to evaluate the robustness of Arabic-AD. Three extensive experiments were conducted to measure the proposed method and compare it to well-known benchmarks in the literature. As a result, Arabic-AD outperformed other state-of-The-Art methods with the lowest EER rate (0.027%), and high detection accuracy (97%) while avoiding the need for excessive training. © 2013 IEEE.",Arabic-AD method; Audio deepfake; deep learning (DL); imitation fakeness; machine learning (ML); modern standard Arabic (MSA),Deep learning; Fake detection; Feature extraction; Speech recognition; Supervised learning; Arabic-AD method; Audio deepfake; Computational modelling; Deep learning; Features extraction; Imitation fakeness; Machine learning; Machine-learning; Modern standard arabic ) modern standard arabic); Modern standards; Robustness; Standard arabics; Synthetic data; Data mining,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85162721139
Nanjappa S.K.; Prakash S.; Burle A.; Nagabhushan N.; Kumar C.S.,"Nanjappa, Sowmya Kyathanahalli (57192682441); Prakash, Sowmya (57191415295); Burle, Aiswarya (58189789000); Nagabhushan, Nandish (58189789100); Kumar, Chaitanya Shashi (58590232400)",57192682441; 57191415295; 58189789000; 58189789100; 58590232400,mySmartCart: a smart shopping list for day-to-day supplies,2023,IAES International Journal of Artificial Intelligence,12,3,,1484,1490,6,1,10.11591/ijai.v12.i3.pp1484-1490,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152965347&doi=10.11591%2fijai.v12.i3.pp1484-1490&partnerID=40&md5=b9b5d96c244e82cf11de616a93a09c9c,"Shopping of day-to-day items and keeping track of the shopping list can be a tedious and a time-consuming procedure, especially if it has to be done frequently. mySmartCart is a mobile application design proposed to transform the traditional way of writing a shopping list to a digitalized smart list which implements voice recognition and handwriting recognition for processing the natural language input of the user. The system design comprises four modules: i) input-which takes voice and handwritten list image input from the user; ii) processing-natural language processing of input data and converted to digital shopping list; iii) classification-list items classified into respective categories using machine learning algorithms; iv) output-searching on e-commerce applications and adding to shopping cart. The design proposed utilizes natural languages to communicate with the user thus enhancing their shopping experience. Google cloud speech recognition and Tesseract optical character recognition (OCR) for natural language processing have been utilized in the prototype along with support vector machine classifier for categorization. © 2023, Institute of Advanced Engineering and Science. All rights reserved.",E-commerce; Machine learning; Natural language processing; Optical character recognition; Smart shopping list; Speech recognition; Support vector machine,,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85152965347
Huang S.-F.; Lin C.-J.; Liu D.-R.; Chen Y.-C.; Lee H.-Y.,"Huang, Sung-Feng (57207859512); Lin, Chyi-Jiunn (57343028900); Liu, Da-Rong (57204215851); Chen, Yi-Chen (57312582900); Lee, Hung-Yi (34969292900)",57207859512; 57343028900; 57204215851; 57312582900; 34969292900,Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,1558,1571,13,39,10.1109/TASLP.2022.3167258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128692277&doi=10.1109%2fTASLP.2022.3167258&partnerID=40&md5=d118c091123c84a4b2aa71975833b1a7,"Personalizing a speech synthesis system is a highly desired application, where the system can generate speech with the user's voice with rare enrolled recordings. There are two main approaches to build such a system in recent works: speaker adaptation and speaker encoding. On the one hand, speaker adaptation methods fine-tune a trained multi-speaker text-to-speech (TTS) model with few enrolled samples. However, they require at least thousands of fine-tuning steps for high-quality adaptation, making it hard to apply on devices. On the other hand, speaker encoding methods encode enrollment utterances into a speaker embedding. The trained TTS model can synthesize the user's speech conditioned on the corresponding speaker embedding. Nevertheless, the speaker encoder suffers from the generalization gap between the seen and unseen speakers. In this paper, we propose applying a meta-learning algorithm to the speaker adaptation method. More specifically, we use Model Agnostic Meta-Learning (MAML) as the training algorithm of a multi-speaker TTS model, which aims to find a great meta-initialization to adapt the model to any few-shot speaker adaptation tasks quickly. Therefore, we can also adapt the meta-trained TTS model to unseen speakers efficiently. Our experiments compare the proposed method (Meta-TTS) with two baselines: a speaker adaptation method baseline and a speaker encoding method baseline. The evaluation results show that Meta-TTS can synthesize high speaker-similarity speech from few enrollment samples with fewer adaptation steps than the speaker adaptation baseline and outperforms the speaker encoding baseline under the same training scheme. When the speaker encoder of the baseline is pre-trained with extra 8371 speakers of data, Meta-TTS can still outperform the baseline on LibriTTS dataset and achieve comparable results on VCTK dataset.  © 2014 IEEE.",few-shot; MAML; meta-learning; speaker adaptation; TTS,Encoding (symbols); Learning algorithms; Speech recognition; Speech synthesis; Adaptation methods; Encoding methods; Encodings; Few-shot; Metalearning; Model agnostic meta-learning; Speaker adaptation; Speech models; Text to speech; Signal encoding,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85128692277
Tao R.; Lee K.A.; Das R.K.; Hautamaki V.; Li H.,"Tao, Ruijie (57219785028); Lee, Kong Aik (7501503733); Das, Rohan Kumar (55120153000); Hautamaki, Ville (6507506881); Li, Haizhou (8615868400)",57219785028; 7501503733; 55120153000; 6507506881; 8615868400,Self-Supervised Training of Speaker Encoder With Multi-Modal Diverse Positive Pairs,2023,IEEE/ACM Transactions on Audio Speech and Language Processing,31,,,1706,1719,13,4,10.1109/TASLP.2023.3268568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153801343&doi=10.1109%2fTASLP.2023.3268568&partnerID=40&md5=68519528a58f771c633e5865a342c1e2,"We study a novel neural speaker encoder and its training strategies for speaker recognition without using any identity labels. The speaker encoder is trained to extract a fixed dimensional speaker embedding from a spoken utterance of variable length. Contrastive learning is a typical self-supervised learning technique. However, the contrastive learning of the speaker encoder depends very much on the sampling strategy of positive and negative pairs. It is common that we sample a positive pair of segments from the same utterance. Unfortunately, such a strategy, denoted as poor-man's positive pairs (PPP), lacks the necessary diversity. In this work, we propose a multi-modal contrastive learning technique with novel sampling strategies. By cross-referencing between speech and face data, we find diverse positive pairs (DPP) for contrastive learning, thus improving the robustness of speaker encoder. We train the speaker encoder on the VoxCeleb2 dataset without any speaker labels, and achieve an equal error rate (EER) of 2.89%, 3.17% and 6.27% under the proposed progressive clustering strategy, and an EER of 1.44%, 1.77% and 3.27% under the two-stage learning strategy with pseudo labels, on the three test sets of VoxCeleb1. This novel solution outperforms the state-of-the-art self-supervised learning methods by a large margin, at the same time, achieves comparable results with the supervised learning counterpart. We also evaluate our self-supervised learning technique on the LRS2 and LRW datasets, where speaker information is unavailable. All experiments suggest that the proposed neural architecture and sampling strategies are robust across datasets.  © 2014 IEEE.",diverse positive pairs; multi-modal; progressive clustering; Self-supervised learning; speaker recognition,Face recognition; Job analysis; Learning algorithms; Neural networks; Signal encoding; Speech recognition; Clusterings; Diverse positive pair; Multi-modal; Neural-networks; Progressive clustering; Self-supervised learning; Speaker recognition; Task analysis; Supervised learning,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85153801343
Wang Q.; Zhou X.; Li H.,"Wang, Qinyi (57211639728); Zhou, Xinyuan (57219732932); Li, Haizhou (8615868400)",57211639728; 57219732932; 8615868400,Speech-and-Text Transformer: Exploiting Unpaired Text for End-to-End Speech Recognition,2023,APSIPA Transactions on Signal and Information Processing,12,1,e27,,,,1,10.1561/116.00000001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159592386&doi=10.1561%2f116.00000001&partnerID=40&md5=268f9a043eced6f52b8b38602e201091,"End-to-end automatic speech recognition (ASR) models are typically data-hungry, which depend on a large paired speech-text dataset for the models to be effective. It remains an active area how to increase the linguistic competence of such ASR models with unpaired text data. The conventional techniques that employ an external language model (LM) suffer from high decoding complexity. Pre-training methods have problems of catastrophic forgetting and model capacity gap between the pre-trained modules and the actual tasks. This paper introduces a speech-and-text Transformer to leverage unpaired text and address the above issues. The decoder of the proposed speech-and-text Transformer contains three parallel branches to learn strong text representations from unpaired text and reduce the mismatch between the speech and text representations. An on-demand dual-modality attention mechanism is proposed to automatically select one or two modalities to learn from. Besides, we introduce a novel alternate training algorithm to load speech and text batches alternately and accumulate their gradients. The proposed model is trained with an auxiliary language modeling task. Intra-domain and cross-domain speech recognition experiments are conducted on AISHELL-1, LibriSpeech, and WenetSpeech corpora. Results show competitive performance to the conventional shallow fusion method with negligible computation overheads during inference. © 2023 Q. Wang, X. Zhou and H. Li.",language model; semi-supervised learning; Speech recognition; transformer; unpaired data,Character recognition; Computational linguistics; Decoding; Large dataset; Modeling languages; Supervised learning; Active area; Automatic speech recognition; End to end; Language model; Learn+; Recognition models; Semi-supervised learning; Text representation; Transformer; Unpaired data; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85159592386
Lee S.; Yun J.S.; Yoo S.B.,"Lee, Sungjin (57288989200); Yun, Jun Seok (57415662200); Yoo, Seok Bong (57211812382)",57288989200; 57415662200; 57211812382,Alternative Collaborative Learning for Character Recognition in Low-Resolution Images,2022,IEEE Access,10,,,22003,22017,14,6,10.1109/ACCESS.2022.3153116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125320727&doi=10.1109%2fACCESS.2022.3153116&partnerID=40&md5=6197f16dc7945a8295d69067ae14b8ec,"Character recognition in a single image is a technology utilized in various sensor platforms, such as smart parking and text-to-speech systems, and numerous studies are being conducted to improve its performance by experimenting with novel approaches. However, when low-quality images were inputted to a character recognition neural network for recognition, a difference in the resolution of the training image and low-quality image results in poor accuracy. To resolve this problem, this study proposes a collaborative trainable mechanism that integrates a global image feature extraction-based super-resolution neural network with a character recognition neural network. This collaborative trainable mechanism helps the character recognizer to be robust to inputs with varying quality in the real world. The alternative collaborative learning and character recognition performance test was conducted using the license plate image dataset among various character images, and the effectiveness of the proposed algorithm was verified using a performance test.  © 2013 IEEE.",character recognition; Collaborative learning; global image feature extraction; image super-resolution,Extraction; Image enhancement; Image recognition; License plates (automobile); Neural networks; Optical character recognition; Optical resolving power; Speech recognition; Statistical tests; Collaborative learning; Collaborative Work; Features extraction; Global image feature extraction; Image feature extractions; Image super resolutions; License; Low qualities; Neural-networks; Quality image; Feature extraction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85125320727
Xie Y.; Liang R.; Zhao X.; Liang Z.; Du J.,"Xie, Yue (57189250869); Liang, Ruiyu (25651831600); Zhao, Xiaoyan (57825984200); Liang, Zhenlin (57210142514); Du, Jing (57225991917)",57189250869; 25651831600; 57825984200; 57210142514; 57225991917,Weighted Gradient Pretrain for Low-Resource Speech Emotion Recognition,2022,IEICE Transactions on Information and Systems,E105D,7,,1352,1355,3,0,10.1587/transinf.2022EDL8014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135211708&doi=10.1587%2ftransinf.2022EDL8014&partnerID=40&md5=4e8dace7728f7bfbbc0a1e9212edcb77,"To alleviate the problem of the dependency on the quantity of the training sample data in speech emotion recognition, a weighted gradient pre-train algorithm for low-resource speech emotion recognition is proposed. Multiple public emotion corpora are used for pre-training to generate shared hidden layer (SHL) parameters with the generalization ability. The parameters are used to initialize the downsteam network of the recognition task for the low-resource dataset, thereby improving the recognition performance on low-resource emotion corpora. However, the emotion categories are different among the public corpora, and the number of samples varies greatly, which will increase the difficulty of joint training on multiple emotion datasets. To this end, a weighted gradient (WG) algorithm is proposed to enable the shared layer to learn the generalized representation of different datasets without affecting the priority of the emotion recognition on each corpus. Experiments show that the accuracy is improved by using CASIA, IEMOCAP, and eNTERFACE as the known datasets to pre-train the emotion models of GEMEP, and the performance could be improved further by combining WG with gradient reversal layer. Copyright © 2022 The Institute of Electronics, Information and Communication Engineers.",low-resource; shared hidden layer; speech emotion recognition; weighted gradient,Speech recognition; Emotion corpora; Hidden layers; Low-resource; Performance; Pre-training; Sample data; Shared hidden layer; Speech emotion recognition; Training sample; Weighted gradient; Emotion Recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85135211708
Kwak I.-Y.; Kwag S.; Lee J.; Jeon Y.; Hwang J.; Choi H.-J.; Yang J.-H.; Han S.-Y.; Huh J.H.; Lee C.-H.; Yoon J.W.,"Kwak, Il-Youp (37021669100); Kwag, Sungsu (57194776178); Lee, Junhee (57226124574); Jeon, Youngbae (57198883129); Hwang, Jeonghwan (57212454389); Choi, Hyo-Jung (57927160900); Yang, Jong-Hoon (57458911900); Han, So-Yul (57959279400); Huh, Jun Ho (35113169000); Lee, Choong-Hoon (57226116327); Yoon, Ji Won (55252331400)",37021669100; 57194776178; 57226124574; 57198883129; 57212454389; 57927160900; 57458911900; 57959279400; 35113169000; 57226116327; 55252331400,"Voice Spoofing Detection Through Residual Network, Max Feature Map, and Depthwise Separable Convolution",2023,IEEE Access,11,,,49140,49152,12,12,10.1109/ACCESS.2023.3275790,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161259283&doi=10.1109%2fACCESS.2023.3275790&partnerID=40&md5=390cadd71af6d13bf23dda1681b893db,"The goal of the '2019 Automatic Speaker Verification Spoofing and Countermeasures Challenge' (ASVspoof) was to make it easier to create systems that could identify voice spoofing attacks with high levels of accuracy. However, model complexity and latency requirements were not emphasized in the competition, despite the fact that they are stringent requirements for implementation in the real world. The majority of the top-performing solutions from the competition used an ensemble technique that merged numerous sophisticated deep learning models to maximize detection accuracy. Those approaches struggle with real-world deployment restrictions for voice assistants which would have restricted resources. We merged skip connection (from ResNet) and max feature map (from Light CNN) to create a compact system, and we tested its performance using the ASVspoof 2019 dataset. Our single model achieved a replay attack detection equal error rate (EER) of 0.30% on the evaluation set using an optimized constant Q transform (CQT) feature, outperforming the top ensemble system in the competition, which scored an EER of 0.39%. We experimented using depthwise separable convolutions (from MobileNet) to reduce model sizes; this resulted in an 84.3 percent reduction in parameter count (from 286K to 45K), while maintaining similar performance (EER of 0.36%). Additionally, we used Grad-CAM to clarify which spectrogram regions significantly contribute to the detection of fake data.  © 2013 IEEE.",Voice assistant security; voice presentation attack detection; voice spoofing attack; voice synthesis attack,Deep learning; Fake detection; Feature extraction; Speech recognition; Attack detection; Automatic speaker verification; Equal error rate; Feature map; Performance; Spoofing attacks; Voice assistant security; Voice presentation attack detection; Voice spoofing attack; Voice synthesis attack; Convolution,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85161259283
Khosravani Pour L.; Farrokhi A.,"Khosravani Pour, L. (58500489200); Farrokhi, A. (56528201800)",58500489200; 56528201800,Language recognition by convolutional neural networks,2023,Scientia Iranica,30,1D,,116,123,7,3,10.24200/sci.2022.59110.6064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165468356&doi=10.24200%2fsci.2022.59110.6064&partnerID=40&md5=eccfcca7fa76f7c858aa76731ca0e9d8,"Speech recognition representing a communication between computers and human as a sub eld of computational linguistics or natural language processing has a long history. Automatic Speech Recognition (ASR), Text To Speech (TTS), speech to text, Continuous Speech Recognition (CSR), and interactive voice response systems are di erent approaches to solving problems in this area. The performance improvement is partially attributed to the ability of the Deep Neural Network (DNN) to model complex correlations in speech features. In this paper, unlike the use of conventional model for sequential data like voice that employs Recurrent Neural Networks (RNNs) with the emergence of di erent architectures in deep networks and good performance of Conventional Neural Networks (CNNs) in image processing and feature extraction, the application of CNNs was developed in other domains. It was shown that prosodic features for Persian language could be extracted via CNNs for segmentation and labeling speech for short texts. By using 128 and 200 lters for CNN and special architectures, 19.46 error in detection rate and better time consumption than RNNs were obtained. In addition, CNN simpli es the learning procedure. Experimental results show that CNN networks can be a good feature extractor for speech recognition in various languages. © 2023 Sharif University of Technology. All rights reserved.",Arti cial intelligence; Convolutional neural networks; Persian language; Speech recognition; Speech segmentation,Character recognition; Convolution; Convolutional neural networks; Deep neural networks; Image processing; Natural language processing systems; Network architecture; Recurrent neural networks; Speech communication; Arti cial intelligence; Automatic speech recognition; Convolutional neural network; Language processing; Language recognition; Natural languages; Neural-networks; Performance; Persian languages; Speech segmentation; artificial neural network; experimental study; image processing; language; recognition; segmentation; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85165468356
Dong M.; Yan D.; Wang R.,"Dong, Mingyu (57221932422); Yan, Diqun (24825818400); Wang, Rangding (7405340909)",57221932422; 24825818400; 7405340909,Adversarial Examples Protect Your Privacy on Speech Enhancement System,2023,Computer Systems Science and Engineering,46,1,,1,12,11,0,10.32604/csse.2023.034568,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147447618&doi=10.32604%2fcsse.2023.034568&partnerID=40&md5=4f81025b76a790242d9a95932863e3c0,"Speech is easily leaked imperceptibly. When people use their phones, the personal voice assistant is constantly listening and waiting to be activated. Private content in speech may be maliciously extracted through automatic speech recognition (ASR) technology by some applications on phone devices. To guarantee that the recognized speech content is accurate, speech enhancement technology is used to denoise the input speech. Speech enhancement technology has developed rapidly along with deep neural networks (DNNs), but adversarial examples can cause DNNs to fail. Considering that the vulnerability of DNN can be used to protect the privacy in speech. In this work, we propose an adversarial method to degrade speech enhancement systems, which can prevent the malicious extraction of private information in speech. Experimental results show that the generated enhanced adversarial examples can be removed most content of the target speech or replaced with target speech content by speech enhancement. The word error rate (WER) between the enhanced original example and enhanced adversarial example recognition result can reach 89.0%. WER of target attack between enhanced adversarial example and target example is low at 33.75%. The adversarial perturbation in the adversarial example can bring much more change than itself. The rate of difference between two enhanced examples and adversarial perturbation can reach more than 1.4430. Meanwhile, the transferability between different speech enhancement models is also investigated. The low transferability of the method can be used to ensure the content in the adversarial example is not damaged, the useful information can be extracted by the friendly ASR. This work can prevent the malicious extraction of speech. © 2023 CRL Publishing. All rights reserved.",Adversarial example; deep neural network; privacy protection; speech enhancement,Extraction; Speech enhancement; Speech recognition; Telephone sets; Adversarial example; Automatic Speech Recognition Technology; De-Noise; Enhancement technologies; Privacy protection; Private information; Speech content; Speech enhancement system; Target speech; Word error rate; Deep neural networks,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85147447618
Higuchi Y.; Moritz N.; Le Roux J.; Hori T.,"Higuchi, Yosuke (57211641457); Moritz, Niko (56707728400); Le Roux, Jonathan (18042059200); Hori, Takaaki (7402411522)",57211641457; 56707728400; 18042059200; 7402411522,Momentum Pseudo-Labeling: Semi-Supervised ASR With Continuously Improving Pseudo-Labels,2022,IEEE Journal on Selected Topics in Signal Processing,16,6,,1424,1438,14,17,10.1109/JSTSP.2022.3195367,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135752129&doi=10.1109%2fJSTSP.2022.3195367&partnerID=40&md5=dc95f8be7d0f6626897f277cdede1c2e,"End-to-end automatic speech recognition (ASR) has become a popular alternative to traditional module-based systems, simplifying the model-building process with a single deep neural network architecture. However, the training of end-to-end ASR systems is generally data-hungry: a large amount of labeled data (speech-text pairs) is necessary to learn direct speech-to-text conversion effectively. To make the training less dependent on labeled data, pseudo-labeling, a semi-supervised learning approach, has been successfully introduced to end-to-end ASR, where a seed model is self-trained with pseudo-labels generated from unlabeled (speech-only) data. Here, we propose momentum pseudo-labeling (MPL), a simple yet effective strategy for semi-supervised ASR. MPL consists of a pair of online and offline models that interact and learn from each other, inspired by the mean teacher method. The online model is trained to predict pseudo-labels generated on the fly by the offline model. The offline model maintains an exponential moving average of the online model parameters. The interaction between the two models allows better ASR training on unlabeled data by continuously improving the quality of pseudo-labels. We apply MPL to a connectionist temporal classification-based model and evaluate it on various semi-supervised scenarios with varying amounts of data or domain mismatch. The results demonstrate that MPL significantly improves the seed model by stabilizing the training on unlabeled data. Moreover, we present additional techniques, e.g., the use of Conformer and an external language model, to further enhance MPL, which leads to better performance than other semi-supervised methods based on pseudo-labeling.  © 2007-2012 IEEE.",Deep learning; end-to-end speech recognition; pseudo-labeling; self-training; semi-supervised learning,Deep neural networks; Learning algorithms; Network architecture; Supervised learning; Teaching; Deep learning; End to end; End-to-end speech recognition; Labelings; Predictive models; Pseudo-labeling; Self-training; Semi-supervised learning; Transformer; Speech recognition,Article,Final,All Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85135752129
Atif M.; Franzoni V.,"Atif, Muhammad (57223124418); Franzoni, Valentina (25521823700)",57223124418; 25521823700,Tell Me More: Automating Emojis Classification for Better Accessibility and Emotional Context Recognition†,2022,Future Internet,14,5,142,,,,9,10.3390/fi14050142,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130638125&doi=10.3390%2ffi14050142&partnerID=40&md5=0c48277720572bc4e54fd15c079bb399,"Users of web or chat social networks typically use emojis (e.g., smilies, memes, hearts) to convey in their textual interactions the emotions underlying the context of the communication, aiming for better interpretability, especially for short polysemous phrases. Semantic-based context recognition tools, employed in any chat or social network, can directly comprehend text-based emoticons (i.e., emojis created from a combination of symbols and characters) and translate them into audio information (e.g., text-to-speech readers for individuals with vision impairment). On the other hand, for a comprehensive understanding of the semantic context, image-based emojis require image-recognition algorithms. This study aims to explore and compare different classification methods for pictograms, applied to emojis collected from Internet sources. Each emoji is labeled according to the basic Ekman model of six emotional states. The first step involves extraction of emoji features through convolutional neural networks, which are then used to train conventional supervised machine learning classifiers for purposes of comparison. The second experimental step broadens the comparison to deep learning networks. The results reveal that both the conventional and deep learning classification approaches accomplish the goal effectively, with deep transfer learning exhibiting a highly satisfactory performance, as expected. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",affective computing; artificial intelligence; context information; deep learning; emoticon; emotion recognition; image classification; machine learning; meme; sentic computing,Character recognition; Classification (of information); Convolutional neural networks; Deep learning; Image recognition; Semantics; Speech recognition; Supervised learning; Affective Computing; Context information; Context recognition; Deep learning; Emoticon; Emotion recognition; Images classification; Interpretability; Meme; Sentic Computing; Image classification,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85130638125
Qian Y.; Gong X.; Huang H.,"Qian, Yanmin (35103128400); Gong, Xun (57220842525); Huang, Houjun (57215929660)",35103128400; 57220842525; 57215929660,Layer-Wise Fast Adaptation for End-to-End Multi-Accent Speech Recognition,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,2842,2853,11,30,10.1109/TASLP.2022.3198546,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136906044&doi=10.1109%2fTASLP.2022.3198546&partnerID=40&md5=efaa3e54c60785202d5a824657ba1384,"The variety and complexity of accents pose a huge challenge to robust Automatic Speech Recognition (ASR). Some previous work has attempted to address such problems, however most of the current approaches either require prior knowledge about the target accent, or cannot handle unseen accents and accent-unspecific standard speech. In this work, we aim to improve multi-accent speech recognition in the end-to-end (E2E) framework with a novel layer-wise adaptation architecture. Firstly, we propose a robust deep accent representation learning architecture to obtain accurate accent embedding, and some advanced schemes are designed to further boost the quality of accent embeddings, including phone posteriorgram (PPG) feature, TTS based data augmentation in the training stage, test-time augmentation and multi-embedding fusion in the testing stage. Then, the layer-wise adaptation with accent embeddings is developed for fast accent adaptation in ASR, and two types of adapter layers are designed, including the gated adapter layer and multi-basis adapter layer. Compared to the usual two-pass adaptation, these adapter layers are injected between the ASR encoder layers to encode the accent information in ASR flexibly, and perform fast adaption on the corresponding speech accent. The experiments on Accent AESRC corpus show that the proposed deep accent representation learning can capture accurate accent knowledge, and get high performance on accent classification. The new layer-wise adaptation architecture with the accurate accent embedding outperforms the other traditional methods, and obtains consistent ∼15% relative word error rate (WER) reduction on all kinds of testing scenarios, including seen accents, unseen accents and accent-unspecific standard speech.  © 2014 IEEE.",accent embedding; End-to-end speech recognition; layer-wise adaptation; multi-accent,Architecture; Deep learning; Job analysis; Personnel training; Speech recognition; Accent embedding; Adaptation models; Decoding; Embeddings; End to end; End-to-end speech recognition; Layer-wise; Layer-wise adaptation; Multi-accent; Representation learning; Task analysis; Embeddings,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85136906044
Song H.; Kim M.; Shin J.W.,"Song, Hyungchan (57216730179); Kim, Minseung (57737496200); Shin, Jong Won (56973022900)",57216730179; 57737496200; 56973022900,Speech Enhancement Using MLP-Based Architecture with Convolutional Token Mixing Module and Squeeze-and-Excitation Network,2022,IEEE Access,10,,,119283,119289,6,3,10.1109/ACCESS.2022.3221440,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141646142&doi=10.1109%2fACCESS.2022.3221440&partnerID=40&md5=f231e1c589403ba1d6f74adc9848cbd8,"The Conformer has shown impressive performance for speech enhancement by exploiting the local and global contextual information, although it requires high computational complexity and many parameters. Recently, multi-layer perceptron (MLP)-based models such as MLP-mixer and gMLP have demonstrated comparable performances with much less computational complexity in the computer vision area. These models showed that all-MLP architectures may perform as good as more advanced structures, but the nature of the MLP limits the application of these architectures to the input with a variable length such as speech and audio. In this paper, we propose the cgMLP-SE model, which is a gMLP-based architecture with convolutional token mixing modules and squeeze-and-excitation network to utilize both local and global contextual information as in the Conformer. Specifically, the token-mixing modules in gMLP are replaced by convolutional layers, squeeze-and-excitation network-based gating is applied on top of the convolutional gating module, and additional feed-forward layers are added to make the cgMLP-SE module a macaron-like structure sandwiched by feed-forward layers like a Conformer block. Experimental results on the TIMIT-DNS noise dataset and the Voice Bank-DEMAND dataset showed that the proposed method exhibited similar speech quality and intelligibility to the Conformer with a smaller model size and less computational complexity.  © 2013 IEEE.",local and global information; low computational complexity; Speech enhancement,Complex networks; Computer architecture; Convolution; Deep learning; Network architecture; Speech enhancement; Speech intelligibility; Speech recognition; Computational modelling; Contextual information; Feed forward; Global informations; Local information; Low computational complexity; Multilayers perceptrons; Perceptron architecture; Performance; Task analysis; Computational complexity,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85141646142
Manohar K.; Jayan A.R.; Rajan R.,"Manohar, Kavya (57219057165); Jayan, A.R. (24479268100); Rajan, Rajeev (57197984309)",57219057165; 24479268100; 57197984309,Mlphon: A Multifunctional Grapheme-Phoneme Conversion Tool Using Finite State Transducers,2022,IEEE Access,10,,,97555,97575,20,10,10.1109/ACCESS.2022.3204403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137849393&doi=10.1109%2fACCESS.2022.3204403&partnerID=40&md5=0089e6eb46637cad9d83e1f343e8fd08,"In this article we present the design and the development of a knowledge based computational linguistic tool, Mlphon for Malayalam language. Mlphon computationally models linguistic rules using finite state transducers and performs multiple functions including grapheme to phoneme (g2p) and phoneme to grapheme (p2g) conversions, syllabification, phonetic feature analysis and script grammar check. This open source software tool, released under MIT license, is developed as a one-stop solution to handle different speech related text processing tasks for automatic speech recognition, text to speech synthesis and non-speech natural language processing tasks including syllable subword based language modeling, phoneme diversity analysis and text sanity check. The tool is evaluated on a manually crafted gold standard lexicon. Mlphon performs orthographic syllabification with 99% accuracy with a syllable error rate of 0.62% on the gold standard lexicon. For grapheme to phoneme conversion task, overall phoneme recognition accuracy of 99% with a phoneme error rate of 0.55% is obtained on gold standard lexicon. Additionally an extrinsic evaluation of Mlphon is performed by employing the pronunciation lexicon created using Mlphon, in Malayalam automatic speech recognition (ASR) task. Performance analysis in terms of the computation time of lexicon creation process and the word error rate (WER) on ASR task are presented along with a comparison over other automated tools for lexicon creation. Pronunciation lexicons with more than 100k commonly used Malayalam words in phonemised and syllabified forms is created and they are published as open language resources along with this work. We also demonstrate the usage of Mlphon on different natural language processing applications - syllable subword ASR, assisted pronunciation learning, phoneme diversity analysis and text sanity check. Being a knowledge based solution with open source code, Mlphon can be adapted to other languages of similar script nature. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Computational Phonology; Low Resource Languages; Malayalam; Pronunciation Lexicon; Software Tool; Speech Recognition; Syllabification,Character recognition; Computational linguistics; Errors; Modeling languages; Natural language processing systems; Open source software; Open systems; Speech recognition; Speech synthesis; Transducers; Automatic speech recognition; Computational modelling; Computational phonology; Knowledge-based systems; Language processing; Low resource languages; Malayalams; Natural language processing; Natural languages; Pronunciation lexicon; Software-tools; Syllabification; Text-processing; Vocabulary; Knowledge based systems,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85137849393
Burggraf P.; Beyer M.; Ganser J.-P.; Adlon T.; Muller K.; Riess C.; Zollner K.; Sabmannshausen T.; Kammerer V.,"Burggraf, Peter (36637002600); Beyer, Moritz (57735399800); Ganser, Jan-Philip (57203636754); Adlon, Tobias (57193856420); Muller, Katharina (57209691216); Riess, Constantin (57735281300); Zollner, Kaspar (57735254400); Sabmannshausen, Till (57728470200); Kammerer, Vincent (57735310100)",36637002600; 57735399800; 57203636754; 57193856420; 57209691216; 57735281300; 57735254400; 57728470200; 57735310100,Preferences for Single-Turn vs. Multiturn Voice Dialogs in Automotive Use Cases - Results of an Interactive User Survey in Germany,2022,IEEE Access,10,,,55020,55033,13,2,10.1109/ACCESS.2022.3174592,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131702836&doi=10.1109%2fACCESS.2022.3174592&partnerID=40&md5=4a586e175056b128d491c3ca3a6f74cc,"Voice assistants have manifested their existence in the vehicle over the last decade. Further technological developments in the area of voice recognition and interactions with users open up the opportunity for new customer-centric user scenarios. In the following work, eight dialog use cases and two different interaction types were examined in detail. The focus of this work is to answer the research question to which extent do users prefer task-oriented multi-turn dialogs over the question-and-answer single-turn dialogs in certain driving situations. Employing a three-step online survey in 2020, participants were asked about their preferences for the assistant's interaction type, use case as well as the perceived usefulness and pleasantness. The authors found that users preferred multi-turn conversations over single-turn conversations for all defined use case scenarios. As further challenges for the future development of voice assistants in the automotive context, the changed driving situation due to e.g. progress in autonomous driving and the focus on an integration of the voice modality as a direct function should be considered.  © 2013 IEEE.",Automotive; Chatbot; Voice communication; Voice control; Voice dialog,Interactive computer systems; Job analysis; Man machine systems; Speech communication; Surveys; Automotives; Chatbots; Driving situations; Multi-turn; Oral communication; Task analysis; Virtual assistants; Voice control; Voice dialogue; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85131702836
Jiang P.; Pan W.; Zhang J.; Wang T.; Huang J.,"Jiang, Peiyuan (58220494300); Pan, Weijun (36449413900); Zhang, Jian (58560268600); Wang, Teng (58600347600); Huang, Junxiang (57214112817)",58220494300; 36449413900; 58560268600; 58600347600; 57214112817,A Robust Conformer-Based Speech Recognition Model for Mandarin Air Traffic Control,2023,"Computers, Materials and Continua",77,1,,911,940,29,1,10.32604/cmc.2023.041772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176473150&doi=10.32604%2fcmc.2023.041772&partnerID=40&md5=b6fe623ae27e929b35be4f4e5024689e,"This study aims to address the deviation in downstream tasks caused by inaccurate recognition results when applying Automatic Speech Recognition (ASR) technology in the Air Traffic Control (ATC) field. This paper presents a novel cascaded model architecture, namely Conformer-CTC/Attention-T5 (CCAT), to build a highly accurate and robust ATC speech recognition model. To tackle the challenges posed by noise and fast speech rate in ATC, the Conformer model is employed to extract robust and discriminative speech representations from raw waveforms. On the decoding side, the Attention mechanism is integrated to facilitate precise alignment between input features and output characters. The Text-To-Text Transfer Transformer (T5) language model is also introduced to handle particular pronunciations and code-mixing issues, providing more accurate and concise textual output for downstream tasks. To enhance the model’s robustness, transfer learning and data augmentation techniques are utilized in the training strategy. The model’s performance is optimized by performing hyperparameter tunings, such as adjusting the number of attention heads, encoder layers, and the weights of the loss function. The experimental results demonstrate the significant contributions of data augmentation, hyperparameter tuning, and error correction models to the overall model performance. On the Our ATC Corpus dataset, the proposed model achieves a Character Error Rate (CER) of 3.44%, representing a 3.64% improvement compared to the baseline model. Moreover, the effectiveness of the proposed model is validated on two publicly available datasets. On the AISHELL-1 dataset, the CCAT model achieves a CER of 3.42%, showcasing a 1.23% improvement over the baseline model. Similarly, on the LibriSpeech dataset, the CCAT model achieves a Word Error Rate (WER) of 5.27%, demonstrating a performance improvement of 7.67% compared to the baseline model. Additionally, this paper proposes an evaluation criterion for assessing the robustness of ATC speech recognition systems. In robustness evaluation experiments based on this criterion, the proposed model demonstrates a performance improvement of 22% compared to the baseline model. © 2023 Tech Science Press. All rights reserved.",Air traffic control; automatic speech recognition; conformer; robustness evaluation; T5 error correction model,Air navigation; Error correction; Robustness (control systems); Speech recognition; Automatic speech recognition; Baseline models; Conformer; Data augmentation; Down-stream; Error correction models; Performance; Recognition models; Robustness evaluation; T5 error correction model; Air traffic control,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85176473150
Li X.,"Li, Xi (57750035500)",57750035500,Construction of English Translation Model Based on Improved Fuzzy Semantic Optimal Control of GLR Algorithm,2022,Scientific Programming,2022,,3376361,,,,2,10.1155/2022/3376361,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132223713&doi=10.1155%2f2022%2f3376361&partnerID=40&md5=b38df2a0779e33b42fc6ebd6453471be,"Data point overlap exists in the model translation recognition results of generalized maximum likelihood ratio detection (GLR) algorithm. A fuzzy semantic optimal control intelligent recognition model for English translation based on improved GLR algorithm is proposed. This algorithm is used to create a phrase corpus for marking tens of thousands of English and Chinese words, so that phrases can be searched automatically. The algorithm builds a phrase corpus of about 710,000 Chinese and English words. Phrase structure is constructed through phrase centers. Partial speech recognition results can be obtained. According to the syntactic function of analytic linear list, the ambiguity of Chinese and English structures in part of speech recognition results is corrected. Finally get the content of the identifier on the basis of comprehensive evaluation. The recognition accuracy based on the improved algorithm is more than 95%. The overall score was 92.3. This algorithm overcomes the disadvantages of GLR. Compared with statistical algorithm and dynamic memory algorithm, the algorithm improves the operation speed and processing performance and is more suitable for machine translation tasks. It provides a new idea in the field of machine translation.  © 2022 Xi Li.",,Computer aided language translation; Machine translation; Maximum likelihood; Semantics; Speech recognition; Datapoints; Fuzzy semantics; Generalized maximum likelihood ratios; Intelligent recognition; Machine translations; Model translation; Model-based OPC; Optimal controls; Recognition models; Translation models; Computational linguistics,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85132223713
Seymour W.; Van Kleek M.,"Seymour, William (57202435989); Van Kleek, Max (6507917811)",57202435989; 6507917811,"Exploring Interactions between Trust, Anthropomorphism, and Relationship Development in Voice Assistants",2021,Proceedings of the ACM on Human-Computer Interaction,5,CSCW2,371,,,,45,10.1145/3479515,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117883291&doi=10.1145%2f3479515&partnerID=40&md5=92c53c1e63eeb5bd575b1d22621799a7,"Modern conversational agents such as Alexa and Google Assistant represent significant progress in speech recognition, natural language processing, and speech synthesis. But as these agents have grown more realistic, concerns have been raised over how their social nature might unconsciously shape our interactions with them. Through a survey of 500 voice assistant users, we explore whether users' relationships with their voice assistants can be quantified using the same metrics as social, interpersonal relationships; as well as if this correlates with how much they trust their devices and the extent to which they anthropomorphise them. Using Knapp's staircase model of human relationships, we find that not only can human-device interactions be modelled in this way, but also that relationship development with voice assistants correlates with increased trust and anthropomorphism.  © 2021 ACM.",anthropomorphism; relationship development; trust; voice assistants,Natural language processing systems; Speech synthesis; User interfaces; Anthropomorphism; Conversational agents; Google+; Human relationships; Interpersonal relationship; Relationship development; Trust; Users' relationships; Voice assistant; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85117883291
Shi X.; Cooper E.; Yamagishi J.,"Shi, Xuan (57226609517); Cooper, Erica (57220570427); Yamagishi, Junichi (7004695833)",57226609517; 57220570427; 7004695833,Use of Speaker Recognition Approaches for Learning and Evaluating Embedding Representations of Musical Instrument Sounds,2022,IEEE/ACM Transactions on Audio Speech and Language Processing,30,,,367,377,10,8,10.1109/TASLP.2022.3140549,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122592704&doi=10.1109%2fTASLP.2022.3140549&partnerID=40&md5=b533318f8622ce22dce485416167e7fb,"Constructing an embedding space for musical instrument sounds that can meaningfully represent new and unseen instruments is important for downstream music generation tasks such as multi-instrument synthesis and timbre transfer. The framework of Automatic Speaker Verification (ASV) provides us with architectures and evaluation methodologies for verifying the identities of unseen speakers, and these can be repurposed for the task of learning and evaluating a musical instrument sound embedding space that can support unseen instruments. Borrowing from state-of-the-art ASV techniques, we construct a musical instrument recognition model that uses a SincNet front-end, a ResNet architecture, and an angular softmax objective function. Experiments on the NSynth and RWC datasets show our model's effectiveness in terms of equal error rate (EER) for unseen instruments, and ablation studies show the importance of data augmentation and the angular softmax objective. Experiments also show the benefit of using a CQT-based filterbank for initializing SincNet over a Mel filterbank initialization. Further complementary analysis of the learned embedding space is conducted with t-SNE visualizations and probing classification tasks, which show that including instrument family labels as a multi-task learning target can help to regularize the embedding space and incorporate useful structure, and that meaningful information such as playing style, which was not included during training, is contained in the embeddings of unseen instruments. © 2014 IEEE.",automatic speaker verification; deep learning; Musical instrument embeddings; speaker recognition,Classification (of information); Deep learning; Filter banks; Musical instruments; Speech processing; Speech recognition; Automatic speaker verification; Deep learning; Down-stream; Embeddings; Features extraction; Musical instrument embedding; Musical instrument sounds; Speaker recognition; Task analysis; Timbre; Music,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85122592704
Laptev A.; Andrusenko A.; Podluzhny I.; Mitrofanov A.; Medennikov I.; Matveev Y.,"Laptev, Aleksandr (57219470555); Andrusenko, Andrei (57211637170); Podluzhny, Ivan (57219762584); Mitrofanov, Anton (57208009720); Medennikov, Ivan (56406445900); Matveev, Yuri (7006613471)",57219470555; 57211637170; 57219762584; 57208009720; 56406445900; 7006613471,Dynamic acoustic unit augmentation with bpe-dropout for low-resource end-to-end speech recognition,2021,Sensors,21,9,3063,,,,12,10.3390/s21093063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104833718&doi=10.3390%2fs21093063&partnerID=40&md5=e0273bedfa44d462929da6cb67f37ddb,"With the rapid development of speech assistants, adapting server-intended automatic speech recognition (ASR) solutions to a direct device has become crucial. For on-device speech recognition tasks, researchers and industry prefer end-to-end ASR systems as they can be made resource-efficient while maintaining a higher quality compared to hybrid systems. However, building end-to-end models requires a significant amount of speech data. Personalization, which is mainly handling out-of-vocabulary (OOV) words, is another challenging task associated with speech assis-tants. In this work, we consider building an effective end-to-end ASR system in low-resource setups with a high OOV rate, embodied in Babel Turkish and Babel Georgian tasks. We propose a method of dynamic acoustic unit augmentation based on the Byte Pair Encoding with dropout (BPE-dropout) technique. The method non-deterministically tokenizes utterances to extend the token’s contexts and to regularize their distribution for the model’s recognition of unseen words. It also reduces the need for optimal subword vocabulary size search. The technique provides a steady improvement in regular and personalized (OOV-oriented) speech recognition tasks (at least 6% relative word error rate (WER) and 25% relative F-score) at no additional computational cost. Owing to the BPE-dropout use, our monolingual Turkish Conformer has achieved a competitive result with 22.2% character error rate (CER) and 38.9% WER, which is close to the best published multilingual system. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Augmentation; BABEL Georgian; BABEL Turkish; BPE-dropout; End-to-end speech recognition; Low-resource; Out-of-vocabulary; Transformer,Acoustics; Speech; Speech Perception; Speech Recognition Software; Vocabulary; Hybrid systems; Speech; Automatic speech recognition; Byte-pair encoding; Character error rates; Computational costs; End-to-end models; Multilingual system; Out of vocabulary words; Resource-efficient; acoustics; automatic speech recognition; speech; speech perception; vocabulary; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85104833718
Uddin I.; Ramli D.A.; Khan A.; Bangash J.I.; Fayyaz N.; Khan A.; Kundi M.,"Uddin, Imran (57212860147); Ramli, Dzati A. (22433855400); Khan, Abdullah (55803876600); Bangash, Javed Iqbal (55647373700); Fayyaz, Nosheen (55973905100); Khan, Asfandyar (7404910098); Kundi, Mahwish (57221951401)",57212860147; 22433855400; 55803876600; 55647373700; 55973905100; 7404910098; 57221951401,Benchmark Pashto Handwritten Character Dataset and Pashto Object Character Recognition (OCR) Using Deep Neural Network with Rule Activation Function,2021,Complexity,2021,,6669672,,,,14,10.1155/2021/6669672,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102651206&doi=10.1155%2f2021%2f6669672&partnerID=40&md5=ab8b0d82c118e019bf292091822db624,"In the area of machine learning, different techniques are used to train machines and perform different tasks like computer vision, data analysis, natural language processing, and speech recognition. Computer vision is one of the main branches where machine learning and deep learning techniques are being applied. Optical character recognition (OCR) is the ability of a machine to recognize the character of a language. Pashto is one of the most ancient and historical languages of the world, spoken in Afghanistan and Pakistan. OCR application has been developed for various cursive languages like Urdu, Chinese, and Japanese, but very little work is done for the recognition of the Pashto language. When it comes to handwritten character recognition, it becomes more difficult for OCR to recognize the characters as every handwritten character's shape is influenced by the writer's hand motion dynamics. The reason for the lack of research in Pashto handwritten character data as compared to other languages is because there is no benchmark dataset available for experimental purposes. This study focuses on the creation of such a dataset, and then for the evaluation purpose, a machine is trained to correctly recognize unseen Pashto handwritten characters. To achieve this objective, a dataset of 43000 images was created. Three Feed Forward Neural Network models with backpropagation algorithm using different Rectified Linear Unit (ReLU) layer configurations (Model 1 with 1-ReLU Layer, Model 2 with 2-ReLU layers, and Model 3 with 3-ReLU Layers) were trained and tested with this dataset. The simulation shows that Model 1 achieved accuracy up to 87.6% on unseen data while Model 2 achieved an accuracy of 81.60% and 3% accuracy, respectively. Similarly, loss (cross-entropy) was the lowest for Model 1 with 0.15 and 3.17 for training and testing, followed by Model 2 with 0.7 and 4.2 for training and testing, while Model 3 was the last with loss values of 6.4 and 3.69. The precision, recall, and f-measure values of Model 1 were better than those of both Model 2 and Model 3. Based on results, Model 1 (with 1 ReLU activation layer) is found to be the most efficient as compared to the other two models in terms of accuracy to recognize Pashto handwritten characters. © 2021 Imran Uddin et al.",,Backpropagation; Chemical activation; Computer vision; Deep learning; Deep neural networks; Feedforward neural networks; Multilayer neural networks; Natural language processing systems; Optical character recognition; Speech recognition; Turing machines; Activation functions; Hand written character recognition; Hand-written characters; Layer configuration; Learning techniques; NAtural language processing; Optical character recognition (OCR); Training and testing; Learning systems,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85102651206
Kabir M.M.; Mridha M.F.; Shin J.; Jahan I.; Ohi A.Q.,"Kabir, Muhammad Mohsin (57220863661); Mridha, M.F. (36761314400); Shin, Jungpil (7402723945); Jahan, Israt (57213223740); Ohi, Abu Quwsar (57217523197)",57220863661; 36761314400; 7402723945; 57213223740; 57217523197,"A Survey of Speaker Recognition: Fundamental Theories, Recognition Methods and Opportunities",2021,IEEE Access,9,,9442674,79236,79263,27,83,10.1109/ACCESS.2021.3084299,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107218376&doi=10.1109%2fACCESS.2021.3084299&partnerID=40&md5=62a8622c362f11bec48bbf7d5741ea2a,"Humans can identify a speaker by listening to their voice, over the telephone, or on any digital devices. Acquiring this congenital human competency, authentication technologies based on voice biometrics, such as automatic speaker recognition (ASR), have been introduced. An ASR recognizes speakers by analyzing speech signals and characteristics extracted from speaker's voices. ASR has recently become an effective research area as an essential aspect of voice biometrics. Specifically, this literature survey gives a concise introduction to ASR and provides an overview of the general architectures dealing with speaker recognition technologies, and upholds the past, present, and future research trends in this area. This paper briefly describes all the main aspects of ASR, such as speaker identification, verification, diarization etc. Further, the performance of current speaker recognition systems are investigated in this survey with the limitations and possible ways of improvement. Finally, a few unsolved challenges of speaker recognition are presented at the closure of this survey.  © 2013 IEEE.",Automatic speaker recognition; challenges; feature extraction; performance measures; recognition techniques,Biometrics; Digital devices; Speech recognition; Automatic speaker recognition; Challenge; Features extraction; Fundamental theory; Human competencies; Performance measure; Recognition methods; Recognition technique; Speaker recognition; Voice biometrics; Surveys,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85107218376
NAKAYAMA S.; TJANDRA A.; SAKTI S.; NAKAMURA S.,"NAKAYAMA, Sahoko (57207857078); TJANDRA, Andros (56118795700); SAKTI, Sakriani (12807978200); NAKAMURA, Satoshi (55628545896)",57207857078; 56118795700; 12807978200; 55628545896,Code-switching asr and tts using semisupervised learning with machine speech chain,2021,IEICE Transactions on Information and Systems,E104D,10,,1661,1677,16,0,10.1587/transinf.2021EDP7005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116566126&doi=10.1587%2ftransinf.2021EDP7005&partnerID=40&md5=088271b7b938e346cdaae6828aa5e9c6,"The phenomenon where a speaker mixes two or more languages within the same conversation is called code-switching (CS). Handling CS is challenging for automatic speech recognition (ASR) and textto- speech (TTS) because it requires coping with multilingual input. Although CS text or speech may be found in social media, the datasets of CS speech and corresponding CS transcriptions are hard to obtain even though they are required for supervised training. This work adopts a deep learning-based machine speech chain to train CS ASR and CS TTS with each other with semisupervised learning. After supervised learning with monolingual data, the machine speech chain is then carried out with unsupervised learning of either the CS text or speech. The results show that the machine speech chain trains ASR and TTS together and improves performance without requiring the pair of CS speech and corresponding CS text. We also integrate language embedding and language identification into the CS machine speech chain in order to handle CS better by giving language information. We demonstrate that our proposed approach can improve the performance on both a single CS language pair and multiple CS language pairs, including the unknown CS excluded from training data.  © 2021 The Institute of Electronics.",ASR; Code-switching; Language identification; Machine speech chain; Semisupervised learning; TTS,Codes (symbols); Deep learning; Learning algorithms; Natural language processing systems; Speech; Supervised learning; Automatic speech recognition; Code-switching; Embeddings; Improve performance; Language identification; Language pairs; Machine speech chain; Social media; Supervised trainings; Text to speech; Speech recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85116566126
Zhang Y.; Jiang F.; Duan Z.,"Zhang, You (57213457101); Jiang, Fei (55167140300); Duan, Zhiyao (24450312900)",57213457101; 55167140300; 24450312900,One-Class Learning towards Synthetic Voice Spoofing Detection,2021,IEEE Signal Processing Letters,28,,9417604,937,941,4,173,10.1109/LSP.2021.3076358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105097239&doi=10.1109%2fLSP.2021.3076358&partnerID=40&md5=75942a85d229c7a5fae6de88610f4872,"Human voices can be used to authenticate the identity of the speaker, but the automatic speaker verification (ASV) systems are vulnerable to voice spoofing attacks, such as impersonation, replay, text-to-speech, and voice conversion. Recently, researchers developed anti-spoofing techniques to improve the reliability of ASV systems against spoofing attacks. However, most methods encounter difficulties in detecting unknown attacks in practical use, which often have different statistical distributions from known attacks. Especially, the fast development of synthetic voice spoofing algorithms is generating increasingly powerful attacks, putting the ASV systems at risk of unseen attacks. In this work, we propose an anti-spoofing system to detect unknown synthetic voice spoofing attacks (i.e., text-to-speech or voice conversion) using one-class learning. The key idea is to compact the bona fide speech representation and inject an angular margin to separate the spoofing attacks in the embedding space. Without resorting to any data augmentation methods, our proposed system achieves an equal error rate (EER) of 2.19% on the evaluation set of ASVspoof 2019 Challenge logical access scenario, outperforming all existing single systems (i.e., those without model ensemble).  © 1994-2012 IEEE.",Anti-spoofing; feature learning; generalization ability; one-class classification; speaker verification,Electrical engineering; Signal processing; Automatic speaker verification; Data augmentation; Equal error rate; One-class learning; Spoofing attacks; Statistical distribution; Synthetic voices; Voice conversion; Speech recognition,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85105097239
Liu X.; Singh P.K.; Pavlovich P.A.,"Liu, Xiaofeng (57226321153); Singh, Pradeep Kumar (55568517829); Pavlovich, Pljonkin Anton (57190492977)",57226321153; 55568517829; 57190492977,Accent labeling algorithm based on morphological rules and machine learning in English conversion system,2021,Journal of Intelligent Systems,30,1,,881,892,11,3,10.1515/jisys-2020-0144,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111130029&doi=10.1515%2fjisys-2020-0144&partnerID=40&md5=615532419490a1cd32fc966286b8338f,"The dependency of a speech recognition system on the accent of a user leads to the variation in its performance, as the people from different backgrounds have different accents. Accent labeling and conversion have been reported as a prospective solution for the challenges faced in language learning and various other voice-based advents. In the English TTS system, the accent labeling of unregistered words is another very important link besides the phonetic conversion. Since the importance of the primary stress is much greater than that of the secondary stress, and the primary stress is easier to call than the secondary stress, the labeling of the primary stress is separated from the secondary stress. In this work, the labeling of primary accents uses a labeling algorithm that combines morphological rules and machine learning; the labeling of secondary accents is done entirely through machine learning algorithms. After 10 rounds of cross-validation, the average tagging accuracy rate of primary stress was 94%, the average tagging accuracy rate of secondary stress was 94%, and the total tagging accuracy rate was 83.6%. This perceptual study separates the labeling of primary and secondary accents providing the promising outcomes.  © 2021 Xiaofeng Liu et al., published by De Gruyter 2021.",accent labeling; machine learning; stress annotation; text-to-speech conversion; unregistered words,Machine learning; Speech recognition; Conversion systems; Cross validation; Labeling algorithms; Language learning; Morphological rules; Primary stress; Secondary stress; Speech recognition systems; Learning algorithms,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85111130029
Effendi J.; Tjandra A.; Sakti S.; Nakamura S.,"Effendi, Johanes (57202967121); Tjandra, Andros (56118795700); Sakti, Sakriani (12807978200); Nakamura, Satoshi (55628545896)",57202967121; 56118795700; 12807978200; 55628545896,"Multimodal Chain: Cross-Modal Collaboration through Listening, Speaking, and Visualizing",2021,IEEE Access,9,,9424554,70286,70299,13,2,10.1109/ACCESS.2021.3077886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105853597&doi=10.1109%2fACCESS.2021.3077886&partnerID=40&md5=50a42af5f25ff962d2dd68fd98f7323c,"Language is an integral part of human interpersonal communication, which is conveyed through multiple sensory channels. This multisensory communication skill has motivated an extensive number of studies on multimodal information processing, which are trying to develop a system that mimics this natural behaviour. For example, automatic speech recognition (ASR) represents listening activity, text-to-speech (TTS) represents speaking, and various image processing models to represent visual perception. Most are trained and tuned independently using parallel examples from the source to the target modality. However, this is not the case in real-life situations, where a lot of paired data are unavailable. Inspired by this self-supervision of the human auditory and visual perception system, we proposed a multimodal chain mechanism with a weakly-supervised chain training strategy that is trained and tuned jointly. In our proposed framework, when the amount of paired training data are insufficient, collaboration among ASR and TTS, image captioning (IC), and image production models can improve their performance through single or dual-loop chain mechanisms. Our experiment result showed that by using such a closed-loop chain mechanism, we can improve a model with both unpaired and unrelated data from different modalities in a semi-supervised manner. Through the collaboration of speech and visual chains, we improve an ASR model performance with an image-only dataset while maintaining the performance of other models.  © 2013 IEEE.",automatic speech recognition; multimodal machine chain; Semi-supervised learning; speech gchain,Character recognition; Mechanisms; Speech recognition; Vision; Automatic speech recognition; Closed-loop chains; Inter-personal communications; Model performance; Multi-modal information; Multimodal chains; Multisensory communications; Visual perception; Image enhancement,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85105853597
Liu S.; Cao Y.; Wang D.; Wu X.; Liu X.; Meng H.,"Liu, Songxiang (57204043583); Cao, Yuewen (57204211476); Wang, Disong (57218450802); Wu, Xixin (55611772100); Liu, Xunying (13608959800); Meng, Helen (7202279063)",57204043583; 57204211476; 57218450802; 55611772100; 13608959800; 7202279063,Any-to-Many Voice Conversion with Location-Relative Sequence-to-Sequence Modeling,2021,IEEE/ACM Transactions on Audio Speech and Language Processing,29,,9420297,1717,1728,11,72,10.1109/TASLP.2021.3076867,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105104197&doi=10.1109%2fTASLP.2021.3076867&partnerID=40&md5=14c98e6d06ce7afefefe9f322bf3303b,"This paper proposes an any-to-many location-relative, sequence-to-sequence (seq2seq), non-parallel voice conversion approach, which utilizes text supervision during training. In this approach, we combine a bottle-neck feature extractor (BNE) with a seq2seq synthesis module. During the training stage, an encoder-decoder-based hybrid connectionist-temporal-classification-attention (CTC-attention) phoneme recognizer is trained, whose encoder has a bottle-neck layer. A BNE is obtained from the phoneme recognizer and is utilized to extract speaker-independent, dense and rich spoken content representations from spectral features. Then a multi-speaker location-relative attention based seq2seq synthesis model is trained to reconstruct spectral features from the bottle-neck features, conditioning on speaker representations for speaker identity control in the generated speech. To mitigate the difficulties of using seq2seq models to align long sequences, we down-sample the input spectral feature along the temporal dimension and equip the synthesis model with a discretized mixture of logistic (MoL) attention mechanism. Since the phoneme recognizer is trained with large speech recognition data corpus, the proposed approach can conduct any-to-many voice conversion. Objective and subjective evaluations show that the proposed any-to-many approach has superior voice conversion performance in terms of both naturalness and speaker similarity. Ablation studies are conducted to confirm the effectiveness of feature selection and model design strategies in the proposed approach. The proposed VC approach can readily be extended to support any-to-any VC (also known as one/few-shot VC), and achieve high performance according to objective and subjective evaluations.  © 2014 IEEE.",Any-to-many; location relative attention; sequence-to-sequence modeling; voice conversion,Bottles; Location; Signal encoding; Temperature control; Attention mechanisms; Content representation; Feature extractor; Objective and subjective evaluations; Sequence modeling; Speaker independents; Temporal classification; Temporal dimensions; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85105104197
Chadha A.; Abdullah A.; Angeline L.,"Chadha, Ankita (56592253900); Abdullah, Azween (24733713900); Angeline, Lorita (46061117300)",56592253900; 24733713900; 46061117300,A Unique Glottal Flow Parameters based Features for Anti-spoofing Countermeasures in Automatic Speaker Verification,2021,International Journal of Advanced Computer Science and Applications,12,8,,827,835,8,1,10.14569/IJACSA.2021.0120894,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119004035&doi=10.14569%2fIJACSA.2021.0120894&partnerID=40&md5=dde6d3e7f0e988aeee41b846d8ed5aed,"The domain of Automatic Speaker Verification(ASV) is blooming with growing developments in feature engineering and artificial intelligence. Inspite of this, the system is liable to spoofing attacks in the form of synthetic or replayed speech. The difficulty in detecting synthetic speech is due to recent advancements in the Voice conversion and Text-to-speech systems which produce natural, indistinguishable speech. To prevent such attacks, there is a need to develop robust spoof detection systems. In order to achieve this goal, we are proposing estimation of Glottal Flow Parameters (GFP) from speech of genuine speech and synthetic spoof samples. The GFP are further parameterized using time, frequency and Liljencrants–Fant (LF) models. Along with GFP features, the Linear Prediction Cepstrum Co-efficient (LFCC) and statistical parameters are computed. The GFP features are investigated to prove their usefulness in detecting spoofed and genuine speech. The ASV spoof 2019 corpus is used to test the framework and evaluated against the baseline models. The proposed spoof detection framework produces an Equal Error Rate (EER) of 2.39% and tandem Detection Cost Function (t-DCF) of 0.0562 which is found to be better than the state-of-the art technique. © 2021. International Journal of Advanced Computer Science and Applications. All Rights Reserved.",glottal excitation; speaker verification; Spoof detection; synthetic speech; text-to-speech; voice conversion,Cost functions; Feature extraction; Speech synthesis; Antispoofing; Automatic speaker verification; Flow parameters; Glottal excitation; Glottal flow; Speaker verification; Spoof detection; Synthetic speech; Text to speech; Voice conversion; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85119004035
Świetlicka I.; Kuniszyk-Jóźkowiak W.; Świetlicki M.,"Świetlicka, Izabela (55241708800); Kuniszyk-Jóźkowiak, Wiesława (6603420073); Świetlicki, Michał (57189688031)",55241708800; 6603420073; 57189688031,Artificial neural networks combined with the principal component analysis for non-fluent speech recognition,2022,Sensors,22,1,321,,,,16,10.3390/s22010321,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121964448&doi=10.3390%2fs22010321&partnerID=40&md5=975e31cec83fe9c12b81aba0126fde71,"The presented paper introduces principal component analysis application for dimensionality reduction of variables describing speech signal and applicability of obtained results for the disturbed and fluent speech recognition process. A set of fluent speech signals and three speech disturbances—blocks before words starting with plosives, syllable repetitions, and sound-initial prolongations—was transformed using principal component analysis. The result was a model containing four principal components describing analysed utterances. Distances between standardised original variables and elements of the observation matrix in a new system of coordinates were calculated and then applied in the recognition process. As a classifying algorithm, the multilayer perceptron network was used. Achieved results were compared with outcomes from previous experiments where speech samples were parameterised with the Kohonen network application. The classifying network achieved overall accuracy at 76% (from 50% to 91%, depending on the dysfluency type). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Artificial neural networks; Principal component analysis; Speech recognition; Stuttering,"Humans; Neural Networks, Computer; Principal Component Analysis; Speech; Speech Perception; Stuttering; Neural networks; Speech; Speech communication; Speech recognition; Dimensionality reduction; Fluents; Multi layer perceptron networks; Observation matrix; Principal Components; Principal-component analysis; Recognition process; Speech disturbances; Speech signals; Stuttering; human; principal component analysis; speech; speech perception; stuttering; Principal component analysis",Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85121964448
Siyaev A.; Jo G.-S.,"Siyaev, Aziz (57215897021); Jo, Geun-Sik (6701387317)",57215897021; 6701387317,Neuro-Symbolic Speech Understanding in Aircraft Maintenance Metaverse,2021,IEEE Access,9,,,154484,154499,15,88,10.1109/ACCESS.2021.3128616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120497485&doi=10.1109%2fACCESS.2021.3128616&partnerID=40&md5=cb3b6bf8a193bd6cebf47085842f7801,"In the emerging world of metaverses, it is essential for speech communication systems to be aware of context to interact with virtual assets in the 3D world. This paper proposes the metaverse for aircraft maintenance training and education of Boeing-737, supplied with legacy manuals, 3D models, 3D simulators, and aircraft maintenance knowledge. Furthermore, to navigate and control operational flow in the metaverse, which is strictly followed by maintenance manuals, the context-aware speech understanding module Neuro-Symbolic Speech Executor (NSSE) is presented. Unlike conventional speech recognition methods, NSSE applies Neuro-Symbolic AI, which combines neural networks and traditional symbolic reasoning, to understand users' requests and reply based on context and aircraft-specific knowledge. NSSE is developed with an industrially flexible approach by applying only synthetic data for training. Nevertheless, the evaluation process performed with various automatic speech recognition metrics on real users' data showed sustainable results with an average accuracy of 94.7%, Word Error Rate (WER) of 7.5%, and the generalization ability to handle speech requests of users with the non-native pronunciation. The proposed Aircraft Maintenance Metaverse is a cheap and scalable solution for aviation colleges since it replaces expensive physical aircraft with virtual one that can be easily modified and updated. Moreover, the Neuro-Symbolic Speech Executor, playing the role of field expert, provides technical guidance and all the resources to facilitate effective training and education of aircraft maintenance.  © 2013 IEEE.",Aircraft maintenance education; Boeing-737; deep learning; industry 40; metaverse; mixed reality; neuro-symbolic AI; smart glasses; speech recognition; transformer,Air navigation; Deep learning; Maintenance; Mixed reality; Speech; Training aircraft; Aircraft maintenance; Aircraft maintenance education; Boeing 737; Deep learning; Industry 40; Metaverses; Mixed reality; Neuro-symbolic AI; Smart glass; Transformer; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85120497485
Yan C.; Zhang G.; Ji X.; Zhang T.; Zhang T.; Xu W.,"Yan, Chen (57200512381); Zhang, Guoming (57200511711); Ji, Xiaoyu (56081745800); Zhang, Tianchen (57200512800); Zhang, Taimin (57195430593); Xu, Wenyuan (36242752200)",57200512381; 57200511711; 56081745800; 57200512800; 57195430593; 36242752200,The Feasibility of Injecting Inaudible Voice Commands to Voice Assistants,2021,IEEE Transactions on Dependable and Secure Computing,18,3,8669818,1108,1124,16,37,10.1109/TDSC.2019.2906165,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105993934&doi=10.1109%2fTDSC.2019.2906165&partnerID=40&md5=d1ef8025e6664b7488dd177396f15546,"Voice assistants (VAs) such as Siri and Google Now have become an increasingly popular human-machine interaction method and have made various systems voice controllable. Prior work on attacking voice assistants shows that the hidden voice commands that are incomprehensible to people can control the VAs. Hidden voice commands, though 'hidden', are nonetheless audible. In this work, we design a completely inaudible attack, DolphinAttack, that modulates voice commands on ultrasonic carriers to achieve inaudibility. By leveraging the nonlinearity of the microphone circuits, the modulated low-frequency audio commands can be successfully demodulated, recovered, and more importantly interpreted by the voice assistants. We validate DolphinAttack on popular voice assistants, including Siri, Google Now, S Voice, HiVoice, Cortana, Alexa, etc. By injecting a sequence of inaudible voice commands, we show a few proof-of-concept attacks, which include activating Siri to initiate a FaceTime call on iPhone, activating Google Now to turn on the airplane mode, and even manipulating the navigation system in an Audi automobile. We propose hardware and software defense solutions. We validate that it is feasible to detect DolphinAttack by classifying the audios using supported vector machine (SVM), and suggest to re-design voice assistants to be resilient to inaudible voice command attacks.  © 2004-2012 IEEE.",defense; Inaudible voice commands; microphones; security analysis; speech recognition; voice assistants,Air navigation; Navigation systems; Ultrasonic applications; Hardware and software; Human machine interaction; Low-frequency; Proof of concept; Supported vector machines; Voice command; Chemical activation,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85105993934
Pérez A.; Díaz-Munío G.G.; Giménez A.; Silvestre-Cerdà J.A.; Sanchis A.; Civera J.; Jiménez M.; Turró C.; Juan A.,"Pérez, Alejandro (55535904600); Díaz-Munío, Gonçal Garcés (56239474000); Giménez, Adrià (23396306500); Silvestre-Cerdà, Joan Albert (42263028000); Sanchis, Albert (56213222600); Civera, Jorge (35227121900); Jiménez, Manuel (36990070400); Turró, Carlos (18435401600); Juan, Alfons (7005475190)",55535904600; 56239474000; 23396306500; 42263028000; 56213222600; 35227121900; 36990070400; 18435401600; 7005475190,Towards cross-lingual voice cloning in higher education,2021,Engineering Applications of Artificial Intelligence,105,,104413,,,,6,10.1016/j.engappai.2021.104413,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112598607&doi=10.1016%2fj.engappai.2021.104413&partnerID=40&md5=b5dbe607c0c9a1733bc8d2f6e93f8819,"The rapid progress of modern AI tools for automatic speech recognition and machine translation is leading to a progressive cost reduction to produce publishable subtitles for educational videos in multiple languages. Similarly, text-to-speech technology is experiencing large improvements in terms of quality, flexibility and capabilities. In particular, state-of-the-art systems are now capable of seamlessly dealing with multiple languages and speakers in an integrated manner, thus enabling lecturer's voice cloning in languages she/he might not even speak. This work is to report the experience gained on using such systems at the Universitat Politècnica de València (UPV), mainly as a guidance for other educational organizations willing to conduct similar studies. It builds on previous work on the UPV's main repository of educational videos, MediaUPV, to produce multilingual subtitles at scale and low cost. Here, a detailed account is given on how this work has been extended to also allow for massive machine dubbing of MediaUPV. This includes collecting 59 h of clean speech data from UPV's academic staff, and extending our production pipeline of subtitles with a state-of-the-art multilingual and multi-speaker text-to-speech system trained from the collected data. Our main result comes from an extensive, subjective evaluation of this system by lecturers contributing to data collection. In brief, it is shown that text-to-speech technology is not only mature enough for its application to MediaUPV, but also needed as soon as possible by students to improve its accessibility and bridge language barriers. © 2021 The Author(s)",Cross-lingual voice conversion; Educational resources; Multilinguality; OER; Text-to-speech,Clone cells; Cost reduction; Data acquisition; Speech; Speech recognition; Cross-lingual; Cross-lingual voice conversion; Educational resource; Educational videos; High educations; Multilinguality; Multiple languages; OER; Speech technology; Text to speech; Cloning,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85112598607
Bell P.; Fainberg J.; Klejch O.; Li J.; Renals S.; Swietojanski P.,"Bell, Peter (23092749700); Fainberg, Joachim (57191857441); Klejch, Ondrej (57022010500); Li, Jinyu (35488220000); Renals, Steve (7003714982); Swietojanski, Pawel (37125204800)",23092749700; 57191857441; 57022010500; 35488220000; 7003714982; 37125204800,Adaptation Algorithms for Neural Network-Based Speech Recognition: An Overview,2021,IEEE Open Journal of Signal Processing,2,,9296327,33,66,33,62,10.1109/OJSP.2020.3045349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126667298&doi=10.1109%2fOJSP.2020.3045349&partnerID=40&md5=932138248b1896ecafb833dee7dc7a9f,"We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model/neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature. © 2020 IEEE.",Accent adaptation; data augmentation; domain adaptation; regularization; semi-supervised learning; speaker adaptation; speaker embeddings; speech recognition; structured linear transforms,Hidden Markov models; Neural networks; Adaptation algorithms; Data augmentation; Domain adaptation; Model parameters; Network systems; Neural network systems; Relative error rates; Speaker adaptation; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85126667298
Arslan R.S.; Barişçi N.; Arici N.; Koçer S.,"Arslan, Recep Sinan (57205619566); Barişçi, Necaattin (56038499500); Arici, Nursal (42460944900); Koçer, Sabri (8590703300)",57205619566; 56038499500; 42460944900; 8590703300,Detecting and correcting automatic speech recognition errors with a new model,2021,Turkish Journal of Electrical Engineering and Computer Sciences,25,9,,2298,2311,13,1,10.3906/ELK-2010-117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117134759&doi=10.3906%2fELK-2010-117&partnerID=40&md5=62bbd1e25f54c82e885c08b4463fb2f8,"The purpose of automatic speech recognition (ASR) systems is to recognize speech signals obtained from people and convert them into text so that they can be processed by a computer. Although many ASR applications are versatile and widely used in the real world, they still generate relatively inaccurate results. They tend to generate spelling errors in recognized words, especially in noisy environments, in situations where the vocabulary size is increased, and at times when the input speech is of poor quality. The permanent presence of errors in ASR systems has led to the need to find alternative methods for automatic detection and correction of such errors. In this study, the basic principles of ASR evaluation are first summarized, and then a new approach based on the suggestion of an alternative hypothesis is proposed for the detection and correction of these errors generated by ASR systems. The proposed method involves a series of processes such as identifying incorrect words, selecting the ones that can be corrected, and identifying candidate words to replace these words. As a result of the tests carried out by creating different test environments, significant performance improvements for Turkish were achieved and an average of 4.60 % performance improvement was provided. © 2021 Turkiye Klinikleri. All rights reserved.",Alternative hypothesis suggestion; Artificial intelligence; Automatic speech recognition; Automatic speech recognition error correction; Natural language processing,Character recognition; Error correction; Natural language processing systems; Signal processing; Speech; Alternative hypothesis; Alternative hypothesis suggestion; Automatic speech recognition; Automatic speech recognition error correction; Automatic speech recognition system; Errors correction; Performance; Recognition error; Speech signals; Speech recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85117134759
Borrelli C.; Bestagini P.; Antonacci F.; Sarti A.; Tubaro S.,"Borrelli, Clara (57204830553); Bestagini, Paolo (21638596100); Antonacci, Fabio (24340514700); Sarti, Augusto (55074405100); Tubaro, Stefano (7003411765)",57204830553; 21638596100; 24340514700; 55074405100; 7003411765,Synthetic speech detection through short-term and long-term prediction traces,2021,Eurasip Journal on Information Security,2021,1,2,,,,64,10.1186/s13635-021-00116-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104086933&doi=10.1186%2fs13635-021-00116-3&partnerID=40&md5=06bd254e0c864fa17ff17e808833f904,"Several methods for synthetic audio speech generation have been developed in the literature through the years. With the great technological advances brought by deep learning, many novel synthetic speech techniques achieving incredible realistic results have been recently proposed. As these methods generate convincing fake human voices, they can be used in a malicious way to negatively impact on today’s society (e.g., people impersonation, fake news spreading, opinion formation). For this reason, the ability of detecting whether a speech recording is synthetic or pristine is becoming an urgent necessity. In this work, we develop a synthetic speech detector. This takes as input an audio recording, extracts a series of hand-crafted features motivated by the speech-processing literature, and classify them in either closed-set or open-set. The proposed detector is validated on a publicly available dataset consisting of 17 synthetic speech generation algorithms ranging from old fashioned vocoders to modern deep learning solutions. Results show that the proposed method outperforms recently proposed detectors in the forensics literature. © 2021, The Author(s).",Audio; Deepfake; Forensics; Speech,Audio recordings; Deep learning; Speech processing; Speech synthesis; Human voice; Long-term prediction; Opinion formation; Short term; Speech generation; Speech recording; Synthetic speech; Technological advances; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85104086933
Yang L.; Xie K.; Wen C.; He J.-B.,"Yang, Lei (57222964399); Xie, Kai (8972776100); Wen, Chang (56811792800); He, Jian-Biao (14032744200)",57222964399; 8972776100; 56811792800; 14032744200,Speech Emotion Analysis of Netizens Based on Bidirectional LSTM and PGCDBN,2021,IEEE Access,9,,9404168,59860,59872,12,9,10.1109/ACCESS.2021.3073234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104228313&doi=10.1109%2fACCESS.2021.3073234&partnerID=40&md5=2ba950266013c0023957ee56a8817bb3,"In recent years, the application of speech emotion recognition (SER) in the supervision of Internet public opinion has received increasing attention. This study proposes a new SER algorithm to analyze the public opinion information of network platforms. Firstly, we extract different spectrum features from speech signals and combine them into frame level speech features. Then, we select conditional deep confidence network (CDBN) which has the ability to learn sequential features as the final classification model. We apply particle swarm optimization (PSO) and genetic algorithm (GA) during the fine-tuning stage of the CDBN to obtain more suitable optimal weights of the whole network, and propose the PSO-GA-CDBN (PGCDBN) model. Compare with the traditional back propagation (BP) algorithm, our training method accelerates the convergence speed of the network and improves the robustness and recognition performance of the network. In our experiment, we used the Chinese Academy of Sciences' Institute of automation (CASIA) Chinese emotional corpus and self-collected Chinese speech datasets, which were collected from Sina Weibo, Tik tok and other online social media platforms. Compare with the popular emotion classifiers such as support vector machine (SVM), deep residual network (ResNet), long short-term memory (LSTM) neural network, DBN, our proposed PGCDBN achieves the best recognition results from both datasets. In addition, we use bidirectional LSTM before PGCDBN to further process the extracted speech features, and the result of bidirectional LSTM has stronger speech signal expression ability. The average recognition accuracy of this new hybrid deep learning model algorithm in two datasets is 98.67%, which can be used for the supervision of netizens' opinions. © 2013 IEEE.",bidirectional LSTM; conditional deep belief network; genetic algorithm; particle swarm optimization; Speech emotion recognition,Backpropagation; Classification (of information); Deep learning; Genetic algorithms; Long short-term memory; Particle swarm optimization (PSO); Social aspects; Social networking (online); Spectrum analysis; Speech; Speech communication; Support vector machines; Chinese Academy of Sciences; Classification models; Internet public opinions; Online social medias; Public opinion informations; Recognition accuracy; Spectrum features; Speech emotion recognition; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85104228313
Eskimez S.E.; Zhang Y.; Duan Z.,"Eskimez, Sefik Emre (57189600042); Zhang, You (57213457101); Duan, Zhiyao (24450312900)",57189600042; 57213457101; 24450312900,Speech Driven Talking Face Generation From a Single Image and an Emotion Condition,2022,IEEE Transactions on Multimedia,24,,,3480,3490,10,50,10.1109/TMM.2021.3099900,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111589330&doi=10.1109%2fTMM.2021.3099900&partnerID=40&md5=1200f654605435dfa478144da477f968,"Visual emotion expression plays an important role in audiovisual speech communication. In this work, we propose a novel approach to rendering visual emotion expression in speech-driven talking face generation. Specifically, we design an end-to-end talking face generation system that takes a speech utterance, a single face image, and a categorical emotion label as input to render a talking face video synchronized with the speech and expressing the conditioned emotion. Objective evaluation on image quality, audiovisual synchronization, and visual emotion expression shows that the proposed system outperforms a state-of-the-art baseline system. Subjective evaluation of visual emotion expression and video realness also demonstrates the superiority of the proposed system. Furthermore, we conduct a human emotion recognition pilot study using generated videos with mismatched emotions among the audio and visual modalities. Results show that humans respond to the visual modality more significantly than the audio modality on this task.  © 1999-2012 IEEE.",Audiovisual; emotion; multimodal; talking face generation,Face recognition; Quality control; Rendering (computer graphics); Speech communication; Speech processing; Speech recognition; Synchronization; Emotion; Emotion expression; Emotion recognition; Face; Face generation; Lip; Multi-modal; Single images; Talking face generation; Visual modalities; Emotion Recognition,Article,Final,All Open Access; Bronze Open Access; Green Open Access,Scopus,2-s2.0-85111589330
Cai Y.; Li L.; Abel A.; Zhu X.; Wang D.,"Cai, Yunqi (57215929816); Li, Lantian (56622319300); Abel, Andrew (27367568600); Zhu, Xiaoyan (7406185137); Wang, Dong (57198729548)",57215929816; 56622319300; 27367568600; 7406185137; 57198729548,Deep Normalization for Speaker Vectors,2021,IEEE/ACM Transactions on Audio Speech and Language Processing,29,,9296778,733,744,11,19,10.1109/TASLP.2020.3039573,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098762552&doi=10.1109%2fTASLP.2020.3039573&partnerID=40&md5=0f66e29c04e6e0a3e3bfc152c88a3c5f,"Deep speaker embedding has demonstrated state-of-the-art performance in speaker recognition tasks. However, one potential issue with this approach is that the speaker vectors derived from deep embedding models tend to be non-Gaussian for each individual speaker, and non-homogeneous for distributions of different speakers. These irregular distributions can seriously impact speaker recognition performance, especially with the popular PLDA scoring method, which assumes homogeneous Gaussian distribution. In this article, we argue that deep speaker vectors require deep normalization, and propose a deep normalization approach based on a novel discriminative normalization flow (DNF) model. We demonstrate the effectiveness of the proposed approach with experiments using the widely used SITW and CNCeleb corpora. In these experiments, the DNF-based normalization delivered substantial performance gains and also showed strong generalization capability in out-of-domain tests.  © 2014 IEEE.",Normalization flow; speaker embedding; speaker recognition,Embeddings; Generalization capability; Non-Gaussian; Non-homogeneous; Performance Gain; Scoring methods; Speaker recognition; State-of-the-art performance; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85098762552
Emiru E.D.; Xiong S.; Li Y.; Fesseha A.; Diallo M.,"Emiru, Eshete Derb (57204217156); Xiong, Shengwu (57203905556); Li, Yaxing (55370893700); Fesseha, Awet (57214835293); Diallo, Moussa (57214835072)",57204217156; 57203905556; 55370893700; 57214835293; 57214835072,Improving amharic speech recognition system using connectionist temporal classification with attention model and phoneme-based byte-pair-encodings,2021,Information (Switzerland),12,2,62,1,22,21,15,10.3390/info12020062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100745172&doi=10.3390%2finfo12020062&partnerID=40&md5=3fb7c2e152ee619196c49b1fdf6f20b2,"Out-of-vocabulary (OOV) words are the most challenging problem in automatic speech recognition (ASR), especially for morphologically rich languages. Most end-to-end speech recognition systems are performed at word and character levels of a language. Amharic is a poorly resourced but morphologically rich language. This paper proposes hybrid connectionist temporal classification with attention end-to-end architecture and a syllabification algorithm for Amharic automatic speech recognition system (AASR) using its phoneme-based subword units. This algorithm helps to insert the epithetic vowel እ[1], which is not included in our Grapheme-to-Phoneme (G2P) conversion algorithm developed using consonant–vowel (CV) representations of Amharic graphemes. The proposed end-to-end model was trained in various Amharic subwords, namely characters, phonemes, character-based subwords, and phoneme-based subwords generated by the byte-pair-encoding (BPE) segmentation algorithm. Experimental results showed that context-dependent phoneme-based subwords tend to result in more accurate speech recognition systems than the character-based, phoneme-based, and character-based subword counterparts. Further improvement was also obtained in proposed phoneme-based subwords with the syllabification algorithm and SpecAugment data augmentation technique. The word error rate (WER) reduction was 18.38% compared to character-based acoustic modeling with the word-based recurrent neural network language modeling (RNNLM) baseline. These phoneme-based subword models are also useful to improve machine and speech translation tasks. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Amharic; Automatic speech recognition; Connectionist temporal classification with attention; Low resource language; Natural language processing; Out-of-vocabulary,Encoding (symbols); Linguistics; Modeling languages; Recurrent neural networks; Speech; Automatic speech recognition; Automatic speech recognition system; Grapheme-to-phoneme conversion; Out of vocabulary words; Segmentation algorithms; Speech recognition systems; Syllabification algorithms; Temporal classification; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85100745172
Novitasari S.; Sakti S.; Nakamura S.,"Novitasari, Sashi (57205401384); Sakti, Sakriani (12807978200); Nakamura, Satoshi (55628545896)",57205401384; 12807978200; 55628545896,Neural incremental speech recognition toward real-time machine speech translation,2021,IEICE Transactions on Information and Systems,E104D,12,,2195,2208,13,3,10.1587/transinf.2021EDP7014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120952028&doi=10.1587%2ftransinf.2021EDP7014&partnerID=40&md5=7434bd54bcb1751fe17d48cb23f3f73b,"Real-time machine speech translation systems mimic human interpreters and translate incoming speech from a source language to the target language in real-time. Such systems can be achieved by performing low-latency processing in ASR (automatic speech recognition) module before passing the output to MT (machine translation) and TTS (text-to-speech synthesis) modules. Although several studies recently proposed sequence mechanisms for neural incremental ASR (ISR), these frameworks have a more complicated training mechanism than the standard attention-based ASR because they have to decide the incremental step and learn the alignment between speech and text. In this paper, we propose attention-transfer ISR (AT-ISR) that learns the knowledge from attention-based non-incremental ASR for a low delay end-to-end speech recognition. ISR comes with a trade-off between delay and performance, so we investigate how to reduce AT-ISR delay without a significant performance drop. Our experiment shows that AT-ISR achieves a comparable performance to the non-incremental ASR when the incremental recognition begins after the speech utterance reaches 25% of the complete utterance length. Additional experiments to investigate the effect of ISR on translation tasks are also performed. The focus is to find the optimum granularity of the output unit. The results reveal that our end-to-end subword-level ISR resulted in the best translation quality with the lowest WER and the lowest uncovered-word rate. Copyright © 2021 The Institute of Electronics, Information and Communication Engineers",Attention transfer; Incremental speech recognition; Real-time speech translation,Character recognition; Computational linguistics; Computer aided language translation; Economic and social effects; Machine translation; Real time systems; Speech synthesis; Speech transmission; Attention transfer; Automatic speech recognition; End to end; Incremental speech recognition; Learn+; Performance; Real- time; Real-time speech translation; Speech translation; Time machine; Speech recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85120952028
HimaBindu G.; Lakshmeeswari G.; Lalitha G.; Subhashini P.P.S.,"HimaBindu, Gottumukkala (57188623927); Lakshmeeswari, Gondi (55446684500); Lalitha, Giddaluru (57223314882); Subhashini, Pedalanka P.S. (57212146797)",57188623927; 55446684500; 57223314882; 57212146797,Recognition using DNN with bacterial foraging optimization using MFCC coefficients,2021,Journal Europeen des Systemes Automatises,54,2,,283,287,4,0,10.18280/JESA.540210,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105556238&doi=10.18280%2fJESA.540210&partnerID=40&md5=0ebb396dd2c69393a9d20cb788d2c1fe,"Speech is an important mode of communication for people. For a long time, researchers have been working hard to develop conversational machines which will communicate with speech technology. Voice recognition is a part of a science called signal processing. Speech recognition is becoming more successful for providing user authentication. The process of user recognition is becoming more popular now a days for providing security by authenticating the users. With the rising importance of automated information processing and telecommunications, the usefulness of recognizing an individual from the features of user voice is increasing. In this paper, the three stages of speech recognition processing are defined as pre-processing, feature extraction and decoding. Speech comprehension has been significantly enhanced by using foreign languages. Automatic Speech Recognition (ASR) aims to translate text to speech. Speaker recognition is the method of recognizing an individual through his/her voice signals. The new speaker initially privileges identity for speaker authentication, and then the stated model is used for identification. The identity argument is approved when the match is above a predefined threshold. The speech used for these tasks may be either text-dependent or text-independent. The article uses Bacterial Foraging Optimization Algorithm (BFO) for accurate speech recognition through Mel Frequency Cepstral Coefficients (MFCC) model using DNN. Speech recognition efficiency is compared to that of the conventional system. © 2021 Lavoisier. All rights reserved.",Bacterial foraging optimization; Deep neural network; Noise removal; Segmentation; Speech recognition,,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85105556238
Algabri M.; Mathkour H.; Alsulaiman M.M.; Bencherif M.A.,"Algabri, Mohammed (56073285900); Mathkour, Hassan (15925954200); Alsulaiman, Mansour M. (26667487200); Bencherif, Mohamed A. (36056175300)",56073285900; 15925954200; 26667487200; 36056175300,Deep learning-based detection of articulatory features in arabic and english speech,2021,Sensors (Switzerland),21,4,1205,1,23,22,5,10.3390/s21041205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100573637&doi=10.3390%2fs21041205&partnerID=40&md5=1c63c365a41d2993102889d8d439acab,"This study proposes using object detection techniques to recognize sequences of articula-tory features (AFs) from speech utterances by treating AFs of phonemes as multi-label objects in speech spectrogram. The proposed system, called AFD-Obj, recognizes sequence of multi-label AFs in speech signal and localizes them. AFD-Obj consists of two main stages: firstly, we formulate the problem of AFs detection as an object detection problem and prepare the data to fulfill requirement of object detectors by generating a spectral three-channel image from the speech signal and creating the corresponding annotation for each utterance. Secondly, we use annotated images to train the proposed system to detect sequences of AFs and their boundaries. We test the system by feeding spectrogram images to the system, which will recognize and localize multi-label AFs. We investi-gated using these AFs to detect the utterance phonemes. YOLOv3-tiny detector is selected because of its real-time property and its support for multi-label detection. We test our AFD-Obj system on Arabic and English languages using KAPD and TIMIT corpora, respectively. Additionally, we pro-pose using YOLOv3-tiny as an Arabic phoneme detection system (i.e., PD-Obj) to recognize and localize a sequence of Arabic phonemes from whole speech utterances. The proposed AFD-Obj and PD-Obj systems achieve excellent results for Arabic corpus and comparable to the state-of-the-art method for English corpus. Moreover, we showed that using only one-scale detection is suitable for AFs detection or phoneme recognition. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Articulatory features detection; Object detection; PER; YOLO,Deep learning; Feature extraction; Object detection; Object recognition; Spectrographs; Speech; Speech communication; Articulatory features; Detection problems; English languages; Phoneme detection; Phoneme recognition; Real-time properties; Speech spectrogram; State-of-the-art methods; article; deep learning; feeding; human; human experiment; phoneme; speech; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85100573637
Hua G.; Teoh A.B.J.; Zhang H.,"Hua, Guang (55208575800); Teoh, Andrew Beng Jin (9733887900); Zhang, Haijian (14827518200)",55208575800; 9733887900; 14827518200,Towards end-to-end synthetic speech detection,2021,IEEE Signal Processing Letters,28,,9456037,1265,1269,4,117,10.1109/LSP.2021.3089437,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112110314&doi=10.1109%2fLSP.2021.3089437&partnerID=40&md5=e1fe50a7deb0b090c14ed54b3cfe6f6f,"The constant Q transform (CQT) has been shown to be one of the most effective speech signal pre-transforms to facilitate synthetic speech detection, followed by either hand-crafted (subband) constant Q cepstral coefficient (CQCC) feature extraction and a back-end binary classifier, or a deep neural network (DNN) directly for further feature extraction and classification. Despite the rich literature on such a pipeline, we show in this paper that the pre-transform and hand-crafted features could simply be replaced by end-to-end DNNs. Specifically, we experimentally verify that by only using standard components, a light-weight neural network could outperform the state-of-the-art methods for the ASVspoof2019 challenge. The proposed model is termed Time-domain Synthetic Speech Detection Net (TSSDNet), having ResNet- or Inception-style structures. We further demonstrate that the proposed models also have attractive generalization capability. Trained on ASVspoof2019, they could achieve promising detection performance when tested on disjoint ASVspoof2015, significantly better than the existing cross-dataset results. This paper reveals the great potential of end-to-end DNNs for synthetic speech detection, without hand-crafted features. © 2021 IEEE.",ASVspoof2015; ASVspoof2019; Cross-dataset testing; End-to-end; Speech forensics; Synthetic speech detection,Deep neural networks; Extraction; Feature extraction; Neural networks; Palmprint recognition; Speech synthesis; Binary classifiers; Cepstral coefficients; Constant q transforms; Detection performance; Feature extraction and classification; Generalization capability; Standard components; State-of-the-art methods; Speech recognition,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85112110314
Lee C.-C.; Sridhar K.; Li J.-L.; Lin W.-C.; Su B.-H.; Busso C.,"Lee, Chi-Chun (7410153704); Sridhar, Kusha (57204211764); Li, Jeng-Lin (57202163561); Lin, Wei-Cheng (55574214770); Su, Bo-Hao (56440809600); Busso, Carlos (35742852700)",7410153704; 57204211764; 57202163561; 55574214770; 56440809600; 35742852700,Deep Representation Learning for Affective Speech Signal Analysis and Processing: Preventing unwanted signal disparities,2021,IEEE Signal Processing Magazine,38,6,,22,38,16,20,10.1109/MSP.2021.3105939,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118595811&doi=10.1109%2fMSP.2021.3105939&partnerID=40&md5=4a2ec9b6fccfcfb14877c694ecd819cc,"Speech emotion recognition (SER) is an important research area, with direct impacts in applications of our daily lives, spanning education, health care, security and defense, entertainment, and human-computer interaction. The advances in many other speech signal modeling tasks, such as automatic speech recognition, text-to-speech synthesis, and speaker identification, have led to the current proliferation of speech-based technology. Incorporating SER solutions into existing and future systems can take these voice-based solutions to the next level. Speech is a highly nonstationary signal, with dynamically evolving spatialoral patterns. It often requires a sophisticated representation modeling framework to develop algorithms capable of handling real-life complexities. © 1991-2012 IEEE.",,Deep learning; Human computer interaction; Medical computing; Signal analysis; Speech communication; Speech processing; Speech recognition; Speech synthesis; Affective speech; Daily lives; Direct impact; Education healths; Health care security; Modeling task; Research areas; Signals analysis; Speech emotion recognition; Speech signal modeling; Character recognition,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85118595811
Arif T.; Javed A.; Alhameed M.; Jeribi F.; Tahir A.,"Arif, Tuba (57376027900); Javed, Ali (57190125008); Alhameed, Mohammed (57201776470); Jeribi, Fathe (57204498461); Tahir, Ali (57215526486)",57376027900; 57190125008; 57201776470; 57204498461; 57215526486,Voice Spoofing Countermeasure for Logical Access Attacks Detection,2021,IEEE Access,9,,,162857,162868,11,28,10.1109/ACCESS.2021.3133134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121339265&doi=10.1109%2fACCESS.2021.3133134&partnerID=40&md5=5c892fb363ef58f42c2d860e727074af,"Voice-driven devices (VDDs) like Google Home and Amazon Alexa, which are well-known connected devices in consumer IoT, have applications in various domains i.e., home appliances automation, next-generation vehicles, voice banking, and so on. However, these VDDs that are based on automatic speaker verification systems (ASVs) are vulnerable to voice based logical access (LA) attacks like Text-to-Speech (TTS) synthesis and converted voice signals. Intruders can exploit these attacks to bypass the security of such systems and gain access of victim's bank account or home control. Thus, there exists a need to develop an effective voice spoofing countermeasure that can reliably be used to protect these VDDs against such malicious attacks. This work presents a novel audio features descriptor named as extended local ternary pattern (ELTP) to capture the vocal tract dynamically induced attributes of bonafide speech and algorithmic artifacts in synthetic and converted speeches. We fused our novel ELTP features with the linear frequency cepstral coefficients (LFCC) to further strengthen the capability of our features for capturing the traits of bonafide and spoofed signals. We employ the proposed ELTP-LFCC features to train the deep bidirectional Long Short-Term Memory (DBiLSTM) network for classification of the bonafide and spoof signal (i.e., TTS synthesis, converted speech). Performance of our spoofing countermeasure is measured on the large-scale and diverse ASVspoof 2019 logical access dataset. Experimental results demonstrate that the proposed audio spoofing countermeasure can reliably be used to detect the LA spoofing attacks.  © 2021 IEEE.",Extended local ternary pattern; logical access attacks; text-to-speech synthesis; voice conversion; voice spoofing countermeasure,Access control; Domestic appliances; Heuristic algorithms; Large dataset; Network security; Reliability analysis; Robustness (control systems); Speech recognition; Speech synthesis; Biometric (access control); Cepstral analysis; Cepstral coefficients; Extended local ternary pattern; Features extraction; Heuristics algorithm; Linear frequency; Logical access attack; Voice conversion; Voice spoofing countermeasure; Feature extraction,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85121339265
Fritsch J.; Magimai-Doss M.,"Fritsch, Julian (57209885151); Magimai-Doss, Mathew (8555953600)",57209885151; 8555953600,Utterance Verification-Based Dysarthric Speech Intelligibility Assessment Using Phonetic Posterior Features,2021,IEEE Signal Processing Letters,28,,9317735,224,228,4,17,10.1109/LSP.2021.3050362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099532216&doi=10.1109%2fLSP.2021.3050362&partnerID=40&md5=0be07747ff3fcbe388f37ea692e6595a,"In the literature, the task of dysarthric speech intelligibility assessment has been approached through development of different low-level feature representations, subspace modeling, phone confidence estimation or measurement of automatic speech recognition system accuracy. This paper proposes a novel approach where the intelligibility is estimated as the percentage of correct words uttered by a speaker with dysarthria by matching and verifying utterances of the speaker with dysarthria against control speakers' utterances in phone posterior feature space and broad phonetic posterior feature space. Experimental validation of the proposed approach on the UA-Speech database, with posterior feature estimators trained on the data from auxiliary domain and language, obtained a best Pearson's correlation coefficient (r) of 0.950 and Spearman's correlation coefficient (rho) of 0.957. Furthermore, replacing control speakers' speech with speech synthesized by a neural text-to-speech system obtained a best r of 0.931 and rho of 0.961. © 1994-2012 IEEE.",Dysarthric speech; objective intelligibility assessment; posterior features; utterance verification,Correlation methods; Linguistics; Speech recognition; Telephone sets; Automatic speech recognition system; Confidence estimation; Correlation coefficient; Experimental validations; Intelligibility assessment; Pearson's correlation coefficients; Text-to-speech system; Utterance verification; Speech intelligibility,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85099532216
Letaifa L.B.; Torres M.I.,"Letaifa, Leila Ben (57219989162); Torres, M. Ines (15756450800)",57219989162; 15756450800,Perceptual Borderline for Balancing Multi-Class Spontaneous Emotional Data,2021,IEEE Access,9,,9398699,55939,55954,15,12,10.1109/ACCESS.2021.3071485,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103884960&doi=10.1109%2fACCESS.2021.3071485&partnerID=40&md5=67db0bd745f01d80feb537948933a502,"Speech is a behavioural biometric signal that can provide important information to understand the human intends as well as their emotional status. The paper is centered on the speech-based identification of the seniors's emotional status during their interaction with a virtual agent playing the role of a health professional coach. Under real conditions, we can just identify a small set oftask-dependent spontaneous emotions. The number of identified samples is largely different for eachemotion, which results in an imbalanced dataset problem. This research proposes the dimensional model of emotions as a perceptual representation space alternative to the generally used acoustic one.The main contribution of the paper is the definition of a perceptual borderline for the oversampling of minority emotion classes in this space. This limit, based on arousal and valence criteria, leads to two methods of balancing the data: the Perceptual Borderline oversampling and the Perceptual Borderline SMOTE (Synthetic Minority Oversampling TEchnique). Both methods are implemented and compared to state-of-the-art approaches of Random oversampling and SMOTE. The experimental evaluation was carried out on three imbalanced datasets of spontaneous emotions acquired in human-machine scenarios in three different cultures: Spain, France and Norway. The emotion recognition results obtained by neural networks classifiers show that the proposed perceptual oversampling methods led to significant improvements when compared with the state-of-the art, for all scenarios and languages.  © 2013 IEEE.",Dimensional model of emotions; emotion recognition; multi-class classification; perceptual borderline; speech analysis; speech processing,Balancing; Behavioural Biometric; Experimental evaluation; Imbalanced Data-sets; Imbalanced dataset problems; Neural networks classifiers; Perceptual representations; State-of-the-art approach; Synthetic minority over-sampling techniques; Speech recognition,Article,Final,All Open Access; Gold Open Access; Green Open Access,Scopus,2-s2.0-85103884960
Huang W.-C.; Hayashi T.; Wu Y.-C.; Kameoka H.; Toda T.,"Huang, Wen-Chin (57208822576); Hayashi, Tomoki (57216552967); Wu, Yi-Chiao (57191855460); Kameoka, Hirokazu (7006771405); Toda, Tomoki (7202683282)",57208822576; 57216552967; 57191855460; 7006771405; 7202683282,Pretraining Techniques for Sequence-to-Sequence Voice Conversion,2021,IEEE/ACM Transactions on Audio Speech and Language Processing,29,,9314100,745,755,10,36,10.1109/TASLP.2021.3049336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099425236&doi=10.1109%2fTASLP.2021.3049336&partnerID=40&md5=c695cb98a98d751bc55c7d9232258298,"Sequence-to-sequence (seq2seq) voice conversion (VC) models are attractive owing to their ability to convert prosody. Nonetheless, without sufficient data, seq2seq VC models can suffer from unstable training and mispronunciation problems in the converted speech, thus far from practical. To tackle these shortcomings, we propose to transfer knowledge from other speech processing tasks where large-scale corpora are easily available, typically text-to-speech (TTS) and automatic speech recognition (ASR). We argue that VC models initialized with such pretrained ASR or TTS model parameters can generate effective hidden representations for high-fidelity, highly intelligible converted speech. In this work, we examine our proposed method in a parallel, one-to-one setting. We employed recurrent neural network (RNN)-based and Transformer based models, and through systematical experiments, we demonstrate the effectiveness of the pretraining scheme and the superiority of Transformer based models over RNN-based models in terms of intelligibility, naturalness, and similarity.  © 2014 IEEE.",pretraining; sequence-to-sequence; transformer; Voice conversion,Character recognition; Speech processing; Speech recognition; Automatic speech recognition; High-fidelity; Model parameters; Pre-training; Recurrent neural network (RNN); Text to speech; Voice conversion; Recurrent neural networks,Article,Final,All Open Access; Green Open Access; Hybrid Gold Open Access,Scopus,2-s2.0-85099425236
Meftah A.H.; Qamhan M.A.; Seddiq Y.; Alotaibi Y.A.; Selouani S.A.,"Meftah, Ali Hamid (36634782600); Qamhan, Mustafa A. (57210603646); Seddiq, Yasser (35776840400); Alotaibi, Yousef A. (6507866160); Selouani, Sid Ahmed (6507508063)",36634782600; 57210603646; 35776840400; 6507866160; 6507508063,"King Saud University Emotions Corpus: Construction, Analysis, Evaluation, and Comparison",2021,IEEE Access,9,,9393909,54201,54219,18,15,10.1109/ACCESS.2021.3070751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103757460&doi=10.1109%2fACCESS.2021.3070751&partnerID=40&md5=af50ab10ff66df37d1287bd7cf23245d,"Emotional speech recognition for the Arabic language is insufficiently tackled in the literature compared to other languages. In this paper, we present the work of creating and verifying the King Saud University Emotions (KSUEmotions) corpus, which was released by the Linguistic Data Consortium (LDC) in 2017 as the first public Arabic emotional speech corpus. KSUEmotions contains an emotional speech of twenty-three speakers from Saudi Arabia, Syria, and Yemen, and includes the emotions: neutral, happiness, sadness, surprise, and anger. The corpus content is verified in two differentways: a human perceptual test by nine listeners who rate emotional performance in audio files, and automatic emotion recognition. Two automatic emotion recognition systems are experimented with: Residual Neural Network and Convolutional Neural Network. This work also experiments with emotion recognition for the English language using the Emotional Prosody Speech and Transcripts Corpus (EPST). The current experimental work is conducted in three tracks: (i) monolingual, where independent experiments for Arabic and English are carried out, (ii) multilingual, where the Arabic and English corpora are merged in as mixed corpus, and (iii) cross-lingual, where models are trained using one language and tested using the other. A challenge encountered in this work is that the two corpora do not contain the same emotions. That problem is tackled by mapping the emotions to the arousal-valance space.  © 2013 IEEE.",Arabic language; corpus; CRNN; digital speech processing; emotion; ResNet; spectrogram,Convolutional neural networks; Speech analysis; Arabic languages; Automatic emotion recognition; Emotion recognition; Emotional prosody; Emotional speech; Emotional Speech Recognition; English languages; Linguistic data consortiums; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85103757460
Lin J.; Van Wijngaarden A.J.D.L.; Wang K.-C.; Smith M.C.,"Lin, Ju (57191868086); Van Wijngaarden, Adriaan J. De Lind (7003590502); Wang, Kuang-Ching (8928410200); Smith, Melissa C. (55495907700)",57191868086; 7003590502; 8928410200; 55495907700,Speech Enhancement Using Multi-Stage Self-Attentive Temporal Convolutional Networks,2021,IEEE/ACM Transactions on Audio Speech and Language Processing,29,,,3440,3450,10,52,10.1109/TASLP.2021.3125143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118676386&doi=10.1109%2fTASLP.2021.3125143&partnerID=40&md5=9b1126f1c2a795fcf9c97ed19e4baccc,"Multi-stage learning is an effective technique for invoking multiple deep-learning modules sequentially. This paper applies multi-stage learning to speech enhancement by using a multi-stage structure, where each stage comprises a self-attention (SA) block followed by stacks of temporal convolutional network (TCN) blocks with doubling dilation factors. Each stage generates a prediction that is refined in a subsequent stage. A feature fusion block is inserted at the input of later stages to re-inject original information. The resulting multi-stage speech enhancement system, multi-stage SA-TCN, is compared with state-of-the-art deep-learning speech enhancement methods using the LibriSpeech and VCTK datasets. The multi-stage SA-TCN system's hyperparameters are fine-tuned, and the impact of the SA block, the feature fusion block, and the number of stages are determined. The use of a multi-stage SA-TCN system as a front-end for automatic speech recognition systems is also investigated. It is shown that the multi-stage SA-TCN systems perform well relative to other state-of-the-art systems in terms of speech enhancement and speech recognition scores.  © 2014 IEEE.",multi-stage architectures; neural networks; self-attention; Speech enhancement; speech recognition; temporal convolutional networks,Job analysis; Recurrent neural networks; Speech enhancement; Speech recognition; Convolutional networks; Multi-stages; Multistage architecture; Network systems; Neural-networks; Noise measurements; Self-attention; Spectrograms; Task analysis; Temporal convolutional network; Convolution,Article,Final,All Open Access; Green Open Access,Scopus,2-s2.0-85118676386
Kim J.-W.; Yoon H.; Jung H.-Y.,"Kim, June-Woo (57219550643); Yoon, Hyekyung (57273359500); Jung, Ho-Young (57198760619)",57219550643; 57273359500; 57198760619,Linguistic-Coupled Age-to-Age Voice Translation to Improve Speech Recognition Performance in Real Environments,2021,IEEE Access,9,,,136476,136486,10,11,10.1109/ACCESS.2021.3115608,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115801012&doi=10.1109%2fACCESS.2021.3115608&partnerID=40&md5=6a4101ffe0a491ea08b22d84b4dea47b,"We address a low-performance problem of the elderly in automatic speech recognition (ASR) through feature adaptation agnostic to the ASR. Most of the datasets for speech recognition models consist of datasets collected from adult speakers. Consequently, the majority of commercial speech recognition systems typically tend to perform well on adult speakers. In other words, the limited diversity of speakers in the training datasets yields unreliable performance for minority (e.g., elderly) speakers due to the infeasible acquisition of training data. In response, this paper suggests a neural network-based voice conversion framework to enhance speech recognition of the minority. To this end, we propose a voice translation model including an unsupervised phonology clustering to extract linguistic information to fit the minority's speech to a current acoustic model frame. Our proposal is a spectral feature adaptation method that can be placed in front of any commercial or open ASR system, avoiding directly modifying the speech recognizer. The experimental results and analysis demonstrate the effectiveness of our proposed method through improvement in elderly speech recognition accuracy.  © 2013 IEEE.",age-on-demand speech recognition; spectral feature transform; Speech recognition; voice translation,Linguistics; Speech processing; Translation (languages); Age-on-demand speech recognition; Automatic speech recognition; Feature adaptation; Feature transform; On demands; Real environments; Spectral feature; Spectral feature transform; Speech recognition performance; Voice translation; Speech recognition,Article,Final,All Open Access; Gold Open Access,Scopus,2-s2.0-85115801012
Fodor Á.; Kopácsi L.; Milacski Z.Á.; Lorincz A.,"Fodor, Ádám (57191043308); Kopácsi, László (57211267928); Milacski, Zoltán Á. (56578585500); Lorincz, András (26643373200)",57191043308; 57211267928; 56578585500; 26643373200,Speech De-identification with Deep Neural Networks,2021,Acta Cybernetica,25,2,,257,269,12,1,10.14232/ACTACYB.288282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123454928&doi=10.14232%2fACTACYB.288282&partnerID=40&md5=beecf718dfd88129022ca53aeb86c271,"Cloud-based speech services are powerful practical tools but the privacy of the speakers raises important legal concerns when exposed to the Internet. We propose a deep neural network solution that removes personal characteristics from human speech by converting it to the voice of a Text-to-Speech (TTS) system before sending the utterance to the cloud. The network learns to transcode sequences of vocoder parameters, delta and delta-delta features of human speech to those of the TTS engine. We evaluated several TTS systems, vocoders and audio alignment techniques. We measured the performance of our method by (i) comparing the result of speech recognition on the de-identified utterances with the original texts, (ii) computing the Mel-Cepstral Distortion of the aligned TTS and the transcoded sequences, and (iii) questioning human participants in A-not-B, 2AFC and 6AFC (Alternative Forced-Choice) tasks. Our approach achieves the level required by diverse applications. © 2021 University of Szeged, Institute of Informatics. All rights reserved.",Deep neural network; Speaker privacy; Speech processing; Text-to-speech; Voice conversion,Audio systems; Speech processing; Speech recognition; Vocoders; Cloud-based; De-identification; Exposed to; Human speech; Legal concern; Speaker privacy; Speech services; Text to speech; Text-to-speech system; Voice conversion; Deep neural networks,Article,Final,All Open Access; Bronze Open Access,Scopus,2-s2.0-85123454928
